<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-262 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-262</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-262</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-265456551</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.14737v1.pdf" target="_blank">Positional Description Matters for Transformers Arithmetic</a></p>
                <p><strong>Paper Abstract:</strong> Transformers, central to the successes in modern Natural Language Processing, often falter on arithmetic tasks despite their vast capabilities --which paradoxically include remarkable coding abilities. We observe that a crucial challenge is their naive reliance on positional information to solve arithmetic problems with a small number of digits, leading to poor performance on larger numbers. Herein, we delve deeper into the role of positional encoding, and propose several ways to fix the issue, either by modifying the positional encoding directly, or by modifying the representation of the arithmetic task to leverage standard positional encoding differently. We investigate the value of these modifications for three tasks: (i) classical multiplication, (ii) length extrapolation in addition, and (iii) addition in natural language context. For (i) we train a small model on a small dataset (100M parameters and 300k samples) with remarkable aptitude in (direct, no scratchpad) 15 digits multiplication and essentially perfect up to 12 digits, while usual training in this context would give a model failing at 4 digits multiplication. In the experiments on addition, we use a mere 120k samples to demonstrate: for (ii) extrapolation from 10 digits to testing on 12 digits numbers while usual training would have no extrapolation, and for (iii) almost perfect accuracy up to 5 digits while usual training would be correct only up to 3 digits (which is essentially memorization with a training set of 120k samples).</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e262.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e262.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT2-small multiplication (padded+reversed)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT2-small (124M) — multiplication with padding and reversed product</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Fine-tuned GPT2-small (124M, decoder-only transformer) on synthetic multiplication data with zero-padding to fixed width and reversal of the product (least-significant-digit first); enabled accurate direct (no-scratchpad) outputs for large-digit multiplications up to 15×15 in this setup.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT2-small</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>124M</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>multiplication (direct product output, digit-wise multiplication implied)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>up to 15-digit × 15-digit (trained on n-maximum-digit datasets; training included many smaller-digit examples)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>fine-tuning on generated multiplication dataset (300k examples), zero-padding factors to equal length, reversing the product digits (LSB first), random initialization; no scratchpad (direct output)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Essentially perfect (≈100% accuracy) up through 12-digit × 12-digit multiplication; still high accuracy on many 13–15 digit combinations though some entries decline (paper reports very high accuracy overall and cites ability to accurately compute 15×15 in many cases; Table 5 and text: 'remarkable aptitude in 15 digits multiplication and essentially perfect up to 12 digits').</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Padding standardizes absolute token positions across training examples so the model can learn a single position-relative rule; reversing the product makes the model produce least-significant digits first so each output digit depends on a local window of input digits and carries propagate forward in generation order, simplifying carry handling; these format changes reduce the model's naive reliance on brittle absolute positional patterns and enable learning a digit-wise algorithmic procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Shown on a small model (124M) — the authors emphasize that a small fraction of model parameters sufficed; they note performance declines for very large maximum-digit settings (e.g., 20 digits) and that larger datasets / tuning could push capacity further. No systematic scaling across larger model sizes reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Without padding and reversal, the same training regimen fails quickly (poor beyond 1-2 digits or beyond combinations where sum of digit lengths >5); accuracy degrades for some 13–15 digit combinations and becomes unsatisfactory by 20 digits with the same training budget; relies on dataset choices (digit distribution) and architectural conventions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to 'Basic' (no padding/no reverse) format and other augmentations (n×1 augmentation, % first-step operator), the padding+reverse intervention gave the largest improvement for large-number multiplication.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Simple data-format modifications (fixed-width padding and reversing product digits) let a small decoder-only transformer directly learn to output large multiplications (essentially perfect to 12 digits, strong performance to 15) by removing positional irregularities and making carry computation local in generation order.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Positional Description Matters for Transformers Arithmetic', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e262.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e262.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT2-small multiplication (basic/no-padding)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT2-small (124M) — multiplication in 'Basic' (no padding, normal product order) format</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Fine-tuning the same GPT2-small architecture on basic (non-padded) synthetic multiplication data yields good performance on tiny multiplications but fails to generalize to larger numbers because of variable absolute token positions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT2-small</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>124M</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>multiplication (direct product output)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>1–n digits; experiments show competence on 1–2 digit multiplications but poor performance as digits increase (esp. when sum of digits >5)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>fine-tuning on generated multiplication dataset (300k examples) with basic representation: 'digit digit ... * digit ... # product' (no padding, normal order)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>High accuracy on 1–2 digit multiplication, but performance drops sharply for larger numbers; model often fails when the sum of the digit-lengths of the two factors exceeds ~5 — e.g., model trained with n=5 performed well on 1–2 digits but poorly on 4×4 and larger.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Model overfits to absolute positional patterns in the basic format: because operator and operand tokens occupy variable absolute positions, the model learns brittle position-specific shortcuts instead of a position-relative digit-wise algorithm, preventing length extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>No evidence of length extrapolation with this format under the paper's training budget; adding more small-number samples alone did not enable extrapolation to larger numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Fails to generalize to larger digit counts; likely memorizes frequent small multiplications; inability to handle variable operator positions leads to errant or missing digits in output.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly to padded/reversed formats and to datasets augmented with n×1 examples or first-step (%) examples; basic format is consistently inferior for large-number generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Without controlling positional structure, small transformers tend to exploit absolute-position shortcuts and cannot reliably scale to larger-digit multiplication tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Positional Description Matters for Transformers Arithmetic', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e262.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e262.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Addition length extrapolation — data-format interventions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Addition length extrapolation experiments comparing Basic, Random Space, and Recursive Scratchpad formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Fine-tuning experiments (mostly on pretrained GPT2-small) for addition (digit-wise) where changing data formats (inserting random spaces; providing recursive, redundant scratchpad steps) greatly improves extrapolation to longer addends compared to a Basic format.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT2-small (fine-tuned from pretrained for main experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>124M</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition (digit-wise addition with carry, shown via scratchpad variants)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>training on 2–10 digit additions; testing on 9–13 digit additions to measure length generalization</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>fine-tuning pretrained GPT2-small for 5 epochs on 120k generated addition examples in three formats: Basic (straightforward digit-wise scratchpad), Random Space (random extra spaces inserted between characters with probability), and Recursive Scratchpad (redundant/repeated recursive digit-wise steps and reversed addend order).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Table 6: Basic trained 2–10 digits → test accuracies: 9 digits 1.00, 10 digits 1.00, 11–13 digits 0.00 (no generalization). Random Space: 9:1.00, 10:1.00, 11:0.99, 12:0.09, 13:0.00. Recursive Scratchpad: 9:1.00, 10:1.00, 11:1.00, 12:0.96, 13:0.55 — i.e., Recursive Scratchpad shows substantial extrapolation (nearly perfect to 12 digits, substantial to 13).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Random spaces disrupt absolute-position shortcuts, forcing models to rely on content or local structure rather than fixed indices; recursive scratchpad increases redundancy of per-digit information so the model must learn the recursive carry rule (an algorithmic digit-wise procedure) rather than memorize positional patterns. Pretraining status matters: recursive scratchpad required starting from a pretrained model to be effective.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Demonstrates data-format-dependent phase transitions (from no generalization to substantial extrapolation) for a fixed model size; no cross-model-size scaling reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Basic format models trained on limited lengths often 'omit' digits when prompted with longer addends (outputting only the number of steps seen during training); Random Space only extends length by a small margin unless combined with redundancy; recursive scratchpad sometimes fails on certain examples (failure cases listed) but overall generalizes best.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Direct comparison of three data formats (Basic vs Random Space vs Recursive Scratchpad) and pretrained vs randomly initialized models; recursive scratchpad + pretrained outperforms others for length extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Data-format choices that reduce reliance on absolute positional cues (random spacing) or that provide redundant, recursive per-digit information (recursive scratchpad) enable a small pretrained transformer to generalize addition algorithms to longer lengths that were not in training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Positional Description Matters for Transformers Arithmetic', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e262.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e262.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Positional-embedding ablations & random embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablation of absolute positional embeddings and introduction of random (hash) embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Experiments removing the transformer’s absolute positional embeddings and alternatively injecting random per-token 'hash' vectors (random embedding) to reduce brittle absolute-position dependence and improve length generalization in digit-reversal and addition tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT2-small (both pretrained and randomly initialized variants tested)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>124M</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>digit-reversal (reverse digits) and addition (digit-wise)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>train on numbers up to 10 digits and test on longer digits including repetitive-digit tests (regular and repetitive data)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>remove absolute positional embedding entirely, or add a 'random embedding' (n_hash-dim Gaussian vectors split across attention heads, regenerated per epoch and test) appended to token embeddings; compare pretrained vs random-init models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Removing positional embedding improves length generalization by roughly ~2 digits on digit-reversal tasks for both pretrained and random-init models (Figure 1). Adding random embedding further improves generalization on both regular and repetitive data (Figure 2). On addition tasks, models with random embedding achieved similar generalization to the Recursive Scratchpad format.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>The dominant failure mode is overreliance on absolute-position signals. Removing positional embeddings forces the model to adopt alternative positional encodings; random per-token tags (random embedding) provide unique identifiers that let the model distinguish identical digits at different positions and avoid brittle absolute-index shortcuts. Pretrained weights provide additional learned components that make it easier to exploit non-positional cues, particularly on repetitive data.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Improvements are demonstrated for the 124M model; random embedding combined with removing absolute positional embedding gives larger generalization gains than removing positional embedding alone, especially for randomly initialized models.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Removing positional embeddings can cause instability and difficulty unless alternative positional cues (like random embedding) are provided; randomly initialized models without positional embedding but without random embedding perform poorly on repetitive-digit inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared models with absolute positional embedding vs with positional embedding removed vs with added random embedding; also compared pretrained vs randomly initialized models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Absolute positional embeddings drive brittle, non-generalizable solutions on arithmetic tasks; providing position information via randomized per-token tags or removing absolute positions (and using alternative encodings) enables the model to learn more algorithmic, generalizable digit-wise procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Positional Description Matters for Transformers Arithmetic', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e262.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e262.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Natural-language + arithmetic integration</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Integration experiments mixing dialogue (natural language) and arithmetic data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Tests whether arithmetic skills learned on synthetic arithmetic data transfer to arithmetic problems embedded in natural language dialogue prompts, and how data formats and positional-embedding choices affect this integration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT2-small (randomly initialized and pretrained variants tested)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>124M</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition (2–5 digit) in dialogue and pure arithmetic contexts</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>dialogue dataset: many 2–3-digit additions and fewer 4–5-digit additions; arithmetic dataset: 2–5 digit additions (120k samples)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>train on (i) dialogue-only data, (ii) dialogue plus Basic arithmetic data, (iii) dialogue plus Random Space arithmetic data; compare models with positional embedding, without positional embedding, and with random embedding; test in dialogue prompt and pure arithmetic prompt contexts; training for 50–100 epochs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Models trained on dialogue-only fail at 4–5 digit addition. Naively mixing arithmetic data in Basic format with absolute positional embeddings provides little improvement. Using Random Space format, removing positional embeddings, or adding random embedding substantially improves the model's ability to perform addition within dialogue prompts. Quantitatively (Table 9): models with Random Space or NoPosEmbed or RandEmbed show much higher accuracies on 4–5-digit dialogue additions than Basic+PosEmbed baselines (exact matrices in Table 9 show improvements to near 0.9–1.0 in many settings).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>When arithmetic examples follow a highly predictable positional pattern, models learn a position-specific shortcut which does not transfer to natural language contexts where positions differ; disrupting absolute position cues in arithmetic training (Random Space) or removing/altering positional encodings enables models to associate arithmetic reasoning with natural-language prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Improvements shown at 124M scale with both pretrained and random-init variants; pretrained models benefit differently depending on embedding choices (pretraining helps recursive scratchpad).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Basic-format arithmetic examples with absolute positional embeddings fail to transfer into dialogue contexts (format mismatch); dialogue-only training lacks sufficient hard examples and contains noise, leading to poor accuracy on larger-digit dialogue additions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared Dia (dialogue-only) vs Dia+Basic vs Dia+RandomSpace, and compared PosEmbed vs NoPosEmbed vs RandEmbed across these datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>To integrate arithmetic competence into natural-language contexts, you must avoid teaching arithmetic via brittle absolute-positional patterns; either modify arithmetic data format (random spaces) or alter positional embeddings (remove pos embedding or add random per-token tags) so the arithmetic procedure generalizes into dialogue prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Positional Description Matters for Transformers Arithmetic', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e262.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e262.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>n×1 augmentation and first-step (%) operator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dataset interventions: augmenting multiplication training with many n×1 examples and with an explicit first-step (%) operation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two dataset-level interventions to bridge simple n×1 multiplications and full n×m multiplications: (1) artificially increase fraction of n×1 (multi-digit × single-digit) examples; (2) add a new operator (%) that explicitly denotes multiplication by b mod 10, exposing the first step.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT2-small</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>124M</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>multiplication (n×1 augmentation and first-step % decomposition)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>experiments shown for up to 5- and 6-maximum-digit datasets (training 300k samples, reversed product but no padding in some ablations)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>modify training dataset so that every 3rd example is either (a) changed so the second factor is single-digit (n×1 augmentation), or (b) changed so the operator is '%' representing a×(b mod 10) (first-step explicit supervision).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Both augmentations improve multiplication accuracy relative to the baseline non-padded, non-augmented setup; first-step (%) augmentation gives a further improvement beyond simple n×1 sampling. Improvements are modest compared to padding+reverse but are measurable (see Table 10a–d and comparisons to Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Providing many n×1 examples or an explicit first-step operation encourages the model to represent full multiplications as compositions of simple subproblems (decomposition into single-digit multiplications and accumulation), which helps the model apply learned subroutines to larger problems.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Augmentations help at the 124M scale under the paper's training budget but do not match the gains from padding+reverse; the benefits depend on the relative prevalence of n×1 examples (paper shows decreasing the ratio hurts performance).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Without sufficient n×1 exposure or explicit decomposition examples, the model struggles to apply simple-case knowledge to full n×m multiplications; mixing two distinct data types (basic and % examples) increases dataset complexity and requires the model to learn to interpret the new operator semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared baseline (no augmentation) vs n×1 augmentation every 3 datapoints vs first-step (%) augmentation every 3 datapoints, and vs padding/reversed interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Teaching subcomponents of an algorithm (many n×1 examples or explicit first-step supervision) helps small transformers compose solutions for larger multiplications, though data-format fixes like padding+reverse yield larger gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Positional Description Matters for Transformers Arithmetic', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Teaching arithmetic to small transformers <em>(Rating: 2)</em></li>
                <li>Length generalization in arithmetic transformers <em>(Rating: 2)</em></li>
                <li>Randomized positional encodings boost length generalization of transformers <em>(Rating: 2)</em></li>
                <li>Faith and fate: Limits of transformers on compositionality <em>(Rating: 1)</em></li>
                <li>Investigating the limitations of transformers with simple arithmetic tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-262",
    "paper_id": "paper-265456551",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "GPT2-small multiplication (padded+reversed)",
            "name_full": "GPT2-small (124M) — multiplication with padding and reversed product",
            "brief_description": "Fine-tuned GPT2-small (124M, decoder-only transformer) on synthetic multiplication data with zero-padding to fixed width and reversal of the product (least-significant-digit first); enabled accurate direct (no-scratchpad) outputs for large-digit multiplications up to 15×15 in this setup.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT2-small",
            "model_size": "124M",
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "multiplication (direct product output, digit-wise multiplication implied)",
            "number_range_or_complexity": "up to 15-digit × 15-digit (trained on n-maximum-digit datasets; training included many smaller-digit examples)",
            "method_or_intervention": "fine-tuning on generated multiplication dataset (300k examples), zero-padding factors to equal length, reversing the product digits (LSB first), random initialization; no scratchpad (direct output)",
            "performance_result": "Essentially perfect (≈100% accuracy) up through 12-digit × 12-digit multiplication; still high accuracy on many 13–15 digit combinations though some entries decline (paper reports very high accuracy overall and cites ability to accurately compute 15×15 in many cases; Table 5 and text: 'remarkable aptitude in 15 digits multiplication and essentially perfect up to 12 digits').",
            "mechanistic_insight": "Padding standardizes absolute token positions across training examples so the model can learn a single position-relative rule; reversing the product makes the model produce least-significant digits first so each output digit depends on a local window of input digits and carries propagate forward in generation order, simplifying carry handling; these format changes reduce the model's naive reliance on brittle absolute positional patterns and enable learning a digit-wise algorithmic procedure.",
            "performance_scaling": "Shown on a small model (124M) — the authors emphasize that a small fraction of model parameters sufficed; they note performance declines for very large maximum-digit settings (e.g., 20 digits) and that larger datasets / tuning could push capacity further. No systematic scaling across larger model sizes reported.",
            "failure_modes": "Without padding and reversal, the same training regimen fails quickly (poor beyond 1-2 digits or beyond combinations where sum of digit lengths &gt;5); accuracy degrades for some 13–15 digit combinations and becomes unsatisfactory by 20 digits with the same training budget; relies on dataset choices (digit distribution) and architectural conventions.",
            "comparison_baseline": "Compared to 'Basic' (no padding/no reverse) format and other augmentations (n×1 augmentation, % first-step operator), the padding+reverse intervention gave the largest improvement for large-number multiplication.",
            "key_finding": "Simple data-format modifications (fixed-width padding and reversing product digits) let a small decoder-only transformer directly learn to output large multiplications (essentially perfect to 12 digits, strong performance to 15) by removing positional irregularities and making carry computation local in generation order.",
            "uuid": "e262.0",
            "source_info": {
                "paper_title": "Positional Description Matters for Transformers Arithmetic",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "GPT2-small multiplication (basic/no-padding)",
            "name_full": "GPT2-small (124M) — multiplication in 'Basic' (no padding, normal product order) format",
            "brief_description": "Fine-tuning the same GPT2-small architecture on basic (non-padded) synthetic multiplication data yields good performance on tiny multiplications but fails to generalize to larger numbers because of variable absolute token positions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT2-small",
            "model_size": "124M",
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "multiplication (direct product output)",
            "number_range_or_complexity": "1–n digits; experiments show competence on 1–2 digit multiplications but poor performance as digits increase (esp. when sum of digits &gt;5)",
            "method_or_intervention": "fine-tuning on generated multiplication dataset (300k examples) with basic representation: 'digit digit ... * digit ... # product' (no padding, normal order)",
            "performance_result": "High accuracy on 1–2 digit multiplication, but performance drops sharply for larger numbers; model often fails when the sum of the digit-lengths of the two factors exceeds ~5 — e.g., model trained with n=5 performed well on 1–2 digits but poorly on 4×4 and larger.",
            "mechanistic_insight": "Model overfits to absolute positional patterns in the basic format: because operator and operand tokens occupy variable absolute positions, the model learns brittle position-specific shortcuts instead of a position-relative digit-wise algorithm, preventing length extrapolation.",
            "performance_scaling": "No evidence of length extrapolation with this format under the paper's training budget; adding more small-number samples alone did not enable extrapolation to larger numbers.",
            "failure_modes": "Fails to generalize to larger digit counts; likely memorizes frequent small multiplications; inability to handle variable operator positions leads to errant or missing digits in output.",
            "comparison_baseline": "Compared directly to padded/reversed formats and to datasets augmented with n×1 examples or first-step (%) examples; basic format is consistently inferior for large-number generalization.",
            "key_finding": "Without controlling positional structure, small transformers tend to exploit absolute-position shortcuts and cannot reliably scale to larger-digit multiplication tasks.",
            "uuid": "e262.1",
            "source_info": {
                "paper_title": "Positional Description Matters for Transformers Arithmetic",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Addition length extrapolation — data-format interventions",
            "name_full": "Addition length extrapolation experiments comparing Basic, Random Space, and Recursive Scratchpad formats",
            "brief_description": "Fine-tuning experiments (mostly on pretrained GPT2-small) for addition (digit-wise) where changing data formats (inserting random spaces; providing recursive, redundant scratchpad steps) greatly improves extrapolation to longer addends compared to a Basic format.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT2-small (fine-tuned from pretrained for main experiments)",
            "model_size": "124M",
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "addition (digit-wise addition with carry, shown via scratchpad variants)",
            "number_range_or_complexity": "training on 2–10 digit additions; testing on 9–13 digit additions to measure length generalization",
            "method_or_intervention": "fine-tuning pretrained GPT2-small for 5 epochs on 120k generated addition examples in three formats: Basic (straightforward digit-wise scratchpad), Random Space (random extra spaces inserted between characters with probability), and Recursive Scratchpad (redundant/repeated recursive digit-wise steps and reversed addend order).",
            "performance_result": "Table 6: Basic trained 2–10 digits → test accuracies: 9 digits 1.00, 10 digits 1.00, 11–13 digits 0.00 (no generalization). Random Space: 9:1.00, 10:1.00, 11:0.99, 12:0.09, 13:0.00. Recursive Scratchpad: 9:1.00, 10:1.00, 11:1.00, 12:0.96, 13:0.55 — i.e., Recursive Scratchpad shows substantial extrapolation (nearly perfect to 12 digits, substantial to 13).",
            "mechanistic_insight": "Random spaces disrupt absolute-position shortcuts, forcing models to rely on content or local structure rather than fixed indices; recursive scratchpad increases redundancy of per-digit information so the model must learn the recursive carry rule (an algorithmic digit-wise procedure) rather than memorize positional patterns. Pretraining status matters: recursive scratchpad required starting from a pretrained model to be effective.",
            "performance_scaling": "Demonstrates data-format-dependent phase transitions (from no generalization to substantial extrapolation) for a fixed model size; no cross-model-size scaling reported.",
            "failure_modes": "Basic format models trained on limited lengths often 'omit' digits when prompted with longer addends (outputting only the number of steps seen during training); Random Space only extends length by a small margin unless combined with redundancy; recursive scratchpad sometimes fails on certain examples (failure cases listed) but overall generalizes best.",
            "comparison_baseline": "Direct comparison of three data formats (Basic vs Random Space vs Recursive Scratchpad) and pretrained vs randomly initialized models; recursive scratchpad + pretrained outperforms others for length extrapolation.",
            "key_finding": "Data-format choices that reduce reliance on absolute positional cues (random spacing) or that provide redundant, recursive per-digit information (recursive scratchpad) enable a small pretrained transformer to generalize addition algorithms to longer lengths that were not in training.",
            "uuid": "e262.2",
            "source_info": {
                "paper_title": "Positional Description Matters for Transformers Arithmetic",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Positional-embedding ablations & random embeddings",
            "name_full": "Ablation of absolute positional embeddings and introduction of random (hash) embeddings",
            "brief_description": "Experiments removing the transformer’s absolute positional embeddings and alternatively injecting random per-token 'hash' vectors (random embedding) to reduce brittle absolute-position dependence and improve length generalization in digit-reversal and addition tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT2-small (both pretrained and randomly initialized variants tested)",
            "model_size": "124M",
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "digit-reversal (reverse digits) and addition (digit-wise)",
            "number_range_or_complexity": "train on numbers up to 10 digits and test on longer digits including repetitive-digit tests (regular and repetitive data)",
            "method_or_intervention": "remove absolute positional embedding entirely, or add a 'random embedding' (n_hash-dim Gaussian vectors split across attention heads, regenerated per epoch and test) appended to token embeddings; compare pretrained vs random-init models.",
            "performance_result": "Removing positional embedding improves length generalization by roughly ~2 digits on digit-reversal tasks for both pretrained and random-init models (Figure 1). Adding random embedding further improves generalization on both regular and repetitive data (Figure 2). On addition tasks, models with random embedding achieved similar generalization to the Recursive Scratchpad format.",
            "mechanistic_insight": "The dominant failure mode is overreliance on absolute-position signals. Removing positional embeddings forces the model to adopt alternative positional encodings; random per-token tags (random embedding) provide unique identifiers that let the model distinguish identical digits at different positions and avoid brittle absolute-index shortcuts. Pretrained weights provide additional learned components that make it easier to exploit non-positional cues, particularly on repetitive data.",
            "performance_scaling": "Improvements are demonstrated for the 124M model; random embedding combined with removing absolute positional embedding gives larger generalization gains than removing positional embedding alone, especially for randomly initialized models.",
            "failure_modes": "Removing positional embeddings can cause instability and difficulty unless alternative positional cues (like random embedding) are provided; randomly initialized models without positional embedding but without random embedding perform poorly on repetitive-digit inputs.",
            "comparison_baseline": "Compared models with absolute positional embedding vs with positional embedding removed vs with added random embedding; also compared pretrained vs randomly initialized models.",
            "key_finding": "Absolute positional embeddings drive brittle, non-generalizable solutions on arithmetic tasks; providing position information via randomized per-token tags or removing absolute positions (and using alternative encodings) enables the model to learn more algorithmic, generalizable digit-wise procedures.",
            "uuid": "e262.3",
            "source_info": {
                "paper_title": "Positional Description Matters for Transformers Arithmetic",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Natural-language + arithmetic integration",
            "name_full": "Integration experiments mixing dialogue (natural language) and arithmetic data",
            "brief_description": "Tests whether arithmetic skills learned on synthetic arithmetic data transfer to arithmetic problems embedded in natural language dialogue prompts, and how data formats and positional-embedding choices affect this integration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT2-small (randomly initialized and pretrained variants tested)",
            "model_size": "124M",
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "addition (2–5 digit) in dialogue and pure arithmetic contexts",
            "number_range_or_complexity": "dialogue dataset: many 2–3-digit additions and fewer 4–5-digit additions; arithmetic dataset: 2–5 digit additions (120k samples)",
            "method_or_intervention": "train on (i) dialogue-only data, (ii) dialogue plus Basic arithmetic data, (iii) dialogue plus Random Space arithmetic data; compare models with positional embedding, without positional embedding, and with random embedding; test in dialogue prompt and pure arithmetic prompt contexts; training for 50–100 epochs.",
            "performance_result": "Models trained on dialogue-only fail at 4–5 digit addition. Naively mixing arithmetic data in Basic format with absolute positional embeddings provides little improvement. Using Random Space format, removing positional embeddings, or adding random embedding substantially improves the model's ability to perform addition within dialogue prompts. Quantitatively (Table 9): models with Random Space or NoPosEmbed or RandEmbed show much higher accuracies on 4–5-digit dialogue additions than Basic+PosEmbed baselines (exact matrices in Table 9 show improvements to near 0.9–1.0 in many settings).",
            "mechanistic_insight": "When arithmetic examples follow a highly predictable positional pattern, models learn a position-specific shortcut which does not transfer to natural language contexts where positions differ; disrupting absolute position cues in arithmetic training (Random Space) or removing/altering positional encodings enables models to associate arithmetic reasoning with natural-language prompts.",
            "performance_scaling": "Improvements shown at 124M scale with both pretrained and random-init variants; pretrained models benefit differently depending on embedding choices (pretraining helps recursive scratchpad).",
            "failure_modes": "Basic-format arithmetic examples with absolute positional embeddings fail to transfer into dialogue contexts (format mismatch); dialogue-only training lacks sufficient hard examples and contains noise, leading to poor accuracy on larger-digit dialogue additions.",
            "comparison_baseline": "Compared Dia (dialogue-only) vs Dia+Basic vs Dia+RandomSpace, and compared PosEmbed vs NoPosEmbed vs RandEmbed across these datasets.",
            "key_finding": "To integrate arithmetic competence into natural-language contexts, you must avoid teaching arithmetic via brittle absolute-positional patterns; either modify arithmetic data format (random spaces) or alter positional embeddings (remove pos embedding or add random per-token tags) so the arithmetic procedure generalizes into dialogue prompts.",
            "uuid": "e262.4",
            "source_info": {
                "paper_title": "Positional Description Matters for Transformers Arithmetic",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "n×1 augmentation and first-step (%) operator",
            "name_full": "Dataset interventions: augmenting multiplication training with many n×1 examples and with an explicit first-step (%) operation",
            "brief_description": "Two dataset-level interventions to bridge simple n×1 multiplications and full n×m multiplications: (1) artificially increase fraction of n×1 (multi-digit × single-digit) examples; (2) add a new operator (%) that explicitly denotes multiplication by b mod 10, exposing the first step.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT2-small",
            "model_size": "124M",
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "multiplication (n×1 augmentation and first-step % decomposition)",
            "number_range_or_complexity": "experiments shown for up to 5- and 6-maximum-digit datasets (training 300k samples, reversed product but no padding in some ablations)",
            "method_or_intervention": "modify training dataset so that every 3rd example is either (a) changed so the second factor is single-digit (n×1 augmentation), or (b) changed so the operator is '%' representing a×(b mod 10) (first-step explicit supervision).",
            "performance_result": "Both augmentations improve multiplication accuracy relative to the baseline non-padded, non-augmented setup; first-step (%) augmentation gives a further improvement beyond simple n×1 sampling. Improvements are modest compared to padding+reverse but are measurable (see Table 10a–d and comparisons to Table 4).",
            "mechanistic_insight": "Providing many n×1 examples or an explicit first-step operation encourages the model to represent full multiplications as compositions of simple subproblems (decomposition into single-digit multiplications and accumulation), which helps the model apply learned subroutines to larger problems.",
            "performance_scaling": "Augmentations help at the 124M scale under the paper's training budget but do not match the gains from padding+reverse; the benefits depend on the relative prevalence of n×1 examples (paper shows decreasing the ratio hurts performance).",
            "failure_modes": "Without sufficient n×1 exposure or explicit decomposition examples, the model struggles to apply simple-case knowledge to full n×m multiplications; mixing two distinct data types (basic and % examples) increases dataset complexity and requires the model to learn to interpret the new operator semantics.",
            "comparison_baseline": "Compared baseline (no augmentation) vs n×1 augmentation every 3 datapoints vs first-step (%) augmentation every 3 datapoints, and vs padding/reversed interventions.",
            "key_finding": "Teaching subcomponents of an algorithm (many n×1 examples or explicit first-step supervision) helps small transformers compose solutions for larger multiplications, though data-format fixes like padding+reverse yield larger gains.",
            "uuid": "e262.5",
            "source_info": {
                "paper_title": "Positional Description Matters for Transformers Arithmetic",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Teaching arithmetic to small transformers",
            "rating": 2,
            "sanitized_title": "teaching_arithmetic_to_small_transformers"
        },
        {
            "paper_title": "Length generalization in arithmetic transformers",
            "rating": 2,
            "sanitized_title": "length_generalization_in_arithmetic_transformers"
        },
        {
            "paper_title": "Randomized positional encodings boost length generalization of transformers",
            "rating": 2,
            "sanitized_title": "randomized_positional_encodings_boost_length_generalization_of_transformers"
        },
        {
            "paper_title": "Faith and fate: Limits of transformers on compositionality",
            "rating": 1,
            "sanitized_title": "faith_and_fate_limits_of_transformers_on_compositionality"
        },
        {
            "paper_title": "Investigating the limitations of transformers with simple arithmetic tasks",
            "rating": 1,
            "sanitized_title": "investigating_the_limitations_of_transformers_with_simple_arithmetic_tasks"
        }
    ],
    "cost": 0.01578925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Positional Description Matters for Transformers Arithmetic
22 Nov 2023</p>
<p>Ruoqi Shen shenr3@cs.washington.edu 
University of Washington</p>
<p>Sébastien Bubeck 
Ronen Eldan 
Yin Tat Lee 
Yuanzhi Li 
Yi Zhang </p>
<p>Microsoft Research</p>
<p>Microsoft Research</p>
<p>Positional Description Matters for Transformers Arithmetic
22 Nov 202313B981EE8A5F4705DC2F264FA335E53BarXiv:2311.14737v1[cs.CL]
Transformers, central to the successes in modern Natural Language Processing, often falter on arithmetic tasks despite their vast capabilities -which paradoxically include remarkable coding abilities.We observe that a crucial challenge is their naive reliance on positional information to solve arithmetic problems with a small number of digits, leading to poor performance on larger numbers.Herein, we delve deeper into the role of positional encoding, and propose several ways to fix the issue, either by modifying the positional encoding directly, or by modifying the representation of the arithmetic task to leverage standard positional encoding differently.We investigate the value of these modifications for three tasks: (i) classical multiplication, (ii) length extrapolation in addition, and (iii) addition in natural language context.For (i) we train a small model on a small dataset (100M parameters and 300k samples) with remarkable aptitude in (direct, no scratchpad) 15 digits multiplication and essentially perfect up to 12 digits, while usual training in this context would give a model failing at 4 digits multiplication.In the experiments on addition, we use a mere 120k samples to demonstrate: for (ii) extrapolation from 10 digits to testing on 12 digits numbers while usual training would have no extrapolation, and for (iii) almost perfect accuracy up to 5 digits while usual training would be correct only up to 3 digits (which is essentially memorization with a training set of 120k samples).</p>
<p>Introduction</p>
<p>In the world of Large Language Models (LLMs) advancements, arithmetic operations stand as an important yet frequently underestimated challenge.The emergence and triumph of models like GPT-4 (OpenAI, 2023;Bubeck et al., 2023) have had a transformative impact on various sectors, illuminating new potentials in Natural Language Processing and more.However, as we delve deeper into the diverse applications of these models, arithmetic tasks continually pose obstacles (Dziri et al., 2023), e.g., even GPT-4's struggles with tasks such as 4-digit multiplication.</p>
<p>Arithmetic tasks differ significantly from typical natural language tasks.The primary distinction lies in their execution: arithmetic operations might demand intricate intermediate steps internally, whereas most other tasks might not need such extensive internal processing.Furthermore, they possess a distinct data format, being based on a concise vocabulary with vast potential combinations.They also showcase more predictable patterns, and each token in an arithmetic sentence can hold equal significance.This contrasts with other tasks where omitting some non-essential words might not affect the overall meaning.Given the stark differences between arithmetic and other tasks, it is uncertain whether there is a straightforward way to bolster a language model's proficiency in arithmetic.Specifically, it's unclear if the prevailing architecture-tailored mainly for natural language tasks-can efficiently and accurately tackle arithmetic tasks.Moreover, this uniqueness of arithmetic also presents an opportunity: the structured nature of arithmetic, with its transparent steps and definitive outcomes, offers an ideal framework for a deeper understanding of the models.Addressing the challenges of arithmetic tasks and enhancing the arithmetic proficiency of LLMs can also contribute to a deeper understanding of their strengths and limitations.</p>
<p>In this work, we investigate the capabilities of language models concerning arithmetic operations, emphasizing techniques related to efficient position information utilization.Before delving into our approaches, we first identify the important challenges that arithmetic tasks face.Three such challenges, central to this study, are:</p>
<p>Complicated Calculation Large-number multiplication and similar arithmetic tasks often involve intricate intermediate steps.Human solutions without using a calculator typically involve digit-wise multiplication, followed by summation.However, allowing a model to record each step can be verbose and inefficient for larger numbers.We investigate the feasibility of enabling a small transformer to directly output the product of large multiplication tasks.</p>
<p>Length Extrapolation Arithmetic data, unlike natural language data, typically exhibits highly regular patterns.As a result, models often depend on absolute position information to solve such tasks (Lee et al., 2023).For instance, in an addition operation like a 1 b 1 c 1 + a 2 b 2 c 2 , aligning digits in corresponding positions (e.g., a 1 in position 1 and a 2 in position 5) presents the simplest solution.However, for a four-digit addition task like a 1 b 1 c 1 d 1 + a 2 b 2 c 2 d 2 that hasn't appeared in the training, it's unclear how to handle d 1 at position 4.</p>
<p>Arithmetic and Language Integration</p>
<p>The poor performance of transformers on arithmetic data may be partly due to the lack of arithmetic data in the training set.However, it's uncertain whether simply supplementing the model with arithmetic data will resolve the problem.It's unclear if the model can successfully integrate arithmetic and natural language data due to their substantial differences.</p>
<p>We tackle the above challenges through two types of approaches, both aimed at utilizing position information more efficiently.A first approach is to alter the positional encoding directly.In this work, we explore an alternative positional encoding, namely randomized embedding, which is simple yet efficient for arithmetic tasks.A less direct approach for better position information utilization is to modify the representation of the arithmetic data to leverage standard positional encoding differently.We investigate how altering the data format can lead the model to learn the arithmetic task differently and exhibit varying properties.</p>
<p>In this work, we focus on small models of a GPT2-small size (124M).Our findings reveal that even such modest-sized models can adeptly execute intricate arithmetic tasks.This underscores not only the capability of the transformer architecture to handle arithmetic but also highlights that a small subset of model parameters can integrate arithmetic proficiency into language models, without affecting the model's capacity on other tasks.</p>
<p>We study large-number multiplication in Section 2, length generalization in Section 3 and arithmetic and language integration in Section 4. In this work, we tackle the three challenges outlined separately.However, in practice, we would need a single model that is able to show all the properties we want.This can be done by combining the approaches used in this paper, which we leave as a future work.For the purposes of this paper, we've maintained consistency in data size, model size, and training epochs, though it's conceivable that our observed outcomes could be achieved with reduced data sizes, smaller models, and fewer training epochs.</p>
<p>Related Works Several recent works have studied using transformers to solve arithmetic tasks.Charton (2021Charton ( , 2022) ) studied using transformers to do linear algebra.Nogueira et al. (2021) studied how the surface form of a number affects learning.Zhang et al. (2022) studied variable assignment task.Qian et al. (2022) demonstrated the limitation of language models on arithmetic tasks.Hanna et al. (2023) studied the ability of GPT2 on arithmetic tasks from an interpretation viewpoint.Dziri et al. (2023) showed that even fine-tuned GPT3 has trouble performing 3-digit multiplication.Yang et al. (2023) trained a model of size 2B to perform arithmetic tasks and beat the performance of GPT4, but the accuracy obtained is not perfect even for 5-digit numbers.Lee et al. (2023) focused on the sample efficiency of using various data formats for arithmetic tasks and also studied the challenges we address in this paper, focusing on small numbers such as 3-digit addition and 2-digit multiplication.We are not aware of any previous work that is able to output the product of two 15-digit number multiplication, essentially perfect up to 12-digit, as demonstrated in our paper.Lee et al. (2023) also illustrates a model's ability to learn arithmetic and language simultaneously, but the two types of data remain separated.We refer the readers to Testolin (2023) for a survey on the recent progress in using neural networks to do arithmetic tasks.</p>
<p>A long list of works has focused on length generalization of transformers by modifying the positional encoding or model architecture, including Su et al. (2021), Press et al. (2021), Kiyono et al. (2021), Csordás et al. (2021), Li and McClelland (2022), Kazemnejad et al. (2023), Ruoss et al. (2023).Jelassi et al. (2023) shows that relative position embedding (Su et al., 2021), the encoder-only model can generalize to significantly longer lengths on arithmetic tasks.</p>
<p>To solve math problems using transformers, Uesato et al. ( 2022</p>
<p>Large Number Multiplication</p>
<p>Multiplication entails a sequence of intermediate steps, especially when dealing with large numbers.Modern language models like GPT-4 often find it challenging to handle these extensive multiplications (see Table 1).One test we can do is to ask the model to output the product directly, without using a scratchpad.We believe studying how the model can output the answer directly, bypassing intermediary steps, is an important research direction because in practice, outputting every step can be laborious and time-consuming.More importantly, always outputting the full steps can also prevent the model from using the most efficient method to solve the problem.In Section 2.1, we show a 12-layer transformer can output the product of 15 × 15-multiplication directly, demonstrating the immense potential of transformers.Constraining the model to use the scratchpad can force the model to adopt suboptimal strategies.While it can be hard for the model to learn to output the answers directly without using a scratchpad, our experiment indicates that given the right dataset and training regimen, it is feasible.</p>
<p>Large number multiplication is complicated, so it can be hard for the model to detect the rules for multiplication if we train the model directly with complicated multiplication tasks.However, there exist simple cases such as one-digit multiplications.By starting with these straightforward cases, the model can initially grasp rules from the basics and then extrapolate to more complex situations.For our initial attempt, we included a lot of small-number multiplication in our datset.Our aim was to ensure the model had ample exposure to basic multiplications, enabling it to grasp multiplication rules.We create a dataset with 300k samples on 1-to-n-digit multiplication.We generate the two numbers in a way such that the number of digits of the two numbers is sampled uniformly randomly from {1, ..., n}.Although this uniform distribution ensures a balanced representation of numbers of different lengths, our emphasis leans towards smaller numbers.For example, our training set consists of around 8k single-digit number times a single-digit number, but there are only 100 different one-digit multiplications, so there will be a lot of repeated single-digit multiplication in our training set.On the contrary, the training set contains only less than 0.0002% of 5-digit times 5-digit numbers.In the "Basic" format, the multiplier, multiplicant, and their product are presented straightforwardly.For instance, for two numbers 73866 and 1001, we write down "7 3 8 6 6 * 1 0 0 1 # 7 3 9 3 9 8 6 6".1 We show in Table 2 the performance of a randomly initialized GPT2-small trained for 300 epochs when n = 5 and in Table 13 the performance when n = 10.The model performs well on 1-2-digit multiplication, but very poorly on large numbers.Notably, we see a trend that the model performs poorly when the sum of the number of digits in the two factors is greater than 5.When the sum is smaller than 5, the training set includes more than 10% of all possible number combinations, leading to uncertainty regarding whether the model's proficiency with smaller numbers stems from genuine understanding or mere memorization.</p>
<p>Our findings show that emphasizing the small numbers is not enough for the model to perform well on large numbers.As the next step, we will focus on modifying the simple case, where the model can grasp the rule, so that the model can extrapolate to the hard cases efficiently.In Section 2.1 and Section A.1, we will present two distinct approaches designed to help the model draw connections between simpler and more complex tasks.These two approaches follow different principles and we hope they can inspire innovative simple case formulations not only for this multiplication task but for other tasks as well.</p>
<p>Padding</p>
<p>For datapoints on multiplications of numbers with different numbers of digits, the position of the product sign varies.Consequently, the model needs to figure out the position of the product sign first and then perform the operation based on the relative position.This makes the rule of operation unnecessarily hard.A simple modification we can adopt is to add zero-padding to the training samples so that all the numbers are given in the same length.In this way, all multiplications will follow one rule no matter how many the number of digits the two factors have.If the maximum number of digits for the factors is n, we pad 0 so that both factors contain n digits and the product contains 2n digit.</p>
<p>In addition, to make the task even easier, we can reverse the digits in the product.The rationale behind this is that to get the most significant digit of the product, we need to compute the carry from each digit accurately but to get the least significant digit, we only need to use the least significant digits of the two factors.As a result, starting with the least significant digit and progressing to the most significant digit is more straightforward.This intuitive approach has been used in previous works such as Lee et al. (2023).</p>
<p>Data Format</p>
<p>Example Basic 7 3 8 6 6 * 1 0 0 1 # 7 3 9 3 9 8 6 6 Reverse Product 7 3 8 6 6 * 1 0 0 1 # 6 6 8 9 3 9 3 7 Add Padding 7 3 8 6 6 * 0 1 0 0 1 # 0 0 7 3 9 3 9 8 6 6 Add Padding + Reverse Product 7 3 8 6 6 * 0 1 0 0 1 # 6 6 8 9 3 9 3 7 0 0 Table 4: Testing accuracy for models trained on data with padding and(or) reversed product when the maximum number of digits is 5.</p>
<p>In Table 3, we present examples of our data format.We give more details on the dataset and the setup in Appendix B. The accuracy by GPT2-small on 300k samples achieved using padding and/or reversed product for multiplications with a maximum of 5 and 10 digits is detailed in Table 4 and Table 14 respectively.The results indicate that padding markedly boosts accuracy while reversing the product further elevates it to near perfection.Utilizing both padding and reversed product allows us to accurately compute up to 15 × 15 multiplications, as shown in Table 5.This is a remarkable enhancement when compared to the non-padded data format, which encountered difficulties even with 4 × 4 multiplication.The benefit of padding is that it standardizes the format between basic and more complex cases, enabling them to be addressed by a singular rule and enhancing the link between them.</p>
<p>However, when evaluating accuracy for a maximum of 20 digits in Table 15, the results for larger numbers are unsatisfactory.We did not fine-tune the parameters in our experiment, so it is possible we can achieve high accuracy for even more digits if we use a larger training set, a more optimal digit distribution, or a more fine-tuned learning rate, etc.</p>
<p>Length Extrapolation</p>
<p>In this section, we tackle a different challenge from the previous section, length extrapolation.While relying on position information can help complicated arithmetic tasks, overreliance on position can hurt generalization to additional digits.Based on the idea of reducing the reliance on absolute positional information, in Section 3.1, we delve into various data formats that can help generalization to additional digits, and in section 3.2, we investigate the role of vanilla positional embedding in arithmetic tasks and explore alternative positional embedding that better suits the needs of arithmetic tasks.</p>
<p>Data format</p>
<p>In this section, we explore the impact of different data formats on the generalization performance of models when faced with additional digits in the addition task.We propose two distinct data formats that aid in improving the models' ability to generalize.One straightforward data format is a chain-of-thought (Wei et al., 2022) style scratchpad.In this format, we first write down the two addends of the addition, followed by the digit-wise summation steps and the final sum.However, as expected, this format struggles to generalize to numbers longer than those encountered during training.A common mistake made by the model is omitting digits while recording the digit-wise summation steps.To address this issue, we explore new data formats based on two key ideas.The first idea involves introducing random spaces between characters in the data.By doing so, we make it more challenging for the model to rely on absolute positional embedding to solve the task.This disruption encourages the model to consider other cues beyond the absolute positional information.</p>
<p>The second idea is based on repeating more information for each digit-wise step.This approach allows the model to access additional information, enabling it to learn the actual steps rather than solely relying on memorizing positional patterns.The increased redundancy makes it harder for the model to overfit to positional information.We found that data formats based on both of these ideas significantly improve generalization performance.By incorporating random spaces and increasing information repetition, the models gain the ability to better handle numbers with more digits and exhibit enhanced generalization performance.</p>
<p>We test our two ideas on three data formats.Table 7 shows the examples of these three data formats, where random space is based on the first idea and recursive scratchpad is based on the second idea.We give the formal definition of the data formats and the setup in Appendix C.1.We show in Table 6 the accuracy of the model on the three types of data formats.We further give a few failure examples of the models trained on each data format in Table 16.Our experimental results corroborate our conjectures.</p>
<p>In the "Basic" data format, when trained on addends up to 10 digits, the model fails to generalize numbers exceeding 10 digits.If the model is given two addends that exceed this length, it simply omits some digits and outputs a result with only 10 steps.However, incorporating random spaces into the training set compels the model to move away from relying on absolute positional embedding since it can no longer retrieve the digits from fixed positions.Despite the model's accurate prediction only extending by one digit, this progression represents a significant improvement, demonstrating a phase transition from a complete lack of generalization to some degree of it.We observe an even more significant improvement in generalization performance when we increase the information provided in each digit-wise step.This suggests that adding more information can encourage the model to learn the fundamental recursive steps required for addition, as opposed to overfitting to positional information.</p>
<p>Data Format</p>
<p>9 Digits 10 Digits 11 Digits 12 Digits 13 Digits Basic 1.00 1.00 0.00 0.00 0.00 Random Space 1.00 1.00 0.99 0.09 0.00 Recursive Scratchpad 1.00 1.00 1.00 0.96 0.55</p>
<p>Table 6: Testing accuracies on 9-13-digit-addition of models trained on the three data formats of 2-10-digit-addition.</p>
<p>Data Format Example Basic 2 3 9 + 8 2 1 : 0 + 9 + 1 = 1 0, 1 + 3 + 2 = 6, 0 + 2 + 8 = 1 0, 1 0 6 0 Random Space 2 3 9 + 8 2 1 : 0 + 9 + 1 = 1 0, 1 + 3 + 2 = 6, 0 + 2 + 8 = 1 0, 1 0 6 0 Recursive Scratchpad 2 3 9 + 8 2 1 : 0 + 9 + 1 = 1 0 , = 0 , 3 2 + 2 8 : 1 + 3 + 2 = 6 , = 6 0 , 2 + 8 : 0 + 2 + 8 = 1 0 , = 0 6 0 , = 1 0 6 0 In addition, we would like to make the following remarks.Pretrained vs. Randomly Initialized We found that in this task, using a pretrained model is important for "recursive scratchpad".Without using a pretrained model, "recursive scratchpad" won't help generalization to additional digits.However, it does not make much difference for "random space".For both pretrained and randomly initialized models, "basic" does not generalize to additional digits.We will have more discussion on training from scratch on the addition task in Section 3.2.</p>
<p>Reverse the order of the digits in the addends For "Recursive Scratchpad", we found that reversing the order of the digits of the addends can help the generalization performance.However, reversing the order of both the addends and the sum will not help as much.</p>
<p>Positional Embedding</p>
<p>As we discussed in Section 3.1, the data format can greatly influence a model's dependency on positional information, which subsequently affects its generalization capacity.In this section, we directly examine positional embedding by studying its limitations and exploring potential alternatives.</p>
<p>To better understand the significance of positional embedding, we first consider a simpler task: given a number, the model outputs its digits in reverse order.For instance, if the input number is 12345, the output should be 54321.We evaluate the model's performance on numbers that are longer than those in the training set and investigate challenging cases such as numbers with many repeated digits.We give the formal definition of the two types of data in Appendix C.2.</p>
<p>As an initial step, we eliminated the positional embedding of the GPT2-small while leaving the rest of the architecture intact.It appears that for both the pre-trained model and the model trained from scratch, the removal of positional embedding enhances the generalization capacity across more digits.We show in Figure 1 the test accuracy of both models on regular and repetitive data.Figure 1a indicates that upon deletion of the positional embedding, both models exhibit an improvement in generalization by approximately two digits on the regular data.While we don't observe a significant accuracy discrepancy between the two models on regular data, their performance on repetitive data varies considerably.As shown in Figure 1b, the repetitive data does not pose a difficult challenge for the model with positional embedding.However, it becomes notably difficult for the model trained from scratch, which achieves low accuracy even with 9-digit data.In contrast, it's relatively simple for the pre-trained model, which manages to achieve perfect accuracy with 16-digit data.We speculate that the underlying reason is the inability to differentiate repetitive data  aside from their respective positions.Without absolute positional embedding, the models must resort to alternative methods to encode positional information.Given that the pre-trained model already contains various useful pre-trained components, it has greater flexibility to address this issue.</p>
<p>We propose a simple solution targeting this issue.We mark each token using a random tag so that the model can easily use the tag to distinguish the same tokens appearing at different positions.We call this component a random embedding.We are able to show that this random tag can not only improve the generalization performance on this simple task of digit reversion but also on the more complicated task of addition.</p>
<p>Random Embedding For any chosen hash dimension n hash , we generate a n hash -dimensional random Gaussian vector with mean 0 and identity covariance.Then, we split the Gaussian vector into n head many vectors {h i } n head i=1 each with dimension n hash /n head , set the last n hash /n head dimensions of the input embedding of each head to be h i and keep the remaining (n embed − n hash )/n head dimensions unchanged.After the final layer, we use only the first (n embed − n hash )/n head dimension of each head to decode.We use newly generated random vectors for each epoch and during testing.In Figure 2, we demonstrate the improved generalization capacity of GPT2-small equipped with random embedding.Figure 2a shows that adding random embedding increases the generalization capacity on both the regular data and the repetitive data in the digit reversion task.</p>
<p>Back to the more complicated task of addition, we show in Figure 2b that if we simply delete the positional embedding, the random initialized model does not perform well.If we keep the positional embedding, the model does not generalize to more digits.The random embedding shows significant improvement by achieving about the same generalization capacity as the "Recursive Scratchpad" data format as we show in Section 3.1.</p>
<p>Addition in Natural Language Setting</p>
<p>In the previous sections, we focused on the case where the training data consists of solely arithmetic data.However, in practice, we need to do the arithmetic operations in the natural language setting.Training data consisting exclusively of arithmetic data is usually easy to collect as it can be generated programmatically in large quantities.On the contrary, obtaining arithmetic information embedded in natural language is a more arduous task due to its rarity in natural language content.Consequently, it is important to understand if training on purely arithmetic data can equip the model with the ability to perform arithmetic tasks within a natural language setting.</p>
<p>In this section, we explore a task that involves mixing natural language data with purely arithmetic data to investigate the model's ability to integrate both data types.The natural language data in this case includes dialogues on solving addition problems, with a substantial amount of samples for easy addition questions and a smaller portion for difficult ones.Such dataset structure reflects the real-world challenge of readily collecting easy tasks, while struggling to find natural language data that solves more complex problems.Alongside this, we incorporate purely arithmetic data, which is always correct and can be effortlessly produced using computer programs.Our primary objective is to examine whether the accurate arithmetic data can help the model solve the complex tasks embedded in the natural language context.</p>
<p>Our experiments show that in our setting, training solely on the natural language data can't guarantee an accurate solution to the difficult problems due to the lack of difficult samples in the training set and the errors present.If we naively mix the arithmetic data with the natural language data, we don't see a significant boost in accuracy, which shows that integrating the two types of data is challenging if they follow different formats.One obvious difficulty arises when the arithmetic data follows a certain pattern; the model can easily learn the arithmetic task relying on the positional information.However, when the arithmetic task appears in the natural language context, it won't follow the same positional pattern, causing the model to struggle to connect the two types of data.Overreliance on positional embedding is a recurring issue when using transformers for arithmetic tasks, and this represents the main challenge we discuss in Section 3. In Section 3, we tackle this issue from two aspects: data format and alternative position embedding.We show in our experiments that similar ideas can be applied to the integration of natural language and arithmetic data, thus facilitating the merging of these two types of data.</p>
<p>We use three types of data formats, formally defined in Appendix D with examples shown in Table 9.Our dialogue dataset contains a large number of 2-3-digit addition, but not enough 4-5-digit addition while the addition data set containd a large number of both 2-3-digit addition and 4-5-digit addition.We compare in Table 9 models trained on datasets that combine dialogue data with addition data (Dia+Basic and Dia+RandomSpace) to those trained solely on dialogue data (Dia).We show the results for random initialized models trained 50 epochs and 100 epochs.Without any arithmetic data, models trained exclusively on dialogue struggle to accurately perform 4-5-digit addition.This confirms our hypothesis, given that the dialogue lacks a sufficient number of correct 4-5-digit examples.With arithmetic data, for models with the absolute position embedding, "Basic" doesn't significantly enhance their ability to tackle addition tasks within the dialogue prompt.In contrast, using "Random space", removing the absolute position embedding, and integrating random embedding all improve the model's ability to leverage addition data in supporting dialogue-based addition tasks.For models that exclude absolute position embedding, as well as those with random embedding, the testing accuracy for "Basic" and "Random space" is similar when trained for long enough.Nevertheless, models can learn the "Random space" format slightly faster, as shown in thing.The answer is 726.Addition Data -Basic 4 8 + 4 = 5 2 3 7 5 + 2 6 1 = 6 3 6 5 0 5 1 + 8 5 3 9 = 1 3 5 9 0 Addition Data -Random Space 4 8 4 5 2 3 7 5 2 6 1 6 3 6 5 0 5 1 8 5 3 9 1 3 5 9 0 Table 9: Testing accuracy for models with and without the positional embedding and with the random embedding on the dataset solely consists of dialogue data and the data set consists of a mix of dialogue data and addition data.</p>
<p>and Table 9b.Models without position embedding exhibit slightly better accuracy compared to those with random embedding in dialogue contexts.Conversely, models with random embedding outperform those lacking position embedding in pure addition scenarios, as highlighted in Table 9c and Table 9d.</p>
<p>In conclusion, to allow language and arithmetic integration, we need either data format modification such as random space, or position embedding modification such as excluding absolute positional embedding or adding random embedding.Our conclusions here align with those in Section 3.For models with absolute position embedding, the "Basic" format is less effective due to its highly predictable pattern, allowing models to overly depend on positional information.Removing position embedding addresses this, but can create new stability issues as the model needs alternative ways to interpret position data.Introducing random embedding can offset the drawbacks of removing position embedding, resulting in a more stable performance.Testolin, A. (2023).Can neural networks do arithmetic?a survey on the elementary numerical skills of state-of-the-art deep learning models.arXiv preprint arXiv:2303.07735.</p>
<p>Uesato, J., Kushman, N., Kumar, R., Song, F., Siegel, N., Wang, L., Creswell, A., Irving, G., and Higgins, I. (2022).Solving math word problems with process-and outcome-based feedback.arXiv preprint arXiv:2211.14275.</p>
<p>Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. (2022).Chain-ofthought prompting elicits reasoning in large language models.Advances in Neural Information Processing Systems, 35:24824-24837.</p>
<p>Yang, Z., Ding, M., Lv, Q., Jiang, Z., He, Z., Guo, Y., Bai, J., and Tang, J. ( 2023 In multiplication, n × 1-multiplication serves as a very important simple case.If our dataset does not contain enough n × 1-multiplication, the models will not be able to learn multiplication successfully even with padding and reversed product.We show in Figure 11 how decreasing the ratio of n × 1-multiplication hurts the model's performance.With this insight, we amplify the presence of n × 1 multiplication in the training set, hypothesizing that this could potentially improve multiplication accuracy, even in the absence of padding.As an initial step, for every three data points of the dataset, we modify the second factor of the multiplication to a single-digit number.In other words, given the original dataset, we choose 1/3 of the datapoints a * b and replace b with b%10.In this way, our dataset consists of a large number of n × 1 multiplication.We give an example of such a dataset with n × 1 multiplication every 3 datapoints in Table 12.Our training datapoints have reversed product but no padding.We show the accuracy on 5-maximum-digit and 6-maximum-digit in Table 10a and 10b.A comparison between Table 10a and Table 4 reveals that this augmentation does somewhat enhance the model accuracy.</p>
<p>To use the simple n × 1-multiplication data, the model must recognize that n × m-multiplication can be decomposed into m n × 1 multiplication and use the knowledge gained from n × 1-multiplication to solve the problem.To better bridge the understanding between n × 1 and n × m multiplications, we undertook a more bold adjustment to our dataset.Rather than merely incorporating more n × 1 multiplications, we introduced an entirely new type of data that explicitly tells the model that n × 1 multiplication is a foundational step for n × m multiplication.To this end, we introduce a new operation % where for two numbers a and b, we define a%b = a × (b mod 10).In this way, we can view % as guidance on the first step of solving a * b.We change the operation from * to % for every 3 datapoints in our training set.We show in Table 12 examples of data with first-step multiplication % every 3 datapoints.We show in Table 10c and 10d the accuracy on 5-maximum-digit and 6-maximum-digit.We see a further performance improvement from Table 10a and Table 10b.</p>
<p>While the enhancements observed from introducing the % operation might not be as pronounced as those achieved with padding, as discussed in Section 2.1, the outcomes of this experiment are both intriguing and enlightening.When comparing the two experiments -one adding additional n × 1 multiplications, and the other integrating first-step multiplications -the latter introduces a more intricate dataset.This dataset comprises two markedly distinct types of data, requiring the models to discern the function of the novel %  Example n × 1 multiplication every 3 datapoints 6 5 1 2 5 * 6 # 0 5 7 0 9 3 (with reversed product) 5 1 4 * 5 9 6 9 # 6 6 0 8 6 0 3 3 8 6 3 1 * 6 1 6 # 6 9 6 6 9 7 3 2 2 2 * 9 # 8 9 1 7 9 8 0 4 * 8 8 8 9 2 # 8 6 1 7 3 9 3 9 0 7  1 9 6 6 5 * 9 7 0 # 0 5 0 5 7 0 9 1  2 6 4 9 * 6 # 4 9 8 5 1  4 3 5 0 * 8 8 1 5 # 0 5 2 5 4 3 8 3  2 0 2 0 * 9 9 8 9 # 0 8 7 7 7 1 0 2  6 2 2 7 4 * 5 # 0 7 3 1 1 3 ... First-step multiplication every 3 datapoints 6 5 1 2 5 * 1 5 3 0 6 % 0 5 7 0 9 3 (with reversed product) 5 1 4 * 5 9 6 9 # 6 6 0 8 6 0 3 3 8 6 3 1 * 6 1 6 # 6 9 6 6 9 7 3 2 2 2 * 8 9 % 8 9 1 7 9 8 0 4 * 8 8 8 9 2 # 8 6 1 7 3 9 3 9 0 7 1 9 6 6 5 * 9 7 0 # 0 5 0 5 7 0 9 1 2 6 4 9 * 6 7 9 6 % 4 9 8 5 1 4 3 5 0 * 8 8 1 5 # 0 5 2 5 4 3 8 3 2 0 2 0 * 9 9 8 9 # 0 8 7 7 7 0 2 6 2 2 7 4 * 9 5 % 0 7 3 1 1 3 ... connections between the simple case and the harder case can be an even more essential step.Exposure to the harder problem when presenting the simple case can be an efficient way to associate the two.Our findings suggest that showing solutions to sub-components of a complex issue can be a more effective instructional approach than merely presenting elementary problems.Indeed, constructing datasets around these subproblems might be more straightforward than building an array of incrementally challenging tasks, when aiming for a specific complexity level.For example, if we want to solve high-school math problems, we can easily collect a dataset consisting of high-school examination problems, but it can be hard to collect a series of problems with increasing difficulty that culminates at the high school level.Using primary school math as foundational problems might not guarantee a seamless transition in complexity or methodology.Instead, decomposing high school problems into their core sub-issues may offer a more coherent learning trajectory.As a result, subproblems of the hard problems can be an efficient simple case.</p>
<p>B Additional Details for Section 2</p>
<p>Setup For all experiments in this section, our training data set consists of 300k samples, where the number of digits each factor has is sampled independently from a uniform distribution on {1, ..., n}.We call such a dataset a n-maximum-digit dataset.We train a random initialized GPT2-small for 300 epochs with a learning rate 2e − 5. We test on 100 samples for each (m 1 , m 2 ) combination, where m 1 and m 2 are the number of digits in the two factors.We replace L with 50 random numbers.We generate L in the following way.For 2-to-3-digit addition, we generate each element of L i.i.d 2-digit numbers uniformly with probability 0.3, and i.i.d.3-digit numbers uniformly with probability 0.7.For 4-to-5-digit addition, we generate each element of L i.i.d 4-digit numbers uniformly with probability 0.5, and i.i.d.4-digit numbers uniformly with probability 0.5.We generate around 12200 dialogues on 2-to-3-digit addition and 1040 dialogues on 4-to-5-digit addition.In this way, our natural language data contains many more 2-to-3-digit numbers than 4-to-5-digit numbers.Our training data contains only one dialogue in one sentence.Although we specify in the prompt the format, around 3% of the data does not follow the prompt and output the dialogue in two lines or output a separator between two dialogues.For the 4-to-5-digit addition, there is an error rate of 0.2%.We did not correct these errors to reflect the noisy data collected in practice.</p>
<p>Addition Data We study two types of arithmetic data.2. Random Space For two numbers a and b, we first write down a n ...a 1 , followed by n s many random spaces.Then, we write down b m ...b 1 , followed n ′ s many random spaces.Finally, we write down the sum s l ...s 1 .n s and n ′ s are uniform numbers from {1, ..., 5}.</p>
<p>For each type, generate 120k samples.</p>
<p>Setup We compare the performance of the models trained on purely dialogue data and those trained the mixture of dialogue and addition data.To test the model in the dialogue context, we prompt the model using a simple student question, "Student: Hi, what is the sum of #a and #b?", where we replace #a and #b with the randomly generated numbers.To test the model in the pure arithmetic context we prompt the model using "#a + #b" or "#a #b" depending on the data format.We check the accuracy of the teacher's response on 100 samples for each number of digits.We compare GPT2-small models with positional embedding, without positional embedding and with random embedding (see Section 3.2 for definition).The models with random embedding do not have positional embedding.For all the runs, we train from a randomly initialized model for 100 epochs with a learning rate 2e-5.</p>
<p>),Cobbe et al. (2021) and Lightman et al. (2023) used verifier and feedback.Zhou et al. (2022) used advanced prompting technique.</p>
<p>( a )
a
Test accuracy on regular data.(b) Test accuracy on repetitive data.</p>
<p>Figure 1 :
1
Figure1: Comparison of pretrained model and trained from the scratch model with and without absolute positional embedding on 100 regular testing samples and repetitive samples.We use pretrained and random initialized GPT2small with/without the positional embedding and fine-tune/train for 10 epochs with a learning rate 2e-5.</p>
<p>( a )
a
Test accuracy on repetitive data in the digit reversion task for models trained from randomly initialized weights.(b)Test accuracy on addition with "Basic" step task for models trained from randomly initialized weights.</p>
<p>Figure 2 :
2
Figure 2: Comparison of trained from scratch model with and without hash embedding on 100 regular testing samples and repetitive samples.We use random initialized GPT2-small (124M) without the positional embedding and train for 25 epochs with a learning rate 1e-5.2</p>
<p>1.</p>
<p>Basic For two numbers a and b with n and m digits, a n ...a 1 and b m ...b 1 .We write down a n ...a 1 + b m ...b 1 = s l s l−1 ...s 1 , where s l ...s 1 is the sum of a and b.</p>
<p>Table 3 :
3
Examples of the data format for multiplication.
(a) Add Padding(b) Reverse Product(c) Reverse Product + Add Padding# Digits12345# Digits12345# Digits1234511.001.001.001.000.9911.001.001.001.001.0011.001.001.001.001.0021.000.990.990.940.8821.001.001.001.001.0021.001.001.001.001.0031.000.990.860.820.7531.001.001.000.970.2831.001.001.001.001.0040.990.960.790.730.6841.001.000.970.280.0241.001.001.001.001.0051.000.900.720.620.5951.000.990.290.060.0251.001.001.001.001.00</p>
<p>Table 5 :
5
Testing accuracy for 15-maximum-digit with padding and reversed product.</p>
<h1>Digits12345678910111213141511.001.001.001.001.001.001.001.001.001.001.001.001.001.001.0021.001.001.001.001.001.001.001.001.001.001.001.001.001.001.0031.001.001.001.001.001.001.001.001.001.001.001.001.001.001.0041.001.001.001.001.001.001.001.001.001.001.001.001.001.001.0051.001.001.001.001.001.001.001.001.001.001.001.001.001.001.0061.001.001.001.001.001.001.001.001.001.001.001.001.001.001.0071.001.001.001.001.001.001.001.001.001.001.001.001.001.001.0081.001.001.001.001.001.001.001.001.001.001.001.001.001.001.0091.001.001.001.001.001.001.001.001.001.001.001.001.001.001.00101.001.001.001.001.001.001.001.000.991.001.001.001.001.000.99111.001.001.001.001.001.001.001.001.001.001.001.001.001.000.99121.001.001.001.001.001.001.001.000.991.001.001.001.000.990.99131.001.001.001.001.001.001.001.001.001.000.990.990.950.950.95141.001.001.001.001.001.001.001.001.001.000.991.000.980.950.93151.001.001.001.001.001.001.001.001.001.000.960.980.950.980.84</h1>
<p>Table 7 :
7
Examples of the data format for adding 239 and 821.</p>
<p>Table 9a
9a
Excuse me, can you help me with something?I need to add two numbers, 842 and 62. Teacher: Of course, let me do the calculation for you.The answer is 904.Student: Good morning!Can you help me with a math problem?I need to find the sum of 324 and 402.Teacher: Good morning!Sure
Data FormatExamplesDialogue Data (Dia)Student:</p>
<p>Table 8 :
8
Examples of the natural language and arithmetic data used.Testing accuracy in the dialogue context for models trained for 100 epochs.Testing accuracy in the dialogue context for models trained for 50 epochs.Testing accuracy in the pure addition context for models trained for 100 epochs.Testing accuracy in the pure addition context for models trained for 50 epochs
DiaDia+BasicDia+Random Space#Digit234523452345PosEmbed0.940.920.610.131.00.960.680.121.00.990.910.82NoPosEmbed0.90.970.670.110.991.00.990.990.960.991.00.99RandEmbed0.90.850.570.071.01.00.970.861.01.00.960.91(a) DiaDia+BasicDia+Random Space#Digit234523452345PosEmbed0.970.960.620.140.990.940.640.070.990.980.930.77NoPosEmbed0.830.960.470.081.01.00.980.61.01.00.990.76RandEmbed0.770.690.130.01.00.980.910.360.990.990.890.83(b) Dia+BasicDia+Random Space#Digit23452345PosEmbed0.991.00.980.981.01.01.01.0NoPosEmbed0.20.080.040.010.950.960.960.27RandEmbed0.991.01.01.01.01.01.01.0(c) Dia+BasicDia+Random Space#Digit23452345PosEmbed0.991.01.00.991.01.01.01.0NoPosEmbed0.910.770.40.220.981.00.980.78RandEmbed0.991.01.00.980.981.00.990.97(d)</p>
<p>Table 10 :
10
). Gpt can solve mathematical problems without a calculator.arXiv preprint arXiv:2309.03241.Testing accuracies for models trained on dataset with n × 1-multiplication every 3 datapoints and first-step multiplication every 3 datapoints.
Zhang, Y., Backurs, A., Bubeck, S., Eldan, R., Gunasekar, S., and Wagner, T. (2022). Unveiling transformerswith lego: a synthetic reasoning task. arXiv preprint arXiv:2206.04301.Zhou, H., Nova, A., Larochelle, H., Courville, A., Neyshabur, B., and Sedghi, H. (2022). Teaching algorithmicreasoning via in-context learning. arXiv preprint arXiv:2211.09066.</p>
<p>Table 12 :
12
Examples of datasets with n × 1 multiplication every 3 datapoints and first-step multiplication every 3 datapoints.</p>
<p>the teacher answers it in different ways.Only show the dialogues one by one.Do not show any numbering or anything else.Only use numbers in the list L.</p>
<h1>Digits123456789101112131415161718192011.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.0021.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.001.000.4631.001.001.001.001.001.001.001.000.991.001.001.000.991.001.001.001.001.000.720.03</h1>
<p>In this paper, for every dataset used, a space is inserted before each digit. This ensures the tokenizer tokenizes each digit as an individual token.
The accuracies of "No PosEmbed" differ slightly from the corresponding accuracies in Figure1because the accuracies are measured from different runs with different learning rates and numbers of epochs.
For dataset with 1-digit weight = α, we generate the dataset in a way such that for every mutliplicaiton, we first generate the number of digits for the two factors independently sampled from {1, ..., 10} with weight {α, 1, 1, ..., 1}.operation and subsequently correlate this operation to the first phase of the * operation.Remarkably, the model adeptly forges this connection, resulting in enhanced performance.This shows that while including simple cases is crucial for the model to solve the hard problem, fostering, where a i represents the i-th digit of the first addend a, b i represents the i-th digit of the second addend b, c i represents the carry from the previous digit, and s i represents the sum of a i , b i , and c i .Finally, we write down the sum of the two numbers.2. Random Space: We introduce random spaces between any two characters in the sentence in the form of "Basic".For each position, we generate a random space with a probability of 0.3.3. Recursive Scratchpad: We modify the basic format so that before writing down the digit-wise sum a i + b i + c i = s i , we first record the remaining digits of a and b that haven't been included in their digit-wise sums in the reversed order, denoted as a i ...a n and b i ...b n .Then, we write down the digit-wise sum for digit i, followed by the digit-wise sums obtained so far, s i ...s 2 s 1 .In addition, we reverse the order of the digits in the two addends.Data Generation For the training data, we first choose the number of digits for the two addends, denoted as n, uniformly random from {2, ..., 10}.Then, given n, we generate the two addends independently using a uniform distribution on [0, 1, ..., 10 n − 1].Then, for each pair of addends, we generate the complete scratchpad steps according to the chosen data format.We generate a total of 120k samples independently following this process.For the testing data, for each number of digits n, we sample two addends uniformly random from [10 n−1 , ..., 10 n − 1] independently.Setup We fine-tune pretrained GPT2-small(124M) for five epochs with learning rate 2e − 5 on data in the three types of data formats in Table7.The testing set consists of 100 n-digit plus n digit additions with n ∈ 9, 10, ...13.C.2 Additional Details for Section 3.2Data Generation For the training data, we first choose the number of digits, denoted as n, uniformly random from {2, ..., 10}.Then, given n, we generate number independently using from uniform distribution on [0, 1, ..., 10 n − 1].We generate a total of 120k samples independently following this process.We consider two types of testing data, to generate the regular testing data, for each digit i, we sample the number uniformly random from [10 n−1 , ..., 10 n − 1] independently.To generate the testing data with repetitive digits, for each digit i, we sample a digit from [0, 1, ..., 9] and repeat the sampled digit i times to form a number with i digits.D Additional Details for Section 4Formally, we have the following two types of data.4 1.00 1.00 1.00 0.81 0.12 0.03 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 5 1.00 1.00 1.00 0.05 0.02 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 6 1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 7 1.00 1.00 0.99 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 8 1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 9 1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 10 1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 11 1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 12 1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 13 1.00 1.00 0.98 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 14 1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 15 1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  Data Format Failure Cases Basic (11 digits) 9 0 8 9 4 0 3 0 2 8 7 + 6 7 9 2 4 1 6 0 2 8 1 : 0 + 7 + 1 = 8 , 0 + 2 + 8 = 1 0 , 1 + 0 + 0 = 1 , 0 + 3 + 6 = 9 , 0 + 0 + 1 = 1 , 0 + 4 + 4 = 8 , 0 + 9 + 2 = 1 1 , 1 + 8 + 9 = 1 8 , 1 + 0 + 7 = 8 , 0 + 9 + 6 = 1 5 , 1 + 0 + 0 = 1, : 1 5 8 8 1 8 1 9 1 0 8 Basic (11 digits) 9 7 4 0 4 8 4 9 0 7 0 + 6 3 2 4 3 4 8 2 9 9 7 : 0 + 0 + 7 = 7 , 0 + 0 + 9 = 9 , 0 + 9 + 2 = 1 1 , 1 + 4 + 8 = 1 3 , 1 + 8 + 4 = 1 3, 1 + 4 + 3 = 8 , 0 + 0 + 4 = 4 , 0 + 4 + 2 = 6 , 0 + 7 + 3 = 1 0 , 1 + 9 + 6 = 1 6 , 1 + 0 + 0 = 1, : 1 6 0 6 4 8 3 3 1 9 7 Random Space 2 3 6 6 7 6 8 3 6 0 8 9 + 1 7 0 7 1 2 0 5 9 6 3 0 : 0 + 9 + 0 = 9, (12 digits) 0 + ... (Empty spaces) Recursive Scratchpad 7 3 7 8 0 5 5 5 9 0 1 8 9 + 8 7 8 1 7 8 2 4 6 5 9 7 9 : 0 + 7 + 8 = 1 5, = 5, (13 digits) 3 7 8 0 5 5 5 9 0 1 8 9 + 7 8 1 7 8 2 4 6 5 9 7 9 : 1 + 3 + 7 = 1 1, = 1 5, 7 8 0 5 5 5 9 0 1 8 9 + 8 1 7 8 2 4 6 5 9 7 9 : 1 + 7 + 8 = 1 6, = 6 1 5, 8 0 5 5 5 9 0 1 8 9 + 1 7 8 2 4 6 5 9 7 9 : 1 + 8 + 1 = 1 0, = 0 6 1 5, 0 5 5 5 9 0 1 8 9 + 7 8 2 4 6 5 9 7 9 : 1 + 0 + 7 = 8, = 8 0 6 1 5, 5 5 5 9 0 1 8 9 + 8 2 4 6 5 9 7 9 : 0 + 5 + 8 = 1 3, = 3 8 0 6 1 5, 5 5 9 0 1 8 9 + 2 4 6 5 9 7 9 : 1 + 5 + 2 = 8, = 8 3 8 0 6 1 5, 5 9 0 1 8 9 + 4 6 5 9 7 9 : 0 + 5 + 4 = 9, = 9 8 3 8 0 6 1 5, 9 0 1 8 9 + 6 5 9 7 9 : 0 + 9 + 6 = 1 5, = 5 9 8 3 8 0 6 1 5, 0 1 8 9 + 5 9 7 9 : 1 + 0 + 5 = 6, = 6 5 9 8 3 8 0 6 1 5, 1 8 9 + 9 7 9 : 0 + 1 + 9 = 1 0, = 0 6 5 9 8 3 8 0 6 1 5, 8 9 + 7 9 : 1 + 8 + 7 = 1 6, = 6 0 6 5 9 8 3 8 0 6 1 5, 9 + 9 : 1 + 9 + 9 = 1 9, = 9 6 5 9 8 3 8 0 6 1 5, = 1 9 6 5 9 8 3 8 0 6 1 5 Dialogue Data We use GPT3.5 to generate the natural language data.We use the following prompt: Create 20 dialogues between a student and a teacher where the student asks the teacher the sum of two numbers.Only use two numbers!An example of such dialogue is "Student: Hi, can you help me add two numbers, 34 and 432?Teacher: Sure.466 is the answer."The teacher and the student are very talkative, so the dialogue contains longer sentences than my example.Please make sure your number is random and that different dialogues have different styles.The student asks the question in different ways in each dialogue and
S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, P Lee, Y T Lee, Y Li, S Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>F Charton, arXiv:2112.01898Linear algebra with transformers. 2021arXiv preprint</p>
<p>What is my math transformer doing?-three results on interpretability and generalization. F Charton, arXiv:2211.001702022arXiv preprint</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>R Csordás, K Irie, J Schmidhuber, arXiv:2110.07732The neural data router: Adaptive control flow in transformers improves systematic generalization. 2021arXiv preprint</p>
<p>N Dziri, X Lu, M Sclar, X L Li, L Jian, B Y Lin, P West, C Bhagavatula, R L Bras, J D Hwang, arXiv:2305.18654Faith and fate: Limits of transformers on compositionality. 2023arXiv preprint</p>
<p>How does gpt-2 compute greater-than?. M Hanna, O Liu, A Variengien, arXiv:2305.00586Interpreting mathematical abilities in a pre-trained language model. 2023arXiv preprint</p>
<p>S Jelassi, S Ascoli, C Domingo-Enrich, Y Wu, Y Li, F Charton, arXiv:2306.15400Length generalization in arithmetic transformers. 2023arXiv preprint</p>
<p>The impact of positional encoding on length generalization in transformers. A Kazemnejad, I Padhi, K N Ramamurthy, P Das, S Reddy, arXiv:2305.194662023arXiv preprint</p>
<p>S Kiyono, S Kobayashi, J Suzuki, K Inui, arXiv:2109.05644Shape: Shifted absolute position embedding for transformers. 2021arXiv preprint</p>
<p>Teaching arithmetic to small transformers. N Lee, K Sreenivasan, J D Lee, K Lee, D Papailiopoulos, arXiv:2307.033812023arXiv preprint</p>
<p>Systematic generalization and emergent structures in transformers trained on structured tasks. Y Li, J L Mcclelland, arXiv:2210.004002022arXiv preprint</p>
<p>H Lightman, V Kosaraju, Y Burda, H Edwards, B Baker, T Lee, J Leike, J Schulman, I Sutskever, K Cobbe, arXiv:2305.20050Let's verify step by step. 2023arXiv preprint</p>
<p>Investigating the limitations of transformers with simple arithmetic tasks. R Nogueira, Z Jiang, J Lin, arXiv:2102.130192021arXiv preprint</p>
<p>Openai, arXiv:2309.05463Gpt-4 technical report. 2023arXiv preprint</p>
<p>Train short, test long: Attention with linear biases enables input length extrapolation. O Press, N A Smith, M Lewis, arXiv:2108.124092021arXiv preprint</p>
<p>Limitations of language models in arithmetic and symbolic induction. J Qian, H Wang, Z Li, S Li, X Yan, arXiv:2208.050512022arXiv preprint</p>
<p>A Ruoss, G Delétang, T Genewein, J Grau-Moya, R Csordás, M Bennani, S Legg, J Veness, arXiv:2305.16843Randomized positional encodings boost length generalization of transformers. 2023arXiv preprint</p>
<p>Roformer: Enhanced transformer with rotary position embedding. J Su, Y Lu, S Pan, A Murtadha, B Wen, Y Liu, arXiv:2104.098642021arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
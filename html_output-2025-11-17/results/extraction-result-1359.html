<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1359 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1359</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1359</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-6fcfc9094813ad45f03d4138ef1b90dc6d76d946</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6fcfc9094813ad45f03d4138ef1b90dc6d76d946" target="_blank">Learning Transferable Graph Exploration</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> Experimental results demonstrate that the proposed `learning to explore' framework is extremely effective for exploration of spatial maps and when applied on the challenging problems of coverage-guided software-testing of domain-specific programs and real-world mobile applications, it outperforms methods that have been hand-engineered by human experts.</p>
                <p><strong>Paper Abstract:</strong> This paper considers the problem of efficient exploration of unseen environments, a key challenge in AI. We propose a `learning to explore' framework where we learn a policy from a distribution of environments. At test time, presented with an unseen environment from the same distribution, the policy aims to generalize the exploration strategy to visit the maximum number of unique states in a limited number of steps. We particularly focus on environments with graph-structured state-spaces that are encountered in many important real-world applications like software testing and map building. We formulate this task as a reinforcement learning problem where the `exploration' agent is rewarded for transitioning to previously unseen environment states and employ a graph-structured memory to encode the agent's past trajectory. Experimental results demonstrate that our approach is extremely effective for exploration of spatial maps; and when applied on the challenging problems of coverage-guided software-testing of domain-specific programs and real-world mobile applications, it outperforms methods that have been hand-engineered by human experts.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1359.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1359.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>2D Maze (synthetic)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Synthetic 2D Maze Exploration (6x6 grid)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A grid-based spatial navigation environment where an agent observes only the visited portion plus 1-hop vision and must maximize unique-location coverage within a fixed step budget (T=36). Used to evaluate graph-structured exploration and history encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic 2D Maze (6x6)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Spatial navigation in a 2D maze (grid) with walls; agent sees explored subgraph and 1-hop vision from current location; objective is to visit as many distinct grid cells as possible within a step budget.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Grid graph with walls (sparse local connectivity, degree up to 4 where unobstructed); observed graph grows as agent explores (partially observed environment).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>6x6 grid = 36 grid locations (nodes); agent budget T = 36 steps.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GMetaExp (Graph Meta-Exploration); baselines: Random, RandDFS</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Policy uses a GGNN (gated graph neural network) to encode the observed graph plus a run-time coverage bit per node, attention-based pooling for graph readout, an auto-regressive LSTM aggregator over per-step graph encodings as external graph-structured memory, and is trained with an advantage actor-critic algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Fraction of unique nodes (grid cells) covered (coverage rate), normalized by |V|.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>GMetaExp: 72% average coverage (Table 2). Baselines: Random 33%, RandDFS 54%. Ablations: Graph + Autoregressive ~71.6%–73.4% depending on GNN depth (Tables 5/6).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Memory-based, graph-aware policy (graph-structured external memory + autoregressive history encoding) that leverages multi-hop GNN message passing and attention pooling.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Explicitly reported: using full graph structure (nodes+edges) outperforms node-only or set/pooling representations; autoregressive history aggregation improves coverage (~5% improvement over single-step observation); increasing GNN receptive field (more layers) improves performance; graph-conditioned policies handle non-stationarity (avoid repeatedly visiting already-covered nodes) better than reactive/tabular policies.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Ablations across representation/topology-receptive-field: 'Node' (current node only) yields ~33% coverage; 'Pool' (unordered set of visited nodes) ~41% in a weak setting; full 'Graph' conditioning with autoregressive history achieves ~71.6%–73.4% (Tables 5/6). Increasing GNN depth (larger receptive field) improves performance.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies that incorporate explicit graph memory and an auto-regressive history encoder (LSTM over per-step graph encodings) perform best; purely reactive policies or tabular Q-learning using only current node fail due to non-stationarity. Encoding the exploration history as an evolving graph memory helps remember visited nodes and reduces redundant revisits (backtracking mechanisms learned implicitly).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Transferable Graph Exploration', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1359.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1359.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Karel program graphs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Karel DSL Program Branch Graphs (map-layout input generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Environment is a fixed program-structure graph (nodes are branches/conditions, edges are syntactic/semantic relations). The agent proposes 2D map-layout inputs (actions) to maximize branch coverage when executing the program.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Karel program branch graph</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A program-execution graph derived from Karel DSL programs: nodes represent code branches/conditions, edges represent syntactic/semantic control-flow or dependency relations; agent action space is large (2D map layouts) used as program inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Control-flow / semantic graph of program branches (general directed graph determined by program structure); connectivity depends on program (can include condition splits, loops).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GMetaExp; baselines: Random, human-engineered heuristics, AFL/Neuzz fuzzers, symbolic execution (Z3)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>GGNN-based encoder for the program graph (GnnEnc) optionally conditioned on run-time coverage bits c_t; RNN (sequence decoder) used to generate structured inputs (2D layouts); history encoded autoregressively; trained with advantage actor-critic.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Fraction of unique program branches (branch coverage) or triggered regular-expression sub-expressions (RobustFill); joint coverage across generated inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Karel: GMetaExp average coverage ~0.76 with one input on test programs (reported as average score); outperforms random and human heuristics in Karel. Symbolic solver (Z3) achieved ~0.837 average score over solved cases but is slower and times out on hard nested-loop cases.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Program-aware, graph-conditioned policies (GnnEnc) that incorporate run-time coverage signals and autoregressive history; when program is complex GnnEnc outperforms bag-of-words or BiLSTM encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>GnnEnc (graph-aware encoding of program structure) yields better coverage especially for programs with complex control flow and small input budgets; run-time coverage signal c_t significantly helps in RobustFill; symbolic methods struggle on graphs with many nested loops/paths (combinatorial explosion), while learned graph-conditioned policies find good trade-offs between computation and coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Across DSLs: RobustFill (simpler DSL) shows similar performance across encoders; Karel (more complex control flow) benefits strongly from GNN-based program encodings. Unconditioned policies (UnCond) can still achieve reasonable coverage, but program-aware GnnEnc performs best for complex programs.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policy architectures that condition on a GNN encoding of the program and on run-time coverage (c_t) perform better; LSTM-based autoregressive aggregation over history helps avoid redundant proposals; symbolic (planning) approaches can be optimal but are computationally expensive and brittle on deep nested control-flow.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Transferable Graph Exploration', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1359.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1359.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>App screen graphs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mobile App Screen Transition Graphs (synthetic ER & real apps)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Screen-transition navigation environments built from recorded app interaction logs: nodes are UI screens, edges are user-triggered transitions (clicks, scrolls, search). Used to evaluate exploration under limited interaction budgets (T=15).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>App screen transition graph (synthetic ER and real-world Android apps)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Graph where nodes represent app screens and edges represent observed transitions between screens (actions like click/scroll/search). Two datasets: synthetic Erdős–Rényi graphs (15–20 nodes, p=0.1) and collected real apps (<=20 distinct screens).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Synthetic ER graphs: sparse (N=15–20, edge prob 0.1). Real app graphs: variable but small (<=20 nodes), structure derived from recorded transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Synthetic: 15–20 nodes (ER graphs, p=0.1). Real apps: up to 20 distinct screens per app. Agent budget T = 15 interactions per episode.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GMetaExp; baselines: Random, RandDFS, tabular Q-learning</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>GGNN encoder for observed graph memory plus run-time coverage, attention-based readout, LSTM aggregator for history; policy outputs finite set of user-interaction actions via an MLP decoder for small action sets; trained with advantage actor-critic and deployed zero-shot or fine-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Number / fraction of distinct screens (nodes) visited (coverage) within the interaction budget.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Fine-tuning: ER — Q-learning 0.60 vs GMetaExp 0.68; App (real) — Q-learning 0.58 vs GMetaExp 0.61. Generalization (zero-shot): ER — RandDFS 0.52 vs GMetaExp 0.65; App — RandDFS 0.54 vs GMetaExp 0.58 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Pretrained graph-aware policy that uses graph-structured memory and history encoding; generalizes zero-shot and benefits from fine-tuning on specific graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>GMetaExp outperforms tabular Q-learning and RandDFS in both synthetic sparse ER graphs and real app graphs; tabular Q-learning fails to generalize because MDP is non-stationary when state is only current node ID. Pretraining on graph distributions improves sample efficiency and final coverage; sparse ER connectivity still allows generalization when using graph encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Comparison between synthetic ER graphs and real apps shows GMetaExp generalizes well across both; synthetic ER experiments show larger relative gains over baselines (e.g., +13% vs RandDFS in generalization). Pretrained models converge faster and often surpass models trained from scratch.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Graph-structured memory and history encoding (autoregressive LSTM over per-step graph encodings) are important for handling non-stationarity and for zero-shot generalization to unseen app graphs; reactive/tabular agents that ignore history perform poorly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Transferable Graph Exploration', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning to navigate in complex environments <em>(Rating: 2)</em></li>
                <li>DOM-Q-NET: Grounded RL on Structured Language <em>(Rating: 2)</em></li>
                <li>Gated Graph Sequence Neural Networks <em>(Rating: 2)</em></li>
                <li>Learning combinatorial optimization algorithms over graphs <em>(Rating: 2)</em></li>
                <li>Information-based active SLAM via topological feature graphs <em>(Rating: 1)</em></li>
                <li>Active SLAM and exploration with particle filters using kullback-leibler divergence <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1359",
    "paper_id": "paper-6fcfc9094813ad45f03d4138ef1b90dc6d76d946",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [
        {
            "name_short": "2D Maze (synthetic)",
            "name_full": "Synthetic 2D Maze Exploration (6x6 grid)",
            "brief_description": "A grid-based spatial navigation environment where an agent observes only the visited portion plus 1-hop vision and must maximize unique-location coverage within a fixed step budget (T=36). Used to evaluate graph-structured exploration and history encoding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Synthetic 2D Maze (6x6)",
            "environment_description": "Spatial navigation in a 2D maze (grid) with walls; agent sees explored subgraph and 1-hop vision from current location; objective is to visit as many distinct grid cells as possible within a step budget.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": true,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "Grid graph with walls (sparse local connectivity, degree up to 4 where unobstructed); observed graph grows as agent explores (partially observed environment).",
            "environment_size": "6x6 grid = 36 grid locations (nodes); agent budget T = 36 steps.",
            "agent_name": "GMetaExp (Graph Meta-Exploration); baselines: Random, RandDFS",
            "agent_description": "Policy uses a GGNN (gated graph neural network) to encode the observed graph plus a run-time coverage bit per node, attention-based pooling for graph readout, an auto-regressive LSTM aggregator over per-step graph encodings as external graph-structured memory, and is trained with an advantage actor-critic algorithm.",
            "exploration_efficiency_metric": "Fraction of unique nodes (grid cells) covered (coverage rate), normalized by |V|.",
            "exploration_efficiency_value": "GMetaExp: 72% average coverage (Table 2). Baselines: Random 33%, RandDFS 54%. Ablations: Graph + Autoregressive ~71.6%–73.4% depending on GNN depth (Tables 5/6).",
            "success_rate": null,
            "optimal_policy_type": "Memory-based, graph-aware policy (graph-structured external memory + autoregressive history encoding) that leverages multi-hop GNN message passing and attention pooling.",
            "topology_performance_relationship": "Explicitly reported: using full graph structure (nodes+edges) outperforms node-only or set/pooling representations; autoregressive history aggregation improves coverage (~5% improvement over single-step observation); increasing GNN receptive field (more layers) improves performance; graph-conditioned policies handle non-stationarity (avoid repeatedly visiting already-covered nodes) better than reactive/tabular policies.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Ablations across representation/topology-receptive-field: 'Node' (current node only) yields ~33% coverage; 'Pool' (unordered set of visited nodes) ~41% in a weak setting; full 'Graph' conditioning with autoregressive history achieves ~71.6%–73.4% (Tables 5/6). Increasing GNN depth (larger receptive field) improves performance.",
            "policy_structure_findings": "Policies that incorporate explicit graph memory and an auto-regressive history encoder (LSTM over per-step graph encodings) perform best; purely reactive policies or tabular Q-learning using only current node fail due to non-stationarity. Encoding the exploration history as an evolving graph memory helps remember visited nodes and reduces redundant revisits (backtracking mechanisms learned implicitly).",
            "uuid": "e1359.0",
            "source_info": {
                "paper_title": "Learning Transferable Graph Exploration",
                "publication_date_yy_mm": "2019-10"
            }
        },
        {
            "name_short": "Karel program graphs",
            "name_full": "Karel DSL Program Branch Graphs (map-layout input generation)",
            "brief_description": "Environment is a fixed program-structure graph (nodes are branches/conditions, edges are syntactic/semantic relations). The agent proposes 2D map-layout inputs (actions) to maximize branch coverage when executing the program.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Karel program branch graph",
            "environment_description": "A program-execution graph derived from Karel DSL programs: nodes represent code branches/conditions, edges represent syntactic/semantic control-flow or dependency relations; agent action space is large (2D map layouts) used as program inputs.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "Control-flow / semantic graph of program branches (general directed graph determined by program structure); connectivity depends on program (can include condition splits, loops).",
            "environment_size": null,
            "agent_name": "GMetaExp; baselines: Random, human-engineered heuristics, AFL/Neuzz fuzzers, symbolic execution (Z3)",
            "agent_description": "GGNN-based encoder for the program graph (GnnEnc) optionally conditioned on run-time coverage bits c_t; RNN (sequence decoder) used to generate structured inputs (2D layouts); history encoded autoregressively; trained with advantage actor-critic.",
            "exploration_efficiency_metric": "Fraction of unique program branches (branch coverage) or triggered regular-expression sub-expressions (RobustFill); joint coverage across generated inputs.",
            "exploration_efficiency_value": "Karel: GMetaExp average coverage ~0.76 with one input on test programs (reported as average score); outperforms random and human heuristics in Karel. Symbolic solver (Z3) achieved ~0.837 average score over solved cases but is slower and times out on hard nested-loop cases.",
            "success_rate": null,
            "optimal_policy_type": "Program-aware, graph-conditioned policies (GnnEnc) that incorporate run-time coverage signals and autoregressive history; when program is complex GnnEnc outperforms bag-of-words or BiLSTM encoders.",
            "topology_performance_relationship": "GnnEnc (graph-aware encoding of program structure) yields better coverage especially for programs with complex control flow and small input budgets; run-time coverage signal c_t significantly helps in RobustFill; symbolic methods struggle on graphs with many nested loops/paths (combinatorial explosion), while learned graph-conditioned policies find good trade-offs between computation and coverage.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Across DSLs: RobustFill (simpler DSL) shows similar performance across encoders; Karel (more complex control flow) benefits strongly from GNN-based program encodings. Unconditioned policies (UnCond) can still achieve reasonable coverage, but program-aware GnnEnc performs best for complex programs.",
            "policy_structure_findings": "Policy architectures that condition on a GNN encoding of the program and on run-time coverage (c_t) perform better; LSTM-based autoregressive aggregation over history helps avoid redundant proposals; symbolic (planning) approaches can be optimal but are computationally expensive and brittle on deep nested control-flow.",
            "uuid": "e1359.1",
            "source_info": {
                "paper_title": "Learning Transferable Graph Exploration",
                "publication_date_yy_mm": "2019-10"
            }
        },
        {
            "name_short": "App screen graphs",
            "name_full": "Mobile App Screen Transition Graphs (synthetic ER & real apps)",
            "brief_description": "Screen-transition navigation environments built from recorded app interaction logs: nodes are UI screens, edges are user-triggered transitions (clicks, scrolls, search). Used to evaluate exploration under limited interaction budgets (T=15).",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "App screen transition graph (synthetic ER and real-world Android apps)",
            "environment_description": "Graph where nodes represent app screens and edges represent observed transitions between screens (actions like click/scroll/search). Two datasets: synthetic Erdős–Rényi graphs (15–20 nodes, p=0.1) and collected real apps (&lt;=20 distinct screens).",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "",
            "graph_connectivity": "Synthetic ER graphs: sparse (N=15–20, edge prob 0.1). Real app graphs: variable but small (&lt;=20 nodes), structure derived from recorded transitions.",
            "environment_size": "Synthetic: 15–20 nodes (ER graphs, p=0.1). Real apps: up to 20 distinct screens per app. Agent budget T = 15 interactions per episode.",
            "agent_name": "GMetaExp; baselines: Random, RandDFS, tabular Q-learning",
            "agent_description": "GGNN encoder for observed graph memory plus run-time coverage, attention-based readout, LSTM aggregator for history; policy outputs finite set of user-interaction actions via an MLP decoder for small action sets; trained with advantage actor-critic and deployed zero-shot or fine-tuned.",
            "exploration_efficiency_metric": "Number / fraction of distinct screens (nodes) visited (coverage) within the interaction budget.",
            "exploration_efficiency_value": "Fine-tuning: ER — Q-learning 0.60 vs GMetaExp 0.68; App (real) — Q-learning 0.58 vs GMetaExp 0.61. Generalization (zero-shot): ER — RandDFS 0.52 vs GMetaExp 0.65; App — RandDFS 0.54 vs GMetaExp 0.58 (Table 4).",
            "success_rate": null,
            "optimal_policy_type": "Pretrained graph-aware policy that uses graph-structured memory and history encoding; generalizes zero-shot and benefits from fine-tuning on specific graphs.",
            "topology_performance_relationship": "GMetaExp outperforms tabular Q-learning and RandDFS in both synthetic sparse ER graphs and real app graphs; tabular Q-learning fails to generalize because MDP is non-stationary when state is only current node ID. Pretraining on graph distributions improves sample efficiency and final coverage; sparse ER connectivity still allows generalization when using graph encodings.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Comparison between synthetic ER graphs and real apps shows GMetaExp generalizes well across both; synthetic ER experiments show larger relative gains over baselines (e.g., +13% vs RandDFS in generalization). Pretrained models converge faster and often surpass models trained from scratch.",
            "policy_structure_findings": "Graph-structured memory and history encoding (autoregressive LSTM over per-step graph encodings) are important for handling non-stationarity and for zero-shot generalization to unseen app graphs; reactive/tabular agents that ignore history perform poorly.",
            "uuid": "e1359.2",
            "source_info": {
                "paper_title": "Learning Transferable Graph Exploration",
                "publication_date_yy_mm": "2019-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning to navigate in complex environments",
            "rating": 2
        },
        {
            "paper_title": "DOM-Q-NET: Grounded RL on Structured Language",
            "rating": 2
        },
        {
            "paper_title": "Gated Graph Sequence Neural Networks",
            "rating": 2
        },
        {
            "paper_title": "Learning combinatorial optimization algorithms over graphs",
            "rating": 2
        },
        {
            "paper_title": "Information-based active SLAM via topological feature graphs",
            "rating": 1
        },
        {
            "paper_title": "Active SLAM and exploration with particle filters using kullback-leibler divergence",
            "rating": 1
        }
    ],
    "cost": 0.015391,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Learning Transferable Graph Exploration</h1>
<p>Hanjun Dai ${ }^{\dagger \dagger}$, Yujia $\mathbf{L i}^{\S}$, Chenglong Wang ${ }^{\ddagger}$, Rishabh Singh ${ }^{\dagger}$, Po-Sen Huang ${ }^{\S}$, Pushmeet Kohli ${ }^{\S}$<br>${ }^{\dagger}$ Georgia Institute of Technology<br>${ }^{\dagger}$ Google Brain, {hadai, rising}@google.com<br>${ }^{\ddagger}$ University of Washington, clwang@cs.washington.edu<br>${ }^{\S}$ DeepMind, {yujiali, posenhuang, pushmeet} @google.com</p>
<h4>Abstract</h4>
<p>This paper considers the problem of efficient exploration of unseen environments, a key challenge in AI. We propose a 'learning to explore' framework where we learn a policy from a distribution of environments. At test time, presented with an unseen environment from the same distribution, the policy aims to generalize the exploration strategy to visit the maximum number of unique states in a limited number of steps. We particularly focus on environments with graph-structured state-spaces that are encountered in many important real-world applications like software testing and map building. We formulate this task as a reinforcement learning problem where the 'exploration' agent is rewarded for transitioning to previously unseen environment states and employ a graph-structured memory to encode the agent's past trajectory. Experimental results demonstrate that our approach is extremely effective for exploration of spatial maps; and when applied on the challenging problems of coverage-guided software-testing of domain-specific programs and real-world mobile applications, it outperforms methods that have been hand-engineered by human experts.</p>
<h2>1 Introduction</h2>
<p>Exploration is a fundamental problem in AI; appearing in the context of reinforcement learning as a surrogate for the underlying target task $[1,2,3]$ or to balance exploration and exploitation [4]. In this paper, we consider a coverage variant of the exploration problem where given a (possibly unknown) environment, the goal is to reach as many distinct states as possible, within a given interaction budget.
The above-mentioned state-space coverage exploration problem appears in many important real-world applications like software testing and map building which we consider in this paper. The goal of software testing is to find as many potential bugs as possible with carefully designed or generated test inputs. To quantify the effectiveness of program exploration, program coverage (e.g. number of branches of code triggered by the inputs) is typically used as a surrogate objective [5]. One popular automated testing technique is fuzzing, which tries to maximize code coverage via randomly generated inputs [6]. In active map building, a robot needs to construct the map for an unknown environment while also keeping track of its locations [7]. The more locations one can visit, the better the map reconstruction could be. Most of these problems have limited budget (e.g. limited time or simulation trials), thus having a good exploration strategy is important.
A crucial challenge for these problems is of generalization to unseen environments. Take software testing as an example, in most traditional fuzzing methods, the fuzzing procedure will start from scratch for a new program, where the knowledge about the previously tested programs is not utilized. Different programs may share common design patterns and semantics, which could be exploited during exploration. Motivated by this problem, this paper proposes a 'learning to explore' framework</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>where we learn a policy from a distribution of environments with the aim of achieving transferable exploration efficiency. At test time, presented with an unseen environment from the same distribution, the policy aims to generalize the exploration strategy to visit the maximum number of unique states in a limited number of steps.</p>
<p>We formulate the state-space coverage problem using Reinforcement Learning (RL). The reward mechanism of the corresponding Markov Decision Process (MDP) is non-stationary as it changes drastically as the episode proceeds. In particular, visiting an unobserved state will be rewarded, but visiting it more than once is a waste of exploration budget. In other words, the environment is always expecting something new from the agent.</p>
<p>States in many such exploration problems are typically structured. For example, programs have syntactic or semantic structures [8], and efficiently covering the program statements require reasoning about the graph structure. The states in a generic RL environment may also form a graph with edges indicating reachability. To utilize the structure of these environments, we augment our RL agent with a graph neural network (GNNs) [9] to encode and represent the graph structured states. This model gives our agent the ability to generalize across problem instances (environments). We also use a graph structured external memory to capture the interaction history of the agent with the environment. Adding this information to the agent’s state, allows us to handle the non-stationarity challenge of the coverage exploration problem. The key contributions of this paper can be summarized as:</p>
<ul>
<li>We propose a new problem framework of exploration in graph structured spaces for several important applications.</li>
<li>We propose to use GNNs for modeling graph-structured states, and model the exploration history as a sequence of evolving graphs. The modeling of a sequence of evolving graphs in particular is as far as we know the first such attempt in the learning and program testing literature.</li>
<li>We successfully apply the graph exploration agent on a range of challenging problems, from exploring synthetic 2D mazes, to generating inputs for software testing, and finally testing realworld Android apps. Experimental evaluation shows that our approach is comparable or better in terms of exploration efficiency than strong baselines such as heuristics designed by human experts, and symbolic execution using the Z3 SMT (satisfiability modulo theories) solver [10].</li>
</ul>
<h2>2 Problem Formulation</h2>
<p>We consider two different exploration settings. The first setting concerns exploration in an unknown environment, where the agent observes a graph at each step, with each node corresponding to a visited unique environment state, and each edge corresponding to an experienced transition. In this setting, the graph grows in size during an episode, and the agent maximizes the speed of this growth.</p>
<p>The second setting is about exploration in a known but complex environment, and is motivated by program testing. In this setting, we have access to the program source code and thus also its graph structure, where the nodes in the graph correspond to the program branches and edges correspond to the syntactic and semantic relationship between branches. The challenge here is to reason about and understand the graph structure, and come up with the right actions to increase graph coverage. Each action corresponds to a test input which resides in a huge action space and has rich structures. Finding such valuable inputs is highly non-trivial in automated testing literature [5, 11, 12, 13, 14], because of challenges in modeling complex program semantics for precise logical reasoning.</p>
<p>We formalize both settings with the same formulation. At each step $t$, the agent observes a graph $G_{t-1}=\left(V_{t-1},E_{t-1}\right)$ and a coverage mask $c_{t-1}:V_{t-1} \mapsto{0,1}$, indicating which nodes have been covered in the exploration process so far. The agent generates an action $x_{t}$, the environment takes this action and returns a new graph $G_{t}=\left(V_{t}, E_{t}\right)$ with a new $c_{t}$. In the first setting above, the coverage mask $c_{t}$ is 1 for any node $v \in V_{t}$ as the graph only contains visited nodes. While in the second setting, the graph $G_{t}$ is constant from step to step, and the coverage mask $c_{t}(v)=1$ if $v$ is covered in the past by some actions and 0 otherwise. We set the initial observation for $t=0$ to be $c_{0}$ mapping any node to 0 , and in the first exploration setting $G_{0}$ to be an empty graph. The exploration process for a graph structured environment can be seen as a finite horizon Markov Decision Process (MDP), with the number of actions or steps $T$ being the budget for exploration.</p>
<p>Action The space for actions $x_{t}$ is problem specific. We used the letter $x$ instead of the more common letter $a$ to highlight that these actions are sometimes closer to the typical inputs to a neural network,</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of our meta exploration model for exploring a known but complicated graph structured environment. The GGNN [15] module captures the graph structures at each step, and the representations of each step are pooled together to form a representation of the exploration history.</p>
<p>which lives in an exponentially large space with rich structures, than to the more common fixed finite action spaces in typical RL environments. In particular, for testing programs, each action is a test input to the program, which can be text (sequences of characters) or images (2D array of characters).</p>
<p>Our task is to provide a sequence of $T$ actions $x_{1},x_{2}, \ldots, x_{T}$ to maximize an exploration objective. An obvious choice is the number of unique nodes (environment states) covered, i.e. $\sum_{v \in V_{T}} c_{T}(v)$. To handle different graph sizes during training, we further normalize this objective by the maximum possible size of the graph $|\mathcal{V}|^2$, which is the number of nodes in the underlying full graph (for the second exploration setting this is the same as $\left|V_T\right|$ ). We therefore get the objective in Eq. (1).</p>
<p>$$
\max_{\left{x_1, x_2, \ldots, x_T\right}} \sum_{v \in V_T} c_T(v)/|\mathcal{V}| \qquad \qquad \qquad \left. \quad \text{(1)} \qquad \qquad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad r_t = \sum_{v \in V_t} c_t(v)/|\mathcal{V}| - \sum_{v \in V_{t-1}} c_{t-1}(v)/|\mathcal{V}| \right), \quad (2)
$$</p>
<p><strong>Reward</strong> Given the above objective, we can define the per-step reward $r_t$ as in Eq. (2). It is easy to verify that $\sum_{t=1}^T r_t = \sum_{v \in V_T} c_T(v)/|\mathcal{V}|$, i.e., the cumulative reward of the MDP is the same as the objective in Eq. (1), as $\sum_{v \in V_T} c_0(v) = 0$. In this definition, the reward at time step $t$ is given to only the additional coverage introduced by the action $x_t$.</p>
<p><strong>State</strong> Instead of feeding in only the observation $(G_t, c_t)$ at each step to the agent, we use an <em>agent state</em> representation that contains the full interaction history in the episode $h_t = {(x_{\tau}, G_{\tau}, c_{\tau})}_{\tau=0}^{t-1}$, with $x_0 = \emptyset$. An agent policy maps each $h_t$ to an action $x_t$.</p>
<h1>3 Model</h1>
<p><strong>Overview of the Framework</strong> We aim to learn an action policy $\pi(x|h_t; \theta_t)$ at each time step $t$, which is parameterized by $\theta_t$. The objective of this specific MDP is formulated as: $\max_{[\theta_1, \ldots, \theta_T]} \sum_{t=1}^T \mathbb{E}_{x_t \sim \pi(x|h_t; \theta_t)} r_t$. Note that, we could share $\theta$ across time steps and learn a single policy $\pi(x|h_t, \theta)$ for all $t$, but in a finite horizon MDP we found it beneficial to use different $\theta$s for different time steps $t$.</p>
<p>In this paper, we are not only interested in efficient exploration for a single graph structured environment, but also the <em>generalization</em> and <em>transferrability</em> of learned exploration strategies that can be used without fine-tuning or retraining on unseen graphs. More concretely, let $\mathcal{G}$ denote each graph structured environment, we are interested in the following <em>meta</em>-reinforcement learning problem:</p>
<p>$$
\max_{[\theta_1, \ldots, \theta_T]} \mathbb{E}<em t="1">{\mathcal{G} \sim \mathcal{D}} \left( \sum</em>
$$}^T \mathbb{E}_{x_t^{(\mathcal{G})} \sim \pi(x|h_t^{(\mathcal{G})}; \theta_t)} r_t^{(\mathcal{G})}} \right) \tag{3</p>
<p>where $\mathcal{D}$ is the distribution of graph exploration problems we are interested in, and we share the parameters ${\theta_t}$ across graphs. After training, the learned policy can generalize to new graphs $\mathcal{G}' \sim \mathcal{D}$ from the same distribution, as the parameters are not tied to any particular $\mathcal{G}$.</p>
<p><sup>2</sup>When it is unknown, we can simply divide the reward by $T$ to normalize the total reward.</p>
<p>Graph Structured Agent and Exploration History The key to developing an agent that can learn to optimize Eq. (3) well is to have a model that can: 1) effectively exploit and represent the graph structure of the problem; and 2) encode and incorporate the history of exploration.</p>
<p>Fig. 1 shows an overview of the agent structure. Since the observations are graph structured in our formulation, we use a variant of the Graph Neural Network [9] to embed them into a continuous vector space. We implement a mapping $g:(G, c) \mapsto \mathbb{R}^{d}$ using a GNN. The mapping starts from initial node features $\mu_{v}^{(0)}$, which is problem specific, and can be e.g. the syntax information of a program branch, or app screen features. We also pad these features with one extra bit $c_{t}(v)$ to add in run-time coverage information. These representations are then updated through an iterative message passing process,</p>
<p>$$
\mu_{v}^{(l+1)}=f\left(\mu_{v}^{l},\left{\left(e_{u v}, \mu_{u}^{(l)}\right)\right}_{u \in \mathcal{N}(v)}\right)
$$</p>
<p>where $\mathcal{N}(v)$ is the neighbors of node $v, e_{u v}$ is the feature for edge $(u, v)$. This iterative process goes for $L$ iterations, aggregating information from $L$-hop neighborhoods. We use the parameterization of GGNN [15] to implement this update function $f($.$) . To get the graph representation g(G, c)$, we aggregate node embeddings $\mu_{v}^{(L)}$ from the last message passing step through an attention-based weighted-sum following [15], which performs better than a simple sum empirically.</p>
<p>Capturing the exploration history is particularly important. Like many other similar problems in RL, the exploration reward is only consistent when taking the history into account, as repeatedly visiting a 'good' state can only be rewarded once. Here we treat the $h_{t}$ as the evolving graph structured memory for the history. The representation of the full history is obtained by aggregating the per-step representations. Formally, we structure the representation function $F$ for history $h_{t}$ as $F\left(h_{t}\right)=F\left(\left[g_{x}\left(x_{0}\right), g\left(G_{0}, c_{0}\right)\right], \ldots,\left[g_{x}\left(x_{t-1}\right), g\left(G_{t-1}, c_{t-1}\right)\right]\right)$, where $g_{x}$ is an encoder for actions and $[\cdot]$ is the concatenation operator. The function $F$ can take a variety of forms, for example: 1) take the most recent element; or 2) auto-regressive aggregation across $t$ steps. We explored a few different settings for this, and obtained the best results with an auto-regressive $F$ (more in Appendix B.1).</p>
<p>The action policy $\pi\left(x_{t} \mid h_{t}\right)=\pi\left(x_{t} \mid F\left(h_{t}\right)\right)$ conditioned on an encoding of the history $h_{t}$ is parameterized by a domain specific neural network. In program testing where the actions are the generated program inputs, $\pi$ is an RNN sequence decoder; while in other problems where we have a small finite set of available actions, an MLP is used instead.</p>
<p>Learning To train this agent, we adopt the advantage actor critic algorithm [16], in the synchronized distributed setting. We use 32 distributed actors to collect on-policy trajectories in parallel, and aggregate them into a single machine to perform parameter update.</p>
<h1>4 Experiments</h1>
<p>In this section, we first illustrate the effectiveness of learning an exploration strategy on synthetic 2D mazes, and then study the problem of program testing (coverage guided fuzzing) through learning, where our model generates test cases for programs. Lastly we evaluate our algorithm on exploring both synthetic and real-world mobile Apps. We use GMetaExp (Graph Meta-Exploration) to denote our proposed method. More details about experiment setup and more results are included in Appendix B.</p>
<p>To train our agent, we adopt the advantage actor critic algorithm [16] in the synchronized distributed setting. For the synthetic experiments where we have the data generator, we generate the training graph environments on the fly. During inference, the agent is deployed on unseen graphs and not allowed to fine-tune its parameters (zero-shot generalization).</p>
<p>The baselines we compare against fall into the following categories:</p>
<ul>
<li>Random exploration: which randomly picks an action at each step;</li>
<li>Heuristics: including exploration heuristics like depth-first-search (DFS) or expert designed ones;</li>
<li>Exact Solver: we compare with Z3 when testing programs. This is a state-of-the-art SMT solver that can find provably optimal solutions given enough computation time.</li>
<li>Fuzzer: we compare with state-of-the-art fuzzing tools like AFL ${ }^{3}$ and Neuzz for program testing.</li>
<li>RL baselines: we also compare with RL models that use different amount of history information, or different history encoding models.</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>DSL</th>
<th># train</th>
<th># valid</th>
<th># test</th>
<th>Coverage</th>
</tr>
</thead>
<tbody>
<tr>
<td>RobustFill</td>
<td>1M</td>
<td>1,000</td>
<td>1,000</td>
<td>RegEx</td>
</tr>
<tr>
<td>Karel</td>
<td>212,524</td>
<td>490</td>
<td>467</td>
<td>Branches</td>
</tr>
</tbody>
</table>
<p>Table 1: DSL program dataset information.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Random</th>
<th>RandomDFS</th>
<th>GMetaExp</th>
</tr>
</thead>
<tbody>
<tr>
<td>Coverage</td>
<td>33%</td>
<td>54%</td>
<td>72%</td>
</tr>
</tbody>
</table>
<p>Table 2: Fraction of the mazes covered via different exploration methods.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Full Maze Random RandDFS GMetaExp
Figure 2: Maze exploration visualizations. Note the mazes are 6x6 but the walls also take up 1 pixel in the visualizations. The start position is marked red in the first column.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Test cases (2D grid world layouts) generated for Karel. Covered program branches are marked. The generated layout on the right by our model GMetaExp covers all statements in the program, while the program exits after the first statement using the layout on the left.</p>
<h1>4.1 Synthetic 2D Maze Exploration</h1>
<p>We start with a simple exploration task in synthetic 2D mazes. The goal is to visit as much of a maze as possible within a fixed number of steps. This is inspired by applications like map building, where an agent explores an environment and builds a map using e.g. SLAM [7]. In this setup, the agent only observes a small neighborhood around its current location, and does not know the 2D coordinates of the grids it has visited. At each time step, it has at most 4 actions (corresponding to the 4 directions).</p>
<p>More concretely, we use the following practical protocol to setup this task:</p>
<ul>
<li>Observation: the observed $G_{t}$ contains the locations (nodes) and the connectivities (edges) for the part of the maze the agent has traversed up to time $t$, plus 1-hop vision for the current location.</li>
<li>Reward: as defined in Eq. (2), a positive reward will only be received if a new location is visited;</li>
<li>Termination: when the agent has visited all the nodes, or has used up the exploration budget $T$.</li>
</ul>
<p>We train on random mazes of size $6 \times 6$, and test on 100 held-out mazes from the same distribution. The starting location is chosen randomly. We allow the agent to traverse for $T=36$ steps, and report the average fraction of the maze grid locations covered on the 100 held-out mazes.</p>
<p>Table 2 shows the quantitative performance of our method and random exploration baselines. As baselines we have uniform random policy (denoted by Random), and a depth-first search policy with random next-step selection (denoted by RandDFS) which allows the agent to backtrack and avoid blindly visiting a node in the current DFS stack. Note that for such DFS, the exploration order of actions is randomized instead of being fixed. Our learned exploration strategy performs significantly better. Fig. 2 shows some example maze exploration trajectories using different approaches.</p>
<p>We conducted an ablation study on the importance of utilizing graph structure, and different variants for modeling the history (see more details in Appendix B.4). We found that 1) exploiting the full graph structure performs significantly better than only using the current node (33\%) as the observation or treating all the nodes in a graph as a set ( $41 \%$ ) and ignoring the edges; 2) autoregressive aggregation over the history performs significantly better than only using the last step, our best performance is improved by $5 \%$ by modeling the full history compared to using a single step observation.</p>
<h1>4.2 Generating Inputs for Testing Domain Specific Programs</h1>
<p>In this section, we study the effectiveness of transferable exploration in the domain of program testing (a.k.a. coverage guided fuzzing). In this setup, our model proposes inputs (test cases) to the program being tested, with the goal of covering as many code branches as possible.
We test our algorithms on two datasets of programs written in two domain specific languages (DSLs), RobustFill [17] and Karel [18]. The RobustFill DSL is a regular expression based string manipulation language, with primitives like concatenation, substring, etc. The Karel DSL is an educational language used to define agents that programmatically explore a grid world. This language is more complex than RobustFill, as it contains conditional statements like if/then/else blocks, and loops like for/while. Table 1 summarizes the statistics of the two datasets. For Karel, we use the published benchmark dataset ${ }^{4}$ with the train/val/test splits; while for RobustFill, the training data was generated using a program synthesizer that is described in [17]. Note that for RobustFill, the agent actions are sequences of characters to generate an input string, while for Karel the actions are 2D arrays of characters to generate map layouts (see Fig. 3 for an example program and two generated map layouts), both generated and encoded by RNNs. Training these RNNs for the huge action spaces jointly with the rest of the model using RL is a challenging task in itself.
Main Results We compare against two baselines: 1) uniform random policy, and 2) specialized heuristic algorithms designed by a human expert. The objective we optimize is the fraction of unique code branches (for Karel) or regular expressions (for RobustFill) covered (triggered when executing the program on the generated inputs) by the test cases, which is a good indicator of the quality of the generated inputs. Modern fuzzing tools like AFL are also coverage-guided.
Fig. 4(a) summarizes the coverage performance of different methods. In RobustFill, our method approaches the human expert level performance, where both of them achieve above $90 \%$ coverage. Note that the dataset is generated in a way that the programs are sampled to get the most coverage on human generated inputs[17], so the evaluation is biased towards human expert. Nevertheless, GMetaExp still gets comparable performance, which is much better than the random fuzzing approach which is widely used in software testing [19]. For Karel programs, GMetaExp gets significantly better results than even the human expert, as it is much harder for a human expert to develop heuristic algorithms to generate inputs for programs with complex conditional and loop statements.
Comparing to fuzzers To compare with fuzzing approaches, we adapted AFL and Neuzz to our problems. We translated all Karel programs into C programs as afl-gcc is required in both AFL and Neuzz. We limit the vocabulary and fuzzing strategies to provide guidance in generating valid test cases with AFL. We run AFL for 10 mins for each program, and report coverage using the test cases with distinct execution traces. Note that to get $n$ distinct execution traces AFL or Neuzz may propose $N \gg n$ test cases. Neuzz is set up similarly, but with the output from AFL as initialization.</p>
<p>|  | AFL |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | Neuzz |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | </p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Testing DSL programs. (a) Program coverage results. The joint coverage of multiple inputs is reported. (b) Ablation study on different history encoding models.</p>
<table>
<thead>
<tr>
<th>Fine-tuning</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Data</td>
<td>Q-learning</td>
<td>GMetaExp</td>
</tr>
<tr>
<td>ER</td>
<td>0.60</td>
<td>$\mathbf{0 . 6 8}$</td>
</tr>
<tr>
<td>App</td>
<td>0.58</td>
<td>$\mathbf{0 . 6 1}$</td>
</tr>
<tr>
<td>Generalization</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Data</td>
<td>RandDFS</td>
<td>GMetaExp</td>
</tr>
<tr>
<td>ER</td>
<td>0.52</td>
<td>$\mathbf{0 . 6 5}$</td>
</tr>
<tr>
<td>App</td>
<td>0.54</td>
<td>$\mathbf{0 . 5 8}$</td>
</tr>
</tbody>
</table>
<p>Table 4: App testing results.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Comparing learning curve of GMetaExp with different initializations.</p>
<p>out of 467 test programs within the time budget, which takes about 4 hours in total. The average score for the solved 412 cases using a single input is 0.837 , which is roughly an 'upper bound' on the single input coverage (not guaranteed to be optimal as we restrict the maximum number of paths to check to 100 and the maximum number of expansions of while loops to 3 to make the solving time tractable). In contrast, GMetaExp gets 0.76 average score with one input and takes only seconds to run for all the test programs. While the symbolic execution approach achieves higher average coverage, it is slow and often fails to solve cases with highly nested loop structures (i.e., nested repeat or while loops). On the hard cases where the SMT solver failed (i.e., cannot find a solution after checking all top 100 potential paths), our approach still gets 0.698 coverage. This shows that the GMetaExp achieves a good balance between computation cost and accuracy.</p>
<p>We visualize the test inputs generated by GMetaExp for two example test programs in Fig. 3 and Fig. 6 in appendix. The covered regular expressions or branches are highlighted. We can observe that the randomly generated inputs can only cover a small fraction of the program. In contrast, our proposed input can trigger many more branches. Moreover, the program also performs interesting manipulations on our generated inputs after execution.</p>
<p>Effectiveness of program-aware inputs: When compared with randomly generated program inputs, our learned model does significantly better in coverage. Random generation, however, can trade off efficiency with speed, as generating a random input is very fast. We therefore evaluated random generation with a much higher sample budget, and found that with 10 inputs, random generation can reach a joint coverage (i.e., the union of the coverage of graph nodes/program branches using multiple generated test inputs) of 73\%, but the coverage maxed out to only 85\% even with 100 inputs (Fig 7 in appendix). This shows the usefulness of our learned model, and the generated program-aware inputs, as we get 93\% coverage with just one input.</p>
<p>Comparison of different conditioning models: We also study the effectiveness of different exploration history encoders. =The encoders we consider here are (1) UnCond, where the policy network knows nothing about the programs or the task, and blindly proposes generally 'good' test cases. Note that it still knows the past test cases it proposed through the autoregressive parameterization of $F\left(h_{t}\right)$ (2) EnvCond, where the policy network is blind to the program but takes the external reward obtained with previous actions into account when generating new actions. This is similar to meta RL [20, 21], where the agent learns an adaptive policy based on the historical interactions with the environment. (3) program-aware models, where the policy network conditions on an encoding of the program. We use BowEnc, BiLstmEnc, and GnnEnc to denote the bag of words encoder, bidirectional LSTM encoder, and graph neural network encoder, respectively.</p>
<p>Fig 4(b) shows the ablation results on different encoders. For RobustFill, since the DSL is relatively simpler, the models conditioned on the program get similar performance; for Karel, we observe that the GnnEnc gets best performance, especially when the exploration budget, i.e., the number of inputs is small. One interesting observation is that UnCond, which does not rely on the program, also achieves good performance. This shows that, one can find some universally good exploration strategies with RL for these datasets. This is also consistent with the software testing practice, where there are common strategies for testing corner cases, like empty strings, null pointers, etc.</p>
<h1>4.3 App Testing</h1>
<p>In this section, we study the exploration and testing problem for mobile apps. Since mobile apps can be very large and the source code is not available for commercial apps, measuring and modeling coverage at the code branch level is very expensive and often impossible. An alternative practice is to measure the number of distinct 'screens' that are covered by test user interactions [22]. Here each 'screen' packs a number of features and UI elements a user can interact with, and testing different interactions on different screens to explore different transitions between screens is a good way to discover bugs and crashes [22, 23].
In this section we explore the screen transition graph for each app with a fixed interaction budget $T=15$, in the explore unknown environment setting. At each step, the agent can choose from a finite set of user interaction actions like search query, click, scroll, etc. Features of a node may come from an encoding of the visual appearance of the screen, the layout or UI elements visible, or an encoding of the past test logs, e.g., in a continuous testing scenario. More details about the setup and results are included in Appendix B.7.
Datasets We scraped a set of apps from the Android app store, and collected 1,000 apps with at most 20 distinct screens as our dataset. We use 5\% of them for held-out evaluation. To avoid the expensive interaction with the Android app simulator during learning, we instead used random user inputs to test these apps offline and extracted a screen transition graph for each app. We then built a light-weight offline app simulator that transitions between screens based on the recorded graph. Interacting with this offline simulator is cheap.
In addition to the real world app dataset, we also created a dataset of synthetic apps to further test the capabilities of our approach. We collected randomly sampled Erdős-Rényi (denoted ER in the experiment results) graphs with 15-20 nodes and edge probability 0.1 , and used these graphs as the underlying screen transition graph for the synthetic apps. For training we generate random graphs on the fly, and we use 100 held-out graphs for testing the generalization performance.
Baselines Besides the RandDFS baselines defined in Sec 4.1, we also evaluate a tabular Q-learning baseline. This Q-learning baseline uses node ID as states and does not model the exploration history. This limitation makes Q-learning impossible to learn the optimal strategy, as the MDP is non-stationary when the state representation only contains the current node ID. Moreover, since this approach is tabular, it does not generalize to new graphs and cannot be used in the generalization setting. We train this baseline on each graph separately for a fixed number of iterations and report the best performance it can reach on those graphs.
Evaluation setup We evaluate our algorithms in two scenarios, namely fine-tuning and generalization. In the fine-tuning case, the agent is allowed to interact with the App simulator for as many episodes as needed, and we report the performance of the algorithms after they have been fine-tuned on the apps. Alternatively, this can be thought of as the 'train on some apps, and evaluate on the same set of apps' setting, which is standard for many RL tasks. For the generalization scenario, the agent is asked to get as much reward as possible within one single episode on apps not seen during training. We compare to the tabular Q-learning approach in the first scenario as it is stronger than random exploration; for the second scenario, since the tabular policy is not generalizable, random exploration is used as baseline instead.</p>
<h2>Results</h2>
<p>Table 4 summarizes the results for different approaches on different datasets. As we can see, with our modeling of the graph structure and exploration history and learning setup, GMEtaExp performs better in both fine-tuning and generalization experiments compared to Q-learning and random exploration baselines. Furthermore, our zero-shot generalization performance is even better than the fine-tuned performance of tabular Q-learning. This further shows the importance of embedding the structural history when proposing the user inputs for exploration.</p>
<p>We show the learning curves of our model for learning from scratch versus fine-tuning on the 100 test graphs for the synthetic app graphs in Fig 5. For fine-tuning, we initialize with the trained model, and perform reinforcement learning for each individual graph. For learning from scratch, we directly learn on each individual graph separately. We observe that (1) the generalization performance is quite effective in this case, where it achieves performance close to the fine tuned model; (2) learning from the pretrained model is beneficial; it converges faster and converges to a model with slightly better coverage than learning from scratch.</p>
<h1>5 Related work</h1>
<p>Balancing between exploration and exploitation is a fundamental topic in reinforcement learning. To tackle this challenge, many mechanisms have been designed, ranging from simple $\epsilon$-greedy, pseudo-count [1, 3], intrinsic motivation [2], diversity [24], to meta learning approaches that learns the algorithm itself [20, 21], or combining structural noise that address the multi-modality policy distribution [25]. In SLAM literature, the exploration problem is typically known as active SLAM with different uncertainty criteria [26] such as entropy/information based approach [27, 28]. Our work focuses purely on exploring distinct states in graph.
Exploration for Fuzzing: Fuzzing explores corner cases in a software, with coverage guided search [23] or learned proposal distributions [29]. To explore the program semantics with input examples, there have been heuristics designed by human expert [17], sampling from manually tuned distributions [30] or greedy approaches [31]. Some recent learning based fuzzing approaches like Learn\&amp;Fuzz [29] and DeepFuzz [32] build language models of inputs and sample from it to generate new inputs, but such a paradigm is not directly applicable for program conditional testing. Neuzz [33] builds a smooth surrogate function on top of AFL that allows gradient guided input generation. Rajpal et al. [34] learn a function to predict which bytes might lead to new coverage using supervised learning on previous fuzzing explorations. Different from these approaches that explore a specific task, we learn a transferable exploration strategy, which is encoded in the graph memory based agent that can be directly rolled out in new unseen environments.</p>
<p>Representation Learning over Structures: The representation of our external graph memory is built on recent advances in graph representation learning [35]. The graph neural network [9] and the variants have shown superior results in domains including program modeling [15, 8], semisupervised learning [36], bioinformatics and chemistry [37, 38, 39, 40]. In this paper, we adapt the parameterization from Li et al. [15], the graph sequence modeling [41], and also the attention based [42] read-out for the graph.
Optimization over Graphs: Existing papers have studied the path finding problems in graph. The DOM-Q-NET [43] navigates HTML page and finishes certain tasks, while Mirowski et al. [44] learns to handle complex visual sensory inputs. Our task is seeking for optimal traversal tour, which is essentially NP-hard. Our work is also closely related to the recent advances in combinatorial optimization over graph structured data. The graph neural network can be learned with one-bit [45] or full supervision [46, 47] and generalize to new combinatorial optimization problem instances. In the case that lacks supervision, the reinforcement learning are adapted [48]. Khalil et al. [49] uses finite horizon DQN to learn the action policy. Our work mainly differs in two ways: 1) the full structure of graph is not always observed and instead needs be explored by the agent; 2) we model the exploration history as a sequence of evolving graphs, rather than learning Q-function of a single graph.</p>
<h2>6 Conclusion</h2>
<p>In this paper, we study the problem of transferable graph exploration. We propose to use a sequence of graph structured external memory to encode the exploration history. By encoding the graph structure with GNN, we can also obtain transferable history memory representations. We demonstrate our method on domains including synthetic 2D maze exploration and real world program and app testing, and show comparable or better performance than human engineered methods. Future work includes scaling up the graph external memory to handle large software or code base.</p>
<h2>Acknowledgments</h2>
<p>We would like to thank Hengxiang Hu, Shu-Wei Cheng and other members in the team for providing data and engineering suggestions. We also want to thank Arthur Guez, Georg Ostrovski, Jonathan Uesato, Tejas Kulkarni, and anonymous reviewers for providing constructive feedbacks.</p>
<h1>References</h1>
<p>[1] Georg Ostrovski, Marc G Bellemare, Aaron van den Oord, and Rémi Munos. Count-based exploration with neural density models. arXiv preprint arXiv:1703.01310, 2017.
[2] Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In ICML, 2017.
[3] Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems, pages 1471-1479, 2016.
[4] Arthur Guez, Théophane Weber, Ioannis Antonoglou, Karen Simonyan, Oriol Vinyals, Daan Wierstra, Rémi Munos, and David Silver. Learning to search with mctsnets. arXiv preprint arXiv:1802.04697, 2018.
[5] Patrice Godefroid, Michael Y. Levin, and David A. Molnar. Automated whitebox fuzz testing. In Proceedings of the Network and Distributed System Security Symposium, NDSS 2008, San Diego, California, USA, 10th February - 13th February 2008, 2008.
[6] Barton P. Miller, Lars Fredriksen, and Bryan So. An empirical study of the reliability of UNIX utilities. Commun. ACM, 33(12):32-44, 1990.
[7] Hugh Durrant-Whyte and Tim Bailey. Simultaneous localization and mapping: part i. IEEE robotics \&amp; automation magazine, 13(2):99-110, 2006.
[8] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to represent programs with graphs. arXiv preprint arXiv:1711.00740, 2017.
[9] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80, 2009.
[10] Leonardo Mendonça de Moura and Nikolaj Bjørner. Z3: an efficient SMT solver. In Tools and Algorithms for the Construction and Analysis of Systems, 14th International Conference, TACAS 2008, Held as Part of the Joint European Conferences on Theory and Practice of Software, ETAPS 2008, Budapest, Hungary, March 29-April 6, 2008. Proceedings, pages 337-340, 2008. doi: 10.1007/978-3-540-78800-31_24. URL https://doi.org/10.1007/978-3-540-788003_24.
[11] Koushik Sen. DART: directed automated random testing. In Hardware and Software: Verification and Testing - 5th International Haifa Verification Conference, HVC 2009, Haifa, Israel, October 19-22, 2009, Revised Selected Papers, page 4, 2009. doi: 10.1007/978-3-642-19237-11_4. URL https://doi.org/10.1007/978-3-642-19237-1_4.
[12] Koushik Sen, Darko Marinov, and Gul Agha. CUTE: a concolic unit testing engine for C. In Proceedings of the 10th European Software Engineering Conference held jointly with 13th ACM SIGSOFT International Symposium on Foundations of Software Engineering, 2005, Lisbon, Portugal, September 5-9, 2005, pages 263-272, 2005. doi: 10.1145/1081706.1081750. URL https://doi.org/10.1145/1081706.1081750.
[13] Cristian Cadar and Koushik Sen. Symbolic execution for software testing: three decades later. Commun. ACM, 56(2):82-90, 2013. doi: 10.1145/2408776.2408795. URL https://doi.org/10.1145/2408776.2408795.
[14] Caroline Lemieux and Koushik Sen. Fairfuzz: a targeted mutation strategy for increasing greybox fuzz testing coverage. In Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering, ASE 2018, Montpellier, France, September 3-7, 2018, pages 475-485, 2018. doi: 10.1145/3238147.3238176. URL https://doi.org/10.1145/3238147.3238176.
[15] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493, 2015.</p>
<p>[16] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pages 1928-1937, 2016.
[17] Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and Pushmeet Kohli. Robustfill: Neural program learning under noisy i/o. arXiv preprint arXiv:1703.07469, 2017.
[18] Rudy Bunel, Matthew J. Hausknecht, Jacob Devlin, Rishabh Singh, and Pushmeet Kohli. Leveraging grammar and reinforcement learning for neural program synthesis. CoRR, abs/1805.04276, 2018.
[19] Michael Sutton, Adam Greene, and Pedram Amini. Fuzzing: brute force vulnerability discovery. Pearson Education, 2007.
[20] Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl2: Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.
[21] Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv preprint arXiv:1611.05763, 2016.
[22] Tanzirul Azim and Iulian Neamtiu. Targeted and depth-first exploration for systematic testing of android apps. In Acm Sigplan Notices, volume 48, pages 641-660. ACM, 2013.
[23] Ke Mao, Mark Harman, and Yue Jia. Sapienz: Multi-objective automated testing for android applications. In Proceedings of the 25th International Symposium on Software Testing and Analysis, pages 94-105. ACM, 2016.
[24] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without a reward function. arXiv preprint arXiv:1802.06070, 2018.
[25] Abhishek Gupta, Russell Mendonca, Yuxuan Liu, Pieter Abbeel, and Sergey Levine. Metareinforcement learning of structured exploration strategies. CoRR, abs/1802.07245, 2018.
[26] Henry Carrillo, Ian Reid, and José A Castellanos. On the comparison of uncertainty criteria for active slam. In Robotics and Automation (ICRA), 2012 IEEE International Conference on, pages 2080-2087. IEEE, 2012.
[27] Beipeng Mu, Matthew Giamou, Liam Paull, Ali-akbar Agha-mohammadi, John Leonard, and Jonathan How. Information-based active slam via topological feature graphs. In Decision and Control (CDC), 2016 IEEE 55th Conference on, pages 5583-5590. IEEE, 2016.
[28] Luca Carlone, Jingjing Du, Miguel Kaouk Ng, Basilio Bona, and Marina Indri. Active slam and exploration with particle filters using kullback-leibler divergence. Journal of Intelligent \&amp; Robotic Systems, 75(2):291-311, 2014.
[29] Patrice Godefroid, Hila Peleg, and Rishabh Singh. Learn\&amp;fuzz: Machine learning for input fuzzing. In Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering, pages 50-59. IEEE Press, 2017.
[30] Richard Shin, Neel Kant, Kavi Gupta, Christopher Bender, Brandon Trabucco, Rishabh Singh, and Dawn Song. Synthetic datasets for neural program synthesis. 2018.
[31] Yewen Pu, Zachery Miranda, Armando Solar-Lezama, and Leslie Kaelbling. Selecting representative examples for program synthesis. In International Conference on Machine Learning, pages $4158-4167,2018$.
[32] Xiao Liu, Xiaoting Li, Rupesh Prajapati, and Dinghao Wu. Deepfuzz: Automatic generation of syntax valid c programs for fuzz testing. 2019.
[33] Dongdong She, Kexin Pei, Dave Epstein, Junfeng Yang, Baishakhi Ray, and Suman Jana. Neuzz: Efficient fuzzing with neural program learning. arXiv preprint arXiv:1807.05620, 2018.</p>
<p>[34] Mohit Rajpal, William Blum, and Rishabh Singh. Not all bytes are equal: Neural byte sieve for fuzzing. CoRR, abs/1711.04596, 2017.
[35] Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018.
[36] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.
[37] Hanjun Dai, Bo Dai, and Le Song. Discriminative embeddings of latent variable models for structured data. In International Conference on Machine Learning, pages 2702-2711, 2016.
[38] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.
[39] Tao Lei, Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Deriving neural architectures from sequence and graph kernels. arXiv preprint arXiv:1705.09037, 2017.
[40] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, pages 1024-1034, 2017.
[41] Daniel D Johnson. Learning graphical state transitions. 2016.
[42] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.
[43] Sheng Jia, Jamie Kiros, and Jimmy Ba. Dom-q-net: Grounded rl on structured language. arXiv preprint arXiv:1902.07257, 2019.
[44] Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andrew J Ballard, Andrea Banino, Misha Denil, Ross Goroshin, Laurent Sifre, Koray Kavukcuoglu, et al. Learning to navigate in complex environments. arXiv preprint arXiv:1611.03673, 2016.
[45] Daniel Selsam, Matthew Lamm, Benedikt Bunz, Percy Liang, Leonardo de Moura, and David L Dill. Learning a sat solver from single-bit supervision. arXiv preprint arXiv:1802.03685, 2018.
[46] Alex Nowak, Soledad Villar, Afonso S Bandeira, and Joan Bruna. A note on learning algorithms for quadratic assignment with graph neural networks. arXiv preprint arXiv:1706.07450, 2017.
[47] Zhuwen Li, Qifeng Chen, and Vladlen Koltun. Combinatorial optimization with graph convolutional networks and guided tree search. In Advances in Neural Information Processing Systems, pages 537-546, 2018.
[48] Irwan Bello, Hieu Pham, Quoc V Le, Mohammad Norouzi, and Samy Bengio. Neural combinatorial optimization with reinforcement learning. arXiv preprint arXiv:1611.09940, 2016.
[49] Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial optimization algorithms over graphs. In Advances in Neural Information Processing Systems, pages 6348-6358, 2017.</p>
<h1>A Statement of the problem</h1>
<h2>A. 1 App covering is at least as hard as finding Hamiltonian Path</h2>
<p>Suppose we already know all the nodes (screens) in the app graph, then the app covering/exploration problem can be abstracted as: given a graph with $N$ nodes, what's the maximum number of nodes one can visit by traversing the graph with at most $T$ steps.</p>
<p>To show the NP-completeness, we first convert the optimality problem to the decision problem: given a graph with $N$ nodes and a number $M$, where there exists a traversal plan that can visit at least $M$ nodes within $T$ steps.</p>
<p>Hamiltonian Path $\rightarrow$ App Covering: given a Hamiltonian Path problem instance with $n$ nodes, we set $N=n, M=n$ and $T=n$ in App Covering, then solves the App Covering automatically solves Hamiltonian Path.</p>
<p>Note that the above argument is based on the assumption that we already know the entire graph. However our task is to explore the graph, in which we don't know the full graph yet. This makes the problem even harder.</p>
<h2>A. 2 Graph optimal traversal order</h2>
<p>If the graph is a tree, then finding the optimal traversal order reduces to dynamic programming. The best strategy is to find the longest path starting from the beginning node, and explore this path in the end. In this way, once all the nodes are explored, the longest path doesn't need the backtracking, thus saving the exploration budget. If it is a general graph, then it reduces to the problem mentioned in Appendix A.1. Then in both cases, the optimal graph traversal order is not trivial.</p>
<h2>B Experiment Details</h2>
<h2>B. 1 Model architectures</h2>
<p>To parameterize the Eq Eq. (4), we use the following realization:</p>
<p>$$
\begin{aligned}
m_{v}^{(l+1)}= &amp; \text { Aggregate }\left(\left{\operatorname{MLP}\left[\mu_{v}^{(l)}, \mu_{u}^{(l)}, k\right]\right}<em v="v">{u \in \mathcal{N}^{k}(v)}\right. \
&amp; k \in 1,2, \ldots, K) \
\mu</em>\right)
\end{aligned}
$$}^{(l+1)}= &amp; \operatorname{GRU}\left(\mu_{v}^{(l)}, m_{v}^{(l+1)</p>
<p>Here $\operatorname{MLP}(\cdot)$ is a fully connected network, and the aggregation method is tuned within {sum, mean, max}. Finally, following Li et al. [15], we use GRU as gating function for the node embedding update. At the beginning, $\mu_{v}^{(0)}$ is set to be the plain node feature, such as tokens in program, or screen features in app exploration.
To get the representation of entire graph memory, we can simply do the summation over all node embeddings, i.e., $g(G, c)=\sum_{v} \mu_{v}^{(L)}$ where $L$ is the number of message-passing steps used in GGNN. However, we found the attentive aggregation obtains best empirical performance:</p>
<p>$$
\begin{aligned}
\alpha_{v} &amp; =\frac{\exp \left(W^{\top} \mu_{v}\right)}{\sum_{u \in G} \exp \left(W^{\top} \mu_{u}\right)} \
g(G, c) &amp; =\sum_{v \in G} \alpha_{v} \mu_{v}^{(L)}
\end{aligned}
$$</p>
<p>To embed the history $F\left(h_{t}\right)$, the simplest way is to use the graph memory read-out $g\left(G_{t}, c_{t}\right)$, since in many cases the current status of the graph is sufficient to tell the history. A stronger parameterization is to use auto-regressive model, where $F\left(h_{t}\right)=\operatorname{LSTM}\left(F\left(h_{t-1}\right), g\left(G_{t}, c_{t}\right)\right)$ and let $F\left(h_{1}\right)=g\left(G_{0}, c_{0}\right)$.
In most experiments, we use $L=5$ for this graph memory read-out function.</p>
<h1>B. 2 Coverage metric for programs</h1>
<p>The RobustFill language defines programs as concatenation of regular-expression based substring operations. For example, consider the program Concat(e1,e2), where e1 $\equiv$ Substr((Num,3,End),(Proper,2,Start)) and e2 $\equiv$ Substr((',',1,End),(',,4,Start)). This program concatenates two input substrings: i) substring between $3^{\text {rd }}$ number and $2^{\text {nd }}$ propercase, and ii) substring between $1^{\text {st }} ;^{\prime}$ and $4^{\text {th }} ;^{\prime}$. In order to trigger these regular expressions, the input string should consists of at least 3 numbers, 2 propercase tokens, $1^{\text {st }} ;^{\prime}$, and $4^{\text {th }} ;$, tokens. Karel language, on the other hand, uses control statements such as if/else and while loops. To maximize the coverage, the inputs should be able to reach both True/False conditions of each control statement. In both RobustFill and Karel experiments, we measure the coverage as the fraction of statements reached. For Karel, we measure the branch coverage where we check if the inputs can reach both blocks of a conditional, whereas for RobustFill, we measure the number of regular expression based substring expression successfully triggering to generate a non-empty result (without an exception). The coverage is normalized to $[0,1]$, where 1 means the full coverage.</p>
<h2>B. 3 Details about baseline algorithms for program coverage</h2>
<p>Details about human expert design For RobustFill, the human expert generated the test inputs and programs in a different way. In brief, the test inputs are generated first that induces an overapproximation of possible program expressions, and then the programs are sampled in a way that almost every regular expression can be triggered. Ideally this procedure would get $100 \%$ coverage for the program, but it is challenging to handle the conflicts between different regular expression requirements and the resulting position indices when evaluated on concrete inputs while also maintaining a maximum output length constraint. Finally this method gets $96 \%$ coverage when 10 inputs are generated for each program. Note that for our proposed GMEtaExptechnique, we are asked to generate inputs based on the program, which is harder than the human expert's setting. Nevertheless, we still get $\sim 93 \%$ coverage, which is close to human's performance.
For Karel, it is much harder to design the heuristics manually because of the complexity of the DSL. We noticed that quite often the randomly generated inputs lead to runtime errors in a Karel program. These errors include infinite loops, or invalid actions in certain situations. The execution terminates once an error is encountered. Thus the heuristic method being used here is to guarantee the successful execution of the program.</p>
<p>Finding inputs for Karel program using symbolic execution We also implemented a symbolic execution baseline using the Z3 SMT solver to find inputs that maximize coverage of Karel programs. In this baseline, we first represent the input maze symbolically, where each grid in a maze is represented as a symbolic integer $s_{i j} \in{-1,0,1}$ representing either the grid $i, j$ in the maze is a wall, an empty slot, or a slot with one marker. Similarly, we represent the hero as a symbolic tuple $\left(h_{x}, h_{y}, f\right)$, where $h_{x} \in[0,|\text { row }|], h_{y} \in[0,|\text { col }|]$ representing the initial position of the hero and $f \in{0,1,2,3}$ representing the facing of the hero. With this representation, we extract all paths of program, rank them based on the number of branches they covered (i.e., coverage score), and finally use Z3 to check whether there exists an input that follows the path. If a solution is found, then the values of the symbolic values represent the configuration of the maze and the hero, which is also an optimal input for program coverage; otherwise we continue to examine the next path, until finding a satisfying input or timeout (after failing on all of the top 100 paths in our experiment). During the path extraction process, whenever we encounter a condition expression (i.e., If, IfElse, While), we split the path into two. Since while-loop can be infinitely expanded, we only consider a finite expansion (unrolling at most 3 times for each loop in our experiment). As the number of total paths is exponential in the number of conditions and examine all possible paths can be prohibitively expensive, we restrict the baseline to only examine the top ranked 100 paths for a given program. This restriction prevents the baseline to check all paths for satisfiable solutions when the program contains a large number of paths but allows the experiment to finish with 4 hours. As a result, the baseline can fail in finding inputs for some examples (failed 55 out of 467 test cases).</p>
<h2>B. 4 Ablation studies on history conditioning models</h2>
<p>Here we show GMEtaExp with different history conditioning method, and report the exploration coverage for 2D maze task in Table 5 and Table 6. These two tables use different configurations of</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Policy Net</th>
<th style="text-align: center;">History</th>
<th style="text-align: center;">Test Performance</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.329</td>
</tr>
<tr>
<td style="text-align: center;">Random DFS</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.539</td>
</tr>
<tr>
<td style="text-align: center;">Node</td>
<td style="text-align: center;">Non-Autoregressive</td>
<td style="text-align: center;">0.3363</td>
</tr>
<tr>
<td style="text-align: center;">Node</td>
<td style="text-align: center;">Autoregressive</td>
<td style="text-align: center;">0.6603</td>
</tr>
<tr>
<td style="text-align: center;">Pool</td>
<td style="text-align: center;">Non-Autoregressive</td>
<td style="text-align: center;">0.4151</td>
</tr>
<tr>
<td style="text-align: center;">Pool</td>
<td style="text-align: center;">Autoregressive</td>
<td style="text-align: center;">0.6671</td>
</tr>
<tr>
<td style="text-align: center;">Graph</td>
<td style="text-align: center;">Non-Autoregressive</td>
<td style="text-align: center;">0.6634</td>
</tr>
<tr>
<td style="text-align: center;">Graph</td>
<td style="text-align: center;">Autoregressive</td>
<td style="text-align: center;">0.7160</td>
</tr>
</tbody>
</table>
<p>Table 5: Maze results, 1 layer GNN feature</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Policy Net</th>
<th style="text-align: center;">History</th>
<th style="text-align: center;">Test Performance</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.329</td>
</tr>
<tr>
<td style="text-align: center;">Random DFS</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.539</td>
</tr>
<tr>
<td style="text-align: center;">Node</td>
<td style="text-align: center;">Non-Autoregressive</td>
<td style="text-align: center;">0.3409</td>
</tr>
<tr>
<td style="text-align: center;">Node</td>
<td style="text-align: center;">Autoregressive</td>
<td style="text-align: center;">0.7063</td>
</tr>
<tr>
<td style="text-align: center;">Pool</td>
<td style="text-align: center;">Non-Autoregressive</td>
<td style="text-align: center;">0.5217</td>
</tr>
<tr>
<td style="text-align: center;">Pool</td>
<td style="text-align: center;">Autoregressive</td>
<td style="text-align: center;">0.7074</td>
</tr>
<tr>
<td style="text-align: center;">Graph</td>
<td style="text-align: center;">Non-Autoregressive</td>
<td style="text-align: center;">0.6780</td>
</tr>
<tr>
<td style="text-align: center;">Graph</td>
<td style="text-align: center;">Autoregressive</td>
<td style="text-align: center;">0.7340</td>
</tr>
</tbody>
</table>
<p>Table 6: Maze results, 2 layer GNN feature
unsupervised graph neural network features (see Sec. B.6). Basically the more layers used in this feature extractor, the larger receptive field the agent can have in the maze.
We have three different history encoding models, namely 'Node', 'Pool' and 'Graph'. Here 'Node' only encodes the information of current location in the maze, while 'Pool' encodes all the visited nodes with an unordered pooling (i.e., ignores the structural information). The 'Graph' conditioning model instead uses all the information in the graph external memory. We can see if we use nonautoregressive policy network, then the policy with 'Graph' conditioning outperforms the other two significantly. While using autoregressive model, basically all three method should have encoded same amount of information. Nevertheless, the 'Graph' conditioning model still performs better than the others, especially when the node feature quality is low (e.g., in Table 5). This is mainly because of the forgetting issue in LSTM, where the past history is not effectively captured. This shows that encoding the graph history in this way can properly remember the exploration history, thus being more effective in the exploration task. Also it suggests that, the history includes information about how the graph and coverage evolves and how we get to the current state, which might be more beneficial than just a current snapshot of graph state.</p>
<h1>B. 5 Ablation studies on program testing</h1>
<p>Here we compare the effectiveness of learned exploration strategy and random exploration strategy on RobustFill dataset. As is shown in Figure 7, 100 randomly generated inputs for the program can jointly get $85 \%$ coverage on held-out test set, while the learned GMetaExp gets $92 \%$ within at most 5 generated inputs on the test set in zero-shot setting. This shows the significance of the learned exploration strategy. Figure 6 shows an example of effective coverage for the program. In this example, the proposed test case gets full coverage of the program, but the randomly generated one only gets 0.36 .
The importance of the run-time information: The run-time information $c_{t}(\cdot)$ we obtained from executing the program can be important for generating meaningful test cases. Figure 4(c) shows the performance of the program-aware GnnEnc model with and without run-time information $c_{t}$. In RobustFill having access to $c_{t}$ made a big difference, while in Karel the difference is much smaller.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Example test cases generated for RobustFill.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: RegEx coverage comparison on RobustFill.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>(a) RobustFill
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 8: Ablation on with/without history run-time information.</p>
<h1>B.6 Unsupervised Graph Neural Network for Representation Learning</h1>
<p>To have generalizable representation of plain graphs (i.e., graphs with no node or edge features), we use the unsupervised link prediction objective to train a graph neural network (GNN) and thus obtain the plain graph features.</p>
<p>Specifically, given a graph $G=(V, E)$ with node set $V$ and edge set $E \subseteq V \times V$, we try to associate each node with an embedding vector $\mu_{v} \in \mathbb{R}^{d}, \forall v \in V$, so as to decompose the adjacency matrix $A \in \mathbb{R}^{|V| \times|V|}$, i.e.,</p>
<p>$$
\min <em v="v">{\left{\mu</em>\right}<em V="V" _in="\in" i="i">{v \in V}, W \in \mathbb{R}^{d \times d}} \sum</em>
$$} \sum_{j \in V}\left(A_{i, j}-\mu_{i}^{\top} W \mu_{j}\right)^{2</p>
<p>These embedding vectors thus capture the structural information from the graph. To enable the generalization of the features, we cannot directly optimize the vectors for each graph, but instead, we will parameterize the embedding using GNN:</p>
<p>$$
\mu_{v}^{(L+1)}=f\left(\mu_{v}^{(L)},\left{\mu_{u}\right}_{u \in \mathcal{N}(v)}\right)
$$</p>
<p>and let $\mu_{v}=\mu_{v}^{(L+1)}$ be the last outcome of this iterative embedding process. We use GGNN to parameterize $f$. Since we don't have node features, we simply assign a constant to $\mu_{v}^{(0)}=0$. Note that the $L$ denotes the receptive field for each vector. To enable the zero-shot generalization, sometimes one can only observe the local graph, thus we need to have small $L$ in these cases. Overall, the objective is written as:</p>
<p>$$
\min <em G="\left(V^{(G)">{f, W \in \mathbb{R}^{d \times d}} \mathbb{E}</em>\right|
$$}, E^{(G)}\right) \in \mathcal{G}}\left|A^{(G)}-\mu^{(G) \top} W \mu^{(G)</p>
<p>where $\mu^{(G)}$ is the embedding of graph $G$ computed by embedding function $f$.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 9: Example App graphs for exploration experiment. Graphs in the top row are random synthetic ones, while graphs in the bottom row are collected from real-world Android Apps. Each node represent a screen, where the red node is the start screen.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 10: Exploration on synthetic apps starting from the blue node. The darkness of node colors indicates the visit count of each node. The white nodes are not visited.</p>
<h1>B. 7 Building offline App simulator for Reinforcement Learning</h1>
<p>We collect 1,000 small apps from the android app store for this experiment. Instead of interacting with the mobile app simulator during reinforcement learning, we collect the exploration histories from the app. The historical data is a list of tuples in the form $s I D_{i}, t I D_{i}, a c t_{i i}$, namely from screen with $s I D_{i}$ to screen $t I D_{i}$ using action $a c t_{i}$. Using these tuples we can rebuild the transition graph for the app. Now we can treat the app as a graph $\mathcal{G}$, and treat it as graph exploration problem. Fig 9 contains the example graph representation for both synthetic and real-world apps.
For synthetic apps, we also visualize the exploration history in Figure. 10. The darkness on nodes represent the corresponding visit count. The darker the more visits. We can see the random exploration gets stuck in some local neighborhood, while our algorithm explores better with less repetitions on the visited nodes.</p>
<p>Since it is a 'fake' app built from the transition tuples, there won't be bugs. Thus the purpose is to demonstrate the effectiveness of exploration. Also, since in this case no interaction with the mobile app simulator is needed during reinforcement learning, we can quickly test different models and experiment setups.</p>
<p>In the real-world setting, we can still adopt this approach to speed up the testing. Since the testing is continual which can last for several years for a single app (e.g., testing with different versions of the same app continually), we can collect the logs along with the testing procedure, and use these offline logs to train the RL agent.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ http://lcamtuf.coredump.cx/afl/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
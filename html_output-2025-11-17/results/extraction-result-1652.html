<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1652 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1652</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1652</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-53107462</p>
                <p><strong>Paper Title:</strong> Sim-to-Real Transfer with Neural-Augmented Robot Simulation</p>
                <p><strong>Paper Abstract:</strong> : Despite the recent successes of deep reinforcement learning, teaching complex motor skills to a physical robot remains a hard problem. While learning directly on a real system is usually impractical, doing so in simulation has proven to be fast and safe. Nevertheless, because of the "reality gap," policies trained in simulation often perform poorly when deployed on a real system. In this work, we introduce a method for training a recurrent neural network on the differences between simulated and real robot trajectories and then using this model to augment the simulator. This Neural-Augmented Simulation (NAS) can be used to learn control policies that transfer signiﬁcantly better to real environments than policies learned on existing simulators. We demonstrate the potential of our approach through a set of experiments on the Mujoco simulator with added backlash and the Poppy Ergo Jr robot. NAS allows us to learn policies that are competitive with ones that would have been learned directly on the real robot.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1652.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1652.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NAS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural-Augmented Simulation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that trains a recurrent neural network (LSTM) to predict the discrepancy between a simulator and real robot trajectories, and uses that learned correction to augment the simulator so policies trained in the augmented simulator transfer better to the real robot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Poppy Ergo Jr (physical) and MuJoCo-based simulated robot arms (Pusher, Striker, ErgoReacher)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Physical system: Poppy Ergo Jr low-cost 3D-printed robot arm (6 DOF in ErgoShield experiments; Dynamixel XL-320 servos) used in attacker/defender ErgoShield setup. Simulated systems: MuJoCo (and PyBullet for ErgoShield) implementations of 3-DOF to 7-DOF arms for Pusher, Striker and ErgoReacher tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation / sim-to-real transfer</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>MuJoCo (primary); PyBullet (ErgoShield simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Rigid-body physics engines simulating joint kinematics and dynamics, contact interactions, masses, inertias, friction and actuator position commands at high control rates; allows modification of physical parameters and injection of artificial effects (e.g. backlash).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>High-fidelity rigid-body physics engine for jointed manipulators, but still simplified relative to real robot (missing many hardware non-idealities unless explicitly modeled).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Rigid-body dynamics, joint kinematics/velocities, contacts and friction, masses and inertias, actuator position commands (servo targets); time-stepped simulation at high frequency.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Hardware non-idealities not fully modeled by default: gear/backlash effects, motor heating, mechanical compliance/flex of 3D-printed parts, mounting play (plastic rivets), stochastic sensor detection misses, long-term wear, non-linear actuator behavior (stick-slip, stiction), some timing/latency differences between sim and real.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Poppy Ergo Jr physical robots mounted on plank in attacker/defender ErgoShield setup; attacker and defender arms controlled at 100 Hz; hit detection implemented with conductive paint and MakeyMakey, with estimated 10–15% missed hits; noise sources include gear backlash on Dynamixel servos, 3D-printed PLA flex, plastic rivet mounting, motor overheating and wear.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Robotic manipulation tasks: pushing a cylinder to a goal (Pusher), striking a ball toward a goal (Striker), reaching an end-effector target (ErgoReacher), and a physical attacker task to hit a moving shield (ErgoShield).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning (PPO) in simulation augmented by a separately trained recurrent correction model (LSTM). Offline: collect paired trajectories in real robot under a behavior policy, train LSTM φ to predict correction s_t^{real} - s^{sim}; online: use φ to modify simulator states during policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Primary metric: episodic task reward averaged across episodes (e.g., average reward over 100 episodes / number of hits in ErgoShield). Additional metrics: trajectory discrepancy (sum of squared differences over trajectories), qualitative task success comparison to expert policy trained on real robot.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Unmodeled gear backlash and time-dependent joint play, actuator nonlinearities (stiction, asymptotic vs linear approach to position), mechanical compliance of 3D-printed parts, mounting play, motor overheating and wear, missed/partial sensor detection, small differences in mass/inertia/friction, latency/Hz differences between sim and robot.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Paired data collection where simulation and real robot are initialized to the same states and the same actions are executed, training an LSTM to predict correction (Δ = s_real - s_sim), resetting the simulator to the real state during collection, using the learned correction to produce 'estimated real' states during policy learning, leveraging simulator for extrapolation beyond collected data, and using recurrent (time-dependent) model to capture non-Markovian discrepancies. Empirically, >100 paired trajectories sufficed for close-to-expert performance in Pusher; ~500 half-hour dataset used for ErgoShield.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>The paper identifies that modeling time-dependent non-Markovian deviations (e.g., joint backlash) is critical; small changes to masses/frictions did not strongly affect transfer while backlash and actuator nonidealities required explicit correction. No strict numeric fidelity thresholds are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>No policy fine-tuning on the real robot was performed for the transfer experiments; real-world data collection was used only to train the LSTM correction (examples: 250–5k trajectories were used in sim experiments, and 500 episodes (~0.5 h) for ErgoShield). For sim tasks: NAS needed as few as ~100 trajectories to reach near-expert performance on Pusher, while expert training directly in real required thousands of frames/trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Comparisons: NAS vs source-only policies vs forward-model-only policies across environments with varying amounts of artificially added backlash (1–3 joints). NAS significantly outperformed source policy and forward-model policy and matched or approached expert performance in some tasks (Pusher). NAS remained more robust than forward model when sim-target gap increased (e.g., additional mass/inertia/friction changes), and required far fewer real trajectories than training an expert on the real robot.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Learning a time-dependent correction (LSTM) between simulator and real robot and augmenting the simulator with that correction (NAS) enables substantially better sim-to-real transfer than training directly on the simulator or using an open-loop forward model; NAS is sample-efficient in terms of required real-world data (>100 trajectories often sufficient), leverages the simulator for extrapolation, and avoids forward-model overfitting by grounding corrections in simulator predictions. Modeling joint backlash and other time-dependent actuator nonidealities is important for successful transfer.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1652.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1652.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Forward LSTM dynamics model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open-loop Forward Dynamics LSTM Model (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent LSTM trained on real-robot trajectories (without using simulator state) to predict next real state given current real state and action; used (as a baseline) to train policies in model-based fashion, but suffers from compounding open-loop error and overfitting to collected trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Same robotic systems as NAS experiments (MuJoCo simulated arms; Poppy Ergo Jr physical robot)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Baseline forward dynamics model applied to the same manipulation tasks (Pusher, Striker, ErgoReacher, ErgoShield) to predict real robot next states from real states and actions.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation / sim-to-real transfer</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>N/A (this model is trained on real data and used as a learned dynamics model instead of a physics simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Not applicable; model is a learned predictor of real dynamics (LSTM) used to generate rollouts for policy training in place of a simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Learned dynamics model — can match recorded trajectories closely but is lower effective fidelity when used open-loop for unseen policies due to compounding error.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Learns the mapping from (state, action) to next state as seen in the collected dataset, including time-dependent effects captured via LSTM hidden state.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Does not exploit simulator physical structure or allow resets to arbitrary states; cannot reliably extrapolate beyond the distribution of collected trajectories and tends to overfit to training data resulting in poor generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Same as NAS: Poppy Ergo Jr arms operating in the physical ErgoShield and other tasks; data collected via behavior policy (random/expert) used to train this LSTM.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Same manipulation tasks as NAS; used to train policies in a model-based manner and then deploy on the real robot.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised learning of an LSTM forward dynamics model on paired real trajectories; policies trained in simulation of this learned model (open-loop rollouts).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Task reward when policies trained on the forward model are deployed on the real robot; trajectory discrepancy compared to expert and NAS-corrected simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Open-loop rollouts lead to compounding prediction errors; dataset coverage mismatch between dynamics training data and state-distribution induced by learned policy; overfitting to recorded trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Sufficient and representative real-data coverage of task-relevant state-action space would be needed; mitigation would require on-policy data collection or model-correction techniques, but these were not used here.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Paper demonstrates that an accurate forward model on recorded data is insufficient for transfer because of compounding error and lack of generalization; thus fidelity needs not only pointwise accuracy but stability under multi-step rollout and coverage of relevant dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>No further real-world fine-tuning of policies trained on the forward model was reported; forward-model-derived policies performed poorly when deployed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Not applicable beyond qualitative comparison showing forward model achieves low one-step error on recorded data but poor closed-loop policy transfer relative to NAS.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Learning a forward dynamics model solely from real data leads to low one-step errors on recorded trajectories but suffers from compounding multi-step errors and overfitting, resulting in poor policy transfer; augmenting a simulator with a learned correction (NAS) is more robust and generalizes better to unseen policies.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1652.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1652.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Domain randomization (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain Randomization for Sim-to-Real Transfer (related approach)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A related sim-to-real strategy (cited in the paper) that randomizes simulation parameters (masses, frictions, lighting, etc.) during training so that policies generalize to real-world variation; discussed in related work but not used in experiments here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>General; cited as a technique for robotic sim-to-real transfer (not specifically used on the Poppy Ergo Jr in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Not directly applied in this work; domain randomization is a training-time augmentation for simulators to expose agents to wide variation so policies are robust to reality gap.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation / sim-to-real transfer</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>various simulators (cited works use MuJoCo, Unity, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simulation environments where physical and perceptual parameters are randomized during training (physics, textures, lighting, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Approach assumes simpler simulators can be used provided sufficient randomized variability; fidelity is not necessarily high per-instance but diversity is leveraged.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Often includes randomized masses, frictions, contact parameters, visuals/lighting and sensor noise to encourage robust policies.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Individual simulated instances may omit specific hardware non-idealities; relies on variability to cover real-world effects rather than modeling them explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>General manipulation and perception tasks in robotics (as discussed in related literature).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning with randomized simulator parameters (mentioned as related but not implemented in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Addresses gaps due to parameter mismatch by exposing policy to broad parameter ranges, but can be sample/time-consuming and may require very wide randomization to cover target variation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Large-scale randomization of task-relevant physical and perceptual parameters during training; often requires many simulation samples for coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Paper contrasts domain randomization's computational cost and requirement for wide coverage with NAS's data-efficient corrective modeling; no new fidelity thresholds provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as a prominent alternative for sim-to-real transfer; the authors note domain randomization can be effective but is time-consuming because policies must experience a wide range of noise during training, and they position NAS as a more sample-efficient alternative for modeling particular simulator-to-real discrepancies.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world. <em>(Rating: 2)</em></li>
                <li>Sim-to-real transfer of robotic control with dynamics randomization. <em>(Rating: 2)</em></li>
                <li>Grounded action transformation for robot learning in simulation. <em>(Rating: 2)</em></li>
                <li>Progressive Neural Networks. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1652",
    "paper_id": "paper-53107462",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "NAS",
            "name_full": "Neural-Augmented Simulation",
            "brief_description": "A method that trains a recurrent neural network (LSTM) to predict the discrepancy between a simulator and real robot trajectories, and uses that learned correction to augment the simulator so policies trained in the augmented simulator transfer better to the real robot.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Poppy Ergo Jr (physical) and MuJoCo-based simulated robot arms (Pusher, Striker, ErgoReacher)",
            "agent_system_description": "Physical system: Poppy Ergo Jr low-cost 3D-printed robot arm (6 DOF in ErgoShield experiments; Dynamixel XL-320 servos) used in attacker/defender ErgoShield setup. Simulated systems: MuJoCo (and PyBullet for ErgoShield) implementations of 3-DOF to 7-DOF arms for Pusher, Striker and ErgoReacher tasks.",
            "domain": "general robotics manipulation / sim-to-real transfer",
            "virtual_environment_name": "MuJoCo (primary); PyBullet (ErgoShield simulation)",
            "virtual_environment_description": "Rigid-body physics engines simulating joint kinematics and dynamics, contact interactions, masses, inertias, friction and actuator position commands at high control rates; allows modification of physical parameters and injection of artificial effects (e.g. backlash).",
            "simulation_fidelity_level": "High-fidelity rigid-body physics engine for jointed manipulators, but still simplified relative to real robot (missing many hardware non-idealities unless explicitly modeled).",
            "fidelity_aspects_modeled": "Rigid-body dynamics, joint kinematics/velocities, contacts and friction, masses and inertias, actuator position commands (servo targets); time-stepped simulation at high frequency.",
            "fidelity_aspects_simplified": "Hardware non-idealities not fully modeled by default: gear/backlash effects, motor heating, mechanical compliance/flex of 3D-printed parts, mounting play (plastic rivets), stochastic sensor detection misses, long-term wear, non-linear actuator behavior (stick-slip, stiction), some timing/latency differences between sim and real.",
            "real_environment_description": "Poppy Ergo Jr physical robots mounted on plank in attacker/defender ErgoShield setup; attacker and defender arms controlled at 100 Hz; hit detection implemented with conductive paint and MakeyMakey, with estimated 10–15% missed hits; noise sources include gear backlash on Dynamixel servos, 3D-printed PLA flex, plastic rivet mounting, motor overheating and wear.",
            "task_or_skill_transferred": "Robotic manipulation tasks: pushing a cylinder to a goal (Pusher), striking a ball toward a goal (Striker), reaching an end-effector target (ErgoReacher), and a physical attacker task to hit a moving shield (ErgoShield).",
            "training_method": "Reinforcement learning (PPO) in simulation augmented by a separately trained recurrent correction model (LSTM). Offline: collect paired trajectories in real robot under a behavior policy, train LSTM φ to predict correction s_t^{real} - s^{sim}; online: use φ to modify simulator states during policy learning.",
            "transfer_success_metric": "Primary metric: episodic task reward averaged across episodes (e.g., average reward over 100 episodes / number of hits in ErgoShield). Additional metrics: trajectory discrepancy (sum of squared differences over trajectories), qualitative task success comparison to expert policy trained on real robot.",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Unmodeled gear backlash and time-dependent joint play, actuator nonlinearities (stiction, asymptotic vs linear approach to position), mechanical compliance of 3D-printed parts, mounting play, motor overheating and wear, missed/partial sensor detection, small differences in mass/inertia/friction, latency/Hz differences between sim and robot.",
            "transfer_enabling_conditions": "Paired data collection where simulation and real robot are initialized to the same states and the same actions are executed, training an LSTM to predict correction (Δ = s_real - s_sim), resetting the simulator to the real state during collection, using the learned correction to produce 'estimated real' states during policy learning, leveraging simulator for extrapolation beyond collected data, and using recurrent (time-dependent) model to capture non-Markovian discrepancies. Empirically, &gt;100 paired trajectories sufficed for close-to-expert performance in Pusher; ~500 half-hour dataset used for ErgoShield.",
            "fidelity_requirements_identified": "The paper identifies that modeling time-dependent non-Markovian deviations (e.g., joint backlash) is critical; small changes to masses/frictions did not strongly affect transfer while backlash and actuator nonidealities required explicit correction. No strict numeric fidelity thresholds are provided.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": "No policy fine-tuning on the real robot was performed for the transfer experiments; real-world data collection was used only to train the LSTM correction (examples: 250–5k trajectories were used in sim experiments, and 500 episodes (~0.5 h) for ErgoShield). For sim tasks: NAS needed as few as ~100 trajectories to reach near-expert performance on Pusher, while expert training directly in real required thousands of frames/trajectories.",
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Comparisons: NAS vs source-only policies vs forward-model-only policies across environments with varying amounts of artificially added backlash (1–3 joints). NAS significantly outperformed source policy and forward-model policy and matched or approached expert performance in some tasks (Pusher). NAS remained more robust than forward model when sim-target gap increased (e.g., additional mass/inertia/friction changes), and required far fewer real trajectories than training an expert on the real robot.",
            "key_findings": "Learning a time-dependent correction (LSTM) between simulator and real robot and augmenting the simulator with that correction (NAS) enables substantially better sim-to-real transfer than training directly on the simulator or using an open-loop forward model; NAS is sample-efficient in terms of required real-world data (&gt;100 trajectories often sufficient), leverages the simulator for extrapolation, and avoids forward-model overfitting by grounding corrections in simulator predictions. Modeling joint backlash and other time-dependent actuator nonidealities is important for successful transfer.",
            "uuid": "e1652.0"
        },
        {
            "name_short": "Forward LSTM dynamics model",
            "name_full": "Open-loop Forward Dynamics LSTM Model (baseline)",
            "brief_description": "A recurrent LSTM trained on real-robot trajectories (without using simulator state) to predict next real state given current real state and action; used (as a baseline) to train policies in model-based fashion, but suffers from compounding open-loop error and overfitting to collected trajectories.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Same robotic systems as NAS experiments (MuJoCo simulated arms; Poppy Ergo Jr physical robot)",
            "agent_system_description": "Baseline forward dynamics model applied to the same manipulation tasks (Pusher, Striker, ErgoReacher, ErgoShield) to predict real robot next states from real states and actions.",
            "domain": "general robotics manipulation / sim-to-real transfer",
            "virtual_environment_name": "N/A (this model is trained on real data and used as a learned dynamics model instead of a physics simulator)",
            "virtual_environment_description": "Not applicable; model is a learned predictor of real dynamics (LSTM) used to generate rollouts for policy training in place of a simulator.",
            "simulation_fidelity_level": "Learned dynamics model — can match recorded trajectories closely but is lower effective fidelity when used open-loop for unseen policies due to compounding error.",
            "fidelity_aspects_modeled": "Learns the mapping from (state, action) to next state as seen in the collected dataset, including time-dependent effects captured via LSTM hidden state.",
            "fidelity_aspects_simplified": "Does not exploit simulator physical structure or allow resets to arbitrary states; cannot reliably extrapolate beyond the distribution of collected trajectories and tends to overfit to training data resulting in poor generalization.",
            "real_environment_description": "Same as NAS: Poppy Ergo Jr arms operating in the physical ErgoShield and other tasks; data collected via behavior policy (random/expert) used to train this LSTM.",
            "task_or_skill_transferred": "Same manipulation tasks as NAS; used to train policies in a model-based manner and then deploy on the real robot.",
            "training_method": "Supervised learning of an LSTM forward dynamics model on paired real trajectories; policies trained in simulation of this learned model (open-loop rollouts).",
            "transfer_success_metric": "Task reward when policies trained on the forward model are deployed on the real robot; trajectory discrepancy compared to expert and NAS-corrected simulator.",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": false,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Open-loop rollouts lead to compounding prediction errors; dataset coverage mismatch between dynamics training data and state-distribution induced by learned policy; overfitting to recorded trajectories.",
            "transfer_enabling_conditions": "Sufficient and representative real-data coverage of task-relevant state-action space would be needed; mitigation would require on-policy data collection or model-correction techniques, but these were not used here.",
            "fidelity_requirements_identified": "Paper demonstrates that an accurate forward model on recorded data is insufficient for transfer because of compounding error and lack of generalization; thus fidelity needs not only pointwise accuracy but stability under multi-step rollout and coverage of relevant dynamics.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": "No further real-world fine-tuning of policies trained on the forward model was reported; forward-model-derived policies performed poorly when deployed.",
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": "Not applicable beyond qualitative comparison showing forward model achieves low one-step error on recorded data but poor closed-loop policy transfer relative to NAS.",
            "key_findings": "Learning a forward dynamics model solely from real data leads to low one-step errors on recorded trajectories but suffers from compounding multi-step errors and overfitting, resulting in poor policy transfer; augmenting a simulator with a learned correction (NAS) is more robust and generalizes better to unseen policies.",
            "uuid": "e1652.1"
        },
        {
            "name_short": "Domain randomization (related work)",
            "name_full": "Domain Randomization for Sim-to-Real Transfer (related approach)",
            "brief_description": "A related sim-to-real strategy (cited in the paper) that randomizes simulation parameters (masses, frictions, lighting, etc.) during training so that policies generalize to real-world variation; discussed in related work but not used in experiments here.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_system_name": "General; cited as a technique for robotic sim-to-real transfer (not specifically used on the Poppy Ergo Jr in this paper)",
            "agent_system_description": "Not directly applied in this work; domain randomization is a training-time augmentation for simulators to expose agents to wide variation so policies are robust to reality gap.",
            "domain": "general robotics manipulation / sim-to-real transfer",
            "virtual_environment_name": "various simulators (cited works use MuJoCo, Unity, etc.)",
            "virtual_environment_description": "Simulation environments where physical and perceptual parameters are randomized during training (physics, textures, lighting, etc.).",
            "simulation_fidelity_level": "Approach assumes simpler simulators can be used provided sufficient randomized variability; fidelity is not necessarily high per-instance but diversity is leveraged.",
            "fidelity_aspects_modeled": "Often includes randomized masses, frictions, contact parameters, visuals/lighting and sensor noise to encourage robust policies.",
            "fidelity_aspects_simplified": "Individual simulated instances may omit specific hardware non-idealities; relies on variability to cover real-world effects rather than modeling them explicitly.",
            "real_environment_description": null,
            "task_or_skill_transferred": "General manipulation and perception tasks in robotics (as discussed in related literature).",
            "training_method": "Reinforcement learning with randomized simulator parameters (mentioned as related but not implemented in this paper).",
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Addresses gaps due to parameter mismatch by exposing policy to broad parameter ranges, but can be sample/time-consuming and may require very wide randomization to cover target variation.",
            "transfer_enabling_conditions": "Large-scale randomization of task-relevant physical and perceptual parameters during training; often requires many simulation samples for coverage.",
            "fidelity_requirements_identified": "Paper contrasts domain randomization's computational cost and requirement for wide coverage with NAS's data-efficient corrective modeling; no new fidelity thresholds provided here.",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Mentioned as a prominent alternative for sim-to-real transfer; the authors note domain randomization can be effective but is time-consuming because policies must experience a wide range of noise during training, and they position NAS as a more sample-efficient alternative for modeling particular simulator-to-real discrepancies.",
            "uuid": "e1652.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world.",
            "rating": 2,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "Sim-to-real transfer of robotic control with dynamics randomization.",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_of_robotic_control_with_dynamics_randomization"
        },
        {
            "paper_title": "Grounded action transformation for robot learning in simulation.",
            "rating": 2,
            "sanitized_title": "grounded_action_transformation_for_robot_learning_in_simulation"
        },
        {
            "paper_title": "Progressive Neural Networks.",
            "rating": 1,
            "sanitized_title": "progressive_neural_networks"
        }
    ],
    "cost": 0.013190249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Sim-to-Real Transfer with Neural-Augmented Robot Simulation</p>
<p>Florian Golemo florian.golemo@inria.fr 
Adrien Ali Taïga adrien.ali.taiga@umontreal.ca 
Pierre-Yves Oudeyer pierre-yves.oudeyer@inria.fr 
Aaron Courville aaron.courville@umontreal.ca </p>
<p>INRIA Bordeaux &amp; MILA</p>
<p>MILA
Université de Montréal</p>
<p>INRIA Bordeaux</p>
<p>MILA
Université de Montréal</p>
<p>Sim-to-Real Transfer with Neural-Augmented Robot Simulation
8644EB775AE096D82BCB1A00F6BC8D1A
Despite the recent successes of deep reinforcement learning, teaching complex motor skills to a physical robot remains a hard problem.While learning directly on a real system is usually impractical, doing so in simulation has proven to be fast and safe.Nevertheless, because of the "reality gap," policies trained in simulation often perform poorly when deployed on a real system.In this work, we introduce a method for training a recurrent neural network on the differences between simulated and real robot trajectories and then using this model to augment the simulator.This Neural-Augmented Simulation (NAS) can be used to learn control policies that transfer significantly better to real environments than policies learned on existing simulators.We demonstrate the potential of our approach through a set of experiments on the Mujoco simulator with added backlash and the Poppy Ergo Jr robot.NAS allows us to learn policies that are competitive with ones that would have been learned directly on the real robot.</p>
<p>Introduction</p>
<p>Reinforcement learning (RL) with function approximation has demonstrated remarkable performance in recent years.Prominent examples include playing Atari games from raw pixels [1], learning complex policies for continuous control [2,3], and surpassing human performance on the game of Go [4].However, most of these successes were achieved in non-physical environments: simulations, video games, etc. Learning complex policies on physical systems remains an open challenge.Typical reinforcement learning methods require a large amount of interaction with the environment and, in addition, it is also often impractical, if not dangerous, to roll out a partially trained policy.Both these issues make it unsuitable for many tasks to be trained directly on a physical robot.</p>
<p>The fidelity of current simulators and rendering engines provides an incentive to use them as training grounds for control policies, hoping the newly acquired skills would transfer back to reality.However, this is not as easy as one might hope since no simulator perfectly captures reality.This problem is widely known as the reality gap.See Figure 1 for an illustration.</p>
<p>The reality gap can be seen as an instance of a domain adaptation problem, where the input distribution of a model changes between training (in simulation) and testing (in the real world).In the case of image classification and off-policy image-based deep reinforcement learning, this issue has sometimes been tackled by refining images from the source domain -where the training happens -to appear closer to images from the target domain -where evaluation is carried out [5,6].</p>
<p>In this work, we take a similar approach and apply it to continuous control.We use data collected from a real robot to train a recurrent neural network to predict the discrepancies between simulation and the real world.This allows us to improve the quality of our simulation by grounding it in realistic trajectories.We refer to our approach as Neural-Augmented Simulation (NAS).We can use it with any reinforcement learning algorithm, combining the benefits of simulation and fast offline training, Figure 1: Impact of backlash: when setting both the simulated robot (red) and the real robot (white) to the resting position, the small backlash of each joint adds up to a noticeable difference in the end effector position.</p>
<p>while still learning policies that transfer well to robots in real environments.Since we collect data using a non-task-specific policy, NAS can be used to learn policies related to different tasks.Thus, we believe that NAS provides an efficient and effective strategy for multi-task sim-to-real transfer.</p>
<p>With our NAS strategy, we aim to achieve the best of both modeling modalities.While the recurrent neural network compensates for the unrealistically modeled aspects of the simulator; the simulator allows for better extrapolation to dynamic regimes that were not well explored under the data collection policy.Our choice to use a recurrent model2 is motivated by the desire to capture deviations that could violate the standard Markov assumption on the state space.</p>
<p>We evaluate our approach on two OpenAI Gym [8] based simulated environments with an artificially created reality gap in the form of added backlash.We also evaluate on the Poppy Ergo Jr robot arm, a relatively inexpensive arm with dynamics that are not well modeled by the existing simulator.We find that NAS provides an efficient way to learn policies that approach the performance of policies trained directly on the physical system.</p>
<p>Related work</p>
<p>Despite the recent success of deep reinforcement learning, learning on a real robot remains impractical due to poor sample efficiency.Classical methods for sim-to-real transfer include co-evolving the policy and the simulation [9,10], selecting for policies that transfer well [11,12], learning a transfer function of policies [13], and modeling different levels of noise [14].Nonetheless, these methods are usually limited to domains with a low-dimensional state-action space.</p>
<p>An alternative is to try to learn a forward model of the dynamics, a mapping between the current state and an action to the next state [15,16,17].However, despite some successes, learning a forward model of a real robot remains an open challenge.The compounding error of these models quickly deteriorates over time and corrections are needed to compensate for the uncertainty of the model [18].These methods are also usually sensitive to the data used to train the model, which is often different from the state distribution encountered during policy learning [19,20] Domain adaptation has received a lot of focus from the computer vision community.There have been many approaches such as fine-tuning a model trained on the source domain using data from the target domain [21], enforcing invariance in features between data from source and target domain [22] or learning a mapping between source and target domain [23].</p>
<p>Similar ideas have been applied in RLx, such as previous work that focused on learning internal representations robust to change in the input distribution [24].Using data generated from the simulation during training was also used in bootstrapping the target policy [25], Progressive Neural Networks [26] were also used to extend a simulation-learned policy to different target environments, but the most common method remains learning the control policy on the simulator before fine tuning on the real robot.Another quite successful recent method consists of randomizing all task-related physical properties of the simulation [27,28].This approach is very time-consuming because in order for a policy to generalize to all kinds of inputs, it has to experience a very wide range of noise during learning.Recently an approach very similar to our has been developed in the work of [29].However, rather than learning a state space transition, the authors learn a transformation of actions.This learning is done on-policy so once the model is learned, the simulator augmentation doesn't transfer to other tasks.</p>
<p>In summary, existing approaches struggle with either computational complexity or with difficulties in transferability to other tasks.We would like to introduce a technique that is capable of addressing both these issues.</p>
<p>Method</p>
<p>Notation</p>
<p>We consider a domain adaptation problem between a source domain D s (usually a simulator) and a target domain D t (typically a robot in the real world).Each domain is represented by a Markov Decision Process (MDP) S, A, p, r, γ , where S is the state space, A the action space, for s ∈ S and a ∈ S, p(.|s, a) the transition probability of the environment, r the reward and γ the discount factor.We assume that the two domains share the same action space, however, because of the perceptual-reality gap state space, dynamics and rewards can be different even if they are structurally similar, we write D s = S s , A, p s , r s and D t = S t , A, p t , r t for the two MDPs.We also assume that rewards are similar (i.e r s = r t ) and, most importantly, we assume that we are given access to the source domain and can reset it to a specific state.For example, for transfer from simulation to the robot, we assume we are free to reset the joint positions and angular velocities of the simulated robots.</p>
<p>Target domain data collection</p>
<p>We model the difference between the source and target domain, learning a mapping from trajectories in the source domain to trajectories in the target domain.Given a behavior policy µ (which could be random or provided by an expert) we collect trajectories from the target environment (i.e.real robot) follow actions given by µ.</p>
<p>Training the model</p>
<p>To train the LSTM in NAS, we sample an initial state s 0 ∼ p t (s 0 ) from the target domain distribution and set the source domain to start from the same initial state (i.e s s 0 = s t 0 = s 0 ).At each time step an action is sampled following the behavior policy a i ∼ µ(s t i ), then executed on the two domains to get the transition (s i , a i , s s i+1 , s t i+1 ).The source domain is then reset to the target domain state and the procedure is repeated until the end of the episode.The resulting trajectory τ = (s 0 , a 0 , s s 1 , s t 1 , ..., s s T −2 , a T , s s T −1 , s t T −1 ) of length T is then stored, the procedure is described in Algorithm 1.After collecting the data the model φ, an LSTM [7], is trained to predict s t i+1 .The difference between two states is usually small, so the network outputs a correction term φ(s t i , a i , h,
s s i+1 ) = s t i+1 − s s i+1
where h is the hidden state of the network.We also compare our approach with a forward model trained without using information from the source domain, ψ(s i , a i , h) = s t i+1 − s i .We normalize the inputs and outputs, and the model is trained with maximum likelihood using Adam [30].</p>
<p>Transferring to the target domain</p>
<p>Once the model φ is trained, it can be combined with the source environment to learn a policy that will later be transferred to the target environment.We use PPO 3 [31] in all our experiments but any other reinforcement learning algorithm could be used.During learning at each time step the current state transition in the source domain is passed to φ to compute an estimate of the current state in the target environment, an action a i is chosen according to this estimate a i ∼ π(φ(s s i , s s i+1 )), then the source domain state is set to the current estimate of the target domain state ∼ φ(s s i , s s i+1 ).This will allow our model to correct trajectories from the source domain, making them closer to the corresponding trajectory in the target domain.</p>
<p>Experiments</p>
<p>We evaluated our method on a set of simulated robotic control tasks using the MuJoCo physics engine [32] as well as on the open-source 3D-printed physical robot "Poppy Ergo Jr" [33] (see https://www.poppy-project.org/en/robots/poppy-ergo-jr)4 .We created an artificial reality gap using two simulated environments.The source and target environments are identical, except that the target environment has backlash.We also experimented with small changes in joints and link parameters but noted that it did not impact our results.Overall, the difference in backlash between the environments was significant enough to prevent policies trained on the source from performing well on the target environment (though an agent could solve the target environment if trained on it).More details about why we picked backlash can be found in appendix A.2.</p>
<p>We used the following environments from OpenAI Gym for our experiments:</p>
<p>• Pusher: A 3-DOF arm trying to move a cylinder on a plane to a specific location.</p>
<p>• Striker: A 7-DOF arm that has to strike a ball on a table in order to make the ball move into a goal that is out of reach for the robot.</p>
<p>• ErgoReacher: A 4-DOF arm that has to reach a goal spot with its end effector.</p>
<p>While we added backlash to only one joint of the Pusher, to test the limits of our artificial reality gap, we added backlash to three joints of the Striker and two joints or ErgoReacher.We also compare our proposed method with different baselines:</p>
<p>• Expert policy: policy trained directly in the target environment</p>
<p>• Source policy: transferring a policy trained in source environment without any adaption</p>
<p>• Forward model policy: a forward dynamic model ψ is trained using an LSTM and data collected from the target domain then a policy trained using only this model (without insight from the source domain)</p>
<p>• Transfer policy: the policy trained using NAS</p>
<p>Trajectory following</p>
<p>We evaluated the two models learned φ and ψ on a 100-step trajectory rollout on the Pusher (see Figure 4).The forward model ψ is trained without access the source domain and is making predictions in an open-loop.While this model is accurate at the beginning of the rollout, its small error compounds over time.We will see later that this makes it hard to learn policies that will achieve a high reward.On the other hand, the model φ is grounded using the source environment and only needs to correct the simulator predictions, so the difference between the trajectory in the real environment is barely noticeable.It shows that correcting trajectories from the source environment provide an efficient way to model the target environment.</p>
<p>Simulated Environments -Sim to Sim transfer</p>
<p>We tested our method on the simulated environments previously introduced.The number of trajectories used to train the models varied from 250 for the Pusher over 1k for the ErgoReacher to 5k for the Striker.Policies are trained for 2M frames and evaluated over 100 episodes, averaged across 5 seeds, results are given in Figure 5.Additional information about the training can be found in app.A.1.</p>
<p>Our experiments show that in all our environments the policy learned using NAS improve significantly over the source policy, and even reach expert performance on the Pusher.Though it seems that the forward model policy is doing better on the Striker, this is just a consequence of reward hacking; the agent learns a single movement that pushes away the ball without considering the target position.In contrast, the policy learned using NAS aims for the right direction but does not push it hard enough to make it all the way to the target.This happens when, following a random policy in the target domain, we do not record enough interaction between the robot and the ball to model this behavior correctly.An expert policy (e.g.given by human demonstration) could help alleviate this issue and make sure that the relevant parts of the state action space are covered when collecting the data.Altogether it highlights the fact that we cannot hope to learn a good transfer policy for states where there exists both a significant discrepancy between the source and target environments and insufficient data to properly train the LSTM.</p>
<p>We also varied the number of trajectories used to train the model in NAS to see how it influences the final performance of the transferred policy, see Figure 6.When more than 100 trajectories are used, the difference with the expert policy is not visually noticeable and the difference in reward is only due to variance in the evaluation and policy learning.This is in contrast to the 3-4k trajectories required by the expert policy during training to reach its final performance.We use the existing Poppy Ergo Jr robot arm in a novel task called "ErgoShield", a low-cost physical variant of the OpenAI Gym "Reacher" task.Each robot arm has 6 DOF: with respect to the robot's heading 1 perpendicular joint at the base, followed by 2 in parallel, 1 perpendicular, and two more in parallel.All joints can rotate from -90 degrees to +90 degrees from their resting position.</p>
<p>Physical Robot Experiments Overview</p>
<p>For this task we fixed two of these robots onto a wooden plank, facing each other at a distance that left the two tips 5mm apart in resting position.One robot is holding a "sword" (pencil) and the other is holding a shield.The shield robot ("defender") moves every 2.5s to a random position in which the shield is in reach of the opponent.The sword robot ("attacker") is the only robot directly controllable.Each episode lasts 10s and the attacker's goal is to touch the shield as often as possible.</p>
<p>Every time a hit is detected, the robots reset to a resting position (attacker) and different random position (defender).The setup is depicted in Figure 7 and additional specifications can be found in appendix B. This environment is accompanied by a simulation in PyBullet 56 .</p>
<p>The environment can be controlled at 100Hz by sending a 6-element vector in range [-1,1] corresponding to the desired motor position.The environment is observed at 100Hz as a 24-element vector consisting of: attacker joint angles, attacker joint velocities, defender joint angles, defender joint velocities.In the physical robots, the hit detection is implemented by coating both sword and shield in conductive paint, to which a MakeyMakey7 is attached.</p>
<p>For the offline data collection, a single robot with an empty pen holder end-effector was instructed to move to 3 random positions for one seconds each (corresponding to an episode length of 300 frames).</p>
<p>We collected 500 such episodes equivalent to roughly half an hour of robot movement including resetting between episodes.</p>
<p>Results on Sim-to-Real Transfer</p>
<p>We followed the same experimental paradigm as in the sim-to-sim experiments in 4.1: 3 expert policies were trained directly on the real robot, 3 policies were trained in simulation and were evaluated on the real robot, 3 policies were trained using our method, and 3 policies were trained with only the forward dynamics model.All policies were trained with the same PPO hyperparameters, save for the random seed.The hyperparameters can be found in appendix C.</p>
<p>The evaluation results are displayed in Figure 8a.The policies trained directly on the real robot performed significantly worse than all the other policies.The main reasons for this are (a) the hit  detection is not perfect (as in simulation) and since not every hit gets detected the reward is sparser 8 .And (b) since the attacker robot frequently gets their sword stuck in the opponent, in themselves, or in the environment, exploration is not as easy as in simulation.</p>
<p>We did not find a significant difference in the performance of the simulation and forward dynamics policies.However, our approach (the "Transfer policy") yielded a significantly better results than any others.</p>
<p>Figure 8b (and in more detail appendix D) shows for a single joint how the different approaches estimate the joint position under a given action.The simulation approaches the target position asymptotically while the real robot approaches the same value linearly.It is worth noting that even though the forward model can estimate the recorded dataset very well, policies trained using only this model and no simulation tend to perform badly.This is likely due to the forward model overfitting to the training data and not generalizing to other settings.This is a crucial feature of our method: by augmenting the simulator we are able to utilize the same learned augmentation to different tasks.</p>
<p>Conclusion</p>
<p>Currently, deep reinforcement learning algorithms are limited in their application to real world scenarios by their high sample complexity and their lack of guarantees when a partially trained policy is deployed.Transfer learning from simulation-trained agents offers a way to solve both these issues and enjoy more flexibility.To that end, we introduced a new method for learning a model that can be used to augment a robotic simulator and demonstrated the performance of this approach as well as its sample efficiency with respect to real robot data.</p>
<p>Provided the same robot is used, the model only has to be learned once and does not require policy-specific fine-tuning, making this method appropriate for multi-task robotic applications.</p>
<p>In the future, we would like to extend this approach to more extensive tasks and different robotic platforms to evaluate its generality, since working purely in simulation leaves some noise that occurs in real robots unmodeled.We would also like to move to image-based observations because our current action/observation spaces are low-dimensional but have a very high frequency (50Hz in sim, 100Hz on real robot).Since our approach is already neural network-based and neural networks are known to scale well to high dimensionality, this addition should be straightforward.</p>
<p>Another interesting path to investigate would be to combine more intelligent exploration methods for collecting the original dataset.If the initial exploration is guided by intrinsic motivation or count-based exploration it might further improve the sample-efficiency and reduce the amount of random movements that need to be recorded in the real robot.</p>
<p>A Experiment details A.1 Neural Network</p>
<p>For the Pusher, the neural network architecture is a fully connected layers with 128 hidden units with ReLU activations followed by a 3 layers LSTM with 128 hidden units and an other fully connected layers for the outputs.The Striker share the same architecture with 256 hidden units instead.The ErgoReacher environment only needed an LSTM of 3 layers with 100 hidden units each.Networks are trained using the Adam optimizer for 250 epochs with a batch size of 1 and a learning rate of 0.01.We use the PPO implementation and the recommended parameters for Mujoco from https:// github.com/ikostrikov/pytorch-a2c-ppo-acktrA.2 Why Backlash?</p>
<p>Regarding the setup for the sim2sim experiments, we tried increasing the reality gap between the two simulated environments on different simulators (Gazebo, MuJoCo, Unity) by tuning physical parameters like mass, inertia and friction.However, we found that small changes in these properties did not affect the policy trained in the source domain, whereas large changes made the environment unstable and it was not possible to learn a good policy directly in the target environment.Nevertheless, even in this extreme case the message from Figure 5 still holds and the proposed method was doing better than the forward model.We then settled on backlash to solve the previous issues as it offered a good compromise between simulation stability and inter-simulation difference.</p>
<p>As an example of one of these preliminary experiments, we increased the mass and inertia of the arm on the Pusher task by 50%, increased the mass and inertia of the pushable object by 100% (i.e.doubled it), and increased the surface friction of the table by 500% while keeping backlash.We found that with these changes, the difference model was still doing much better than the forward model.Looking at a trajectory rollout, the error was significant but close to zero.It should be noted, that such changes created a significant difference between the source and target environments.The new expert policy only averaged -91.3 in reward instead of the previous -67.5 (calculated over 100 episodes on 5 different seeds) which shows the reliability of our method when the reality gap increases.</p>
<p>B ErgoShield Specifications</p>
<p>The "swords" we used are standard BIC 19.5cm wooden pencils.The pencil is sticking out of the penholder end-effector by about 2cm at the bottom.The shield is 2.35cm in radius and 0.59cm deep in the center.A 3D-printable version is available at https://www.tinkercad.com/things/8UXdY4a5xdJ-ergo-jr-shield#/.</p>
<p>The random sampling ranges in degrees on each joint of the shield robot are [45, 15, 15, 22.5, 15, 15] respectively centered around the resting position.</p>
<p>The connection for training the real robot live has an average latency of 10ms.On the real robot the noise and non-stationarities can stem from</p>
<p>• The gear backlash on the Dynamixel XL-320 servos.</p>
<p>• The body of the robot being 3D-printed out of thin PLA plastic.</p>
<p>• The body parts being mounted to the servos via plastic rivets.</p>
<p>• Overheating of the motors and wear of the body parts.</p>
<p>• The electro-conductive paint thinning out over time.</p>
<p>C PPO Hyperparameters Real Robot</p>
<p>Figure 2 :
2
Figure 2: Left: Overview of the method for training the forward dynamics model.By gathering state differences when running the same action in simulation and on the real robot.Right: When the forward model is learned, it can be applied to simulator states to get the corresponding real state.This correction model (∆) is time-dependent (implemented as LSTM).The policy learning algorithm only has access to the "real" states.</p>
<p>Figure 3 :Figure 4 :
34
Figure 3: Benchmark simulation environments used</p>
<p>Figure 5 :
5
Figure 5: Comparison of the different methods described when deployed on the target environment, for the Pusher (5a) and the Striker (5b).</p>
<p>Figure 6 :
6
Figure 6: We compare the results of our method when the number of trajectories used to train the model φ varies on the Pusher</p>
<p>Figure 7 :
7
Figure 7: The ErgoShield environment in reality and simulation.The attacker (sword, left side) has to hit the randomly-moving defender (shield, right side) as often as possible in 10s.In the left image the defender robot is covered in a sock to mitigate the attacker getting stuck too often.The joint angles were compensated for backlash in the left image to highlight similarities.</p>
<p>(a) Method Comparison (Real Robot) (b) Example Single Joint Position and Estimate over Time</p>
<p>Figure 8 :
8
Figure 8: Results of different simulation to real transfer approaches.Left: comparison of average rewards of 20 rollouts of 3 policies per approach.Right: comparison of single joint behavior when receiving a target position (violet dotted) in simulation (green dashed), on the real robot (blue solid), and estimates from the forward model (red, dashed) and our method (yellow, dotted)</p>
<p>Execute action a i in the source domain, observe a reward r i and a new state s s
Algorithm 1 Offline Data CollectionInitialize model φfor episode= 1, M doInitialize the target domain from a state s t 1 Initialize the source domain to the same state s s 1 = s t 1for i = 1, T doSelect action a i ∼ µ(s t i ) according to the behavior policy Execute action a i on the source domain and observe new state s s i+1 Execute action a i on the target domain and observe new state s t i+1 Update φ with the tuple s s i+1 = s t i+1Set s s i+1 = s t i+1end forend forAlgorithm 2 Policy Learning in Source DomainGiven a RL algorithm AA model φInitialize Afor episode= 1, M doInitialize the source domain from a state s s
1Initialize the φ latent variables h for i = 1, T do Sample action a i ∼ π(s s i ) using the behavioral policy fromA i+1 Set ŝt i+1 = s s i+1 + φ(s i , a i , h, s s i+1) the estimate of s t i+1 given by φ Do a one-step policy update with A using the transition (s s i , a i , ŝt i+1 , r i ) Set the source domain to s s i+1 = ŝt i+1 end for end for</p>
<p>We chose to use a Long Short-Term Model (LSTM)[7] as our recurrent neural network.
A list of all our hyperparameters is included in the supplementary material.
The code for our experiments can be found at https://github.com/aalitaiga/sim-to-real/
https://pybullet.org/wordpress/
The simulated environment is also available on GitHub at https://github.com/fgolemo/gym-ergojr
https://makeymakey.com/
There are no false positives, but we estimate that 10 − 15% hits aren't recognized.
AcknowledgmentsThis work was made possible with the funding and support of CIFAR, CHIST-ERA IGLU, and ANR.The authors would like to thank the MILA lab for being an amazing research environment and the FLOWERS team at INRIA Bordeaux for the ongoing support with the robots.On top of that, the authors would like to thank David Meger for providing a home for the little robots and input on the project, Herke van Hoof for giving valuable advice on the training procedure, and Alexandre Lacoste for making us revisit the method several times over until we were crystal clear.Additional thanks to NVIDIA for providing a Geforce Titan Xp for the INRIA Bordeaux lab.D Quantitative Analysis of TrajectoriesTable1displays the differences between the expert policy (the policy rolled out on the real robot) and (a) the source policy (same policy in simulation), (b) the forward model policy (the policy rolled out through the LSTM), and (c) the transferred model policy (our approach for modifying the simulator output).1st Quartile Mean 3rd Quartile Table1: Quantitative analysis of trajectory differences on the ErgoShield task, calculate by the sums of squared differences at each point in the trajectory over 1000 trajectories.The point-wise difference in (a) indicates the expected significant deviations between simulation and real robot.The low deviations in (b) are specifically what that model was trained for and are therefore near-zero.In practice however, this leads to overfitting, i.e. the so-trained model doesn't perform well on policies for which this model has not been exposed to any trajectories (which is evident from the performance in Figure8a).The differences in (c) show that the modified simulator is closer to the real robot trajectory.In combination with the final performance in Figure8awe can infer that our model does not overfit as the forward model does because it's harnessing the simulator to generalize to previously unseen trajectories.
Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, Nature. 51875402015</p>
<p>Trust region policy optimization. J Schulman, S Levine, P Abbeel, M Jordan, P Moritz, Proceedings of the 32nd International Conference on Machine Learning (ICML-15). the 32nd International Conference on Machine Learning (ICML-15)2015</p>
<p>T P Lillicrap, J J Hunt, A Pritzel, N Heess, T Erez, Y Tassa, D Silver, D Wierstra, arXiv:1509.02971Continuous control with deep reinforcement learning. 2015arXiv preprint</p>
<p>Mastering the game of go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, Nature. 52975872016</p>
<p>Learning from simulated and unsupervised images through adversarial training. A Shrivastava, T Pfister, O Tuzel, J Susskind, W Wang, R Webb, The IEEE Conference on Computer Vision and Pattern Recognition. 201736</p>
<p>Using simulation and domain adaptation to improve efficiency of deep robotic grasping. K Bousmalis, A Irpan, P Wohlhart, Y Bai, M Kelcey, M Kalakrishnan, L Downs, J Ibarz, P Pastor, K Konolige, Robotics and Automation (ICRA). IEEE2018. 2018</p>
<p>Long short-term memory. S Hochreiter, J Schmidhuber, Neural computation. 981997</p>
<p>. G Brockman, V Cheung, L Pettersson, J Schneider, J Schulman, J Tang, W Zaremba, arXiv:1606.015402016Openai gym. arXiv preprint</p>
<p>Back to reality: Crossing the reality gap in evolutionary robotics. J C Zagal, J Ruiz-Del Solar, P Vallejos, IFAC Proceedings Volumes. 200437</p>
<p>Once more unto the breach: Co-evolving a robot and its simulator. J Bongard, H Lipson, Proceedings of the Ninth International Conference on the Simulation and Synthesis of Living Systems (ALIFE9). the Ninth International Conference on the Simulation and Synthesis of Living Systems (ALIFE9)2004</p>
<p>Crossing the reality gap in evolutionary robotics by promoting transferable controllers. S Koos, J.-B Mouret, S Doncieux, Proceedings of the 12th annual conference on Genetic and evolutionary computation. the 12th annual conference on Genetic and evolutionary computationACM2010</p>
<p>Combining simulation and reality in evolutionary robotics. J C Zagal, J Ruiz-Del-Solar, Journal of Intelligent and Robotic Systems. 5012007</p>
<p>Adapting learned robotics behaviours through policy adjustment. J C G Higuera, D Meger, G Dudek, Robotics and Automation (ICRA), 2017 IEEE International Conference on. IEEE2017</p>
<p>Noise and the reality gap: The use of simulation in evolutionary robotics. N Jakobi, P Husbands, I Harvey, European Conference on Artificial Life. Springer1995</p>
<p>Deep learning helicopter dynamics models. A Punjani, P Abbeel, Robotics and Automation (ICRA), 2015 IEEE International Conference on. IEEE2015</p>
<p>One-shot learning of manipulation skills with online dynamics adaptation and neural network priors. J Fu, S Levine, P Abbeel, Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on. IEEE2016</p>
<p>Combining model-based policy search with online model learning for control of physical humanoids. I Mordatch, N Mishra, C Eppner, P Abbeel, Robotics and Automation (ICRA), 2016 IEEE International Conference on. IEEE2016</p>
<p>A Nagabandi, G Kahn, R S Fearing, S Levine, arXiv:1708.02596Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. 2017arXiv preprint</p>
<p>Pilco: A model-based and data-efficient approach to policy search. M Deisenroth, C E Rasmussen, Proceedings of the 28th International Conference on machine learning (ICML-11). the 28th International Conference on machine learning (ICML-11)2011</p>
<p>Guided cost learning: Deep inverse optimal control via policy optimization. C Finn, S Levine, P Abbeel, International Conference on Machine Learning. 2016</p>
<p>How transferable are features in deep neural networks?. J Yosinski, J Clune, Y Bengio, H Lipson, Advances in neural information processing systems. 2014</p>
<p>Domain-adversarial training of neural networks. Y Ganin, E Ustinova, H Ajakan, P Germain, H Larochelle, F Laviolette, M Marchand, V Lempitsky, Journal of Machine Learning Research. 17592016</p>
<p>Unsupervised cross-domain image generation. Y Taigman, A Polyak, L Wolf, arXiv:1611.022002016arXiv preprint</p>
<p>Deep domain confusion: Maximizing for domain invariance. E Tzeng, J Hoffman, N Zhang, K Saenko, T Darrell, arXiv:1412.34742014arXiv preprint</p>
<p>S James, E Johns, arXiv:1609.037593d simulation for robot arm control with deep q-learning. 2016arXiv preprint</p>
<p>A A Rusu, M Vecerik, T Rothörl, N Heess, R Pascanu, R Hadsell, arXiv:1610.04286Sim-to-real robot learning from pixels with progressive nets. 2016arXiv preprint</p>
<p>J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, arXiv:1703.06907Domain randomization for transferring deep neural networks from simulation to the real world. 2017arXiv preprint</p>
<p>X B Peng, M Andrychowicz, W Zaremba, P Abbeel, arXiv:1710.06537Sim-to-real transfer of robotic control with dynamics randomization. 2017arXiv preprint</p>
<p>Grounded action transformation for robot learning in simulation. J Hanna, P Stone, Proceedings of the 31st AAAI Conference on Artificial Intelligence (AAAI). the 31st AAAI Conference on Artificial Intelligence (AAAI)February 2017</p>
<p>Adam: A method for stochastic optimization. D P Kingma, J Ba, Proceedings of the International Conference on Learning Representations. the International Conference on Learning Representations2015</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Mujoco: A physics engine for model-based control. E Todorov, T Erez, Y Tassa, Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on. IEEE2012</p>
<p>Poppy project: open-source fabrication of 3d printed humanoid robot for science, education and art. M Lapeyre, P Rouanet, J Grizou, S Nguyen, F Depraetre, A Le, P.-Y Falher, Oudeyer, Digital Intelligence. 2014. 20146</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1955 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1955</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1955</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-41.html">extraction-schema-41</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <p><strong>Paper ID:</strong> paper-277467951</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.24361v1.pdf" target="_blank">Sim-and-Real Co-Training: A Simple Recipe for Vision-Based Robotic Manipulation</a></p>
                <p><strong>Paper Abstract:</strong> Large real-world robot datasets hold great potential to train generalist robot models, but scaling real-world human data collection is time-consuming and resource-intensive. Simulation has great potential in supplementing large-scale data, especially with recent advances in generative AI and automated data generation tools that enable scalable creation of robot behavior datasets. However, training a policy solely in simulation and transferring it to the real world often demands substantial human effort to bridge the reality gap. A compelling alternative is to co-train the policy on a mixture of simulation and real-world datasets. Preliminary studies have recently shown this strategy to substantially improve the performance of a policy over one trained on a limited amount of real-world data. Nonetheless, the community lacks a systematic understanding of sim-and-real co-training and what it takes to reap the benefits of simulation data for real-robot learning. This work presents a simple yet effective recipe for utilizing simulation data to solve vision-based robotic manipulation tasks. We derive this recipe from comprehensive experiments that validate the co-training strategy on various simulation and real-world datasets. Using two domains--a robot arm and a humanoid--across diverse tasks, we demonstrate that simulation data can enhance real-world task performance by an average of 38%, even with notable differences between the simulation and real-world data. Videos and additional results can be found at https://co-training.github.io/</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1955.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1955.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>C2S-PnP (Panda)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CounterToSink Pick-and-Place (Franka Emika Panda)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sim-to-real co-training experiment where a visuomotor policy is trained on a mix of 50 real demonstrations and up to 10k task-aware digital-cousin (DC) simulation trajectories to pick an object from a counter and place it into a sink basin on a Panda arm.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>pick-and-place (counter to sink)</td>
                        </tr>
                        <tr>
                            <td><strong>task_timescale</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_contact_ratio</strong></td>
                            <td>contact-rich</td>
                        </tr>
                        <tr>
                            <td><strong>task_precision_requirement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_modeled</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level_description</strong></td>
                            <td>high-fidelity physics simulation for scene and robot kinematics using roboCasa renderer / MimicGen-generated trajectories; actuator-level modeling details not reported</td>
                        </tr>
                        <tr>
                            <td><strong>parameter_specific_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>task success rate (fraction of successful place-in-sink trials); e.g., Real+DC achieved 67% in one ablation (10k DC demos) vs Real-only lower (value not listed here for that exact run in-text)</td>
                        </tr>
                        <tr>
                            <td><strong>sim_vs_real_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_performed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_results</strong></td>
                            <td>Reducing DC simulations from 10k to 500 decreased success from 67% to 53%; camera misalignment (unaligned DC) dropped success from 67% to 56%; co-training ratio tuning important (high α favored). Dynamics alignment not reported to affect this task specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_reported</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td>Compared aligned vs default (misaligned) camera views: aligned DC gives substantially better transfer (67% vs 56%). No detailed actuator-fidelity vs performance comparison reported.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Visual / texture diversity applied (rendering with many textures) for DC; no actuator parameter randomization reported.</td>
                        </tr>
                        <tr>
                            <td><strong>robot_type</strong></td>
                            <td>7-DoF Franka Emika Panda arm with parallel-jaw gripper (end-effector delta control)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_for_theory</strong></td>
                            <td>For pick-and-place on the Panda arm, semantic/task alignment and camera viewpoint alignment of task-aware simulation and large quantities of sim trajectories matter far more for sim-to-real transfer than explicit tuning of actuator/dynamics parameters (actuator-level modeling details were not required to achieve strong transfer).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1955.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1955.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>C2C-PnP (Panda)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CounterToCabinet Pick-and-Place (Franka Emika Panda)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sim-and-real co-training experiment for moving objects from counter to cabinet using 50 real demonstrations and task-aware digital-cousin simulation data to improve real-world pick-and-place performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>pick-and-place (counter to cabinet)</td>
                        </tr>
                        <tr>
                            <td><strong>task_timescale</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_contact_ratio</strong></td>
                            <td>contact-rich</td>
                        </tr>
                        <tr>
                            <td><strong>task_precision_requirement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_modeled</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level_description</strong></td>
                            <td>physics-based simulation with task-aware scene alignment (digital cousin); actuator modeling details not specified</td>
                        </tr>
                        <tr>
                            <td><strong>parameter_specific_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>task success rate (placing object into cabinet); aggregated across objects (average success reported in tables but per-task numeric not always explicitly listed in-text)</td>
                        </tr>
                        <tr>
                            <td><strong>sim_vs_real_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_performed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_results</strong></td>
                            <td>Co-training with DC significantly improved average success vs real-only; camera alignment and number of sim demos influenced performance similarly to CounterToSinkPnP. Specific actuator-parameter sensitivity not performed.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_reported</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>DC rendered with many textures for visual diversity; no dynamics randomization reported.</td>
                        </tr>
                        <tr>
                            <td><strong>robot_type</strong></td>
                            <td>7-DoF Franka Emika Panda arm with parallel-jaw gripper</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_for_theory</strong></td>
                            <td>Task-aware simulation that preserves task goals, object categories, and approximate initialization distributions substantially aids transfer for cabinet pick-and-place; actuator/dynamics tuning was not highlighted as necessary.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1955.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1955.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CloseDoor (Panda)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CloseDoor (Franka Emika Panda)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sim-and-real co-training for closing an overhead cabinet door: policies trained with real and DC simulation data show large gains; also examined effect of increasing real-demo count.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>articulated-object manipulation (door closing)</td>
                        </tr>
                        <tr>
                            <td><strong>task_timescale</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_contact_ratio</strong></td>
                            <td>contact-rich (hinged contact)</td>
                        </tr>
                        <tr>
                            <td><strong>task_precision_requirement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_modeled</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level_description</strong></td>
                            <td>task-aware physics simulation (digital cousin) of articulated object interactions; actuator/dynamics fidelity not described</td>
                        </tr>
                        <tr>
                            <td><strong>parameter_specific_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>task success rate (door joint angle < 5° after action); Real-only with 50 demos had large gap to co-trained policies; Real-only with 100 demos achieved 80% success</td>
                        </tr>
                        <tr>
                            <td><strong>sim_vs_real_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_reported</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>visual variations used in DC; no actuator parameter randomization reported</td>
                        </tr>
                        <tr>
                            <td><strong>robot_type</strong></td>
                            <td>7-DoF Franka Emika Panda arm</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_analysis</strong></td>
                            <td>Real policy performance plateaued (80% with 100 demos) indicating task difficulty beyond mere data scaling; no specific actuator cause identified.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_for_theory</strong></td>
                            <td>For articulated-object tasks like closing a door, adding DC simulation helps substantially, but simply increasing real demos does not guarantee perfect success — dynamics/contacts may still limit performance, though the paper did not attribute failures to modeled actuator dynamics.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1955.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1955.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CupPnP (GR-1)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cup Pick-and-Place (Fourier GR-1 humanoid)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Humanoid sim-to-real co-training where policies are trained with 20 real demos and up to 1k DC simulations (DexMimicGen) to move a cup from a plate to the table; reports high transfer success and ablations on dynamics alignment, co-training ratio, and camera alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>pick-and-place (cup, humanoid first-person view)</td>
                        </tr>
                        <tr>
                            <td><strong>task_timescale</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_contact_ratio</strong></td>
                            <td>contact-rich</td>
                        </tr>
                        <tr>
                            <td><strong>task_precision_requirement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_modeled</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level_description</strong></td>
                            <td>physics simulation with DexMimicGen-generated humanoid trajectories; actuator/dynamics modeling specifics not provided (used IK controller in real robot; simulation used same embodiment)</td>
                        </tr>
                        <tr>
                            <td><strong>parameter_specific_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>task success rate; reported values include Real-only on novel objects 10% vs co-trained 80% (generalization); co-trained Real+DC achieved up to 95% success in some ablations (with 1k DC demos and tuned α)</td>
                        </tr>
                        <tr>
                            <td><strong>sim_vs_real_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_performed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_results</strong></td>
                            <td>Tuning co-training ratio strongly affected success: α=99% optimal; α values like 99.5%/99.9% degraded success (from 95% to 60%). Camera alignment improved transfer (aligned vs default: 95% vs 70%). Dynamics alignment attempts (tuning physics parameters) produced no change in success (95% with or without dynamic alignment), suggesting low sensitivity to actuator/dynamics parameter tuning for this task.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_reported</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td>Compared dynamics-aligned vs not: no observed difference in success for GR-1 CupPnP; compared aligned vs default camera views: aligned better (95% vs 70%).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Visual texture randomization and Vid2Vid visual realism augmentations were tested; actuator-parameter randomization not used.</td>
                        </tr>
                        <tr>
                            <td><strong>robot_type</strong></td>
                            <td>Fourier GR-1 humanoid (arms + dexterous hands), first-person RGB camera</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_for_theory</strong></td>
                            <td>For humanoid pick-and-place, co-training with large amounts of task-aware sim data and careful co-training ratio/camera alignment yields strong transfer; explicit dynamic (actuator) alignment/tuning was unnecessary — performance was insensitive to dynamics-parameter tuning in this task.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1955.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1955.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MilkPnP (GR-1)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Milk Pick-and-Place (Fourier GR-1 humanoid)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simulation-and-real co-training for moving a milk carton from table to second-level shelf on the GR-1 humanoid using DexMimicGen-synthesized trajectories and 20 real demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>pick-and-place (milk carton, humanoid)</td>
                        </tr>
                        <tr>
                            <td><strong>task_timescale</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_contact_ratio</strong></td>
                            <td>contact-rich</td>
                        </tr>
                        <tr>
                            <td><strong>task_precision_requirement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_modeled</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level_description</strong></td>
                            <td>task-aware physics simulation (digital cousin) with DexMimicGen; no explicit actuator dynamics modeling details provided</td>
                        </tr>
                        <tr>
                            <td><strong>parameter_specific_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>task success rate (placement into shelf level); reported aggregated success rates for co-trained policies outperform Real-only, exact per-task numeric values provided in appendix tables but not all in main text</td>
                        </tr>
                        <tr>
                            <td><strong>sim_vs_real_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_reported</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>visual diversity applied in DC rendering; no actuator parameter randomization reported</td>
                        </tr>
                        <tr>
                            <td><strong>robot_type</strong></td>
                            <td>Fourier GR-1 humanoid</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_for_theory</strong></td>
                            <td>Task-aware simulation that preserves object categories and approximate initialization distributions improves transfer for humanoid pick-and-place, without needing detailed actuator dynamics tuning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1955.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1955.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pouring (GR-1)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pouring (GR-1 humanoid): pick cup containing ping-pong ball and pour into bowl</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sim-and-real co-training on a contact-rich, non-prehensile-like pouring task with 20 real demonstrations and 1k DC simulated demos; used to test generalization and co-training effects for a humanoid.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>non-prehensile manipulation (pouring with object transfer)</td>
                        </tr>
                        <tr>
                            <td><strong>task_timescale</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_contact_ratio</strong></td>
                            <td>highly contact-rich</td>
                        </tr>
                        <tr>
                            <td><strong>task_precision_requirement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_modeled</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level_description</strong></td>
                            <td>physics simulation modeling articulated object interactions; actuator dynamics descriptions not provided</td>
                        </tr>
                        <tr>
                            <td><strong>parameter_specific_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>task success rate (ball ends up in bowl); co-trained policies outperform Real-only; numeric specifics in appendix</td>
                        </tr>
                        <tr>
                            <td><strong>sim_vs_real_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_reported</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>DC rendered with variability; no actuator randomization reported</td>
                        </tr>
                        <tr>
                            <td><strong>robot_type</strong></td>
                            <td>Fourier GR-1 humanoid</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_for_theory</strong></td>
                            <td>Even for contact-rich pouring, task-aware DC simulation improves real-world performance, but the paper does not report actuator-dynamics modeling details — suggesting visual/semantic alignment and data diversity are primary drivers in these experiments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1955.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1955.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MultiTaskPnP (GR-1)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MultiTask Pick-and-Place (Four tasks, GR-1 humanoid)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multi-task sim-and-real co-training where a single policy is trained across four pick-and-place variations (4k DC demos total) and varying numbers of real demos; used to test co-training benefits in data-rich settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>multi-task pick-and-place (cutting board/mat/plate to basket/pan/bowl)</td>
                        </tr>
                        <tr>
                            <td><strong>task_timescale</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_contact_ratio</strong></td>
                            <td>contact-rich</td>
                        </tr>
                        <tr>
                            <td><strong>task_precision_requirement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_modeled</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level_description</strong></td>
                            <td>task-aware digital-cousin simulations aggregated across tasks; actuator modeling not described</td>
                        </tr>
                        <tr>
                            <td><strong>parameter_specific_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>average task success rate across tasks; e.g., Real-only 30.6% vs Real+DC 70.8% (with 200 real demos and 4k DC demos) and Real+DC+Prior 75.7%</td>
                        </tr>
                        <tr>
                            <td><strong>sim_vs_real_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_performed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_results</strong></td>
                            <td>Co-training remains beneficial even with up to 400 real demos; performance scales with more real data but cotrained policies consistently outperform real-only; no actuator-parameter sensitivity reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_reported</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>visual textures and object instance randomization used in DC; no actuator-dynamics randomization</td>
                        </tr>
                        <tr>
                            <td><strong>robot_type</strong></td>
                            <td>Fourier GR-1 humanoid</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_analysis</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_for_theory</strong></td>
                            <td>In multi-task settings, large amounts of task-aware sim data substantially raise real-world success even when real data is plentiful; actuator-dynamics fidelity was not identified as a limiting factor in these experiments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1955.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1955.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BimanualPnP (GR-1)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bimanual Pick-and-Place (GR-1 humanoid bimanual task)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Bimanual sim-and-real co-training experiment where bimanual behavior was difficult to learn from task-agnostic prior sim datasets but improves substantially when co-training with task-aware DC (50 real demos + 1k DC → 50% vs 15% real-only).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>bimanual pick-and-place (two-hand sequence)</td>
                        </tr>
                        <tr>
                            <td><strong>task_timescale</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_contact_ratio</strong></td>
                            <td>contact-rich and sequential</td>
                        </tr>
                        <tr>
                            <td><strong>task_precision_requirement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_modeled</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level_description</strong></td>
                            <td>DexMimicGen-generated bimanual simulated trajectories in RoboCasa; actuator-level dynamics not specified</td>
                        </tr>
                        <tr>
                            <td><strong>parameter_specific_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>task success rate; real-only (50 demos) 15% → Real+DC (1k DC + 50 real) 50%; 100 real-only gives 30%</td>
                        </tr>
                        <tr>
                            <td><strong>sim_vs_real_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_reported</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td>Task-agnostic prior sim led to wrong behavior (single-arm) when co-trained; task-aware DC necessary to encode correct bimanual behavior. No actuator fidelity comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>visual and instance diversity used in DC; actuator parameter randomization not used</td>
                        </tr>
                        <tr>
                            <td><strong>robot_type</strong></td>
                            <td>Fourier GR-1 humanoid (bimanual)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_analysis</strong></td>
                            <td>When co-training with task-agnostic prior datasets that had only single-arm behaviors, the learned policy adopted single-arm behavior and failed bimanual task — indicates behavior-pattern mismatch between sim and real is critical, more so than low-level actuator fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_for_theory</strong></td>
                            <td>Behavioral consistency between simulation and real (i.e., matching the correct multi-arm behavior) is crucial for sim-to-real transfer in bimanual tasks; having correct behavior patterns in simulation matters more than detailed actuator-parameter tuning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sim-to-real transfer of robotic control with dynamics randomization <em>(Rating: 2)</em></li>
                <li>Bayessim: adaptive domain randomization via probabilistic inference for robotics simulators <em>(Rating: 2)</em></li>
                <li>Neural posterior domain randomization <em>(Rating: 2)</em></li>
                <li>Real2sim2real: Self-supervised learning of physical single-step dynamic actions for planar robot casting <em>(Rating: 2)</em></li>
                <li>Adaptsim: Taskdriven simulation adaptation for sim-to-real transfer <em>(Rating: 2)</em></li>
                <li>Closing the sim-to-real loop: Adapting simulation randomization with real world experience <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1955",
    "paper_id": "paper-277467951",
    "extraction_schema_id": "extraction-schema-41",
    "extracted_data": [
        {
            "name_short": "C2S-PnP (Panda)",
            "name_full": "CounterToSink Pick-and-Place (Franka Emika Panda)",
            "brief_description": "Sim-to-real co-training experiment where a visuomotor policy is trained on a mix of 50 real demonstrations and up to 10k task-aware digital-cousin (DC) simulation trajectories to pick an object from a counter and place it into a sink basin on a Panda arm.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_name": "pick-and-place (counter to sink)",
            "task_timescale": null,
            "task_contact_ratio": "contact-rich",
            "task_precision_requirement": null,
            "actuator_parameters_modeled": null,
            "actuator_parameters_simplified": null,
            "fidelity_level_description": "high-fidelity physics simulation for scene and robot kinematics using roboCasa renderer / MimicGen-generated trajectories; actuator-level modeling details not reported",
            "parameter_specific_fidelity": null,
            "transfer_success_metric": "task success rate (fraction of successful place-in-sink trials); e.g., Real+DC achieved 67% in one ablation (10k DC demos) vs Real-only lower (value not listed here for that exact run in-text)",
            "sim_vs_real_performance": null,
            "sensitivity_analysis_performed": true,
            "sensitivity_analysis_results": "Reducing DC simulations from 10k to 500 decreased success from 67% to 53%; camera misalignment (unaligned DC) dropped success from 67% to 56%; co-training ratio tuning important (high α favored). Dynamics alignment not reported to affect this task specifically.",
            "computational_cost_reported": false,
            "computational_cost_details": null,
            "fidelity_comparison": "Compared aligned vs default (misaligned) camera views: aligned DC gives substantially better transfer (67% vs 56%). No detailed actuator-fidelity vs performance comparison reported.",
            "domain_randomization_used": false,
            "domain_randomization_details": "Visual / texture diversity applied (rendering with many textures) for DC; no actuator parameter randomization reported.",
            "robot_type": "7-DoF Franka Emika Panda arm with parallel-jaw gripper (end-effector delta control)",
            "transfer_failure_analysis": null,
            "key_finding_for_theory": "For pick-and-place on the Panda arm, semantic/task alignment and camera viewpoint alignment of task-aware simulation and large quantities of sim trajectories matter far more for sim-to-real transfer than explicit tuning of actuator/dynamics parameters (actuator-level modeling details were not required to achieve strong transfer).",
            "uuid": "e1955.0"
        },
        {
            "name_short": "C2C-PnP (Panda)",
            "name_full": "CounterToCabinet Pick-and-Place (Franka Emika Panda)",
            "brief_description": "Sim-and-real co-training experiment for moving objects from counter to cabinet using 50 real demonstrations and task-aware digital-cousin simulation data to improve real-world pick-and-place performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_name": "pick-and-place (counter to cabinet)",
            "task_timescale": null,
            "task_contact_ratio": "contact-rich",
            "task_precision_requirement": null,
            "actuator_parameters_modeled": null,
            "actuator_parameters_simplified": null,
            "fidelity_level_description": "physics-based simulation with task-aware scene alignment (digital cousin); actuator modeling details not specified",
            "parameter_specific_fidelity": null,
            "transfer_success_metric": "task success rate (placing object into cabinet); aggregated across objects (average success reported in tables but per-task numeric not always explicitly listed in-text)",
            "sim_vs_real_performance": null,
            "sensitivity_analysis_performed": true,
            "sensitivity_analysis_results": "Co-training with DC significantly improved average success vs real-only; camera alignment and number of sim demos influenced performance similarly to CounterToSinkPnP. Specific actuator-parameter sensitivity not performed.",
            "computational_cost_reported": false,
            "computational_cost_details": null,
            "fidelity_comparison": null,
            "domain_randomization_used": false,
            "domain_randomization_details": "DC rendered with many textures for visual diversity; no dynamics randomization reported.",
            "robot_type": "7-DoF Franka Emika Panda arm with parallel-jaw gripper",
            "transfer_failure_analysis": null,
            "key_finding_for_theory": "Task-aware simulation that preserves task goals, object categories, and approximate initialization distributions substantially aids transfer for cabinet pick-and-place; actuator/dynamics tuning was not highlighted as necessary.",
            "uuid": "e1955.1"
        },
        {
            "name_short": "CloseDoor (Panda)",
            "name_full": "CloseDoor (Franka Emika Panda)",
            "brief_description": "Sim-and-real co-training for closing an overhead cabinet door: policies trained with real and DC simulation data show large gains; also examined effect of increasing real-demo count.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_name": "articulated-object manipulation (door closing)",
            "task_timescale": null,
            "task_contact_ratio": "contact-rich (hinged contact)",
            "task_precision_requirement": null,
            "actuator_parameters_modeled": null,
            "actuator_parameters_simplified": null,
            "fidelity_level_description": "task-aware physics simulation (digital cousin) of articulated object interactions; actuator/dynamics fidelity not described",
            "parameter_specific_fidelity": null,
            "transfer_success_metric": "task success rate (door joint angle &lt; 5° after action); Real-only with 50 demos had large gap to co-trained policies; Real-only with 100 demos achieved 80% success",
            "sim_vs_real_performance": null,
            "sensitivity_analysis_performed": false,
            "sensitivity_analysis_results": null,
            "computational_cost_reported": false,
            "computational_cost_details": null,
            "fidelity_comparison": null,
            "domain_randomization_used": false,
            "domain_randomization_details": "visual variations used in DC; no actuator parameter randomization reported",
            "robot_type": "7-DoF Franka Emika Panda arm",
            "transfer_failure_analysis": "Real policy performance plateaued (80% with 100 demos) indicating task difficulty beyond mere data scaling; no specific actuator cause identified.",
            "key_finding_for_theory": "For articulated-object tasks like closing a door, adding DC simulation helps substantially, but simply increasing real demos does not guarantee perfect success — dynamics/contacts may still limit performance, though the paper did not attribute failures to modeled actuator dynamics.",
            "uuid": "e1955.2"
        },
        {
            "name_short": "CupPnP (GR-1)",
            "name_full": "Cup Pick-and-Place (Fourier GR-1 humanoid)",
            "brief_description": "Humanoid sim-to-real co-training where policies are trained with 20 real demos and up to 1k DC simulations (DexMimicGen) to move a cup from a plate to the table; reports high transfer success and ablations on dynamics alignment, co-training ratio, and camera alignment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_name": "pick-and-place (cup, humanoid first-person view)",
            "task_timescale": null,
            "task_contact_ratio": "contact-rich",
            "task_precision_requirement": null,
            "actuator_parameters_modeled": null,
            "actuator_parameters_simplified": null,
            "fidelity_level_description": "physics simulation with DexMimicGen-generated humanoid trajectories; actuator/dynamics modeling specifics not provided (used IK controller in real robot; simulation used same embodiment)",
            "parameter_specific_fidelity": null,
            "transfer_success_metric": "task success rate; reported values include Real-only on novel objects 10% vs co-trained 80% (generalization); co-trained Real+DC achieved up to 95% success in some ablations (with 1k DC demos and tuned α)",
            "sim_vs_real_performance": null,
            "sensitivity_analysis_performed": true,
            "sensitivity_analysis_results": "Tuning co-training ratio strongly affected success: α=99% optimal; α values like 99.5%/99.9% degraded success (from 95% to 60%). Camera alignment improved transfer (aligned vs default: 95% vs 70%). Dynamics alignment attempts (tuning physics parameters) produced no change in success (95% with or without dynamic alignment), suggesting low sensitivity to actuator/dynamics parameter tuning for this task.",
            "computational_cost_reported": false,
            "computational_cost_details": null,
            "fidelity_comparison": "Compared dynamics-aligned vs not: no observed difference in success for GR-1 CupPnP; compared aligned vs default camera views: aligned better (95% vs 70%).",
            "domain_randomization_used": false,
            "domain_randomization_details": "Visual texture randomization and Vid2Vid visual realism augmentations were tested; actuator-parameter randomization not used.",
            "robot_type": "Fourier GR-1 humanoid (arms + dexterous hands), first-person RGB camera",
            "transfer_failure_analysis": null,
            "key_finding_for_theory": "For humanoid pick-and-place, co-training with large amounts of task-aware sim data and careful co-training ratio/camera alignment yields strong transfer; explicit dynamic (actuator) alignment/tuning was unnecessary — performance was insensitive to dynamics-parameter tuning in this task.",
            "uuid": "e1955.3"
        },
        {
            "name_short": "MilkPnP (GR-1)",
            "name_full": "Milk Pick-and-Place (Fourier GR-1 humanoid)",
            "brief_description": "Simulation-and-real co-training for moving a milk carton from table to second-level shelf on the GR-1 humanoid using DexMimicGen-synthesized trajectories and 20 real demonstrations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_name": "pick-and-place (milk carton, humanoid)",
            "task_timescale": null,
            "task_contact_ratio": "contact-rich",
            "task_precision_requirement": null,
            "actuator_parameters_modeled": null,
            "actuator_parameters_simplified": null,
            "fidelity_level_description": "task-aware physics simulation (digital cousin) with DexMimicGen; no explicit actuator dynamics modeling details provided",
            "parameter_specific_fidelity": null,
            "transfer_success_metric": "task success rate (placement into shelf level); reported aggregated success rates for co-trained policies outperform Real-only, exact per-task numeric values provided in appendix tables but not all in main text",
            "sim_vs_real_performance": null,
            "sensitivity_analysis_performed": false,
            "sensitivity_analysis_results": null,
            "computational_cost_reported": false,
            "computational_cost_details": null,
            "fidelity_comparison": null,
            "domain_randomization_used": false,
            "domain_randomization_details": "visual diversity applied in DC rendering; no actuator parameter randomization reported",
            "robot_type": "Fourier GR-1 humanoid",
            "transfer_failure_analysis": null,
            "key_finding_for_theory": "Task-aware simulation that preserves object categories and approximate initialization distributions improves transfer for humanoid pick-and-place, without needing detailed actuator dynamics tuning.",
            "uuid": "e1955.4"
        },
        {
            "name_short": "Pouring (GR-1)",
            "name_full": "Pouring (GR-1 humanoid): pick cup containing ping-pong ball and pour into bowl",
            "brief_description": "Sim-and-real co-training on a contact-rich, non-prehensile-like pouring task with 20 real demonstrations and 1k DC simulated demos; used to test generalization and co-training effects for a humanoid.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_name": "non-prehensile manipulation (pouring with object transfer)",
            "task_timescale": null,
            "task_contact_ratio": "highly contact-rich",
            "task_precision_requirement": null,
            "actuator_parameters_modeled": null,
            "actuator_parameters_simplified": null,
            "fidelity_level_description": "physics simulation modeling articulated object interactions; actuator dynamics descriptions not provided",
            "parameter_specific_fidelity": null,
            "transfer_success_metric": "task success rate (ball ends up in bowl); co-trained policies outperform Real-only; numeric specifics in appendix",
            "sim_vs_real_performance": null,
            "sensitivity_analysis_performed": false,
            "sensitivity_analysis_results": null,
            "computational_cost_reported": false,
            "computational_cost_details": null,
            "fidelity_comparison": null,
            "domain_randomization_used": false,
            "domain_randomization_details": "DC rendered with variability; no actuator randomization reported",
            "robot_type": "Fourier GR-1 humanoid",
            "transfer_failure_analysis": null,
            "key_finding_for_theory": "Even for contact-rich pouring, task-aware DC simulation improves real-world performance, but the paper does not report actuator-dynamics modeling details — suggesting visual/semantic alignment and data diversity are primary drivers in these experiments.",
            "uuid": "e1955.5"
        },
        {
            "name_short": "MultiTaskPnP (GR-1)",
            "name_full": "MultiTask Pick-and-Place (Four tasks, GR-1 humanoid)",
            "brief_description": "Multi-task sim-and-real co-training where a single policy is trained across four pick-and-place variations (4k DC demos total) and varying numbers of real demos; used to test co-training benefits in data-rich settings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_name": "multi-task pick-and-place (cutting board/mat/plate to basket/pan/bowl)",
            "task_timescale": null,
            "task_contact_ratio": "contact-rich",
            "task_precision_requirement": null,
            "actuator_parameters_modeled": null,
            "actuator_parameters_simplified": null,
            "fidelity_level_description": "task-aware digital-cousin simulations aggregated across tasks; actuator modeling not described",
            "parameter_specific_fidelity": null,
            "transfer_success_metric": "average task success rate across tasks; e.g., Real-only 30.6% vs Real+DC 70.8% (with 200 real demos and 4k DC demos) and Real+DC+Prior 75.7%",
            "sim_vs_real_performance": null,
            "sensitivity_analysis_performed": true,
            "sensitivity_analysis_results": "Co-training remains beneficial even with up to 400 real demos; performance scales with more real data but cotrained policies consistently outperform real-only; no actuator-parameter sensitivity reported.",
            "computational_cost_reported": false,
            "computational_cost_details": null,
            "fidelity_comparison": null,
            "domain_randomization_used": false,
            "domain_randomization_details": "visual textures and object instance randomization used in DC; no actuator-dynamics randomization",
            "robot_type": "Fourier GR-1 humanoid",
            "transfer_failure_analysis": null,
            "key_finding_for_theory": "In multi-task settings, large amounts of task-aware sim data substantially raise real-world success even when real data is plentiful; actuator-dynamics fidelity was not identified as a limiting factor in these experiments.",
            "uuid": "e1955.6"
        },
        {
            "name_short": "BimanualPnP (GR-1)",
            "name_full": "Bimanual Pick-and-Place (GR-1 humanoid bimanual task)",
            "brief_description": "Bimanual sim-and-real co-training experiment where bimanual behavior was difficult to learn from task-agnostic prior sim datasets but improves substantially when co-training with task-aware DC (50 real demos + 1k DC → 50% vs 15% real-only).",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_name": "bimanual pick-and-place (two-hand sequence)",
            "task_timescale": null,
            "task_contact_ratio": "contact-rich and sequential",
            "task_precision_requirement": null,
            "actuator_parameters_modeled": null,
            "actuator_parameters_simplified": null,
            "fidelity_level_description": "DexMimicGen-generated bimanual simulated trajectories in RoboCasa; actuator-level dynamics not specified",
            "parameter_specific_fidelity": null,
            "transfer_success_metric": "task success rate; real-only (50 demos) 15% → Real+DC (1k DC + 50 real) 50%; 100 real-only gives 30%",
            "sim_vs_real_performance": null,
            "sensitivity_analysis_performed": false,
            "sensitivity_analysis_results": null,
            "computational_cost_reported": false,
            "computational_cost_details": null,
            "fidelity_comparison": "Task-agnostic prior sim led to wrong behavior (single-arm) when co-trained; task-aware DC necessary to encode correct bimanual behavior. No actuator fidelity comparisons.",
            "domain_randomization_used": false,
            "domain_randomization_details": "visual and instance diversity used in DC; actuator parameter randomization not used",
            "robot_type": "Fourier GR-1 humanoid (bimanual)",
            "transfer_failure_analysis": "When co-training with task-agnostic prior datasets that had only single-arm behaviors, the learned policy adopted single-arm behavior and failed bimanual task — indicates behavior-pattern mismatch between sim and real is critical, more so than low-level actuator fidelity.",
            "key_finding_for_theory": "Behavioral consistency between simulation and real (i.e., matching the correct multi-arm behavior) is crucial for sim-to-real transfer in bimanual tasks; having correct behavior patterns in simulation matters more than detailed actuator-parameter tuning.",
            "uuid": "e1955.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "rating": 2
        },
        {
            "paper_title": "Bayessim: adaptive domain randomization via probabilistic inference for robotics simulators",
            "rating": 2
        },
        {
            "paper_title": "Neural posterior domain randomization",
            "rating": 2
        },
        {
            "paper_title": "Real2sim2real: Self-supervised learning of physical single-step dynamic actions for planar robot casting",
            "rating": 2
        },
        {
            "paper_title": "Adaptsim: Taskdriven simulation adaptation for sim-to-real transfer",
            "rating": 2
        },
        {
            "paper_title": "Closing the sim-to-real loop: Adapting simulation randomization with real world experience",
            "rating": 1
        }
    ],
    "cost": 0.0168775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Sim-and-Real Co-Training: A Simple Recipe for Vision-Based Robotic Manipulation</p>
<p>Abhiram Maddukuri 
New York University</p>
<p>Zhenyu Jiang 
New York University</p>
<p>Lawrence Yunliang Chen 
New York University</p>
<p>Soroush Nasiriany 
New York University</p>
<p>Yuqi Xie 
New York University</p>
<p>Yu Fang 
New York University</p>
<p>Wenqi Huang 
New York University</p>
<p>Zu Wang 
New York University</p>
<p>Zhenjia Xu 
New York University</p>
<p>Nikita Chernyadev 
New York University</p>
<p>Scott Reed 
New York University</p>
<p>Ken Goldberg 
New York University</p>
<p>Ajay Mandlekar 
New York University</p>
<p>Linxi Fan 
New York University</p>
<p>Yuke Zhu 
New York University</p>
<p>U T Austin 
New York University</p>
<p>U C Berkeley 
New York University</p>
<p>Sim-and-Real Co-Training: A Simple Recipe for Vision-Based Robotic Manipulation
6E8060DE9A3C1EF3A14DA379C85641E9
Large real-world robot datasets hold great potential to train generalist robot models, but scaling real-world human data collection is time-consuming and resource-intensive.Simulation has great potential in supplementing large-scale data, especially with recent advances in generative AI and automated data generation tools that enable scalable creation of robot behavior datasets.However, training a policy solely in simulation and transferring it to the real world often demands substantial human effort to bridge the reality gap.A compelling alternative is to co-train the policy on a mixture of simulation and real-world datasets.Preliminary studies have recently shown this strategy to substantially improve the performance of a policy over one trained on a limited amount of real-world data.Nonetheless, the community lacks a systematic understanding of sim-and-real cotraining and what it takes to reap the benefits of simulation data for real-robot learning.This work presents a simple yet effective recipe for utilizing simulation data to solve vision-based robotic manipulation tasks.We derive this recipe from comprehensive experiments that validate the co-training strategy on various simulation and real-world datasets.Using two domains-a robot arm and a humanoid-across diverse tasks, we demonstrate that simulation data can enhance real-world task performance by an average of 38%, even with notable differences between the simulation and real-world data.Videos and additional results can be found at co-training.github.io.* Equal contribution.† Project leads.Simulation DataPolicy Co-Training DeploymentTraining on real-world and sim data Deploy to realworld environmentsReal-World Data Real-World Task Task-Aware Simulation Task-Agnostic Simulation Deployment task in real world No sim-to-real gap Expensive to collect data Aligned with real-world tasks Automated data generation Simulation tuning required Preexist &amp; use out of the box Many tasks &amp; environments Orders more data Least target task alignment Real-world task Collect dozens of demos via teleoperation Task-agnostic prior simulation datasets Create digital cousin task Real-world data (1x) Collect dozens of demos, multiply via DexMimicGen Digital cousin sim data (100x)</p>
<p>I. INTRODUCTION</p>
<p>The ability to generalize across diverse environments and tasks is a critical step toward realizing generalist robotic systems.Recent advancements in robot foundation models [1]- [3]-trained on a mixture of web-scale vision-language datasets and robot-specific datasets-have demonstrated significant potential for cross-domain generalization.Large realworld robot datasets [4], [5] embody this diversity and are a crucial source of data.Despite this progress, challenges remain in achieving reliable real-world deployment.To bridge this gap, there have recently been multiple efforts on collecting even larger-scale real-robot datasets [1], [6].These works have shown the potential of data-driven methods in acquiring versatile robotic skills.However, they involve considerable cost, time, and scalability challenges, and it remains unclear whether simply scaling real-world data collection alone is sufficient to train generalist robot models.</p>
<p>Simulation is a promising alternative to mitigate the data hunger of large models.The recent proliferation of genera-Fig.1: Sim-and-Real Co-Training.We show how co-training policies on real-world and simulation data can attain superior performance in the real-robot deployment, compared to training solely on real-world data.We specifically study two forms of simulation data: (1) task-aware data from digital cousins built with knowledge of the real-world tasks, and (2) task-agnostic data from multi-task prior simulations covering more diverse settings but with less alignment to the target task.</p>
<p>tive AI tools allows for the automated generation of assets, scenes, and tasks in simulation, all of which can be produced with high-fidelity physics simulators and photorealistic renderers [7], [8].Furthermore, automated data generation tools can be applied to these simulation environments to synthesize large amounts of diverse, high-quality robot trajectories with arXiv:2503.24361v2[cs.RO] 2 Apr 2025 minimal human effort [9]- [12], offering massive training data for generalist manipulation policies.However, approaches that use simulation data must deal with the reality gap since the visuals and physics in simulation do not align perfectly with the real world.Prior approaches on sim-to-real policy transfer typically rely on extensive tuning of simulation to match the real world [13]- [17], or meticulously randomizing a specific set of simulation parameters [18]- [22].Such approaches can require significant human effort.</p>
<p>A compelling alternative to sim-to-real transfer is to directly co-train policies on a mixture of simulation and real-world data.Preliminary findings in recent work [7], [23], [24] suggest that incorporating simulation data in this way can greatly improve policy performance compared to using real-world data alone.Moreover, sim-and-real co-training may not require the high level of alignment between simulation and reality typically needed for sim-to-real transfer, making it a promising strategy to tap into the potential of large synthetic datasets with minimal human effort.Despite its promise, the community lacks a systematic understanding of this strategy and what it takes to reap the benefits of simulation data for real-robot learning.It remains unclear how different the simulation data can be from the real-world data, and what kinds of dataset mixtures and compositions are ideal.</p>
<p>This work presents a simple recipe for supplementing real robot datasets with synthetic simulation datasets to facilitate learning vision-based manipulation policies for real robots.We derive this recipe by conducting a comprehensive set of experiments that co-train robot policies on various simulation and real-world datasets.As shown in Figure 1, we focus on two concrete sources of simulation data -task-aware simulation, in which the simulation environment is intentionally designed to align with the real world loosely, akin to the "digital cousin" concept introduced in Dai et al. [25], and taskagnostic simulation, which comprises prior simulation data made independently of the particular target task.Our study is carried out on two distinct robot embodiments (robot arm and humanoid) across several diverse tasks, spanning pickand-place, articulated object manipulation, and non-prehensile manipulation (e.g., pouring).We investigate a number of critical data composition factors to understand the degree to which simulation and real-world data must be aligned for co-training to be effective.For example, should the tasks, scenes, and objects be the same between the sim and the real?How about the location of workspace cameras and object placements?We make use of synthetic data generation tools [9], [10] to test different simulation dataset compositions with ease, resulting in actionable insights for robotics practitioners.</p>
<p>We summarize our contributions as follows:</p>
<p>1) We establish a systematic study for co-training on realrobot data and synthetically generated data from simulation, resulting in a simple recipe to leverage synthetic simulation data for real-world manipulation; 2) We demonstrate empirically how co-training on synthetic simulation data can be broadly useful in facilitating policy learning for downstream real-world tasks, improving policy performance across two domains by an average of 38%; 3) We derive insight into what types of simulation data are most effective for sim-and-real co-training.Surprisingly, we find that simulation data provides substantial benefits even with notable differences from the real-world data, and that diverse simulation data can facilitate generalization to unseen scenarios in the real world.</p>
<p>II. RELATED WORK</p>
<p>A. Learning Manipulation from Demonstration Data</p>
<p>Behavior cloning [26] is a widely adopted approach for learning robot policies from demonstration data [27]- [37].In this framework, policies are trained to predict actions based on ground truth state-action pairs provided in a demonstration dataset.This method has been extensively applied in robot manipulation tasks [38]- [49].However, its success in realworld applications typically hinges on the availability of large amounts of high-quality demonstration data, which can be prohibitively expensive to collect.To address this limitation, our work explores the use of simulation data to enhance realworld robot manipulation through imitation learning, thereby reducing the dependency on costly real-world data collection.</p>
<p>B. Sim-to-Real and Sim-Real Co-Training</p>
<p>Sim-to-real transfer has been a pivotal focus in robotics research, aimed at enabling models trained in simulation to perform effectively in the real world.One popular approach is domain randomization [18]- [22], [50], [51], which introduces variability into the simulation environment to train policies that are robust to discrepancies between simulation and reality.However, domain randomization approaches can require careful tuning and a significant human burden to determine proper randomization ranges for the parameters that enable the policy to transfer to the real world.Another common approach seeks to minimize the sim-to-real gap by improving simulation fidelity to match the real world closely.Techniques such as system identification [13]- [17], [52]- [54] and the creation of digital twins [55], [56] aim to align simulated dynamics more closely with real-world conditions.These methods often require significant human effort, preventing their applicability to diverse tasks and environments.Instead, recent research has trained real-world manipulation policies using a mixture of simulation and real-world data [3], [7], [23], [24], [57], and demonstrated superior performance to solely using the same quantity of real-world data.Furthermore, the simulation data in these approaches does not necessarily need to be perfectly aligned with the real world, making it a compelling alternative to other approaches.Building on these findings, our work systematically investigates sim-and-real co-training.We examine the effectiveness of co-training real-world policies using two sources of simulation data with varying levels of alignment: task-aware digital cousins [25] and task-agnostic prior simulation, as shown in Figure 1.Our study offers practical insights and methodologies for practitioners aiming to achieve robust performance in real-world scenarios.</p>
<p>C. Dataset Composition in Robot Learning</p>
<p>Recent research [58], [59] has highlighted the importance of dataset composition in robot learning, particularly in understanding how variations in data quality and diversity influence policy generalization [60], [61].Studies such as Mimi-cLabs [62] have conducted large-scale analyses to identify the types of data that maximize the utility of robotic datasets and improve downstream policy performance.Inspired by this line of work, we investigate the optimal composition of simulation and real-world data specifically for real-world robotic manipulation tasks.Our study aims to provide actionable guidelines on how to strategically combine these data sources to achieve superior policy learning outcomes in the real world.</p>
<p>III. PROBLEM STATEMENT AND PRELIMINARIES A. Co-Training on Real-World and Simulation Data</p>
<p>We assume access to robot trajectory demonstrations collected in real environments, D real = {ξ i } N i=1 .Instead of training a policy on demonstrations solely from the real world, we have additional demonstrations from simulation environments,
D sim = {ξ i } M i=1
, where generally M ≫ N .We train a visuomotor policy π θ on these two data sources.We adopt the co-training formulation following prior work [7], where we minimize the behavioral cloning action loss
L total (θ; D real , D sim ) = α•L(θ; D sim )+(1−α)•L(θ; D real ) (1) where L(θ; D) = 1 |D| (oi,ai)∈D − log π θ (a i |o i ) and α ∈ [0, 1]
is the co-training ratio balancing the relative weight of simulation and real-world data.In practice, we use an equivalent formulation of α, which represents the probability of sampling from simulation data in each training batch.Specifically, we reweight each sample (o i , a i ) such that the probability of drawing it from the simulation dataset is P [(o i , a i ) ∈ D sim ] = α, while the probability of drawing it from the real dataset is P [(o i , a i ) ∈ D real ] = 1 − α during training batch sampling.We further detail the relative weighting procedure in Appendix VIII-G.As we will see in experiments, the choice of α is crucial to policy performance.Our end objective is to produce vision-based manipulation policies that maximize task performance on one or multiple downstream tasks in real-world environments.</p>
<p>B. Data Composition Factors</p>
<p>D real and D sim can comprise demonstration trajectories from either a single task or a diverse array of tasks, embodiments, and environments.To reason about how the particular choices in constructing these datasets can affect the success of cotraining, it is useful to decompose these datasets into a set of data composition factors.We assume that each dataset follows a distribution of factors {Z (1) , Z (2) , • • • , Z (K) }, borrowing notation from recent work [62].We do not assume that the simulation dataset is perfectly aligned with the real-world dataset, i.e., for some factors
Z (i) sim ̸ = Z (i)
real .Despite these alignment gaps, we are interested in transferring knowledge from simulation domains to learn a more effective policy π for real-world tasks.We define these parameters in more detail and quantify them in Section IV, when we introduce the domains and tasks, and we study how important it is to align each factor between simulation and the real world for co-training success.</p>
<p>C. Automated Synthetic Data Generation</p>
<p>A key advantage of simulation is ease of data collectionwe leverage automated synthetic data generation tools to generate large, high-quality simulation datasets and use them for co-training with smaller real-world datasets.For each task in the simulation, we first collect dozens of source human demonstrations.We then use MimicGen [9] to generate large synthetic trajectory datasets at scale.For bimanual and humanoid robots, we use DexMimicGen [10], a method that builds on top of MimicGen.The process is as follows.First, we segment these source demonstrations into a sequence of object-centric segments.(Dex)MimicGen then generates new demonstrations by applying linear transformations to selected source demonstration segments and concatenating these transformed segments to form novel trajectories.By leveraging these methods, we can use physics simulations to multiply the number of trajectories by orders of magnitude.(2) Given real-world tasks and prior simulation data, we build simulated digital cousin environments that share semantic similarities with their real-world counterparts but may still hold discrepancies in visual and physical aspects.We leverage synthetic data generation methods to multiply trajectories in digital cousins, producing a large quantity of demonstrations in simulation.From here, we consolidate prior simulation data, digital cousin data, and real-world data; (3) We co-train the policy on a mixture of real-world and simulation data.We sample simulation data according to a sampling ratio of α, which is crucial for the method's effectiveness.After training the policy, we deploy the learned policy directly to the real robot.</p>
<p>IV. STUDY SETUP Our goal is to develop a simple recipe for co-training on real-robot and simulation data to significantly improve real-world policy performance compared to training on real data alone.We are broadly interested in two scenarios:</p>
<p>(1) Co-training with prior large-scale simulation data.Can we use existing large prior simulation datasets as co-training data?Note, these datasets often have significant discrepancies with the real world in terms of visual features, task semantics, and behaviors.We are interested in understanding the extent to which these datasets can help out of the box in learning downstream real-world tasks in spite of these domain gaps.</p>
<p>(2) Co-training with task-aware simulation data.Given knowledge of the real-world tasks, we can create customized simulation datasets that are better aligned with the real-world tasks.However, tuning simulation environments to match the real-world environments precisely is impractical.Which data composition factors are most important to align between simulation and the real-world setup, and can we forgo perfect alignment, allowing us to reduce human effort?See Figure 2 for an overview of our workflow, including the real-world setup, simulation pipeline, and the co-training procedure.We describe all of these components in detail in the following sections.</p>
<p>A. Real-World Domains</p>
<p>We seek a co-training recipe that is broadly applicable to a wide range of embodiments, tasks, and environments.To this end, we conduct a comprehensive study featuring two distinct domains, each with a unique robot embodiment and diverse tasks (see Figure 3 for an illustration): Panda Kitchen.A real-world kitchen environment with the Franka Emika Panda robot.We adopt the DROID tabletop hardware setup [6], with some minor modifications (see Appendix VIII-C for details).We experiment with three real-world tasks and collect 50 human demonstrations for each task:</p>
<p>• CounterToSinkPnP: move an object from the counter to the sink basin.This task features nine object categories with diverse shapes: can, cup, coffee cup, water bottle, lemon, garlic, bowl, granola bar, and pear.• CounterToCabPnP: move an object from the counter to the cabinet.This task features eight object categories.1 • CloseDoor: close the door for an overhead cabinet.Humanoid Tabletop.A real-world tabletop environment with a Fourier GR-1 humanoid robot.We control the robot using a mink-based [63] IK controller.We use a first-person view RGB camera mounted to the head of the humanoid.We choose three tasks and collect 20 human demonstrations for each task.We describe more details in Appendix VIII-C.</p>
<p>• CupPnP: move a cup from the plate to the table.</p>
<p>• MilkPnP: move a box of milk from the table to the second level of the shelf.• Pouring: pick up a cup with a ping-pong ball inside and pour the ball into a bowl on the table.Our study is grounded in the real world-we compare the efficacy of different co-training methods by directly evaluating policies on these real-world tasks.</p>
<p>B. Prior Task-Agnostic Simulation Data</p>
<p>We leverage synthetically generated data to supplement real-robot datasets for policy training.One approach is to directly co-train with existing large-scale simulation datasets, or prior task-agnostic simulation datasets.We define a prior task-agnostic simulation dataset as any simulation dataset that existed before the creation of the downstream, real-world task.For the purpose of direct co-training, we use prior taskagnostic datasets that contain the same robot embodiment and action space, but this is not a strict requirement.We otherwise assume that these datasets cover a broad range of tasks and environments.We are interested in co-training with these datasets out of the box, without expending additional efforts designing new tasks in simulation and collecting new data.These datasets may have numerous discrepancies with real-world data, but they present a simple and convenient way to leverage simulation data.We use the following prior simulation datasets: Panda Kitchen.We use the multi-task RoboCasa dataset [7].We choose RoboCasa, given its focus on kitchen environments, a diverse range of scenes and tasks, and the availability of large robot data.In addition, Nasiriany et al. [7] showed preliminary findings that co-training with simulation data can aid transfer in a real-world kitchen environment.The dataset comprises 72k demonstrations across 24 tasks and 100 scenes; for each task, 3,000 demonstrations were generated from 50 source human demonstrations using the MimicGen data generation system [9].Refer to Appendix VIII-D for in-depth details about the tasks and datasets.Note that three of these tasks semantically correspond to our real-world tasks but include notable discrepancies with the real-world setup, including initial robot joint positions, controller parameters, physical parameters, object categories, and the robot base position.We present a quantitative comparison of data composition for the real-world and prior simulation datasets in Appendix VIII-F.</p>
<p>Camera alignment differences between simulation and real-world data can be a major discrepancy.We address this discrepancy by re-rendering the simulation demonstrations to approximately match the camera poses of the real-world setup. 2 Note that this does not represent perfect alignment, but, as we demonstrate in our experiments, it can significantly help nonetheless.Beyond this simple post-processing operation, we do not make any further changes to the prior data.</p>
<p>Humanoid Tabletop.To mirror the setup for the kitchen domain, we create a prior task-agnostic dataset comprising 10 tasks in RoboCasa involving a single kitchen countertop and a GR-1 robot.Each task involves grasping a specified object from a source receptacle and placing it into a target receptacle (e.g., from bowl to basket).Refer to Appendix VIII-D for additional details about these tasks.While semantically similar to the real-world setup, the prior tasks and datasets were developed independently and involve numerous discrepancies such as object categories, visual textures, distractor objects, and physical parameters.None of these 10 tasks is semantically equivalent to the real-world tasks-they involve different source and/or target receptacles.We use DexMimicGen [10], a data generation framework built on top of MimicGen for humanoid and other bimanual robots, to synthesize robot trajectories from dozens of human demonstrations.We generate 1,000 demonstrations for each task, resulting in 10k total demonstrations.</p>
<p>C. Building Task-Aware Simulation Datasets</p>
<p>The tasks and datasets presented in the previous section may have a number of large discrepancies with the real-world tasks, potentially limiting their utility.Alternatively, we can expend additional effort in creating custom tasks in simulation that are better aligned with the real-world tasks.Creating a perfect digital twin [55], [56], [64] copy of the real-world task is challenging, requiring extensive manual tuning, system identification, and sourcing identical 3D assets.Instead, we opt to create tasks in simulation that share the same task semantics, namely the object categories in the environment and the same behaviors.We refer to these as task-aware digital cousins.The term "digital cousin" was recently introduced by Dai et al. [25] to describe simulation environments that are close to, but not perfectly aligned with, their real-world counterparts.We extend this notion with a more precise definition: a taskaware digital cousin is a simulation dataset that preserves four key elements of the real-world task:</p>
<p>1) The same robot and action space;</p>
<p>2) The same task goal-specifically, the same success check and, if applicable, the same language instructions; 3) The same object categories, though individual instances may differ in geometry or texture; 4) The same environmental fixture categories (e.g., kitchen counters, tabletops, cabinet doors).We outline our efforts to create these tasks as follows:</p>
<p>Panda Kitchen.The real-world Panda Kitchen tasks are already represented in the RoboCasa prior dataset, but have several discrepancies as outlined in the prior section.We outline the changes that we made to the task and dataset as follows.First, we adjust the initial state distribution of the robot joints and robot base position in simulation to match the real environment.In addition, we restrict the objects in the task to a curated list of 10 object categories, which includes all of the nine object categories used in the real-world CounterToSinkPnP task.This is in contrast to the prior dataset, in which the authors feature a wider range of 66 possible object categories.For each task, we then collect 100 source human demonstrations.Finally, we generate 10,000 demonstrations for each task using MimicGen.This is in contrast to the prior dataset, where the authors collected 50 source human demonstrations per task and generated 3,000 demonstrations per task with MimicGen.We provide a more in-depth comparison between the real data, task-agnostic prior simulation data, and task-aware digital cousin simulation data in Appendix VIII-F.Humanoid Tabletop.For each of the three tasks in this domain, we construct a digital cousin of the real-world environment in RoboCasa.In each of the real-world tasks, we use a fixed set of objects for both data collection and evaluation.In the digital cousin, however, we randomly select objects from the same category as those in the real-world task to increase the diversity of simulation demonstrations.Additionally, we align the robot's initial pose and camera position to closely replicate the real-world setup.We then collect 10 source demonstrations and generate 1,000 trajectories for each task using DexMimicGen [10].A detailed analysis of the data composition is provided in Appendix VIII-F.</p>
<p>D. Training and Evaluation Protocol</p>
<p>In our study, we compare the effect of co-training with different forms of real-world and simulation data.For each task, we have access to the following forms of data:</p>
<p>• Real-world data (Real): demonstrations collected for the target task in the real-world.See Section IV-A for the list of tasks.We compare various mixtures of these datasets by cotraining a policy on the data and evaluating the resulting policy on our real-world tasks outlined in Section IV-A.We train visuomotor policies with the Diffusion Policy implementation from Chi et al. [37].The policy takes RGB images and robot proprioceptive information as input and produces a sequence of actions to execute.We outline specific settings and hyperparameters in detail in Appendix VIII-G.Following training, we evaluate the policy across a number of trials and record the success rate.See Appendix VIII-H for details on our evaluation protocol.</p>
<p>V. EXPERIMENTS</p>
<p>In this section, we present a comprehensive empirical study of co-training real-world policies using simulation data.We begin by showcasing the benefits of co-training using our fullfledged recipe, which has been informed by systematic experimentation (Section V-A and Section V-B).Specifically, we demonstrate how co-training with simulation data enhances the real-world policy's in-domain performance (Section V-A) and improves its generalization to novel scenarios (Section V-B).Next, we delve into the systematic experiments that guided the development of our recipe (Section V-D).These experiments identify key elements for effective sim-and-real cotraining, providing insights into what factors matter most.Finally, we conclude with a concise and actionable recipe based on our findings (Section V-E), ensuring clarity for practitioners.</p>
<p>A. Effectiveness of Sim-and-Real Co-Training</p>
<p>Co-training with task-aware digital cousin data significantly enhances real-world performance beyond realonly policies.Table I presents our main results.In the second row, compared to policies trained only on Real, those trained on Real and DC exhibit a 35.8% higher average success rate.These results indicate that incorporating simulation data more closely aligned with real-world tasks significantly enhances real-world policy performance.It is important to note that DC data is generated in task-aware digital cousin simulation environments.These environments share the same task definitions, similar scene setups, and comparable camera views with the real world, though none are perfectly aligned.Nevertheless, with minimal effort, we were able to construct and approximately align these digital cousins with real-world environments (see Section IV-C), demonstrating the feasibility of leveraging such simulations for improved real-world policy training.</p>
<p>Co-training with task-agnostic prior simulation data also improves real-world performance.As shown in the third row of Table I, policies trained on Real and Prior consistently outperform those trained solely on Real across all tasks, achieving an average success rate improvement of 31.5%.This is a particularly surprising and encouraging result, as the Prior data are generated without any prior knowledge of real-world tasks.These results indicate that even without manual alignment of the simulation environment, cotraining with simulation data yields substantial benefits.This finding highlights the potential of leveraging readily available simulation data to enhance real-world policy performance.</p>
<p>Finally, in the last row of Table I, policies trained on the combination of Real, DC, and Prior data in general perform the best, achieving an improvement of 37.9% over the realonly policies on average.</p>
<p>We observe a dramatic performance gap between Real and the other co-trained polices for the CloseDoor task.We further investigate the robustness of this gap by training the Real policy with more demos.See Section VIII-L for more results.</p>
<p>B. Generalization Beyond Real Demonstrations</p>
<p>To understand how simulation data enhances real-world policy performance, we investigate whether exposure to diverse situations in simulation-ones not explicitly covered in real-world demonstrations-can improve a policy's ability to generalize to similar, unseen situations in the real world.This question is particularly important because generating broad-coverage data in simulation is relatively easy, whereas collecting diverse real-world demonstrations is often expensive and impractical.If simulation data can effectively bridge this gap, it would provide a powerful and scalable way to improve real-world policies with minimal real-world data.</p>
<p>To explore this, we evaluate the generalization capabilities of co-trained policies beyond real-world demonstrations.Specifically, we consider two key axes of variation: novel objects and novel initial positions.We use the CounterToSinkPnP and CupPnP tasks as our testbed, where, by default, these factors are randomized in simulation to assess the policy's ability to handle novel scenarios in the real world (see Section IV-C).</p>
<p>Co-training with simulation data enhances policy robustness to novel object entities.For the CounterToSinkPnP task, we evaluate on eight new object categories (carrot, ladle, lime, apple, orange, sponge, cucumber, and banana) and new instances of the original object categories with differing size, color, and shape.For the CupPnP task, we replace the red cup with cups of different colors and introduce novel objects.The settings of generalization experiments are detailed in Appendix VIII-I.As shown in Table II, the policy trained solely on Real achieves a success rate of only 33% and 10% on novel objects, whereas the co-trained policy significantly outperforms it with success rates of 50% and 80%.The diversity in simulation data contributes to improved generalizability in real-world policy performance.</p>
<p>Co-training with simulation data enhances policy robustness to novel object positions.In this experiment, we exclude real demonstrations where the object is placed in the middle of the workspace, retaining only those where the object is positioned along borders or corners of the rectangular sampling   region.During evaluation, we place the objects in the center of the sampling region, which are unseen positions in the real demonstrations.The setups are visualized in Appendix VIII-I.</p>
<p>In simulation, we still include data with objects distributed uniformly in the rectangular region.As shown in Table II, policies co-trained with DC achieve a twice higher success rate compared with the policies trained solely on Real for both humanoid and Panda experiments.This result indicates that diverse simulation data substantially improve policy robustness to spatial variations.</p>
<p>C. Effectiveness of Co-Training in Data-Rich Settings</p>
<p>Our main results (Section V-A) examine the effectiveness of sim-and-real co-training in single-task settings with different embodiment and scene setups.Meanwhile, recent works [1], [2], [65] have demonstrated incredible performance by training policies on large-scale, multi-task real-world manipulation datasets.This raises the question-can co-training with synthetic data still be beneficial with larger real-robot datasets?To evaluate the effectiveness of our co-training approach in a scaled-up setting, we conduct experiments on a new humanoid MultiTaskPnP task.In this task, the robot must pick an object from one container and place it into another, with four different source-target container combinations.</p>
<p>For each real-world task variation, we construct a corresponding task-aware digital cousin.We train policies using a fixed set of 4,000 DC demonstrations (1,000 per task) while varying the number of real-world demonstrations.During  testing, we evaluate the generalizability of the policy using three unseen objects.The detailed task setup is provided in Appendix VIII-J.As shown in Figure 4, increasing the number of real-world demonstrations improves the success rate for both real-only and co-trained policies.Even with 400 real demonstrations, the co-trained policy consistently outperforms the realonly policy, demonstrating that sim-and-real co-training remains beneficial even in data-rich settings.</p>
<p>D. Key Elements of Effective Co-Training</p>
<p>In the sections above, we demonstrated the effectiveness of sim-and-real co-training with our full-fledged recipe.In this section, we present systematic studies that help identify key elements for successful co-training.These elements include the quantity of real and simulation demonstrations, the co-training ratio, and the importance of camera alignment in digital cousin (DC) environments.Our findings highlight practical strategies for optimizing co-training and improving real-world policy performance.</p>
<p>A sufficient number of simulation demonstrations is crucial for effective co-training.First, we analyze the impact of varying the number of simulation demonstrations.We reduce the number of DC demonstrations for the Panda CounterToSinkPnP and GR-1 CupPnP tasks and train policies using Real data combined with the reduced DC data.In the Panda CounterToSinkPnP task, decreasing the number of simulation demonstrations from 10k to 500 causes the success rate to drop from 67% to 53%.Similarly, in the GR-1 CupPnP task, reducing the number of simulation demonstrations from 1k to 100 lowers the success rate from 95% to 75%.Therefore, having sufficient simulation demonstrations is essential for achieving strong performance in cotrained policies.</p>
<p>Tuning the co-training ratio is required for effective co-training.We investigate the impact of the co-training ratio-the proportion of simulation data used during training-on the CupPnP task (Section IV-A).As shown in Figure 5, a 1:1 ratio (50%) is suboptimal.In our experiments, a co-training ratio of 99% yielded the best performance.However, pushing the co-training ratio further to values like 99.5% and 99.9% resulted in drops in success rate, from 95% to 60%.These findings highlight the importance of carefully tuning the co-training ratio to achieve optimal performance.
Real World Digital Cousin (Aligned Camera) Digital Cousin (Default Camera)
Camera alignment is critical for successful co-training with task-aware digital cousin data.In DC, we approximate the alignment of the simulation environment's camera with the real-world camera view.To evaluate the importance of this alignment, we render data using the default, unaligned camera view and trained policies on the resulting misaligned simulation data.The results indicate a significant drop in performance compared to policies co-trained with properly aligned DC data.On the Panda arm CounterToSinkPnP task, the co-training success rate dropped from 67% to 56%, while in the GR-1 humanoid CupPnP task, it declined from 95% to 70%.We visualize the default and aligned camera views in Figure 6.Notably, the aligned camera is not strictly identical to the real-world camera.For example, the camera mounted on the real-world humanoid has a fisheye effect, whereas our aligned simulation camera does not model this distortion.This suggests that while alignment enhances performance, perfect camera alignment is not necessary for effective co-training.</p>
<p>E. A Simple Recipe for Sim-and-Real Co-Training</p>
<p>Based on our empirical findings, we provide a set of recommendations to help practitioners reap the benefits of cotraining with synthetic simulation data.</p>
<p>• Task and scene composition.The greatest gains are observed when co-training with simulation data from task-aware digital cousins, where the task and scene compositions closely mirror those of the real-world setting.Nevertheless, co-training with large multi-task prior simulation data-despite differences in task and scene composition-still provides meaningful benefits.• Object composition and initialization distribution.Incorporating diverse objects and varying their placements in simulation data helps real-world policies generalize to unseen scenarios.• Alignment between the task-aware digital cousin and the real world.It is essential that the simulation task shares the same definition and success criteria as its realworld counterpart.Additionally, maintaining similar camera viewpoints between simulation and real-world settings can improve performance, though perfect alignment is not required.• Co-training hyperparameters.We recommend utilizing a sufficiently large amount of simulation data (ideally, orders of magnitude more than real-world data) and carefully tuning the co-training ratio to optimize performance.</p>
<p>VI. LIMITATIONS</p>
<p>Although we conduct systematic studies on several tasks across both a tabletop manipulator and a humanoid robot, most tasks are centered around pick-and-place.Extending our approach to a broader set of manipulation tasks, such as high-precision insertion, and longer-horizon tasks, is left for future work.While our co-training recipe consistently improves success rates compared to solely training on real-robot data collection, the policy's performance is still not perfect.Future efforts could look into building on top of this recipe to further boost real-world performance.Finally, certain realworld tasks-particularly those involving deformable objects and liquids-remain difficult to simulate accurately, inherently limiting the applicability of simulation data.Applying this cotraining strategy to such tasks presents a challenge.Future work could explore the use of co-training data produced by video generation models and world models [66]- [68] as a way to bridge this gap.</p>
<p>VII. CONCLUSION</p>
<p>In this work, we systematically investigate how to effectively leverage synthetically generated data from physics simulations to solve real-world, vision-based manipulation tasks.By analyzing key factors that impact the dataset distributions and co-training strategies, we demonstrate that large-scale simulation data can effectively complement real-world dataeven in the presence of significant discrepancies-leading to policies that outperform those trained on real-world data alone.</p>
<p>Furthermore, we find that simulation data enhances policy generalization to scenarios not covered in real-world datasets, underscoring its potential for developing more robust and adaptable robotic systems.Our findings highlight the promise of leveraging diverse simulation data to advance generalist robot autonomy.</p>
<p>In addition, we offer a set of practical recommendations for practitioners to harness the benefits of synthetic simulation data without requiring extensive manual effort in constructing or aligning simulation environments.They reinforce the importance of systematically integrating simulation and realworld data.We hope our insights will inspire future research to further unleash the potential of simulation in building generalizable robot models in the real world.1) CounterToPlate: Pick up an object from the tabletop and place it on the nearby plate.2) PnPAppleToPlate: Pick up the apple from the tabletop and place it on the nearby plate.3) PnPCanToBowl: Pick up the can from the tabletop and place it in the nearby bowl.4) PnPMugToPlate: Pick up the mug from the tabletop and place it on the nearby plate.5) PnPFruitPlacement: Pick up the fruit from the tabletop and place it on the nearby plate.6) PnPKettleToPlate: Pick up the kettle from the tabletop and place it on the nearby plate.7) PnPMilkPlateToPlate: Pick up the milk carton located on a plate and transfer it to another plate.8) PnPMilkToBasket: Pick up the milk carton from the tabletop and place it in the nearby basket.9) PnPPlateToPlate: Pick the object located on a plate and transfer it to another plate.10) PnPVegetableBowlToPlate: Pick the vegetable located inside a bowl and transfer it to a plate.</p>
<p>E. Task-Aware Digital Cousin Datasets</p>
<p>Panda Kitchen.We design a digital cousin for each of the three tasks in the Panda Kitchen.Among the set of data composition factors outlined in Section V-D, we focus on approximately aligning the following factors: Task composition, Object composition, and Initialization distribution.See Tables III and V for a summary of the alignments.</p>
<p>• Task composition: We align our digital cousin to contain the same task semantics and motions as in the real world.This involves using identical language instructions to those in the real task and approximately matching the initial joint configurations and positions to ensure similar trajectory distributions.• Object composition: We design the digital cousin for our Pick-and-Place tasks to predominantly include object categories present in the real-world setting.Specifically, in the CounterToSinkPnP DC, 9/10 object categories overlap with those used in real-world tasks.</p>
<p>Similarly, in the CounterToCabPnP DC, 8/10 object categories are shared with the real-world setting.• Initialization distribution: We approximately align starting joint states, initial positions, and object initialization regions for our digital cousins.Specifically, we update the default starting position of the Panda to be farther away from the counterspace and increase the starting height to better match the starting position in the real world.We also reduce the area of the default sampling region to better match the sampling region in the real-world setting.For our CounterToSinkPnP DC, CounterToCabPnP DC, and CloseDoor DC, we generate 10,000 synthetic trajectories each from an initial source of 100 human  We outline the number of object categories, object instances, scenes, and number of demonstrations for the real, digital cousin (DC), and task-agnostic prior (Prior) datasets.</p>
<p>demonstrations using MimicGen [9].We further diversify these demonstrations by rendering them using combinations of the 100 floor, 100 wall, 100 cabinet, and 100 counter AI-generated textures provided by RoboCasa [7].</p>
<p>Humanoid Tabletop.Similarly, we design one digital cousin (DC) for each of the three tasks in the Humanoid Tabletop domain.We focus on the following factors when designing the DC: Task composition, Object composition, and Initialization distribution.</p>
<p>• Task composition: we set up the scene and the success check to make the simulation task semantically similar to the real-world task.During the simulation source demo collection, we try to mimic the motion pattern of the real-world demos.We add a small random offset (uniform distribution between [−0.2, 0.2]) to the initial joint distribution as real demos have different initial joints.• Object composition: since we have a fixed set of objects in the real-world, we use a smaller set of objects in DC for the GR-1 humanoid, as noted in Table IV.• Initialization distribution: we align object initialization regions for our DC with the real-world counterparts.For our CupPnP, MilkPnP, and Pouring digital cousins, we generate 1000 synthetic demonstrations each from an initial source of 10 human demonstrations using DexMimicGen [10].We instantiate the MilkPnP and Pouring digital cousins in RoboCasa, and we render them with 10 table textures.</p>
<p>F. Data Composition Analysis</p>
<p>We provide a detailed comparison of the real-world, taskaware digital cousin and task-agnostic prior datasets in Table III for the Panda Kitchen domain and Table IV for the Humanoid Tabletop domain.</p>
<p>We also add a detailed comparison of additional factors such as camera positions and initialization parameters for the CounterToSinkPnP task in Table V.</p>
<p>G. Training Details</p>
<p>Panda Kitchen.We adopt the open-source Diffusion Policy implementation from Chi et al. [37].We use the transformerbased variant with ResNet visual encoders.The policy takes three 128 × 170 image views and robot proprioception information (end effector position and rotation, gripper joint values), and outputs a 7-DoF action for delta end-effector Dataset Obj.Cat.control and gripper action.We modify the hyperparameters to use a larger transformer network and a larger batch size of 256.We also add language conditioning to facilitate training on diverse multi-task data; we encode language using the CLIP sentence encoder, and we add FiLM conditioning layers [71] to the vision encoder.We set a default co-training ratio of 0.10 for real-world data and 0.90 for simulation data.We do this as we have a significantly higher quantity of simulation demonstrations than real-world demonstrations.</p>
<p>As mentioned in Section III-A, with a training batch of size B, on average, α • B samples are drawn from the simulation dataset D sim , and (1 − α) • B samples are drawn from the realworld dataset D real .To enforce this, we reweight each data sample such that P [(o i , a i ) ∈ D sim ] = α and P [(o i , a i ) ∈ D real ] = 1 − α, where (o i , a i ) denotes an observation-action pair within the batch.We achieve this by first normalizing the weight of each sample according to the size of its dataset, and then multiplying the normalized weight by α if the sample belongs to D sim and by 1 − α if it belongs to D real .Humanoid Tabletop.Since there is no ambiguity in terms of objects to manipulate, we use Diffusion Policy (DP) without language conditioning and train one policy for each task.We use the DP implementation from UMI [72] with Vision Transformers [73] as vision encoders and UNet [74] as the diffusion backbone.The input observation contains an RGB image from a first-person-view camera and joint position observations.The output action is the target joint positions of the arms and the dexterous hands.Figure 5 shows that cotraining ratios α of 0.9 and 0.99 are optimal, and we use 0.99 for all other experiments.</p>
<p>H. Policy Evaluation</p>
<p>We use similar evaluation protocols for the Franka Panda arm and the GR-1 humanoid.We evaluate the model at three checkpoints during training, spaced at equal intervals.At each checkpoint, we assess the model's success rate and use the highest success rate among these evaluations as the final result.Panda Kitchen.For both the CounterToSink and CounterToCab tasks, we perform four random placements on the counter per object and record the overall average success rate for all object categories.For the CloseDoor task we sample 10 random joint angles between [85   and average the success rate across all sampled joint values; this results in 36, 32, and 10 total rollouts per checkpoint for the CounterToSink, CounterToCab, and CloseDoor respectively.For the CounterToSink, we record a success if the policy picks up the object and places it in the left or right basin of the sink, and record a failure otherwise.For the CounterToCab, we record a success if the policy picks up the object and securely places it in the cabinet, and record a failure otherwise.Finally, for the CloseDoor task, we record a success if the door's joint angle is less than 5 • and record a failure otherwise.</p>
<p>The initialization regions and starting conditions (robot joint states, robot positions, object sampling regions) at test time are the same as during training.See Figure 7 for a visualization of the starting state and initialization regions for the tasks.For CounterToCabPnP and CounterToSinkPnP, we evaluate on seen objects, and for CloseDoor, we evaluate on the same door and use the same range of joint angles, [85 • , 115 • ], from the training data.Humanoid Tabletop.For each task, we evaluate the policy performance using the same objects and position distributions.For each checkpoint, we evaluate 20 different initial positions for the CupPnP task and 10 for the MilkPnP and Pouring tasks.We report the highest success rates among the three checkpoints for each training run.We consider partial successes, where a successful pick is counted as 0.5.</p>
<p>I. Generalization Experiment Details</p>
<p>This section visualizes the settings of the generalization experiments described in Section V-B. Figure 8  uses the same red cup.Consequently, we observe a much greater performance improvement in the CupPnP task when employing sim-and-real co-training, as in Table II.</p>
<p>Figure 9 visualizes the training and testing object initialization ranges for the generalization experiments conducted on the Panda CounterToSinkPnP and GR-1 CupPnP tasks.In the training data, objects are always initialized at the borders of the workspace, while testing is performed with objects placed at the center of the workspace.</p>
<p>J. MultiTaskPnP Task Setup</p>
<p>In the multi-task experiment on the humanoid, we consider the following four tasks:</p>
<p>1) Cuttingboard2Basket: Pick up an object from the cutting board and place it into the nearby basket.2) Cuttingboard2Pan: Pick up an object from the cutting board and place it into the nearby pan. 3) Mat2Basket: Pick up an object from the mat and place it into the nearby basket.4) Plate2Bowl: Pick up an object from the plate and place it into the nearby bowl.The tasks are visualized in Figure 10.The train and test objects we used are visualized in Figure 11.We collect 100 demonstrations for each task in the real world, where we randomly select an object and an initial location for each trajectory.At test time, we choose unseen instances of the same object category as the test objects, but the positions are within the demonstration distribution.</p>
<p>We create a digital cousin for each of the four tasks, using target objects and containers of the same category in simulation.Additionally, we align the initial distribution of object positions to approximate the distribution observed in realworld demonstrations.Once the simulation environments are set up, we collect 10 human demonstrations and generate 1000
Cuttingboard2Basket Cuttingboard2Pan Placemat2Basket Plate2Bowl
Fig. 10: MultiTaskPnP visualization.We show the realworld scene setup of the four tasks in MultiTaskPnP.</p>
<p>GR-1 MultiTaskPnP</p>
<p>Test objects Train objects Fig. 11: MultiTaskPnP train and test objects.We show the real-world train and test objects we used for GR-1 MultiTaskPnP.</p>
<p>simulation demonstrations per task using DexMimicGen [10], resulting in a total of 4000 DC demonstrations.</p>
<p>We train a single policy across all four tasks, where the policy implicitly conditions on the image observation to determine the current task.For evaluation, we select three unseen objects-can, lemon, and cucumber (see Figure 11 right)-and assess performance across three different initial positions for each object in each task, yielding a total of 36 evaluations across the four tasks.We report the average performance across all tasks, considering partial successes, where a successful pick is counted as 0.5.</p>
<p>In Figure 4, we compare policies trained on Real + DC, where we use 4,000 total DC demonstrations and vary the number of real-world demonstrations, with 10, 25, 50, 75, and 100 real demonstrations per task.Results show that the cotrained policies consistently outperform the real-only policies.</p>
<p>We also compare co-training with different simulation data in this multi-task setting, similar to the single-task setting we show in Section V-A.We use 200 real demonstrations (50 per task), 4000 DC demos (1,000 per task), and 10,000 Prior demos (1,000 per task).Similar to Table I, we see that the policy trained on the combination of Real, DC, and Prior data performs the best, getting a success rate of 75.7%, followed by Real + DC (70.8%) and Real + Prior (68.8), both of which significantly outperform the Real policy (30.6%).</p>
<p>K. Improving Visual Realism with Vid2Vid</p>
<p>Given our observation that incorporating task-aware digital cousins, such as camera viewpoint alignment, enhances performance, we pose a follow-up question: Would improving visual realism by making simulation rendering closer to reality enhance co-training?</p>
<p>Several studies [23], [75]- [78] have suggested that training generative models (such as CycleGAN) on real and simulated images, and applying them to simulation images to make them visually more realistic, improves sim-to-real policy transfer.Inspired by this, we conduct experiments to assess the extent to which improving the visual realism of simulation data enhances policy performance.Specifically, we fine-tune CogVideo-X [79], a state-of-theart video diffusion model, on CupPnP real demonstration videos in the Text2Video modality.This enables the model to generate realistic-looking videos of a robot performing the task from pure Gaussian noise.</p>
<p>To adapt the model for style transfer on simulation videos-preserving object positions while enhancing visual realism-we adopt a simple strategy: instead of generating videos from pure noise, we introduce noise into reference simulation videos and use them as initialization for video diffusion.By adjusting the noise strength parameter, we control the extent of noise added and the diffusion starting timestep (see Figure 12 for examples).Lower noise levels retain more of the original simulation textures, producing outputs closer to the inputs, while higher noise levels result in more realistic appearances but may distort object poses.</p>
<p>We set the noise strength to 0.6, as lower values yield outputs too similar to simulation textures, whereas higher values cause excessive deviations from object positions, rendering the original action labels from simulation data inapplicable.</p>
<p>We then conduct co-training experiments in the Real + DC setup on the CupPnP task, comparing policy performance with and without Vid2Vid augmentations on DC data under different numbers of sim and real demos similar to Figure 4.</p>
<p>We find that improved visual realism is particularly beneficial in low-data regimes and provides a modest improvement in overall policy performance.Experiment results, reported in Table VI, show that Vid2Vid-enhanced visual realism leads to a 5-10% average improvement in policy performance, with the most significant benefits occurring when the number of simulation or real-world trajectories is low.When sufficient real-world demonstrations are available, simulation data plays a minor role.Conversely, when a large and diverse set of simulation data is available, the importance of visual realism diminishes.</p>
<p>These findings highlight the potential of the role of generative models as part of the synthetic data.While our approach utilizes a renderer to provide reference videos for the video diffusion model, future work could explore synthetic generation without reliance on the graphics renderer.models on policy performance in various numbers of simulation and real demonstrations.We compare the policy co-trained on Real + DC for the CupPnP task where DC is augmented to be visually more realistic using the Vid2Vid model (DC w/ V2V) compared to using the OpenGL-based simulation renderings (DC).</p>
<p>L. Training CloseDoor with More Demos</p>
<p>Due to the large performance gap between real and all other co-trained policies for the CloseDoor task, we investigate whether this gap can be easily closed by training the real policy on more demonstrations.Specifically, we train the real policy on 100 demos instead of 50.We find that the policy still does not achieve 100% success, only getting an 80% success rate despite training with double demos.</p>
<p>M. FAQs</p>
<p>Is dynamic alignment investigated?We have attempted to align dynamics by tuning physics parameters to reduce the gap between open-loop rollout results in simulation and the real world.However, our experiments on GR-1 CupPnP show no difference in success rate (95%) with or without dynamic alignment, suggesting that such alignment is unnecessary for our tasks.</p>
<p>How misaligned is the default camera?Since perfect camera alignment between simulation and the real world is infeasible, quantifying the exact camera pose error is challenging.Instead, we compute the delta pose between the default camera and the "aligned" camera.In Panda, the position and orientation deltas for the third-person cameras are 37 cm and 20 • , respectively, while for the wrist camera, the deltas are 9 cm and 180 • .In GR-1, the position delta is 36 cm, and the orientation delta is 60 • .A visualization of the camera views is provided in Figure 6.</p>
<p>How does co-training compare to domain randomization and domain adaptation?We find that domain randomization and domain adaptation are complementary to co-training but not strictly necessary to reap the benefits of co-training with simulation.For instance, in the Panda Kitchen domain, we observe that adding generative texture randomization to the digital cousin data further improves performance, but even without it, co-training alone still leads to performance gains.Similarly, in the Humanoid Tabletop domain, enhancing visual realism via a Video2Video model can boost co-training results, but the gains are marginal in some cases.</p>
<p>Are there plans for bimanual tasks?We introduce a GR-1 BimanualPnP task, where the humanoid must use its left hand to pick up a Rubik's cube and place it at the center of the table, then use its right hand to pick up the cube and place it into a basket, as shown in Figure 13.Our results show that with 50 real demonstrations, the policy achieves a 15% success rate.When co-trained with 1,000 DC simulation demonstrations and 50 real demonstrations, the success rate improves to 50%.In contrast, a policy trained only with 100 real demonstrations achieves a 30% success rate.We also briefly tested co-training with our current task-agnostic prior datasets.As discussed in Section VIII-D, all our humanoid prior datasets consist of single-arm pick-and-place data.When co-trained with these datasets using a 99% co-training ratio, the policy exhibits single-arm pick-and-place behavior instead of the expected bimanual behavior, resulting in a near-zero success rate.This suggests that for task-agnostic prior datasets to effectively support real-world manipulation, the behavior patterns in simulation and the real-world need to be consistent.</p>
<p>( 1 +Fig. 2 :
12
Fig.2: Method Overview.Our workflow consists of three components:(1) We start with a real-world target task in mind and some prior simulation data; (2) Given real-world tasks and prior simulation data, we build simulated digital cousin environments that share semantic similarities with their real-world counterparts but may still hold discrepancies in visual and physical aspects.We leverage synthetic data generation methods to multiply trajectories in digital cousins, producing a large quantity of demonstrations in simulation.From here, we consolidate prior simulation data, digital cousin data, and real-world data; (3) We co-train the policy on a mixture of real-world and simulation data.We sample simulation data according to a sampling ratio of α, which is crucial for the method's effectiveness.After training the policy, we deploy the learned policy directly to the real robot.</p>
<p>Fig. 3 :
3
Fig.3: Real-World and Simulation Tasks.We experiment with co-training on three data sources on the two robot domains of Kitchen Panda and Humanoid Tabletop: (top) data collected for real-world tasks; (middle) data from task-aware digital cousin environments that resemble the target tasks but are not perfectly aligned; (bottom) prior multi-task data from simulation that comprise a wide range of tasks and environments but have larger discrepancies with real-world tasks.</p>
<p>•</p>
<p>Prior simulation data (Prior): task-agnostic simulation data outlined in Section IV-B.• Task-aware digital cousin data (DC): synthetic simulation data outlined in Section IV-C.See Appendix Tables III and IV for an overview and comparison of these datasets for the Panda Kitchen and Humanoid Tabletop domains.</p>
<p>Fig. 4 :
4
Fig.4: Effect of the quantity of real demonstrations.We use a total of 4,000 simulation DC demos and vary the total number of real demos from 40 to 400 on task MultiTaskPnP.The results show that our co-training recipe remains beneficial with larger real datasets.</p>
<p>Fig. 5 :
5
Fig. 5: Effect of the different co-training ratios.The co-training ratio, α, is the probability of sampling from simulation data in each minibatch.We experiment on the CupPnP task with 20 real demos and 1000 simulation demos from task-aware digital cousins.Tuning the co-training ratio is important for the good performance of cotrained policies.</p>
<p>Fig. 6 :
6
Fig. 6: Camera alignment visualization.We visualize the default and aligned camera views of the GR-1 CupPnP task and Panda CounterToSinkPnP task.</p>
<p>Fig. 7 : 5 )
75
Fig. 7: Visualization of start states for our experiments.We visualize the starting states of the Panda and GR-1 for our experiments, including initialization regions for our pick-andplace tasks.</p>
<p>Fig. 8 :
8
Fig. 8: Visualization of novel object experiment settings.We show the picture of train objects and test objects of the generalization experiment conducted on Panda CounterToSinkPnP and GR-1 CupPnP tasks.</p>
<p>Fig. 9 :
9
Fig. 9: Visualization of novel position experiment settings.We visualize the train and test object initialization range of the generalization experiment conducted on Panda CounterToSinkPnP and GR-1 CupPnP tasks.</p>
<p>Fig. 12 :
12
Fig. 12: Examples of the Video2Video model outputs with different noise strength.Left: An example video frame from the simulation data.Right: The corresponding frames generated by the trained video diffusion model by initializing the noises with the simulation video with different noise strengths.By setting different values for the noise strength parameter, we can control how much noise is added and from which timestep the model starts diffusion.</p>
<p>Fig. 13 :
13
Fig. 13: Visualization of BimanualPnP task.We show one rollout of our co-trained policy on BimanualPnP task.This task involves bimanual and long-horizon manipulation.</p>
<p>TABLE I :
I
Effect of different simulation data in the co-training mix.We compare co-training with different simulation data on six tasks across two robot platforms.Note that we abbreviate the CounterToSinkPnP task as C2SPnP and CounterToCabPnP as C2CPnP.Co-training with Prior (third row) consistently boosts performance over policies trained on real data only (first row).On top of these results, adding better aligned DC further improves the overall performance (last row).</p>
<p>TABLE II :
II
Co-training with sim enhances policy generalization across novel objects and novel positions.We select the CounterToSinkPnP task on Panda and the CupPnP task on the humanoid and evaluate the policies' performance when the object is changed and when the object is placed at unseen positions.
Co-Trained PolicyPolicy Trained only on Real Data100%0.89Success Rate80% 20% 40% 60%0.38 0.170.68 0.270.71 0.310.78 0.310.390%40100200300400Number of Real Demos</p>
<p>TABLE III :
III
Aggregate dataset statistics of the Panda Kitchen domain.</p>
<p>Table Tex.Demos
CupPnP1120RealMilkPnP1120Pouring1120DCCupPnP MilkPnP1 31 101k 1kPouring2101kPriorRoboCasa (10 tasks)661010k</p>
<p>TABLE IV :
IV
Aggregate dataset statistics of the Humanoid Tabletop domain.We outline the number of object categories, table textures, and number of demonstrations for the real, digital cousin (DC), and task-agnostic prior (Prior) datasets.Note that for CupPnP, we randomize the color of the object, although we are using a single object category.</p>
<p>Real [-0.35, 0.49, 0.70] [-0.35, -0.42, 0.72] [-0.029, 0, 0.05] (0.09, -0.20, -0.02, -2.47, -0.01, 2.30, 0.85) [-18, -26, 0] 27x23 DC [-0.35, 0.49, 0.70] [-0.35, -0.42, 0.72] [-0.029, 0, 0.05] (0.09, -0.20, -0.02, -2.47, -0.01, 2.30, 0.85) [0, -32, -4] 27x27 Prior [-0.35, 0.49, 0.70] [-0.35, -0.42, 0.72] [-0.029, 0, 0.05] (-0.02, -1.03, -0.02, -2.28, 0.04, 1.52, 0.70) [0, -20, -22] 30x40
Left Cam Pos.Right Cam Pos.Wrist Cam Pos.Init. Robot JointsInit. Robot Base Pos.Obj. Init. Reg.
• , 115 • ]</p>
<p>TABLE V :
V
Additional dataset statistics for CounterToSinkPnP task.All units except for initial robot joints are in centimeters.Camera positions are (x,y,z) coordinates relative to the robot base.Initial robot base positions are (x, y, z) coordinates relative to the middle edge of the sink.Object initialization region is (depth x width)
Panda CounterToSinkPnPGR-1 CupPnPTrain objectsTest objectsTrain objectsTest objects</p>
<p>Real 20 +Sim 20 Sim 100 Sim 200 Sim 500 Sim 1000
Real + DC48%73%85%88%95%Real + DC w/ V2V70%80%88%93%95%Sim 1000 +Real 1Real 5Real 10Real 20Real 50Real + DC8%40%73%95%95%Real + DC w/ V2V13%53%83%95%95%</p>
<p>TABLE VI :
VI
Effects of improved visual realism with Vid2Vid</p>
<p>We use the same object categories as CounterToSinkPnP but exclude the water bottle due to difficulties in placing it stably into the cabinet.
This operation introduces occlusions for the drawer and stove knob manipulation tasks, so we opt to exclude these. In total, we use 60k demonstrations across 20 tasks.
VIII. APPENDIXA. OverviewThe Appendix contains the following content:• Author Contributions (Appendix VIII-B): list of each author's contributions to the paper • Real-World Tasks (Appendix VIII-C): details about realworld domains and tasksC. Real-World TasksPanda Kitchen.For our Panda Kitchen tasks, we use the DROID[6]setup and make some modifications.We use the controller from Deoxys[69], which supports OSC-based end effector control[70], and opt to keep the original paralleljaw Panda gripper instead of the Robotiq gripper.We use two side-view third-person cameras and an eye-in-hand camera.For each task, we collect data via teleoperation using a SpaceMouse.The robot starts from a fixed initial position and joint configuration, with camera poses remaining constant across each demo.For the CounterToSinkPnP task, we uniformly sample object placements from the rectangular counter region to the left of the sink.For the CounterToCabPnP task, we uniformly sample object placements from the rectangular counter region below the cabinet.Finally, in the CloseDoor task, we uniformly sample the initial angle of the open door from the interval [85 • , 115 • ].Refer to Figure7for a visualization of the start states and sampling regions for each task.Humanoid Tabletop.We use a Fourier GR-1 robot with the default 6-DoF dexterous hands.The joints of the robot's lower body and waist are locked, while the two arms and hands are activated.During data collection, a human operator wears a pair of MANUS gloves with a VIVE tracker positioned on the back of each hand to capture the finger pose and wrist pose, respectively.We implement an inverse kinematic (IK) controller based on the mink framework[63]to control the robot during teleoperation, where the human input consists of wrist pose and finger joint commands.During policy learning and deployment, we use the joint positions computed by the IK solver as the action space.Additionally, an OAK-D camera mounted on the robot's head provides egocentric visual input for the system.Only one of the stereo RGB images (and not the depth) is used for the policy, and no additional third-person view is used.For each of the three single tasks-CupPnP, MilkPnP, and Pouring, we collect 20 demonstrations in the real world.The initial pose of the robot varies around a standard reset pose according to the human teleoperator's pose.Only one set of objects is used for each task, but the object positions are randomized uniformly in the region as visualized in Figure7.For the multi-task setting (Cuttingboard2Basket, Cuttingboard2Pan, Mat2Basket, and Plate2Bowl), we additionally select multiple object instances during data collection.See Appendix VIII-J for details.D. Task-Agnostic Simulation DatasetsPanda Kitchen.We use a subset of the 72k trajectories from the RoboCasa dataset[7].Specifically, we exclude the OpenDrawer, CloseDrawer, TurnOnStove, and TurnOffStove tasks from the original dataset due to lack of visibility introduced by camera pose alignment.This results in 60k trajectories over the following 20 tasks.We cite the task descriptions from RoboCasa here: 1) PickPlaceCounterToCabinet: Pick an object from the counter and place it inside the cabinet.2) PickPlaceCabinetToCounter: Pick an object from the cabinet and place it on the counter.
π 0 : A vision-language-action flow model for general robot control. K Black, N Brown, D Driess, A Esmail, M Equi, C Finn, N Fusai, L Groom, K Hausman, B Ichter, arXiv:2410.241642024arXiv preprint</p>
<p>OpenVLA: An opensource vision-language-action model. M J Kim, K Pertsch, S Karamcheti, T Xiao, A Balakrishna, S Nair, R Rafailov, E Foster, G Lam, P Sanketi, arXiv:2406.092462024arXiv preprint</p>
<p>J Nvidia, F Bjorck, N Castañeda, X Cherniadev, R Da, L Ding, Y Fan, D Fang, F Fox, S Hu, Huang, arXiv:2503.14734Gr00t n1: An open foundation model for generalist humanoid robots. 2025arXiv preprint</p>
<p>Bridge Data: Boosting Generalization of Robotic Skills with Cross-Domain Datasets. F Ebert, Y Yang, K Schmeckpeper, B Bucher, G Georgakis, K Daniilidis, C Finn, S Levine, Proceedings of Robotics: Science and Systems. Robotics: Science and SystemsNew York City, NY, USA62022</p>
<p>RT-1: Robotics transformer for real-world control at scale. A Brohan, N Brown, J Carbajal, Y Chebotar, J Dabis, C Finn, K Gopalakrishnan, K Hausman, A Herzog, J Hsu, arXiv:2212.068172022arXiv preprint</p>
<p>Droid: A large-scale in-the-wild robot manipulation dataset. A Khazatsky, K Pertsch, S Nair, A Balakrishna, S Dasari, S Karamcheti, S Nasiriany, M K Srirama, L Y Chen, Proceedings of Robotics: Science and Systems. Robotics: Science and Systems2024</p>
<p>Robocasa: Large-scale simulation of everyday tasks for generalist robots. S Nasiriany, RSSA Maddukuri, RSSL Zhang, RSSA Parikh, RSSA Lo, RSSA Joshi, RSSA Mandlekar, RSSY Zhu, RSSRobotics: Science and Systems. 2024</p>
<p>Robogen: Towards unleashing infinite data for automated robot learning via generative simulation. Y Wang, Z Xian, F Chen, T.-H Wang, Y Wang, K Fragkiadaki, Z Erickson, D Held, C Gan, Forty-first International Conference on Machine Learning. 2023</p>
<p>Mimicgen: A data generation system for scalable robot learning using human demonstrations. A Mandlekar, S Nasiriany, B Wen, I Akinola, Y Narang, L Fan, Y Zhu, D Fox, Conference on Robot Learning. PMLR2023</p>
<p>Dexmimicgen: Automated data generation for bimanual dexterous manipulation via imitation learning. Z Jiang, Y Xie, K Lin, Z Xu, W Wan, A Mandlekar, L Fan, Y Zhu, 2025 IEEE International Conference on Robotics and Automation (ICRA). 2025</p>
<p>Skillmimicgen: Automated demonstration generation for efficient skill learning and deployment. C Garrett, A Mandlekar, B Wen, D Fox, arXiv:2410.189072024arXiv preprint</p>
<p>Imitating task and motion planning with visuomotor transformers. M Dalal, A Mandlekar, C R Garrett, A Handa, R Salakhutdinov, D Fox, Conference on Robot Learning. PMLR2023</p>
<p>Bayessim: adaptive domain randomization via probabilistic inference for robotics simulators. F Ramos, R C Possas, D Fox, arXiv:1906.017282019arXiv preprint</p>
<p>Neural posterior domain randomization. F Muratore, T Gruner, F Wiese, B Belousov, M Gienger, J Peters, Conference on Robot Learning. PMLR2022</p>
<p>Real2sim2real: Self-supervised learning of physical single-step dynamic actions for planar robot casting. V Lim, H Huang, L Y Chen, J Wang, J Ichnowski, D Seita, M Laskey, K Goldberg, 2022 International Conference on Robotics and Automation (ICRA). IEEE2022</p>
<p>Asid: Active exploration for system identification in robotic manipulation. M Memmel, A Wagenmaker, C Zhu, P Yin, D Fox, A Gupta, arXiv:2404.123082024arXiv preprint</p>
<p>Adaptsim: Taskdriven simulation adaptation for sim-to-real transfer. A Z Ren, H Dai, B Burchfiel, A Majumdar, arXiv:2302.049032023arXiv preprint</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE2017</p>
<p>Sim-to-real transfer of robotic control with dynamics randomization. X B Peng, M Andrychowicz, W Zaremba, P Abbeel, 2018 IEEE international conference on robotics and automation (ICRA). IEEE2018</p>
<p>Reinforcement and imitation learning for diverse visuomotor skills. Y Zhu, Z Wang, J Merel, A Rusu, T Erez, S Cabi, S Tunyasuvunakool, J Kram Ã¡r, R Hadsell, N Freitas, N Heess, Proceedings of Robotics: Science and Systems. Robotics: Science and Systems2018</p>
<p>Learning dexterous in-hand manipulation. O M Andrychowicz, B Baker, M Chociej, R Jozefowicz, B Mcgrew, J Pachocki, A Petron, M Plappert, G Powell, A Ray, The International Journal of Robotics Research. 3912020</p>
<p>Dextreme: Transfer of agile in-hand manipulation from simulation to reality. A Handa, A Allshire, V Makoviychuk, A Petrenko, R Singh, J Liu, D Makoviichuk, K Van Wyk, A Zhurkevich, B Sundaralingam, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Using simulation and domain adaptation to improve efficiency of deep robotic grasping. K Bousmalis, A Irpan, P Wohlhart, Y Bai, M Kelcey, M Kalakrishnan, L Downs, J Ibarz, P Pastor, K Konolige, 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE2018</p>
<p>From imitation to refinement-residual rl for precise assembly. L Ankile, A Simeonov, I Shenfeld, M Torne, P , arXiv:2407.166772024arXiv preprint</p>
<p>Automated creation of digital cousins for robust policy learning. T Dai, J Wong, Y Jiang, C Wang, C Gokmen, R Zhang, J Wu, L Fei-Fei, arXiv:2410.074082024arXiv preprint</p>
<p>Alvinn: An autonomous land vehicle in a neural network. D A Pomerleau, Advances in neural information processing systems. 1989</p>
<p>A comparison of imitation learning algorithms for bimanual manipulation. M Drolet, S Stepputtis, S Kailas, A Jain, J Peters, S Schaal, H Ben Amor, IEEE Robotics and Automation Letters. 2024RA-L</p>
<p>Learning multi-arm manipulation through collaborative teleoperation. A Tung, J Wong, A Mandlekar, R Martín-Martín, Y Zhu, L Fei-Fei, S Savarese, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE2021</p>
<p>Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware. T Z Zhao, V Kumar, S Levine, C Finn, Proceedings of Robotics: Science and Systems. Robotics: Science and SystemsDaegu, Republic of Korea72023</p>
<p>Aloha 2: An enhanced low-cost hardware for bimanual teleoperation. J Aldaco, T Armstrong, R Baruch, J Bingham, S Chan, K Draper, D Dwibedi, C Finn, P Florence, S Goodrich, arXiv:2405.022922024arXiv preprint</p>
<p>Is imitation learning the route to humanoid robots?. S , Trends in cognitive sciences. 361999</p>
<p>Movement imitation with nonlinear dynamical systems in humanoid robots. A J Ijspeert, J Nakanishi, S Schaal, Proceedings 2002 IEEE International Conference on Robotics and Automation. 2002 IEEE International Conference on Robotics and Automation20022</p>
<p>Deep imitation learning for humanoid loco-manipulation through human teleoperation. M Seo, S Han, K Sim, S H Bang, C Gonzalez, L Sentis, Y Zhu, 2023 IEEE-RAS 22nd International Conference on Humanoid Robots (Humanoids). IEEE2023</p>
<p>Bunny-visionpro: Real-time bimanual dexterous teleoperation for imitation learning. R Ding, Y Qin, J Zhu, C Jia, S Yang, R Yang, X Qi, X Wang, arXiv:2407.031622024arXiv preprint</p>
<p>Open-television: teleoperation with immersive active visual feedback. X Cheng, J Li, S Yang, G Yang, X Wang, arXiv:2407.015122024arXiv preprint</p>
<p>What matters in learning from offline human demonstrations for robot manipulation. A Mandlekar, D Xu, J Wong, S Nasiriany, C Wang, R Kulkarni, L Fei-Fei, S Savarese, Y Zhu, R Martín-Martín, Conference on Robot Learning (CoRL). 2021</p>
<p>Diffusion policy: Visuomotor policy learning via action diffusion. C Chi, RSSS Feng, RSSY Du, RSSZ Xu, RSSE Cousineau, RSSB Burchfiel, RSSS Song, RSSProceedings of Robotics: Science and Systems. Robotics: Science and Systems2023</p>
<p>One-shot visual imitation learning via meta-learning. C Finn, T Yu, T Zhang, P Abbeel, S Levine, Conference on robot learning. 2017</p>
<p>Robot programming by demonstration. A Billard, S Calinon, R Dillmann, S Schaal, 2008Springer Handbook of Robotics</p>
<p>Learning and reproduction of gestures by imitation. S Calinon, F D'halluin, E L Sauser, D G Caldwell, A Billard, IEEE Robotics and Automation Magazine. 172010</p>
<p>GTI: Learning to Generalize across Long-Horizon Tasks from Human Demonstrations. A Mandlekar, D Xu, R Martín-Martín, S Savarese, L Fei-Fei, Proceedings of Robotics: Science and Systems. Robotics: Science and SystemsCorvalis, Oregon, USA72020</p>
<p>Transporter networks: Rearranging the visual world for robotic manipulation. A Zeng, P Florence, J Tompson, S Welker, J Chien, M Attarian, T Armstrong, I Krasin, D Duong, V Sindhwani, Conference on Robot Learning. PMLR2021</p>
<p>Generalization through hand-eye coordination: An action space for learning spatially-invariant visuomotor control. C Wang, R Wang, A Mandlekar, L Fei-Fei, S Savarese, D Xu, 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2021</p>
<p>Learning latent plans from play. C Lynch, M Khansari, T Xiao, V Kumar, J Tompson, S Levine, P Sermanet, Conference on Robot Learning. 2019</p>
<p>Demonstration-guided reinforcement learning with learned skills. K Pertsch, Y Lee, Y Wu, J J Lim, Conference on Robot Learning. 2021</p>
<p>Opal: Offline primitive discovery for accelerating offline reinforcement learning. A Ajay, A Kumar, P Agrawal, S Levine, O Nachum, International Conference on Learning Representations. 2021</p>
<p>Hierarchical few-shot imitation with skill transition models. K Hakhamaneshi, R Zhao, A Zhan, P Abbeel, M Laskin, International Conference on Learning Representations. 2021</p>
<p>Bottom-up skill discovery from unsegmented demonstrations for long-horizon robot manipulation. Y Zhu, P Stone, Y Zhu, IEEE Robotics and Automation Letters. 722022</p>
<p>Learning and retrieval from prior data for skill-based imitation learning. S Nasiriany, T Gao, A Mandlekar, Y Zhu, Conference on Robot Learning (CoRL). 2022</p>
<p>Closing the sim-to-real loop: Adapting simulation randomization with real world experience. Y Chebotar, A Handa, V Makoviychuk, M Macklin, J Issac, N Ratliff, D Fox, 2019 International Conference on Robotics and Automation (ICRA). IEEE2019</p>
<p>Active domain randomization. B Mehta, M Diaz, F Golemo, C J Pal, L Paull, Conference on Robot Learning. PMLR2020</p>
<p>What went wrong? closing the sim-to-real gap via differentiable causal discovery. P Huang, X Zhang, Z Cao, S Liu, M Xu, W Ding, J Francis, B Chen, D Zhao, Conference on Robot Learning. PMLR2023</p>
<p>Rma: Rapid motor adaptation for legged robots. A Kumar, Z Fu, D Pathak, J Malik, arXiv:2107.040342021arXiv preprint</p>
<p>Context is everything: Implicit identification for dynamics adaptation. B Evans, A Thankaraj, L Pinto, 2022 International Conference on Robotics and Automation (ICRA). IEEE2022</p>
<p>Ditto: Building digital twins of articulated objects from interaction. Z Jiang, C.-C Hsu, Y Zhu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Reconciling reality through simulation: A realto-sim-to-real approach for robust manipulation. M Torne, A Simeonov, Z Li, A Chan, T Chen, A Gupta, P , arXiv:2403.039492024arXiv preprint</p>
<p>Rt-2: Vision-languageaction models transfer web knowledge to robotic control. A Brohan, N Brown, J Carbajal, Y Chebotar, X Chen, K Choromanski, T Ding, D Driess, A Dubey, C Finn, arXiv:2307.158182023arXiv preprint</p>
<p>Re-mix: Optimizing data mixtures for large scale imitation learning. J Hejna, C Bhateja, Y Jian, K Pertsch, D Sadigh, arXiv:2408.140372024arXiv preprint</p>
<p>Decomposing the generalization gap in imitation learning for visual robotic manipulation. A Xie, L Lee, T Xiao, C Finn, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>The colosseum: A benchmark for evaluating generalization for robotic manipulation. W Pumacay, I Singh, J Duan, R Krishna, J Thomason, D Fox, arXiv:2402.081912024arXiv preprint</p>
<p>Efficient data collection for robotic manipulation via compositional generalization. J Gao, A Xie, T Xiao, C Finn, D Sadigh, arXiv:2403.051102024arXiv preprint</p>
<p>What matters in learning from large-scale datasets for robot manipulation. V Saxena, M Bronars, N R Arachchige, K Wang, W C Shin, S Nasiriany, A Mandlekar, D Xu, CoRL 2024 Workshop on Mastering Robot Manipulation in a World of Abundant Data. </p>
<p>Mink: Python inverse kinematics based on MuJoCo. K Zakka, Jul. 2024</p>
<p>Augmenting reinforcement learning with behavior primitives for diverse manipulation tasks. S Nasiriany, H Liu, Y Zhu, IEEE International Conference on Robotics and Automation (ICRA). 2022</p>
<p>Open X-Embodiment: Robotic learning datasets and RT-X models. X-Embodiment Open, A Collaboration, A O'neill, A Rehman, A Maddukuri, A Gupta, A Padalkar, A Lee, A Pooley, A Gupta, A Mandlekar, Jain, 2023</p>
<p>Cosmos world foundation model platform for physical AI. N Nvidia, A Agarwal, M Ali, Y Bala, E Balaji, T Barker, P Cai, Y Chattopadhyay, Y Chen, Y Cui, D Ding, J Dworakowski, Fan, 2025</p>
<p>Genie: Generative interactive environments. J Bruce, M Dennis, A Edwards, J Parker-Holder, Y Shi, E Hughes, M Lai, A Mavalankar, R Steigerwald, C Apps, Y Aytar, S Bechtle, F Behbahani, S Chan, N Heess, L Gonzalez, S Osindero, S Ozair, S Reed, J Zhang, K Zolna, J Clune, N Freitas, S Singh, T Rocktäschel, 2024</p>
<p>Gaia-1: A generative world model for autonomous driving. A Hu, L Russell, H Yeo, Z Murez, G Fedoseev, A Kendall, J Shotton, G Corrado, 2023</p>
<p>Viola: Imitation learning for vision-based manipulation with object proposal priors. Y Zhu, A Joshi, P Stone, Y Zhu, 6th Annual Conference on Robot Learning. 2022</p>
<p>Inertial properties in robotic manipulation: An object-level framework. O Khatib, IJRR. 1995</p>
<p>Film: Visual reasoning with a general conditioning layer. E Perez, F Strub, H Vries, V Dumoulin, A Courville, 2017</p>
<p>Universal manipulation interface: In-the-wild robot teaching without in-the-wild robots. C Chi, RSSZ Xu, RSSC Pan, RSSE Cousineau, RSSB Burchfiel, RSSS Feng, RSSR Tedrake, RSSS Song, RSSProceedings of Robotics: Science and Systems. Robotics: Science and Systems2024</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. A Dosovitskiy, arXiv:2010.119292020arXiv preprint</p>
<p>U-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, Medical Image Computing and Computer-Assisted Intervention. 2015</p>
<p>Rl-cyclegan: Reinforcement learning aware simulation-to-real. in 2020 ieee. K Rao, C Harris, A Irpan, S Levine, J Ibarz, M Khansari, CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2020163</p>
<p>Retinagan: An object-aware approach to sim-to-real transfer. D Ho, K Rao, Z Xu, E Jang, M Khansari, Y Bai, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE202110926</p>
<p>Sim-to-real transfer for visual reinforcement learning of deformable object manipulation for robot-assisted surgery. P M Scheikl, E Tagliabue, B Gyenes, M Wagner, D Dall'alba, P Fiorini, F Mathis-Ullrich, IEEE Robotics and Automation Letters. 822022</p>
<p>Digital twin (dt)-cyclegan: Enabling zeroshot sim-to-real transfer of visual grasping models. D Liu, Y Chen, Z Wu, IEEE Robotics and Automation Letters. 852023</p>
<p>Cogvideox: Text-to-video diffusion models with an expert transformer. Z Yang, J Teng, W Zheng, M Ding, S Huang, J Xu, Y Yang, W Hong, X Zhang, G Feng, arXiv:2408.060722024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
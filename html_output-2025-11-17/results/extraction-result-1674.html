<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1674 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1674</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1674</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-266573488</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.16741v1.pdf" target="_blank">Bin-Picking of Novel Objects Through Category-Agnostic-Segmentation: RGB Matters</a></p>
                <p><strong>Paper Abstract:</strong> This paper addresses category-agnostic instance segmentation for robotic manipulation, focusing on segmenting objects independent of their class to enable versatile applications like bin-picking in dynamic environments. Existing methods often lack generalizability and object-specific information, leading to grasp failures. We present a novel approach leveraging objectcentric instance segmentation and simulation-based training for effective transfer to real-world scenarios. Notably, our strategy overcomes challenges posed by noisy depth sensors, enhancing the reliability of learning. Our solution accommodates transparent and semi-transparent objects which are historically difficult for depth-based grasping methods. Contributions include domain randomization for successful transfer, our collected dataset for warehouse applications, and an integrated framework for efficient bin-picking. Our trained instance segmentation model achieves state-of-the-art performance over WISDOM public benchmark [1] and also over the custom-created dataset. In a real-world challenging bin-picking setup our bin-picking framework method achieves 98% accuracy for opaque objects and 97% accuracy for non-opaque objects, outperforming the state-of-theart baselines with a greater margin.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1674.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1674.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RGB-only sim-to-real (this work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain-randomized RGB sim-to-real transfer for category-agnostic instance segmentation and bin-picking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that trains a Mask R-CNN instance-segmentation network purely on synthetic RGB images (PyBullet) with heavy domain randomization and transfers the model directly to real-world bin-picking using a UR5 arm with an eye-in-hand Realsense D435i camera and a Schunk WSG-50 gripper; segmentation output drives an analytic grasp planner.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>UR5 manipulator with wrist-mounted Realsense D435i and Schunk WSG-50 gripper</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>UR5 6-DOF robotic arm with an eye-in-hand Realsense D435i RGB-D camera (wrist-mounted) and a two-finger Schunk WSG-50 parallel-jaw gripper used for bin-picking of novel objects (opaque and non-opaque).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>warehouse bin-picking / robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>PyBullet</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>A synthetic bin-on-table environment in PyBullet using scanned 3D object models, randomized object poses and counts, randomized textures and lighting, and rendered RGB (and depth) observations; simulates rigid-body physics and basic camera rendering.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Approximate physics simulation with non-photorealistic rendering and heavy domain randomization (not high-fidelity photorealistic rendering).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Rigid-body object geometry and poses, basic contact/physics via PyBullet, camera viewpoint, randomized lighting, randomized object and bin textures, synthetic RGB rendering (and synthetic depth rendering available but not used for training this variant).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>No photorealistic material/optical modeling (especially for transparent/translucent materials), no explicit modeling of consumer-depth-camera noise characteristics for Realsense D435i, simplified illumination/BRDFs relative to reality, and limited modeling of subtle optical effects (e.g., refraction/partial transparency).</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>A real bin on a table observed by a wrist-mounted Intel Realsense D435i RGB-D camera (commodity-level, produces notably noisy depth maps), objects (daily-use, opaque and transparent/translucent) placed in clutter; UR5 executes open-loop grasps planned from segmentation masks and uses measured depth for 3D conversion where available.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Category-agnostic instance segmentation (segment unseen objects) used to plan grasp poses for bin-picking; analytic grasp evaluation and execution in real world.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised learning: Mask R-CNN (ResNet-50 backbone) trained end-to-end on 30,000 synthetic RGB images with domain randomization (trained for 25 epochs).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Segmentation: COCO-style Average Precision / Average Recall (AP/AR). Grasping: grasp success rate (percentage of successful picks over 100 trials per scenario).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Segmentation: state-of-the-art AP/AR on benchmark and custom dataset (reported qualitatively and in tables); Grasp success rate (real-world bin-picking): 98% for opaque objects, 97% for non-opaque (transparent/translucent) objects.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Randomized object textures (Describable Textures Dataset, 5,640 textures), bin-floor textures (20 wooden textures), randomized lighting parameters (illumination from bright to dim), randomized camera orientation within a small range, randomized object poses and object counts (1–20 per scene).</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Noisy/low-fidelity depth from commodity Realsense sensor (not modeled), limited photorealism/material modeling (especially for transparency), sensor calibration and real-world illumination differences, clutter/occlusion beyond simulation levels.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Training on RGB-only inputs with carefully designed domain randomization (textures, lighting, camera pose, object counts) avoided over-reliance on noisy simulated depth; using analytic grasp evaluation that leverages available real depth for pose conversion while keeping segmentation-driven grasp selection; no real-world fine-tuning required.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Depth-only sim-to-real transfer requires high-precision, near noise-free depth sensors (industrial-grade like Phoxi); commodity-level noisy depth (Realsense D435i) breaks depth-only transfer, so either careful modeling of sensor noise or using RGB domain randomization is required.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Depth-only models (trained on synthetic depth) transferred well to a dataset captured with an industrial Phoxi sensor (high-precision, low-noise) but performed poorly on the authors' dataset captured with a noisy Realsense; RGB-only domain-randomized model achieved state-of-the-art on both WISDOM (Phoxi) and the noisy Realsense dataset. Photo-realistic simulation can also enable transfer but at high computational and engineering cost.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>For bin-picking with commodity (noisy) depth sensors, training instance segmentation on synthetic RGB with heavy domain randomization transfers reliably to real-world, even for transparent and translucent objects; depth-only sim-to-real works only when real depth sensors are high-precision and near noise-free. Camera-specific depth-noise limits generality of depth-trained models; domain randomization for RGB is a practical alternative that avoids camera-specific noise modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bin-Picking of Novel Objects Through Category-Agnostic-Segmentation: RGB Matters', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1674.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1674.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Depth-only sim-to-real (prior works)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sim-to-real transfer using synthetic depth maps for unseen-object instance segmentation / grasping</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior approaches trained CNNs on synthetic depth images (or depth+RGB fusion) and reported direct transfer to real-world bin-picking, but their success depended on using high-precision industrial depth sensors with low noise.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Depth-trained CNNs used with industrial-grade depth sensors (e.g., Phoxi) in bin-picking benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Instance segmentation or grasp-pose prediction networks trained on synthetic depth maps then applied with real depth sensors; used in bin-picking contexts, often with analytic grasp evaluation downstream.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>warehouse bin-picking / robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Training used synthetic depth-image generation (simulator unspecified in paper for these baselines) representing object geometry and depth maps without modeling the specific noise of commodity sensors.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Synthetic depth rendering (approximate geometry-based depth), often assuming low-noise / high-precision depth (i.e., effectively noise-free depth in simulation).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Object geometry and depth rendering, object poses and clutter; some works modeled idealized depth sensing.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Did not model consumer-level depth noise characteristics, material optical effects for transparency, and often lacked photorealistic RGB rendering.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Real-world tests typically used datasets captured with industrial-grade Phoxi depth cameras (high-accuracy, low-noise) and showed good transfer under those conditions; performance degrades with noisy depth sensors.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Instance segmentation and/or direct grasp-pose prediction for bin-picking using depth inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised CNN training on synthetic depth images (sometimes depth+RGB fusion variants exist).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Segmentation AP/AR and grasp success rate in real-world trials.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Reported good real-world performance when used with high-precision industrial depth sensors (exact numbers depend on the cited work); authors report these depth-only models perform poorly on their Realsense-captured dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Dependence on sensor depth quality; unmodeled depth noise from commodity sensors; differences in sensor characteristics between sim and real.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>High-quality, near noise-free depth sensing in the real system (industrial sensors) appears necessary for direct depth-only transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>High-precision (25–500 μm) depth accuracy required for reliable transfer when training on synthetic depth (as measured in WISDOM / industrial setups).</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Depth-only models: strong on datasets captured with industrial depth sensors, weak on datasets captured with noisy commodity sensors; indicates sensitivity to sensor fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Depth-only sim-to-real can work if real depth sensors are high-precision and low-noise; it is fragile when applied with noisy commodity depth sensors, motivating alternatives like RGB domain randomization or sensor-noise modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bin-Picking of Novel Objects Through Category-Agnostic-Segmentation: RGB Matters', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1674.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1674.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Camera-specific depth-noise augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Augmenting simulated depth maps with manually modeled camera-specific noise profiles</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced approach that augments simulated depth maps with manually modeled noise tailored to the target camera to improve sim-to-real transfer for depth-trained models, but it is camera-specific and not a generalized solution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Depth-noise augmented simulation (method)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Technique to modify synthetic depth images by adding modeled noise patterns that mimic the noise characteristics of a particular depth camera to improve transfer of depth-trained models.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotic perception / sim-to-real transfer</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simulated depth rendering with post-hoc injection of a camera-specific noise profile to emulate real sensor artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Synthetic depth rendering augmented with empirically modeled noise (camera-specific).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Intended to model sensor noise characteristics (depth artifacts) specific to a target camera; underlying geometry and depth rendering still approximate.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Noise model is manual and camera-specific—does not generalize across cameras or fully capture complex optical/material interactions (e.g., transparency) unless explicitly modeled.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Intended to match a particular real depth camera's noise profile (e.g., Realsense variants) to make depth-trained models robust to that camera's artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Improve sim-to-real transfer of depth-based segmentation/grasping by matching simulated depth noise to the real camera.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised CNN training on depth images with added synthetic, camera-specific noise.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Segmentation AP/AR or grasp success rate when evaluated on real data captured by the matched camera.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>This approach is an augmentation applied specifically to depth outputs rather than broad domain randomization of textures/lighting.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Requires accurate modeling of camera noise; fails to generalize across different camera models or varied sensing conditions if noise profile mismatched.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Accurate empirical noise model for the target camera; however, approach is limited by camera-specificity.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Paper notes that camera-specific noise modeling can help but is not a generalized solution—depth noise must be modeled to match the target sensor for robust transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Manually modeling depth noise can improve transfer for a specific camera, but it is camera-specific and does not provide a general solution; motivates RGB domain randomization as an alternative when depth noise is high and variable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bin-Picking of Novel Objects Through Category-Agnostic-Segmentation: RGB Matters', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Segmenting unknown 3d objects from real depth images using mask r-cnn trained on synthetic data. <em>(Rating: 2)</em></li>
                <li>Dex-net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics. <em>(Rating: 2)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world. <em>(Rating: 2)</em></li>
                <li>Sim2real in robotics and automation: Applications and challenges. <em>(Rating: 1)</em></li>
                <li>The best of both modes: Separately leveraging rgb and depth for unseen object instance segmentation. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1674",
    "paper_id": "paper-266573488",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "RGB-only sim-to-real (this work)",
            "name_full": "Domain-randomized RGB sim-to-real transfer for category-agnostic instance segmentation and bin-picking",
            "brief_description": "A pipeline that trains a Mask R-CNN instance-segmentation network purely on synthetic RGB images (PyBullet) with heavy domain randomization and transfers the model directly to real-world bin-picking using a UR5 arm with an eye-in-hand Realsense D435i camera and a Schunk WSG-50 gripper; segmentation output drives an analytic grasp planner.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "UR5 manipulator with wrist-mounted Realsense D435i and Schunk WSG-50 gripper",
            "agent_system_description": "UR5 6-DOF robotic arm with an eye-in-hand Realsense D435i RGB-D camera (wrist-mounted) and a two-finger Schunk WSG-50 parallel-jaw gripper used for bin-picking of novel objects (opaque and non-opaque).",
            "domain": "warehouse bin-picking / robotic manipulation",
            "virtual_environment_name": "PyBullet",
            "virtual_environment_description": "A synthetic bin-on-table environment in PyBullet using scanned 3D object models, randomized object poses and counts, randomized textures and lighting, and rendered RGB (and depth) observations; simulates rigid-body physics and basic camera rendering.",
            "simulation_fidelity_level": "Approximate physics simulation with non-photorealistic rendering and heavy domain randomization (not high-fidelity photorealistic rendering).",
            "fidelity_aspects_modeled": "Rigid-body object geometry and poses, basic contact/physics via PyBullet, camera viewpoint, randomized lighting, randomized object and bin textures, synthetic RGB rendering (and synthetic depth rendering available but not used for training this variant).",
            "fidelity_aspects_simplified": "No photorealistic material/optical modeling (especially for transparent/translucent materials), no explicit modeling of consumer-depth-camera noise characteristics for Realsense D435i, simplified illumination/BRDFs relative to reality, and limited modeling of subtle optical effects (e.g., refraction/partial transparency).",
            "real_environment_description": "A real bin on a table observed by a wrist-mounted Intel Realsense D435i RGB-D camera (commodity-level, produces notably noisy depth maps), objects (daily-use, opaque and transparent/translucent) placed in clutter; UR5 executes open-loop grasps planned from segmentation masks and uses measured depth for 3D conversion where available.",
            "task_or_skill_transferred": "Category-agnostic instance segmentation (segment unseen objects) used to plan grasp poses for bin-picking; analytic grasp evaluation and execution in real world.",
            "training_method": "Supervised learning: Mask R-CNN (ResNet-50 backbone) trained end-to-end on 30,000 synthetic RGB images with domain randomization (trained for 25 epochs).",
            "transfer_success_metric": "Segmentation: COCO-style Average Precision / Average Recall (AP/AR). Grasping: grasp success rate (percentage of successful picks over 100 trials per scenario).",
            "transfer_performance_sim": null,
            "transfer_performance_real": "Segmentation: state-of-the-art AP/AR on benchmark and custom dataset (reported qualitatively and in tables); Grasp success rate (real-world bin-picking): 98% for opaque objects, 97% for non-opaque (transparent/translucent) objects.",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Randomized object textures (Describable Textures Dataset, 5,640 textures), bin-floor textures (20 wooden textures), randomized lighting parameters (illumination from bright to dim), randomized camera orientation within a small range, randomized object poses and object counts (1–20 per scene).",
            "sim_to_real_gap_factors": "Noisy/low-fidelity depth from commodity Realsense sensor (not modeled), limited photorealism/material modeling (especially for transparency), sensor calibration and real-world illumination differences, clutter/occlusion beyond simulation levels.",
            "transfer_enabling_conditions": "Training on RGB-only inputs with carefully designed domain randomization (textures, lighting, camera pose, object counts) avoided over-reliance on noisy simulated depth; using analytic grasp evaluation that leverages available real depth for pose conversion while keeping segmentation-driven grasp selection; no real-world fine-tuning required.",
            "fidelity_requirements_identified": "Depth-only sim-to-real transfer requires high-precision, near noise-free depth sensors (industrial-grade like Phoxi); commodity-level noisy depth (Realsense D435i) breaks depth-only transfer, so either careful modeling of sensor noise or using RGB domain randomization is required.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Depth-only models (trained on synthetic depth) transferred well to a dataset captured with an industrial Phoxi sensor (high-precision, low-noise) but performed poorly on the authors' dataset captured with a noisy Realsense; RGB-only domain-randomized model achieved state-of-the-art on both WISDOM (Phoxi) and the noisy Realsense dataset. Photo-realistic simulation can also enable transfer but at high computational and engineering cost.",
            "key_findings": "For bin-picking with commodity (noisy) depth sensors, training instance segmentation on synthetic RGB with heavy domain randomization transfers reliably to real-world, even for transparent and translucent objects; depth-only sim-to-real works only when real depth sensors are high-precision and near noise-free. Camera-specific depth-noise limits generality of depth-trained models; domain randomization for RGB is a practical alternative that avoids camera-specific noise modeling.",
            "uuid": "e1674.0",
            "source_info": {
                "paper_title": "Bin-Picking of Novel Objects Through Category-Agnostic-Segmentation: RGB Matters",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Depth-only sim-to-real (prior works)",
            "name_full": "Sim-to-real transfer using synthetic depth maps for unseen-object instance segmentation / grasping",
            "brief_description": "Prior approaches trained CNNs on synthetic depth images (or depth+RGB fusion) and reported direct transfer to real-world bin-picking, but their success depended on using high-precision industrial depth sensors with low noise.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_system_name": "Depth-trained CNNs used with industrial-grade depth sensors (e.g., Phoxi) in bin-picking benchmarks",
            "agent_system_description": "Instance segmentation or grasp-pose prediction networks trained on synthetic depth maps then applied with real depth sensors; used in bin-picking contexts, often with analytic grasp evaluation downstream.",
            "domain": "warehouse bin-picking / robotic manipulation",
            "virtual_environment_name": null,
            "virtual_environment_description": "Training used synthetic depth-image generation (simulator unspecified in paper for these baselines) representing object geometry and depth maps without modeling the specific noise of commodity sensors.",
            "simulation_fidelity_level": "Synthetic depth rendering (approximate geometry-based depth), often assuming low-noise / high-precision depth (i.e., effectively noise-free depth in simulation).",
            "fidelity_aspects_modeled": "Object geometry and depth rendering, object poses and clutter; some works modeled idealized depth sensing.",
            "fidelity_aspects_simplified": "Did not model consumer-level depth noise characteristics, material optical effects for transparency, and often lacked photorealistic RGB rendering.",
            "real_environment_description": "Real-world tests typically used datasets captured with industrial-grade Phoxi depth cameras (high-accuracy, low-noise) and showed good transfer under those conditions; performance degrades with noisy depth sensors.",
            "task_or_skill_transferred": "Instance segmentation and/or direct grasp-pose prediction for bin-picking using depth inputs.",
            "training_method": "Supervised CNN training on synthetic depth images (sometimes depth+RGB fusion variants exist).",
            "transfer_success_metric": "Segmentation AP/AR and grasp success rate in real-world trials.",
            "transfer_performance_sim": null,
            "transfer_performance_real": "Reported good real-world performance when used with high-precision industrial depth sensors (exact numbers depend on the cited work); authors report these depth-only models perform poorly on their Realsense-captured dataset.",
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Dependence on sensor depth quality; unmodeled depth noise from commodity sensors; differences in sensor characteristics between sim and real.",
            "transfer_enabling_conditions": "High-quality, near noise-free depth sensing in the real system (industrial sensors) appears necessary for direct depth-only transfer.",
            "fidelity_requirements_identified": "High-precision (25–500 μm) depth accuracy required for reliable transfer when training on synthetic depth (as measured in WISDOM / industrial setups).",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Depth-only models: strong on datasets captured with industrial depth sensors, weak on datasets captured with noisy commodity sensors; indicates sensitivity to sensor fidelity.",
            "key_findings": "Depth-only sim-to-real can work if real depth sensors are high-precision and low-noise; it is fragile when applied with noisy commodity depth sensors, motivating alternatives like RGB domain randomization or sensor-noise modeling.",
            "uuid": "e1674.1",
            "source_info": {
                "paper_title": "Bin-Picking of Novel Objects Through Category-Agnostic-Segmentation: RGB Matters",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Camera-specific depth-noise augmentation",
            "name_full": "Augmenting simulated depth maps with manually modeled camera-specific noise profiles",
            "brief_description": "A referenced approach that augments simulated depth maps with manually modeled noise tailored to the target camera to improve sim-to-real transfer for depth-trained models, but it is camera-specific and not a generalized solution.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_system_name": "Depth-noise augmented simulation (method)",
            "agent_system_description": "Technique to modify synthetic depth images by adding modeled noise patterns that mimic the noise characteristics of a particular depth camera to improve transfer of depth-trained models.",
            "domain": "robotic perception / sim-to-real transfer",
            "virtual_environment_name": null,
            "virtual_environment_description": "Simulated depth rendering with post-hoc injection of a camera-specific noise profile to emulate real sensor artifacts.",
            "simulation_fidelity_level": "Synthetic depth rendering augmented with empirically modeled noise (camera-specific).",
            "fidelity_aspects_modeled": "Intended to model sensor noise characteristics (depth artifacts) specific to a target camera; underlying geometry and depth rendering still approximate.",
            "fidelity_aspects_simplified": "Noise model is manual and camera-specific—does not generalize across cameras or fully capture complex optical/material interactions (e.g., transparency) unless explicitly modeled.",
            "real_environment_description": "Intended to match a particular real depth camera's noise profile (e.g., Realsense variants) to make depth-trained models robust to that camera's artifacts.",
            "task_or_skill_transferred": "Improve sim-to-real transfer of depth-based segmentation/grasping by matching simulated depth noise to the real camera.",
            "training_method": "Supervised CNN training on depth images with added synthetic, camera-specific noise.",
            "transfer_success_metric": "Segmentation AP/AR or grasp success rate when evaluated on real data captured by the matched camera.",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": "This approach is an augmentation applied specifically to depth outputs rather than broad domain randomization of textures/lighting.",
            "sim_to_real_gap_factors": "Requires accurate modeling of camera noise; fails to generalize across different camera models or varied sensing conditions if noise profile mismatched.",
            "transfer_enabling_conditions": "Accurate empirical noise model for the target camera; however, approach is limited by camera-specificity.",
            "fidelity_requirements_identified": "Paper notes that camera-specific noise modeling can help but is not a generalized solution—depth noise must be modeled to match the target sensor for robust transfer.",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Manually modeling depth noise can improve transfer for a specific camera, but it is camera-specific and does not provide a general solution; motivates RGB domain randomization as an alternative when depth noise is high and variable.",
            "uuid": "e1674.2",
            "source_info": {
                "paper_title": "Bin-Picking of Novel Objects Through Category-Agnostic-Segmentation: RGB Matters",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Segmenting unknown 3d objects from real depth images using mask r-cnn trained on synthetic data.",
            "rating": 2,
            "sanitized_title": "segmenting_unknown_3d_objects_from_real_depth_images_using_mask_rcnn_trained_on_synthetic_data"
        },
        {
            "paper_title": "Dex-net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics.",
            "rating": 2,
            "sanitized_title": "dexnet_20_deep_learning_to_plan_robust_grasps_with_synthetic_point_clouds_and_analytic_grasp_metrics"
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world.",
            "rating": 2,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "Sim2real in robotics and automation: Applications and challenges.",
            "rating": 1,
            "sanitized_title": "sim2real_in_robotics_and_automation_applications_and_challenges"
        },
        {
            "paper_title": "The best of both modes: Separately leveraging rgb and depth for unseen object instance segmentation.",
            "rating": 1,
            "sanitized_title": "the_best_of_both_modes_separately_leveraging_rgb_and_depth_for_unseen_object_instance_segmentation"
        }
    ],
    "cost": 0.01346875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Bin-picking of novel objects through category-agnostic-segmentation: RGB matters
27 Dec 2023</p>
<p>Prem Raj 
Sachin Bhadang 
Gaurav Chaudhary 
Laxmidhar Behera 
Tushar Sandhan 
Bin-picking of novel objects through category-agnostic-segmentation: RGB matters
27 Dec 2023B1B9BFD47F682B99B482E1E0A9E9E283arXiv:2312.16741v1[cs.RO]Bin-PickingDeep-LearningManipulationClass-agnostic instance segmentation
This paper addresses category-agnostic instance segmentation for robotic manipulation, focusing on segmenting objects independent of their class to enable versatile applications like bin-picking in dynamic environments.Existing methods often lack generalizability and object-specific information, leading to grasp failures.We present a novel approach leveraging objectcentric instance segmentation and simulation-based training for effective transfer to real-world scenarios.Notably, our strategy overcomes challenges posed by noisy depth sensors, enhancing the reliability of learning.Our solution accommodates transparent and semi-transparent objects which are historically difficult for depth-based grasping methods.Contributions include domain randomization for successful transfer, our collected dataset for warehouse applications, and an integrated framework for efficient bin-picking.Our trained instance segmentation model achieves state-of-the-art performance over WISDOM public benchmark[1]and also over the custom-created dataset.In a real-world challenging bin-picking setup our bin-picking framework method achieves 98% accuracy for opaque objects and 97% accuracy for non-opaque objects, outperforming the state-of-theart baselines with a greater margin.</p>
<p>I. INTRODUCTION</p>
<p>Category-agnostic instance segmentation is the technique to segment the individual objects in the scene regardless of their class [1].This method can be utilized for various robotic manipulation applications, such as robotic bin-picking of novel objects.Prominently, the instance-segmentation problem had been studied for cases with predefined semantic classes [2], [3].This might be useful in bin-picking for a limited type of known object.However, this is not feasible for bin-picking in practical scenarios such as warehouse automation where new types of objects are introduced regularly.</p>
<p>There are various state-of-the-art solutions exist for binpicking of novel diverse objects that directly predict the optimal grasp pose without a pre-segmentation step [4]- [8].They have certain disadvantages.First, these are grippercentric solutions, and hence, a solution designed for one type of gripper (e.g. a parallel jaw gripper) can not be easily extendable to other types of gripper (i.e. a suction gripper).The category-agnostic instance segmentation is the object-centric solution and thus can be easily extended to any type of gripper.Secondly, the direct grasp-pose prediction methods do not have any object-specific information, and the predicted optimal grasp-pose might result in a grasp failure during the grasp 1 Intelligence Systems and Control Lab Indian Institute of Technology Kanpur, India {praj, sachinb20, gauravch, lbehera, sandhan}@iitk.ac.in 2 Indian Institute of Technology Mandi, India director@iitmandi.ac.in</p>
<p>RGB-D Camera</p>
<p>Parallel-Jaw Gripper Opaque Objects Non-opaque Objects UR5 Robotic Arm Fig. 1: Experimental setup for our bin-picking problem with unknown diverse objects.The objects consist of opaque and non-opaque (i.e.transparent and translucent) objects.Depth sensing technology has become cheaper and easily available to common users.However, commodity-level sensors often generate noisy depth maps, especially for non-opaque objects.</p>
<p>Our proposed method can grasp opaque and non-opaque objects in the bin, reliably, even in such a challenging scenario.</p>
<p>attempt for various reasons (e.g.object slips when grasped from the corner side rather than grasping from its middle [6]).Convolutional neural networks (CNN) modules are being used by the state-of-the-art solutions for the instancesegmentation tasks.However, the availability of labeled training data remains a major challenge as the process of data labeling is labor-intensive and costlier [9].To overcome this, many solutions have used simulations for auto-generating the training data [4], [10] followed by sim-to-real transfer of the learning for real-world deployment [11].Previously, It was believed that real-world visuals differ significantly from the simulated world and hence the sim-to-real transfer is not promising in the case of models trained with only synthetic RGB data [12], [13].Subsequently, leading work for learning class agnostic segmentation for bin-picking has shown that if the CNN module is trained over only the simulated depth images, the learning can directly be employed in the real world [1].Recently, many similar followed works [12], [14]- [16] have fused RGB features with depth features and shown improved results in this context.However, these methods are tested in the real world with high-quality industrial-grade costlier depth sensors that produce very accurate depth maps with high precision.As verified in our work, this direct transfer of learning with depth-map inputs does not work for noisy depth maps produced with low-cost depth sensors, such as Realsense D435i, that are widely used in the research community in general.The trained network is found to be highly sensitive to noise in the depth maps.</p>
<p>One recent work [17] talks about this issue and purposed to augment the simulated depth maps with manually modeled noise profiles to mimic the real-world noise.However, the modeling of noise is camera-specific and does not provide a generalized solution.In our work, we re-purpose the problem of category-agnostic instance segmentation in the case of notso-high-precision depth sensors and show that a model trained with simulated color (RGB) images can directly transfer to the real world with performance to the level of the state-of-theart if a carefully designed domain randomization strategy [18] is used.Additionally, our method can segment effectively the transparent and the semi-transparent objects, enabling them to be grasped with ease, which always has been a great challenge for depth input modality-based methods as depth sensing is poorer for such objects [5], [6].The effectiveness of the proposed method has also been shown by performing realworld bin-picking trials in a challenging bin-picking setup.The details are further elaborated in Section III.</p>
<p>In summary, the main contributions of our work are as follows:</p>
<p>• Re-purposing the sim-to-real transfer of category-agnostic instance segmentation learning amidst the noisy depth sensing.</p>
<p>• A method to generate simulated training samples with domain randomization for sim-to-real transfer with RGB images.• A simulated as well as a real dataset for category-agnostic instance segmentation in the context of warehouse applications for training and evaluation purposes.• An integrated bin-picking framework that can also grasp transparent and semi-transparent objects effectively.The framework uses the purposed instance-segmentation method and an analytical grasp evaluation method [6].</p>
<p>II. RELATED WORKS</p>
<p>Broadly, our work comprises instance segmentation, simto-real transfer, and bin picking of diverse objects.To gain a better understanding of the existing research in these areas, we will comprehensively review related works in each of these categories.</p>
<p>A. Instance Segmentation</p>
<p>Instance segmentation, which involves simultaneously detecting objects and segmenting them into pixel-level masks, has gained significant attention in recent years due to its practical applications in autonomous driving, robotics, and medical imaging.Mask R-CNN [19], one of the most widely used instance segmentation methods, extends the popular object detection framework, Faster R-CNN [20], by adding a segmentation branch that predicts the object mask in parallel with the object classification and bounding box regression tasks.Building on top of Mask R-CNN, many recent works have aimed to improve the accuracy and efficiency of instance segmentation, including the use of advanced backbones such as shufflenet [21], feature pyramid networks [22], and efficient training strategies [23].Another important area of research in instance segmentation is panoptic segmentation, which combines instance segmentation with semantic segmentation to provide a unified view of the scene [24].</p>
<p>Category-agnostic instance segmentation detects and segments all object instances in an image, without prior knowledge of object categories.This technique is promising for robotics applications, as it enables robust perception and interaction in unstructured environments, where there are no predefined categories of objects [25], [26].It has the potential for complex tasks, such as bin-picking [1], [27]- [29], that require object localization in cluttered settings.</p>
<p>B. Sim-to-real transfer</p>
<p>Sim-to-real transfer is an important research area in robotics that focuses on developing techniques to transfer machine learning models trained in simulation to real-world settings.A variety of methods have been proposed for sim-to-real transfer, including domain randomization [18], [30], data augmentation [31], and adversarial training [32].Recent works have explored the use of sim-to-real transfer in a range of applications, such as robot grasping [5], navigation [33], motion planning [10] and locomotion [34].In the case of the bin-picking problem, recently there are some works [1], [5], [35] that have shown that CNN models trained purely over synthetic depth maps can be directly transferred to the real world.However, in contrast to these findings, we have found that this is only true in the case of high-precision noise-free depth sensing which is costlier.Instead, models trained over only RGB images with appropriate domain randomization can successfully transfer the learning to the real world without any further finetuning.The not-so-perfect depth maps from the low-cost depth sensors can still be useful for subsequent steps in the bin-picking such as grasp pose evaluation [6].</p>
<p>C. Bin-picking</p>
<p>The robotic bin-picking problem has a wide range of formulations depending on the type of objects in the bin (homogeneous or heterogeneous), the target application (warehouse automation or industrial parts handling), and the perception system (camera types, camera positioning, etc.) [36], [37].Specifically, in this paper, we are focusing on the bin-picking solutions that have a use-case in a warehouse automation application where a large number of novel objects with different shapes, sizes, colors, and textures, need to be handled [36].One type of solution for this category of bin-picking is designed to be gripper-specific [4], [5], [7], [38]- [42].On the other hand, the gripper-agnostic works in this category  involve an object-centric approach [1].One type of objectcentric approach uses 3D CAD model of the target object [43] which is not suitable for novel objects whose CAD model is not available.Another type of solution in this category is to apply a category-agnostic instance segmentation [1], [12],</p>
<p>[14]- [17] which is also the focus of our purposed approach.</p>
<p>Our work is most closely related to [1] which is categoryagnostic instance segmentation for bin-picking of unknown novel objects via sim-to-real transfer.</p>
<p>III. OUR METHOD: BIN PICKING WITH UNKNOWN</p>
<p>OBJECTS</p>
<p>Our bin-picking framework mainly consists of two parts.One is the CNN-based model training for class-agnostic instance segmentation and another is the grasp-pose planning using the predicted segmentation mask.First, we will describe the grasp-pose planning part in detail.Next, we will iterate through the deep learning framework for the class-agnostic instance segmentation.A schematic diagram of our proposed method is depicted in the Figure 2 for reference.</p>
<p>A. Class-agnostic Instance Segmentation</p>
<p>Our class-agnostic instance segmentation method aims to segment the previously unseen objects in the bin in a realworld setting.For this, a deep-learning-based framework is utilized via sim-2-real transfer learning.The CNN model is trained entirely in the simulation and learning is transferred directly to the real world.The crucial step for setting up any deep learning framework is to acquire the appropriate training data, choose a befitting CNN model based on the application requirement, and perform proper training.We will describe the details of these components, next.</p>
<p>1) Data Generation: For generating the ground truth training samples, the PyBullet robotic simulator has been used.A synthetic environment has been created within the simulator that consists of a bin kept over a table and objects are spawned in it randomly.The simulated camera is kept just above the bin facing downwards at a distance of 70 cm from the bin floor.The number of objects in the scene ranges from 1 to 20 for different samples.</p>
<p>The 3D object models are taken from an open source google-scanned-objects [44] repository and consist of dailyuse objects such as groceries, medicines, and toys.For each scene, the bin and the scene objects are assigned different textures from a pool of available options.For the bin floor, the textures are pooled from 20 different wooden textures downloaded directly from the web.For objects, the Describable Textures Dataset [45] repository is used which consists of 5,640 texture images of 47 different categories.The camera orientation is randomized for each scene within a short range such that the objects in the scene remain in the camera view.The light parameters of the simulation are also randomized representing a range of scene illuminations varying from the bright day scenes to the dark dim-light scenes.</p>
<p>2) Network design choice and the training: For the CNN network design choice, we choose the standard highperforming Mask-RCNN network with ResNet-50 as the backbone.Our proposed bin-picking framework makes use of an open-loop motion planner in which the grasp pose is predicted once and then the robot executes it without further feedback from the vision.Thus, real-time vision feedback is not necessary, however, the quality of the grasp pose matters.The grasp planning algorithm as described in the next section, depends solely on the segmentation mask predicted by the CNN network.Thus, for our bin-picking framework, the segmentation accuracy is more important than the inference time.</p>
<p>For the training, PyTorch [46] deep learning library is used.The network was trained for 25 epochs with a batch size of 10.The training was carried out using 3 Nvidia-1080Ti GPUs.During the inference, only 1 GPU is used.The training dataset consists of a total of 30,000 samples and a 9:1 ratio is kept for training and validation sets.</p>
<p>B. Grasp Planning Framework</p>
<p>Our bin-picking framework takes the cumulative object instance segmentation mask as input and outputs the final grasp pose for the robot action.The framework for obtaining the instance segmentation mask is described in the previous subsection.To describe our grasp-pose planning method, we define the grasp pose as follows:
G i = (P i , Θ i , W i , Q i )(1)
where P i represents the center point of the grasp pose G i .Θ i denotes the angle of the grasp pose.The grasp pose angle is planner, measured along the vertical axis (i.e.z-axis).The horizontal x-axis is assumed to be the reference zero angle.W i refers to the width of the grasp pose rectangle, and Q i represents the grasp quality index.</p>
<p>The grasp pose is calculated in image coordinates and converted into the robot's world Cartesian frame.This conversion requires using intrinsic and extrinsic camera parameters obtained through a standard calibration procedure.The depth values used for this purpose are expressed in the camera's reference frame.The camera is positioned above the workspace bin at a fixed distance, facing downwards.The grasp-pose evaluation method consists of many sub-steps that are executed sequentially.The overall flow of the method is summarised in Algorithm 1.The details of the different components of the method are described, next.</p>
<ol>
<li>Sampling Candidate Grasp-pose: The algorithm samples grasp poses using segmentation masks generated by our category-agnostic segmentation method (Section III-A).At each segmentation instance, D number of grasp poses are sampled at equally spaced predefined angles (D=6 in our case).Each of these grasp-poses G i is represented by a rectangle of width gw and breadth gb in the image plane.The centers of the segmentation instances become the centers of the corresponding grasp poses.For further processing, the rectangular region corresponding to the grasp pose is cropped from the segmentation mask and horizontally aligned.Then, it is translated such that the top left corner of it coincides with the origin.</li>
</ol>
<p>Grasp Pose Subsectors Identification:</p>
<p>To ensure a comprehensive evaluation of a grasp pose G i , our objective is to partition the complete area within the grasp pose rectangle into three distinct subsectors.The tactile contact sector S tc denotes the section of the target object's area within the grasp pose rectangle.The unobstructed space sector S uo encompasses the region within the grasp pose boundary where the gripping device is unlikely to encounter obstacles during the grasp attempt (i.e. the area corresponding to the background region as per the segmentation mask).The remaining segment constitutes the collision sector S cl indicating the area where the gripping device is prone to collide with other objects.The derivation of these subsectors is done through a simple strategy that uses the obtained segmentation mask.In the grasp pose rectangle area, the pixels corresponding to the target objects are assigned to S tc , the pixels corresponding to the background class are assigned to S uo and the rest pixels are assigned to S cl .For visualization in Figure 2b, the subsectors, namely, S tc , S uo and S cl are depicted with green, white, and red colors.</p>
<ol>
<li>Grasp Pose Filtration: In the process of validating all the sampled grasp poses, we assess the suitability of each one.A grasp pose is deemed unsuitable under the following circumstances: For all the sampled grasp poses, we check for the validity of each one.A grasp pose is not suitable in the following two situations:</li>
</ol>
<p>• If the width of the target object along the orientation of the grasp pose exceeds the maximum potential opening of the gripper, then the grasp becomes unviable.To ascertain this, we compare the maximum width of the tactile contact sector S tc with the gripper's maximum opening capacity.• Adequate space within the free-space sector must be available for the gripper's fingers to enter.To confirm this, we compare the minimum width of the unobstructed space sector S uo on both sides of the grasp pose with the width of the gripper's fingers.</p>
<p>Grasp Pose Finetuning:</p>
<p>We finetune the grasp poses to enhance their effectiveness.Initially, we reposition the grasp pose's center to align with the center of the S tc .This adjustment ensures that the grasp pose's central point aligns more accurately with the center of the target object along the grasp pose orientation, resulting in improved stability during the grasping process.Furthermore, in the process of determining the refined width, we calculate the disparity between the centers of the masks representing the left and right subparts of the region S uo .This method allows us to derive a more precise measurement of the refined width, facilitating a more accurate assessment of the available space for the gripper's fingers.</p>
<ol>
<li>Grasp Quality Assesment: If more than one grasp pose has passed the pose validation step, the grasp quality index Q i is calculated for each of the valid grasp poses for their ranking.For the calculation of Q i , three things are taken into consideration: first is unobstructed-space-score (OSS), which is the normalized area of the unobstructed space sector S uo , second is contact-tangibility-score (CTS), which is the normalized area of the tactile contact sector S tc within a predefined rectangular region around the center, and the third is the segmentation score (SS) which is the confidence score predicted by our instance segmentation network.Each of these components takes values between 0 and 100.The value of the grasp quality index Q i is obtained by taking an average of the above three components.</li>
</ol>
<p>IV. RESULTS AND DISCUSSION</p>
<p>In this section, we assess the effectiveness of our method for bin-picking unknown novel objects through class-agnostic Algorithm 1: Grasp planning algorithm as used in our bin-picking framework.The algorithm is discussed in detail in Section III-B.</p>
<p>Smap ← Predicted instance segmentation map; P i ← Center point of the grasp pose; Θ i ← Angle of the grasp pose; W i ← Width of the grasp pose rectangle; W 0 ← Initial fix width of the grasp pose rectangle; segmentation.First, in the next subsection, we evaluate our proposed framework for class-agnostic segmentation.Subsequently, an evaluation of the proposed bin-picking method is carried out with real-world bin-picking experiments.
Q i ← Grasp quality index (GQI); G i ← Grasp pose = (P i , Θ i , W i , Q i ); Let i</p>
<p>A. Evaluation of the Class-agnostic instance segmentation method</p>
<p>To evaluate our proposed framework for class-agnostic instance segmentation for bin-picking applications, two datasets are considered.First, is WISDOM [1] which is a public benchmark dataset in this domain, and second is our custommade dataset.In Table I the two datasets are compared over various attributes.The notable difference between the two datasets is the depth sensors used for capturing the depth maps.While for the WISDOM dataset, industrial grade costlier Phoxi camera is used that produces high precision (accuracy of 25-500 um) depth maps, for our custom-dataset the commodity level cost-effective Realsense camera is used that produces considerable noisy depth maps (accuracy of 2.5 -5 mm).</p>
<p>As an evaluation metric, the average precision (AP) and average recall (AR) are used as defined by the COCO benchmark [47] for the instance segmentation task.For calculating AP and AR, IoU thresholds from 0.50 to 0.95 with a 0.05 margin were used and top-100 detections were considered.The experimental results are reported in Table II.Our method has achieved better results compared to the considered baseline methods [1], [16], [17].For baselines, we only consider classagnostic segmentation works that are related to the bin-picking applications.All the baselines use their respective customgenerated simulated data for the training.The baseline [1] uses only a depth map as the input.The baseline [17] uses a fusion of depth and RGB features as the input.For [16], two variants are considered, one uses only depth map as the input and another uses photo-realistic rendered RGB images.</p>
<p>As shown in the table, the method that has used depth data as the input performs considerably over the Wisdom dataset while the performance over our custom dataset is poorer.The noisy depth maps are the reason behind the performance decline of these methods.Our method used only synthetic RGB images as the input and was able to transfer well in the real world with the help of domain randomization.Our method achieves state-of-the-art performance over both datasets while using only the RGB image as the input.Photo-realism can also TABLE II: Comparing different model variants based on the segmentation performance evaluated in terms of average precision and average recall (as defined by COCO benchmarks [47]).The performance was measured over the two datasets, namely WISDOM [1] and Ours (See Table I).Our method outperforms all the considered state-of-the-art baselines.For further discussion, please refer to Section IV-A.be an alternative for smooth sim-to-real transfer as shown by the results of the method [16].Nevertheless, this approach mandates meticulously crafted simulations and significant computational expenses, resulting in limited adaptability and impracticality for real-world implementations.</p>
<p>B. Bin Picking Experiments</p>
<p>To evaluate the complete end-to-end bin-picking pipeline, we have performed real-world experiments.For the experiments, a UR5 robotic manipulator arm is used.A Realsense D435i RGB-D camera is mounted upon the wrist of the manipulator as an eye-in-hand configuration.As a gripping tool, the Schunk WSG-50 gripper (two-fingered parallel-jaw gripper) is mounted at the end-effector of the manipulator.The setup is shown in Figure 1.As shown in the figure, the camera is looking directly in the downward direction where the target objects are placed in a bin.</p>
<p>The experiments consist of various daily usable objects including transparent and semi-transparent objects.Our grasp prediction method mainly relies on the segmentation mask generated from the CNN network, which uses only the RGB image.All the grasp-pose parameters are calculated without using the depth map.</p>
<p>For the experiments, we divide the object set into two.One is opaque objects and the other is non-opaque objects (transparent and translucent objects).We evaluate our method along with the considered baseline methods [5], [6], [38] in two different scenarios, all opaque objects only and all nonopaque objects only.</p>
<p>The bin-picking experiments are performed using our proposed method and the selected state-of-the-art baseline methods [5], [6], [38].For each method, a total of 100 grasp trials are performed in each scenario type (i.e.opaque and nonopaque).Initially, 15 and 10 objects are randomly thrown in the bin, respectively for opaque and non-opaque categories.Then, objects are grasped one by one and put into the receptacle.Again, a new iteration is started when either two consecutive failures have occurred or all the objects in the scene are grasped.Consecutive grasp failures at the same location are counted only once.Thus, this process is repeated until the total number of grasp attempts reaches 100.</p>
<p>The results are reported in Table III.Our method outperforms all the considered baselines by a large margin.The methods [38] and [5] were trained over noise-free simulated depth images and thus perform poorly in noisy depth sensing environments.In the case of non-opaque objects, the noise in the depth maps increases further resulting in further decline in the performance.Our method is independent of the depth data and thus performs superior.Furthermore, it is interesting to see that our method performs equally well with non-opaque objects although the training data for our instance-segmentation network does not contain any non-opaque objects.</p>
<p>V. CONCLUSION AND FUTURE WORKS</p>
<p>This study addresses the critical challenge of categoryagnostic instance segmentation for robotic manipulation, enabling versatile applications such as bin-picking with unknown objects in clutter.By focusing on object-centric segmentation and leveraging simulation-based training, our approach is able to segment unknown objects in the real world without a single real-world training sample.The devised strategy effectively addresses the inherent noise in depth sensors and enables reliable picking of objects in the absence of high-precision depth sensing.Notably, our solution accommodates transparent and semi-transparent objects, historically challenging for depthbased techniques.The contributions encompass a successful domain randomization strategy, the provision of benchmark datasets for warehouse applications, and an integrated binpicking framework for enhanced efficiency.</p>
<p>One of the challenges our method faces is that the segmentation quality becomes poorer when the clutter in the bin rises beyond a certain level.It will be interesting to see a method that can selectively segment the objects reliably that are graspable and avoids others that are mostly occluded and will not likely cause any collision during the grasp attempt.Another possible direction for future work is to encompass the depth of information within the learning process for the instance segmentation task while the input to the deep network is still the RGB image only.One way to achieve this is to add a depth estimation as the auxiliary task in the network design.TABLE IV: Qualitative examples of instance-segmentation and grasp pose prediction by our method (i.e.rows 4 and 8, respectively) along with the grasp pose predictions by the considered baseline methods (i.e.rows 5-7).Scenes 1-3 consist of opaque objects and scenes 4-6 consist of non-opaque (i.e.transparent and translucent objects).The results indicate that our grasp-pose planning framework demonstrates superior performance in predicting high-quality grasp-poses compared to existing approaches, particularly in difficult bin-picking scenarios (Best viewed in color).</p>
<p>Grasp-Pose Evaluation process (depicted for a single grasp pose).</p>
<p>Fig. 2 :
2
Fig. 2: Schematic diagram of our proposed method (Best viewed in the color).Part (a) shows the overview of the entire end-to-end grasp planning for the bin-picking.Part (b) highlights the grasp evaluation process and depicts it for a particular grasp pose instance.Details of the instance segmentation module are provided in Section III-A.The grasp pose sampling and grasp pose evaluation are discussed in Section III-B.</p>
<p>TABLE I :
I
[1]ote the i-th grasp pose parameters and the best grasp pose parameters be denoted by * Comparison of two datasets considered for the evaluation of the methods.The two datasets consist of diverse daily-use objects kept within a bin in clutter.WISDOM[1]is a public benchmark while Ours is our custom-created dataset.Each dataset provides RGB images, depth maps, and ground truth segmentation of the object instances.
WISDOMOursDepth SensorIndustrial GradeCommodity LevelDepth Map noiseAlmost noise freeConsiderable noiseDepth Accuracy25 -500 micrometers2.5 -5 millimetersSamples300100Image Size1032x772640x480Object TypesDaily use objectsDaily use objectsSegmentation labelsYesYesData: SmapResult: Best grasp pose G  ValidPose = False;ObjectWidth = MaxWidth(S tc );FslWidth = minWidth(S uo Lef t);FsrWidth = minWidth(S uo Right);if FslWidth &gt; GripperFingerWidth andFsrWidth &gt; GripperFingerWidth andObjectWidth &lt; MaxGripperOpening thenValidPose = True;elseInvalid Poseif ValidPose thenP ose.P i = Centre(S tc );P ose.W i = Centre(S uo Right) -Centre(S uo Lef t);P ose.Q i = OSS + CTS + SS;return P ose;elsereturn None
* Initialize G * = ∅; foreach Mask in Smap do P i = M askCentre; Θ i = M askAngle; candidateGraspP oses = append.(Pi ,Θ i ,W 0 ,N ON E) foreach Pose in candidateGraspP oses do G = PoseEvaluation(Pose); ref inedP oses = append.(G);G * = SelectBestPose (ref inedP oses); return G * ; Function PoseEvaluation(Pose): SubsectorsIdentification : S tc , S uo , S cl ;</p>
<p>TABLE III :
III
Real-world bin-picking experiments with opaque and non-opaque objects in a challenging scenario.Our method outperforms the considered state-of-the-art baselines in each category.The performance is measured in terms of grasp success rate.For the grasp trials, a maximum of 15 and 10 objects are present in the bin, respectively for opaque and non-opaque categories.For further details, please refer to Section IV-B.(D stands for depth in the table.)
MethodInputGrasp SuccessInferenceTypeOpaque Non-opaqueTime (ms)GRConv [38]RGB+D413421DexNet (4.0-PJ) [5]D66541212Raj [6]D8773375OursRGB9897144</p>
<p>Segmenting unknown 3d objects from real depth images using mask r-cnn trained on synthetic data. M Danielczuk, M Matl, S Gupta, A Li, A Lee, J Mahler, K Goldberg, 2019 International Conference on Robotics and Automation (ICRA). IEEE2019</p>
<p>Cartman: The low-cost cartesian manipulator that won the amazon robotics challenge. D Morrison, A W Tow, M Mctaggart, R Smith, N Kelly-Boxall, S Wade-Mccue, J Erskine, R Grinover, A Gurman, T Hunn, 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE2018</p>
<p>Semi supervised deep quick instance detection and segmentation. A Kumar, L Behera, 2019 International Conference on Robotics and Automation (ICRA). IEEE2019</p>
<p>Dex-net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics. J Mahler, RSSJ Liang, RSSS Niyaz, RSSM Laskey, RSSR Doan, RSSX Liu, RSSJ A Ojea, RSSK Goldberg, RSSProceedings of Robotics: Science and Systems. Robotics: Science and SystemsIEEE2017</p>
<p>Learning ambidextrous robot grasping policies. J Mahler, M Matl, V Satish, M Danielczuk, B Derose, S Mckinley, K Goldberg, Science Robotics. 4262019</p>
<p>Towards object agnostic and robust 4-dof table-top grasping. P Raj, A Kumar, V Sanap, T Sandhan, L Behera, IEEE 18th International Conference on Automation Science and Engineering. 2022</p>
<p>Graspfusionnet: a two-stage multi-parameter grasp detection network based on rgb-xyz fusion in dense clutter. W Wang, W Liu, J Hu, Y Fang, Q Shao, J Qi, Machine Vision and Applications. 3172020</p>
<p>Uncertainty-based exploring strategy in densely cluttered scenes for vacuum cup grasping. K Tung, J Su, J Cai, Z Wan, H Cheng, 2022 International Conference on Robotics and Automation (ICRA). </p>
<p>Microsoft coco: Common objects in context. T.-Y Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P Dollár, C L Zitnick, Computer Vision-ECCV 2014: 13th European Conference. Zurich, SwitzerlandSpringerSeptember 6-12, 2014. 2014Proceedings, Part V 13</p>
<p>Learning to switch cnns with model agnostic meta learning for fine precision visual servoing. P Raj, V P Namboodiri, L Behera, 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE202010217</p>
<p>Object detection using sim2real domain randomization for robotic applications. D Horváth, G Erdős, Z Istenes, T Horváth, S Földi, IEEE Transactions on Robotics. 2022</p>
<p>The best of both modes: Separately leveraging rgb and depth for unseen object instance segmentation. C Xie, Y Xiang, A Mousavian, D Fox, Conference on robot learning. PMLR2020</p>
<p>Sim2real in robotics and automation: Applications and challenges. S Höfer, K Bekris, A Handa, J C Gamboa, M Mozifian, F Golemo, C Atkeson, D Fox, K Goldberg, J Leonard, IEEE transactions on automation science and engineering. 1822021</p>
<p>Learning rgb-d feature embeddings for unseen object instance segmentation. Y Xiang, C Xie, A Mousavian, D Fox, Conference on Robot Learning. PMLR2021</p>
<p>Unseen object instance segmentation for robotic environments. C Xie, Y Xiang, A Mousavian, D Fox, IEEE Transactions on Robotics. 3752021</p>
<p>Unseen object amodal instance segmentation via hierarchical occlusion modeling. S Back, J Lee, T Kim, S Noh, R Kang, S Bak, K Lee, 2022 International Conference on Robotics and Automation (ICRA). IEEE2022</p>
<p>Segmenting unseen industrial components in a heavy clutter using rgb-d fusion and synthetic data. S Back, J Kim, R Kang, S Choi, K Lee, 2020 IEEE International Conference on Image Processing. 2020</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE2017</p>
<p>Mask r-cnn. K He, G Gkioxari, P Dollar, R Girshick, Proceedings of the IEEE International Conference on Computer Vision (ICCV). the IEEE International Conference on Computer Vision (ICCV)2017</p>
<p>Faster r-cnn: Towards real-time object detection with region proposal networks. S Ren, K He, R Girshick, J Sun, Proceedings of the Conference on Neural Information Processing Systems (NIPS). the Conference on Neural Information Processing Systems (NIPS)2015</p>
<p>Shufflenet v2: Practical guidelines for efficient cnn architecture design. X Zhang, X Zhou, M Lin, J Sun, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)2018</p>
<p>Feature pyramid networks for object detection. T.-Y Lin, P Doll'ar, R Girshick, K He, B Hariharan, S Belongie, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2017</p>
<p>Big transfer (bit): General visual representation learning. J Chen, S Kornblith, M Norouzi, G Hinton, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021</p>
<p>Panoptic segmentation. A Kirillov, R Girshick, K He, P Doll'ar, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2019</p>
<p>Joint object and pose recognition using conditional latent-variable models. A Mousavian, A Toshev, A Fathi, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2018</p>
<p>An object recognition and localization method based on category-agnostic instance segmentation. Y Zhou, H Zheng, B Zhao, J Liu, Proceedings of the 2020 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA). the 2020 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA)IEEE2020</p>
<p>A real-time system for robotic bin picking based on category-agnostic instance segmentation. Y Li, X Wang, W Cao, J Li, H Lu, 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEE202010438</p>
<p>An efficient and robust category-agnostic instance segmentation method for bin-picking robots. L Qin, S Yang, H Liu, Robotics and Autonomous Systems. 1202019</p>
<p>Bin-picking for multiple objects based on category-agnostic instance segmentation and hand-eye calibration. X Yin, Y Huang, X Li, Y Zhang, J Li, S Li, S Chen, IEEE Transactions on Automation Science and Engineering. 1832021</p>
<p>Transferring end-to-end visuomotor control from simulation to real world for a multi-stage task. S James, A J Davison, E Johns, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2018</p>
<p>A survey on image data augmentation for deep learning. C Shorten, T M Khoshgoftaar, Journal of Big Data. 61602019</p>
<p>Sim-to-real transfer of robotic control with dynamics randomization. S James, A Gupta, S Levine, 2019 International Conference on Robotics and Automation (ICRA). IEEE2019</p>
<p>Sim2real view invariant visual servoing by recurrent control. F Sadeghi, S Levine, Robotics: Science and Systems (RSS). 2018</p>
<p>Deep reinforcement learning for robotic locomotion with asynchronous advantage actor-critic (a3c). X B Peng, G Berseth, C Yin, S Schaal, IEEE International Conference on Robotics and Automation (ICRA). IEEE2018</p>
<p>Driving in the matrix: Can virtual worlds replace humangenerated annotations for real world tasks?. M Johnson-Roberson, C Barto, R Mehta, S N Sridhar, K Rosaen, R Vasudevan, 2017 IEEE International Conference on Robotics and Automation (ICRA). IEEE2017</p>
<p>What are the important technologies for bin picking? technology analysis of robots in competitions based on a set of performance metrics. M Fujita, Y Domae, A Noda, G Garcia Ricardez, T Nagatani, A Zeng, S Song, A Rodriguez, A Causo, I.-M Chen, Advanced Robotics. 347-82020</p>
<p>Bin picking approaches based on deep learning techniques: A state-of-the-art survey. A Cordeiro, L F Rocha, C Costa, P Costa, M F Silva, 2022 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC). 2022</p>
<p>Antipodal robotic grasping using generative residual convolutional neural network. S Kumra, S Joshi, F Sahin, 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). </p>
<p>Domainindependent unsupervised detection of grasp regions to grasp novel objects. S V Pharswan, M Vohra, A Kumar, L Behera, 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2019</p>
<p>Learning hand-eye coordination for robotic grasping with deep learning and largescale data collection. S Levine, P Pastor, A Krizhevsky, J Ibarz, D Quillen, The International Journal of Robotics Research. 374-52018</p>
<p>Domainindependent disperse and pick method for robotic grasping. P Raj, A Singhal, V Sanap, L Behera, R Sinha, 2022 International Joint Conference on Neural Networks (IJCNN). 2022</p>
<p>Scalable and time-efficient binpicking for unknown objects in dense clutter. P Raj, L Behera, T Sandhan, IEEE Transactions on Automation Science and Engineering. 2023</p>
<p>3d matching by combining cad model and computer vision for autonomous bin picking. L D Hanh, K T G Hieu, International Journal on Interactive Design and Manufacturing (IJIDeM). 152021</p>
<p>Google scanned objects: A high-quality dataset of 3d scanned household items. L Downs, A Francis, N Koenig, B Kinman, R Hickman, K Reymann, T B Mchugh, V Vanhoucke, arXiv:2204.119182022arXiv preprint</p>
<p>Describing textures in the wild. M Cimpoi, S Maji, I Kokkinos, S Mohamed, A Vedaldi, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2014</p>
<p>Pytorch: An imperative style, highperformance deep learning library. A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, A Desmaison, A Kopf, E Yang, Z Devito, M Raison, A Tejani, S Chilamkurthy, B Steiner, L Fang, J Bai, S Chintala, Advances in Neural Information Processing Systems. Curran Associates, Inc201932</p>
<p>. T Lin, M Maire, S J Belongie, L D Bourdev, R B Girshick, J Hays, P Perona, D Ramanan, P Doll'a R, C L Zitnick, 2014Microsoft COCO: common objects in context," CoRR, vol. abs/1405.0312</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-384 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-384</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-384</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-16.html">extraction-schema-16</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <p><strong>Paper ID:</strong> paper-258426468</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.00366v1.pdf" target="_blank">S2abEL: A Dataset for Entity Linking from Scientific Tables</a></p>
                <p><strong>Paper Abstract:</strong> Entity linking (EL) is the task of linking a textual mention to its corresponding entry in a knowledge base, and is critical for many knowledge-intensive NLP applications. When applied to tables in scientific papers, EL is a step toward large-scale scientific knowledge bases that could enable advanced scientific question answering and analytics. We present the first dataset for EL in scientific tables. EL for scientific tables is especially challenging because scientific knowledge bases can be very incomplete, and disambiguating table mentions typically requires understanding the papers's tet in addition to the table. Our dataset, S2abEL, focuses on EL in machine learning results tables and includes hand-labeled cell types, attributed sources, and entity links from the PaperswithCode taxonomy for 8,429 cells from 732 tables. We introduce a neural baseline method designed for EL on scientific tables containing many out-of-knowledge-base mentions, and show that it significantly outperforms a state-of-the-art generic table EL method. The best baselines fall below human performance, and our analysis highlights avenues for improvement.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e384.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e384.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TURL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TURL: Table Understanding through Representation Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structure-aware Transformer encoder pre-trained on general-purpose web/table corpora (WikiTable) that produces contextualized embeddings for table cells, rows, and columns for downstream table tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Turl: table understanding through representation learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>TURL (structure-aware Transformer for table representation)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>TURL is a Transformer-based model pre-trained on large corpora of web tables (WikiTable) to learn structure-aware representations of table elements (cells, rows, columns). It encodes cell text together with table layout features to produce contextual embeddings used for downstream tasks such as entity linking via candidate ranking or classification.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / representation learning for tables</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>general web / Wiki-style tables (general-domain table understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>scientific tables in machine learning papers (results tables)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (fine-tuned on inKB cells of the scientific table dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Fine-tuned TURL on the in-KB cells from the S2abEL dataset (scientific ML results tables) using the same candidate set size (50) for comparison. No structural redesign was described; adaptation consisted of supervised fine-tuning on scientific-table examples.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>Partially successful but limited: TURL performed worse than the specialized SciBERT-based approach on scientific tables — the paper reports a substantial improvement by the authors' method over TURL on nine out of ten topic folds (Table 3). Reported micro-average accuracy numbers in Table 3 are approximately 32.5 for TURL versus 44.8 for the authors' method (i.e., TURL underperformed in this scientific-table EL setting).</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>TURL focuses on cell content and table positional features and lacks integration of full-document context, causing failures on short/abbreviated mentions (cells shorter than four characters), highly domain-specific abbreviations, and cases requiring citation/paragraph disambiguation. Also, pretraining on general web tables did not provide sufficient domain-specific signals for the scientific ML tables.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>TURL's structural modeling of tables (cell/row/column embeddings) aligns with table data and likely helped some cases; availability of public code allowed direct fine-tuning and comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Required fine-tuning data from the target domain (inKB scientific table cells) and candidate entity lists for supervised training; performance benefited from additional document-level context which TURL in its standard form does not incorporate.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Limited: while TURL generalizes to table tasks in principle, the paper finds it insufficient for scientific-result tables without additional document-context modeling; it likely needs adaptation (document context, domain-specific pretraining/fine-tuning) to obtain good performance in other specialized table domains.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>instrumental/technical skills (model architecture and pretraining procedure) and explicit procedural steps (fine-tuning protocol)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'S2abEL: A Dataset for Entity Linking from Scientific Tables', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e384.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e384.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dense Retrieval (DR) / Bi-encoder</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dense retrieval with bi-encoder (adapted from Dense Passage Retrieval and Sentence-BERT methods)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bi-encoder dense retrieval approach that embeds table-cell contexts and KB entity descriptions into a shared embedding space to retrieve candidate entities by nearest-neighbor search; adapted from open-domain dense passage retrieval and sentence-embedding methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dense passage retrieval for open-domain question answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Dense retrieval (bi-encoder) for candidate entity retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>A bi-encoder architecture (two separate SciBERT encoders) is fine-tuned with a triplet objective to learn embeddings for table-cell contexts and KB entity textual representations; candidate entities are retrieved by nearest-neighbor search in the embedding space (top-k). Negative examples and triplet loss were used during training to separate true entity embeddings from negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / retrieval & candidate generation</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>open-domain document/question answering and dense passage retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>candidate entity retrieval for entity linking from scientific tables</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (fine-tuned bi-encoder using SciBERT and triplet loss on table-cell↔entity pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Replaced generic encoders with SciBERT encoders specialized for scientific text; trained with a triplet objective on cells whose gold entity exists in the KB; used Euclidean distance with a margin of 1; integrated with other retrieval strategies (interleaving with ASR) to produce final candidate sets; trained only on in-KB training examples.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>Partially successful: dense retrieval captured latent semantics and retrieved many correct candidates but underperformed attributed-source-based retrieval (ASR) on this task. The paper reports that seeding candidates with attributed-paper entities (ASR) significantly outperforms DR alone, and interleaving DR and ASR yields the best recall@K performance overall.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Failures often due to extremely short/generic cell mentions (e.g., 'val', 'ours', 'window'), which lack lexical cues for dense models; domain-specific abbreviations and context-dependent shorthand are hard for DR to resolve. Also, DR trained only on inKB examples cannot recover outKB mentions.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Use of SciBERT (scientific-text pretraining) improved semantic representation; bi-encoder architecture allowed efficient nearest-neighbor retrieval; triplet training with negatives provided discriminative embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Requires sufficient supervised cell↔entity training pairs (in-KB) for fine-tuning, a vector index (Elasticsearch or other ANN) for nearest-neighbor search, and carefully chosen negative sampling. Performance depends on availability of meaningful textual descriptions for KB entities.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Moderate: dense retrieval techniques generalize across retrieval tasks, but effectiveness depends on the similarity between training and target textual distributions and on availability of entity descriptions; combining with domain-specific signals (like ASR) improves robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>computational method / explicit procedural steps (model training, triplet loss, retrieval pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'S2abEL: A Dataset for Entity Linking from Scientific Tables', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e384.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e384.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ASR via ASM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Attributed Source Retrieval (ASR) using Attributed Source Matching (ASM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval method that ranks cited/attributed papers for a table cell and fetches entities associated with those papers from a KB to seed the candidate entity set for entity linking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>Attributed Source Retrieval (ASR) driven by Attributed Source Matching (ASM)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>ASM is a SciBERT-based binary classifier that predicts which papers in the document's reference list (or the document itself) are the attributed sources for a given table cell. ASR then retrieves entities associated with high-probability attributed papers (via Paper-RelatesTo-Entity relations in the KB) and interleaves those entities with DR candidates to form the candidate set.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / knowledge-driven candidate retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>citation/reference analysis and metadata linking (bibliographic/source attribution)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>entity candidate retrieval for table EL in scientific papers</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/combined approach (learned source-attribution model used to drive KB entity retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Trained a SciBERT-based ASM model to take a cell and a candidate paper representation (index, author, year, title, abstract) and output likelihood of attribution; constructed candidate entity lists by fetching entities associated with top-ranked attributed papers and filtered by predicted cell type; interleaved ASR results with DR candidates to produce final candidate list.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>Largely successful as a seed strategy: ASR seeding significantly outperformed DR alone in recall@K; interleaving ASR with DR provided the best candidate recall. However, ASR missed gold entities when papers did not cite the concept (22.8% of error cases at K=100) or when KB Paper-RelatesTo-Entity relations were incomplete.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Failure cases include (1) authors not citing the original or relevant paper for a concept (common for well-known entities) and (2) incomplete or missing Paper-RelatesTo-Entity relations in the KB. Search/ranking noise and citation ambiguity also reduce effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of document reference sections, a KB with paper↔entity relations (Papers with Code), and an effective ASM classifier to rank candidate sources.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Requires (a) access to the document's references and in-document context, (b) a KB containing paper-to-entity relations, and (c) a trained ASM model; manual verification or larger K may be needed when KB coverage is imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Limited-to-moderate: ASR is powerful where document citations map well to KB relations; less effective in domains where authors do not cite canonical works or where KBs lack coverage of paper↔entity relations. The general approach (use source attribution to retrieve candidates) is applicable to other scientific domains given similar metadata and KB structure.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>interpretive frameworks (using attribution as a disambiguation signal) and explicit procedural steps (model training and KB lookup)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'S2abEL: A Dataset for Entity Linking from Scientific Tables', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e384.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e384.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5-small reference parser</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5-small fine-tuned for reference-string parsing (author/year/title extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text-to-text transformer (T5-small) fine-tuned to parse raw bibliography/reference strings into structured metadata (first-author last name, year, title) to enable searching and retrieval of cited papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring the limits of transfer learning with a unified text-to-text transformer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>T5-small fine-tuning for reference metadata extraction</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>A T5-small model was fine-tuned to take raw reference strings from papers and output structured metadata fields (last name of first author, year, title). These extracted fields were then used to query Semantic Scholar to retrieve paper abstracts and metadata, with manual verification of search results.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / information extraction</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>general text-to-text transformer models and NLP (pretrained language modeling)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>bibliographic metadata extraction and attributed-source identification in scientific papers</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>adapted/modified for new context (task-specific fine-tuning of a general text-to-text transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Fine-tuned T5-small specifically on reference-string→(author/year/title) mapping; used outputs to query external search APIs (Semantic Scholar) and manually verified results because search APIs sometimes return non-top matches.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>Successful pragmatically: enabled automated extraction of candidate attributed papers and retrieval of abstracts for ASM use. However, search API noise required manual verification for many queries (i.e., the pipeline was semi-automatic).</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Search API ranking issues (matching paper not always top result), noisy or variable reference string formats, and need for manual verification to ensure high precision.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>T5's flexible text-to-text format made it straightforward to learn reference parsing; availability of Semantic Scholar to retrieve candidate papers and their abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Requires labeled examples of reference strings for fine-tuning, access to bibliographic search APIs (Semantic Scholar), and manual verification workflows to ensure high precision.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High for other document metadata extraction tasks — the approach (fine-tune T5 for structured extraction) is broadly applicable across domains with similar reference string formats, but end-to-end automation depends on the quality of downstream search APIs and KB coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps (fine-tuning and extraction pipeline) and instrumental technical skills (using search APIs and verification)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'S2abEL: A Dataset for Entity Linking from Scientific Tables', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e384.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e384.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciBERT adaptations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciBERT-based task-specific fine-tuning and input augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>SciBERT, a BERT variant pre-trained on scientific text, was adapted and fine-tuned with task-specific input augmentations (token-type embeddings indicating token source like cell or context) for cell type classification, attributed source matching, dense retrieval encoders, and cross-encoder entity disambiguation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SciB-ERT: A pretrained language model for scientific text</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>SciBERT fine-tuning with token-source augmentation for multiple table EL subtasks</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>The authors used SciBERT as the backbone encoder for multiple subtasks (CTC, ASM, DR bi-encoder, and cross-encoder ED). They augmented token embeddings with an additional trainable embedding indicating whether a token is from the table cell, context sentence, paper metadata, or KB entity description. Different heads/losses were applied per task (cross-entropy for CTC, BCE for ASM/ED, triplet loss for DR).</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / domain-adapted language model fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>pretrained scientific-language models (scientific NLP)</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>specialized downstream tasks over scientific tables (cell typing, source attribution, candidate retrieval, entity disambiguation)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application with task-specific adaptations (fine-tuning + input-embedding augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Augmented token embeddings with a trainable embedding to signal token provenance (cell vs. context vs. metadata), averaged output token embeddings for classification, and used task-specific losses (Cross Entropy, BCE, triplet). Oversampling was applied to minority classes in the CTC task to address imbalance.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>Successful as a strong baseline: SciBERT-based models outperformed baseline AxCell (ULMFiT) on cell type classification and formed the backbone of the authors' end-to-end system that outperformed TURL on inKB linking; still, overall EL performance remained substantially below human performance, leaving room for improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>Class imbalance in CTC required oversampling; outKB identification remains challenging; model performance dependent on document-context integration and candidate generation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>SciBERT pretraining on scientific text provided domain-relevant linguistic priors; token-source augmentation allowed the model to incorporate multimodal contextual signals (table vs. document).</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Requires GPU resources for fine-tuning, labeled data per subtask, KB entity textual fields for representation, and careful negative sampling for retrieval/disambiguation training.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>High within scientific NLP tasks; the pattern (use SciBERT + provenance embeddings) should generalize to other scientific subdomains provided labeled data and KB coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>instrumental/technical skills (model fine-tuning and input engineering) and explicit procedural steps</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'S2abEL: A Dataset for Entity Linking from Scientific Tables', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e384.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e384.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experimental methods, techniques, or procedures being transferred, adapted, or applied from one scientific domain or context to another scientific domain or context, including details about the transfer process and outcomes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AxCell (ULMFiT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AxCell: Automatic extraction of results from machine learning papers (ULMFiT-based CTC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>AxCell is a prior system for cell type classification in ML paper tables that uses a ULMFiT LSTM architecture pre-trained on arXiv papers with hand-crafted features to provide table-cell context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AxCell: Automatic extraction of results from machine learning papers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_name</strong></td>
                            <td>ULMFiT-based cell type classification (AxCell)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_description</strong></td>
                            <td>AxCell employs a ULMFiT architecture with LSTM layers pre-trained on arXiv text; it takes table cell content plus hand-crafted context features as input to classify cell types (e.g., method, dataset). The authors used AxCell as a baseline by adapting its publicly available implementation to the 5-class CTC task.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>computational method / classification</td>
                        </tr>
                        <tr>
                            <td><strong>source_domain</strong></td>
                            <td>language-model pretraining on large scientific text corpora (arXiv) and table IE</td>
                        </tr>
                        <tr>
                            <td><strong>target_domain</strong></td>
                            <td>cell type classification in scientific tables (S2abEL dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_type</strong></td>
                            <td>direct application with minor output-layer modification (used as a baseline; output modified for 5-class classification)</td>
                        </tr>
                        <tr>
                            <td><strong>modifications_made</strong></td>
                            <td>Modified AxCell's output layer to accommodate the paper's 5-class CTC schema (other, dataset, method, metric, dataset&metric) and ran it as a baseline against the SciBERT approach.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>Partially successful as a baseline: AxCell provided reasonable baseline performance, but the SciBERT-based approach in this paper outperformed AxCell in F1 scores for CTC (authors state their method outperforms AxCell somewhat).</td>
                        </tr>
                        <tr>
                            <td><strong>barriers_encountered</strong></td>
                            <td>AxCell relied on hand-crafted features and an LSTM-based architecture which may be less effective than transformer-based encoders for capturing complex context; required minor tuning to match the target label set.</td>
                        </tr>
                        <tr>
                            <td><strong>facilitating_factors</strong></td>
                            <td>Availability of an existing implementation and pretraining on large scientific corpora allowed straightforward baseline comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>contextual_requirements</strong></td>
                            <td>Requires cell-level features and contextual fields from the paper; adaptation required output-layer changes for the target label set.</td>
                        </tr>
                        <tr>
                            <td><strong>generalizability</strong></td>
                            <td>Moderate: AxCell's general approach is applicable across ML paper table extraction tasks, but transformer-based models like SciBERT appear to offer advantages on this dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>explicit procedural steps and instrumental technical skills (model adaptation and feature engineering)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'S2abEL: A Dataset for Entity Linking from Scientific Tables', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Turl: table understanding through representation learning <em>(Rating: 2)</em></li>
                <li>Dense passage retrieval for open-domain question answering <em>(Rating: 2)</em></li>
                <li>Exploring the limits of transfer learning with a unified text-to-text transformer <em>(Rating: 2)</em></li>
                <li>SciB-ERT: A pretrained language model for scientific text <em>(Rating: 2)</em></li>
                <li>AxCell: Automatic extraction of results from machine learning papers <em>(Rating: 2)</em></li>
                <li>Sentence-BERT: Sentence embeddings using Siamese BERT-networks <em>(Rating: 1)</em></li>
                <li>Nilinker: Attention-based approach to nil entity linking <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-384",
    "paper_id": "paper-258426468",
    "extraction_schema_id": "extraction-schema-16",
    "extracted_data": [
        {
            "name_short": "TURL",
            "name_full": "TURL: Table Understanding through Representation Learning",
            "brief_description": "A structure-aware Transformer encoder pre-trained on general-purpose web/table corpora (WikiTable) that produces contextualized embeddings for table cells, rows, and columns for downstream table tasks.",
            "citation_title": "Turl: table understanding through representation learning",
            "mention_or_use": "use",
            "procedure_name": "TURL (structure-aware Transformer for table representation)",
            "procedure_description": "TURL is a Transformer-based model pre-trained on large corpora of web tables (WikiTable) to learn structure-aware representations of table elements (cells, rows, columns). It encodes cell text together with table layout features to produce contextual embeddings used for downstream tasks such as entity linking via candidate ranking or classification.",
            "procedure_type": "computational method / representation learning for tables",
            "source_domain": "general web / Wiki-style tables (general-domain table understanding)",
            "target_domain": "scientific tables in machine learning papers (results tables)",
            "transfer_type": "adapted/modified for new context (fine-tuned on inKB cells of the scientific table dataset)",
            "modifications_made": "Fine-tuned TURL on the in-KB cells from the S2abEL dataset (scientific ML results tables) using the same candidate set size (50) for comparison. No structural redesign was described; adaptation consisted of supervised fine-tuning on scientific-table examples.",
            "transfer_success": "Partially successful but limited: TURL performed worse than the specialized SciBERT-based approach on scientific tables — the paper reports a substantial improvement by the authors' method over TURL on nine out of ten topic folds (Table 3). Reported micro-average accuracy numbers in Table 3 are approximately 32.5 for TURL versus 44.8 for the authors' method (i.e., TURL underperformed in this scientific-table EL setting).",
            "barriers_encountered": "TURL focuses on cell content and table positional features and lacks integration of full-document context, causing failures on short/abbreviated mentions (cells shorter than four characters), highly domain-specific abbreviations, and cases requiring citation/paragraph disambiguation. Also, pretraining on general web tables did not provide sufficient domain-specific signals for the scientific ML tables.",
            "facilitating_factors": "TURL's structural modeling of tables (cell/row/column embeddings) aligns with table data and likely helped some cases; availability of public code allowed direct fine-tuning and comparison.",
            "contextual_requirements": "Required fine-tuning data from the target domain (inKB scientific table cells) and candidate entity lists for supervised training; performance benefited from additional document-level context which TURL in its standard form does not incorporate.",
            "generalizability": "Limited: while TURL generalizes to table tasks in principle, the paper finds it insufficient for scientific-result tables without additional document-context modeling; it likely needs adaptation (document context, domain-specific pretraining/fine-tuning) to obtain good performance in other specialized table domains.",
            "knowledge_type": "instrumental/technical skills (model architecture and pretraining procedure) and explicit procedural steps (fine-tuning protocol)",
            "uuid": "e384.0",
            "source_info": {
                "paper_title": "S2abEL: A Dataset for Entity Linking from Scientific Tables",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Dense Retrieval (DR) / Bi-encoder",
            "name_full": "Dense retrieval with bi-encoder (adapted from Dense Passage Retrieval and Sentence-BERT methods)",
            "brief_description": "A bi-encoder dense retrieval approach that embeds table-cell contexts and KB entity descriptions into a shared embedding space to retrieve candidate entities by nearest-neighbor search; adapted from open-domain dense passage retrieval and sentence-embedding methods.",
            "citation_title": "Dense passage retrieval for open-domain question answering",
            "mention_or_use": "use",
            "procedure_name": "Dense retrieval (bi-encoder) for candidate entity retrieval",
            "procedure_description": "A bi-encoder architecture (two separate SciBERT encoders) is fine-tuned with a triplet objective to learn embeddings for table-cell contexts and KB entity textual representations; candidate entities are retrieved by nearest-neighbor search in the embedding space (top-k). Negative examples and triplet loss were used during training to separate true entity embeddings from negatives.",
            "procedure_type": "computational method / retrieval & candidate generation",
            "source_domain": "open-domain document/question answering and dense passage retrieval",
            "target_domain": "candidate entity retrieval for entity linking from scientific tables",
            "transfer_type": "adapted/modified for new context (fine-tuned bi-encoder using SciBERT and triplet loss on table-cell↔entity pairs)",
            "modifications_made": "Replaced generic encoders with SciBERT encoders specialized for scientific text; trained with a triplet objective on cells whose gold entity exists in the KB; used Euclidean distance with a margin of 1; integrated with other retrieval strategies (interleaving with ASR) to produce final candidate sets; trained only on in-KB training examples.",
            "transfer_success": "Partially successful: dense retrieval captured latent semantics and retrieved many correct candidates but underperformed attributed-source-based retrieval (ASR) on this task. The paper reports that seeding candidates with attributed-paper entities (ASR) significantly outperforms DR alone, and interleaving DR and ASR yields the best recall@K performance overall.",
            "barriers_encountered": "Failures often due to extremely short/generic cell mentions (e.g., 'val', 'ours', 'window'), which lack lexical cues for dense models; domain-specific abbreviations and context-dependent shorthand are hard for DR to resolve. Also, DR trained only on inKB examples cannot recover outKB mentions.",
            "facilitating_factors": "Use of SciBERT (scientific-text pretraining) improved semantic representation; bi-encoder architecture allowed efficient nearest-neighbor retrieval; triplet training with negatives provided discriminative embeddings.",
            "contextual_requirements": "Requires sufficient supervised cell↔entity training pairs (in-KB) for fine-tuning, a vector index (Elasticsearch or other ANN) for nearest-neighbor search, and carefully chosen negative sampling. Performance depends on availability of meaningful textual descriptions for KB entities.",
            "generalizability": "Moderate: dense retrieval techniques generalize across retrieval tasks, but effectiveness depends on the similarity between training and target textual distributions and on availability of entity descriptions; combining with domain-specific signals (like ASR) improves robustness.",
            "knowledge_type": "computational method / explicit procedural steps (model training, triplet loss, retrieval pipeline)",
            "uuid": "e384.1",
            "source_info": {
                "paper_title": "S2abEL: A Dataset for Entity Linking from Scientific Tables",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "ASR via ASM",
            "name_full": "Attributed Source Retrieval (ASR) using Attributed Source Matching (ASM)",
            "brief_description": "A retrieval method that ranks cited/attributed papers for a table cell and fetches entities associated with those papers from a KB to seed the candidate entity set for entity linking.",
            "citation_title": "",
            "mention_or_use": "use",
            "procedure_name": "Attributed Source Retrieval (ASR) driven by Attributed Source Matching (ASM)",
            "procedure_description": "ASM is a SciBERT-based binary classifier that predicts which papers in the document's reference list (or the document itself) are the attributed sources for a given table cell. ASR then retrieves entities associated with high-probability attributed papers (via Paper-RelatesTo-Entity relations in the KB) and interleaves those entities with DR candidates to form the candidate set.",
            "procedure_type": "computational method / knowledge-driven candidate retrieval",
            "source_domain": "citation/reference analysis and metadata linking (bibliographic/source attribution)",
            "target_domain": "entity candidate retrieval for table EL in scientific papers",
            "transfer_type": "adapted/combined approach (learned source-attribution model used to drive KB entity retrieval)",
            "modifications_made": "Trained a SciBERT-based ASM model to take a cell and a candidate paper representation (index, author, year, title, abstract) and output likelihood of attribution; constructed candidate entity lists by fetching entities associated with top-ranked attributed papers and filtered by predicted cell type; interleaved ASR results with DR candidates to produce final candidate list.",
            "transfer_success": "Largely successful as a seed strategy: ASR seeding significantly outperformed DR alone in recall@K; interleaving ASR with DR provided the best candidate recall. However, ASR missed gold entities when papers did not cite the concept (22.8% of error cases at K=100) or when KB Paper-RelatesTo-Entity relations were incomplete.",
            "barriers_encountered": "Failure cases include (1) authors not citing the original or relevant paper for a concept (common for well-known entities) and (2) incomplete or missing Paper-RelatesTo-Entity relations in the KB. Search/ranking noise and citation ambiguity also reduce effectiveness.",
            "facilitating_factors": "Availability of document reference sections, a KB with paper↔entity relations (Papers with Code), and an effective ASM classifier to rank candidate sources.",
            "contextual_requirements": "Requires (a) access to the document's references and in-document context, (b) a KB containing paper-to-entity relations, and (c) a trained ASM model; manual verification or larger K may be needed when KB coverage is imperfect.",
            "generalizability": "Limited-to-moderate: ASR is powerful where document citations map well to KB relations; less effective in domains where authors do not cite canonical works or where KBs lack coverage of paper↔entity relations. The general approach (use source attribution to retrieve candidates) is applicable to other scientific domains given similar metadata and KB structure.",
            "knowledge_type": "interpretive frameworks (using attribution as a disambiguation signal) and explicit procedural steps (model training and KB lookup)",
            "uuid": "e384.2",
            "source_info": {
                "paper_title": "S2abEL: A Dataset for Entity Linking from Scientific Tables",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "T5-small reference parser",
            "name_full": "T5-small fine-tuned for reference-string parsing (author/year/title extraction)",
            "brief_description": "A text-to-text transformer (T5-small) fine-tuned to parse raw bibliography/reference strings into structured metadata (first-author last name, year, title) to enable searching and retrieval of cited papers.",
            "citation_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "mention_or_use": "use",
            "procedure_name": "T5-small fine-tuning for reference metadata extraction",
            "procedure_description": "A T5-small model was fine-tuned to take raw reference strings from papers and output structured metadata fields (last name of first author, year, title). These extracted fields were then used to query Semantic Scholar to retrieve paper abstracts and metadata, with manual verification of search results.",
            "procedure_type": "computational method / information extraction",
            "source_domain": "general text-to-text transformer models and NLP (pretrained language modeling)",
            "target_domain": "bibliographic metadata extraction and attributed-source identification in scientific papers",
            "transfer_type": "adapted/modified for new context (task-specific fine-tuning of a general text-to-text transformer)",
            "modifications_made": "Fine-tuned T5-small specifically on reference-string→(author/year/title) mapping; used outputs to query external search APIs (Semantic Scholar) and manually verified results because search APIs sometimes return non-top matches.",
            "transfer_success": "Successful pragmatically: enabled automated extraction of candidate attributed papers and retrieval of abstracts for ASM use. However, search API noise required manual verification for many queries (i.e., the pipeline was semi-automatic).",
            "barriers_encountered": "Search API ranking issues (matching paper not always top result), noisy or variable reference string formats, and need for manual verification to ensure high precision.",
            "facilitating_factors": "T5's flexible text-to-text format made it straightforward to learn reference parsing; availability of Semantic Scholar to retrieve candidate papers and their abstracts.",
            "contextual_requirements": "Requires labeled examples of reference strings for fine-tuning, access to bibliographic search APIs (Semantic Scholar), and manual verification workflows to ensure high precision.",
            "generalizability": "High for other document metadata extraction tasks — the approach (fine-tune T5 for structured extraction) is broadly applicable across domains with similar reference string formats, but end-to-end automation depends on the quality of downstream search APIs and KB coverage.",
            "knowledge_type": "explicit procedural steps (fine-tuning and extraction pipeline) and instrumental technical skills (using search APIs and verification)",
            "uuid": "e384.3",
            "source_info": {
                "paper_title": "S2abEL: A Dataset for Entity Linking from Scientific Tables",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "SciBERT adaptations",
            "name_full": "SciBERT-based task-specific fine-tuning and input augmentation",
            "brief_description": "SciBERT, a BERT variant pre-trained on scientific text, was adapted and fine-tuned with task-specific input augmentations (token-type embeddings indicating token source like cell or context) for cell type classification, attributed source matching, dense retrieval encoders, and cross-encoder entity disambiguation.",
            "citation_title": "SciB-ERT: A pretrained language model for scientific text",
            "mention_or_use": "use",
            "procedure_name": "SciBERT fine-tuning with token-source augmentation for multiple table EL subtasks",
            "procedure_description": "The authors used SciBERT as the backbone encoder for multiple subtasks (CTC, ASM, DR bi-encoder, and cross-encoder ED). They augmented token embeddings with an additional trainable embedding indicating whether a token is from the table cell, context sentence, paper metadata, or KB entity description. Different heads/losses were applied per task (cross-entropy for CTC, BCE for ASM/ED, triplet loss for DR).",
            "procedure_type": "computational method / domain-adapted language model fine-tuning",
            "source_domain": "pretrained scientific-language models (scientific NLP)",
            "target_domain": "specialized downstream tasks over scientific tables (cell typing, source attribution, candidate retrieval, entity disambiguation)",
            "transfer_type": "direct application with task-specific adaptations (fine-tuning + input-embedding augmentation)",
            "modifications_made": "Augmented token embeddings with a trainable embedding to signal token provenance (cell vs. context vs. metadata), averaged output token embeddings for classification, and used task-specific losses (Cross Entropy, BCE, triplet). Oversampling was applied to minority classes in the CTC task to address imbalance.",
            "transfer_success": "Successful as a strong baseline: SciBERT-based models outperformed baseline AxCell (ULMFiT) on cell type classification and formed the backbone of the authors' end-to-end system that outperformed TURL on inKB linking; still, overall EL performance remained substantially below human performance, leaving room for improvement.",
            "barriers_encountered": "Class imbalance in CTC required oversampling; outKB identification remains challenging; model performance dependent on document-context integration and candidate generation quality.",
            "facilitating_factors": "SciBERT pretraining on scientific text provided domain-relevant linguistic priors; token-source augmentation allowed the model to incorporate multimodal contextual signals (table vs. document).",
            "contextual_requirements": "Requires GPU resources for fine-tuning, labeled data per subtask, KB entity textual fields for representation, and careful negative sampling for retrieval/disambiguation training.",
            "generalizability": "High within scientific NLP tasks; the pattern (use SciBERT + provenance embeddings) should generalize to other scientific subdomains provided labeled data and KB coverage.",
            "knowledge_type": "instrumental/technical skills (model fine-tuning and input engineering) and explicit procedural steps",
            "uuid": "e384.4",
            "source_info": {
                "paper_title": "S2abEL: A Dataset for Entity Linking from Scientific Tables",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "AxCell (ULMFiT)",
            "name_full": "AxCell: Automatic extraction of results from machine learning papers (ULMFiT-based CTC)",
            "brief_description": "AxCell is a prior system for cell type classification in ML paper tables that uses a ULMFiT LSTM architecture pre-trained on arXiv papers with hand-crafted features to provide table-cell context.",
            "citation_title": "AxCell: Automatic extraction of results from machine learning papers",
            "mention_or_use": "mention",
            "procedure_name": "ULMFiT-based cell type classification (AxCell)",
            "procedure_description": "AxCell employs a ULMFiT architecture with LSTM layers pre-trained on arXiv text; it takes table cell content plus hand-crafted context features as input to classify cell types (e.g., method, dataset). The authors used AxCell as a baseline by adapting its publicly available implementation to the 5-class CTC task.",
            "procedure_type": "computational method / classification",
            "source_domain": "language-model pretraining on large scientific text corpora (arXiv) and table IE",
            "target_domain": "cell type classification in scientific tables (S2abEL dataset)",
            "transfer_type": "direct application with minor output-layer modification (used as a baseline; output modified for 5-class classification)",
            "modifications_made": "Modified AxCell's output layer to accommodate the paper's 5-class CTC schema (other, dataset, method, metric, dataset&metric) and ran it as a baseline against the SciBERT approach.",
            "transfer_success": "Partially successful as a baseline: AxCell provided reasonable baseline performance, but the SciBERT-based approach in this paper outperformed AxCell in F1 scores for CTC (authors state their method outperforms AxCell somewhat).",
            "barriers_encountered": "AxCell relied on hand-crafted features and an LSTM-based architecture which may be less effective than transformer-based encoders for capturing complex context; required minor tuning to match the target label set.",
            "facilitating_factors": "Availability of an existing implementation and pretraining on large scientific corpora allowed straightforward baseline comparison.",
            "contextual_requirements": "Requires cell-level features and contextual fields from the paper; adaptation required output-layer changes for the target label set.",
            "generalizability": "Moderate: AxCell's general approach is applicable across ML paper table extraction tasks, but transformer-based models like SciBERT appear to offer advantages on this dataset.",
            "knowledge_type": "explicit procedural steps and instrumental technical skills (model adaptation and feature engineering)",
            "uuid": "e384.5",
            "source_info": {
                "paper_title": "S2abEL: A Dataset for Entity Linking from Scientific Tables",
                "publication_date_yy_mm": "2023-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Turl: table understanding through representation learning",
            "rating": 2,
            "sanitized_title": "turl_table_understanding_through_representation_learning"
        },
        {
            "paper_title": "Dense passage retrieval for open-domain question answering",
            "rating": 2,
            "sanitized_title": "dense_passage_retrieval_for_opendomain_question_answering"
        },
        {
            "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "rating": 2,
            "sanitized_title": "exploring_the_limits_of_transfer_learning_with_a_unified_texttotext_transformer"
        },
        {
            "paper_title": "SciB-ERT: A pretrained language model for scientific text",
            "rating": 2,
            "sanitized_title": "scibert_a_pretrained_language_model_for_scientific_text"
        },
        {
            "paper_title": "AxCell: Automatic extraction of results from machine learning papers",
            "rating": 2,
            "sanitized_title": "axcell_automatic_extraction_of_results_from_machine_learning_papers"
        },
        {
            "paper_title": "Sentence-BERT: Sentence embeddings using Siamese BERT-networks",
            "rating": 1,
            "sanitized_title": "sentencebert_sentence_embeddings_using_siamese_bertnetworks"
        },
        {
            "paper_title": "Nilinker: Attention-based approach to nil entity linking",
            "rating": 1,
            "sanitized_title": "nilinker_attentionbased_approach_to_nil_entity_linking"
        }
    ],
    "cost": 0.01801525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>S2abEL: A Dataset for Entity Linking from Scientific Tables</p>
<p>Yuze Lou 
University of Michigan</p>
<p>Bailey Kuehl 
Allen Institute for AI</p>
<p>Erin Bransom 
Allen Institute for AI</p>
<p>Sergey Feldman 
Allen Institute for AI</p>
<p>Aakanksha Naik 
Allen Institute for AI</p>
<p>Doug Downey 
Allen Institute for AI</p>
<p>S2abEL: A Dataset for Entity Linking from Scientific Tables</p>
<p>Entity linking (EL) is the task of linking a textual mention to its corresponding entry in a knowledge base, and is critical for many knowledge-intensive NLP applications. When applied to tables in scientific papers, EL is a step toward large-scale scientific knowledge bases that could enable advanced scientific question answering and analytics. We present the first dataset for EL in scientific tables. EL for scientific tables is especially challenging because scientific knowledge bases can be very incomplete, and disambiguating table mentions typically requires understanding the paper's text in addition to the table. Our dataset, S2abEL 1 , focuses on EL in machine learning results tables and includes handlabeled cell types, attributed sources, and entity links from the PaperswithCode taxonomy for 8,429 cells from 732 tables. We introduce a neural baseline method designed for EL on scientific tables containing many outof-knowledge-base mentions, and show that it significantly outperforms a state-of-the-art generic table EL method. The best baselines fall below human performance, and our analysis highlights avenues for improvement. * This work was conducted during the internship at AI2.</p>
<p>Introduction</p>
<p>Entity Linking (EL) is a longstanding problem in natural language processing and information extraction. The goal of the task is to link textual mentions to their corresponding entities in a knowledge base (KB) (Cucerzan, 2007), and it serves as a building block for various knowledge-intensive applications, including search engines (Blanco et al., 2015), question-answering systems (Dubey et al., 2018), and more. However, existing EL methods and datasets primarily focus on linking mentions from free-form natural language (Gu et al., 2021;De Cao et al., 2021;Yamada et al., 2022). Some consider tabular data, but focus on tables from the general domain (Deng et al., 2020;Tang et al., 2021b;Iida et al., 2021;Yu et al., 2019). Despite significant research in EL, there is a lack of datasets and methods for EL in scientific tables. Linking entities in scientific tables holds promise for accelerating science in multiple ways: from augmented reading applications that help users understand the meaning of table cells without diving into the document (Head et al., 2021) to automated knowledge base construction that unifies disparate tables, enabling complex question answering or hypothesis generation (Hope et al., 2022).</p>
<p>EL in science is challenging because the set of scientific entities is vast and always growing, and existing knowledge bases are highly incomplete. A traditional "closed world" assumption often made in EL systems, whereby all mentions have corresponding entities in the target KB, is not realistic in scientific domains. It is important to detect which mentions are entities not yet in the reference KB, referred to as outKB mentions. Even for human annotators, accurately identifying whether a rarely-seen surface form actually refers to a rarely-mentioned long-tail inKB entity or an outKB entity requires domain expertise and a significant effort to investigate the document and the target KB. A further challenge is that entity mentions in scientific tables are often abbreviated and opaque, and require examining other context in the caption and paper text for disambiguation. An example is shown in Figure 1.</p>
<p>In this paper, we make three main contributions. First, we introduce S2abEL, a high-quality humanannotated dataset for EL in machine learning results tables. The dataset is sufficiently large for training and evaluating models on table EL and relevant sub-tasks, including 52,257 annotations of appropriate types for table cells (e.g. method, dataset), 9,565 annotations of attributed source papers and  (Zhang et al., 2022) showing relevant context needs to be found for entity mentions in the table and part of EL results to PapersWithCode KB. candidate entities for mentions, and 8,429 annotations for entity disambiguation including outKB mentions. To the best of our knowledge, this is the first dataset for table EL in the scientific domain. Second, we propose a model that serves as a strong baseline for each of the sub-tasks, as well as end-to-end table EL. We conduct a comprehensive comparison between our approach and existing approaches, where applicable, for each sub-task. Our method significantly outperforms TURL (Deng et al., 2020), a state-of-the-art method closest to the table EL task, but only designed for generaldomain tables. We also provide a detailed error analysis that emphasizes the need for improved methods to address the unique challenges of EL from scientific tables with outKB mentions.</p>
<p>Related Work</p>
<p>Entity Linking</p>
<p>In recent years, various approaches have been proposed for entity linking from free-form text, leveraging large language models (Gu et al., 2021;De Cao et al., 2021;Yamada et al., 2019). Researchers have also attempted to extend EL to structured Web tables, but they solely rely on table contents and do not have rich surrounding text (Deng et al., 2020;Zhang et al., 2020;Bhagavatula et al., 2015;Mulwad et al., 2023;Tang et al., 2020;Iida et al., 2021). Most of these works focus on general-purpose KBs such as Wikidata (Vrandečić and Krötzsch, 2014) and DBPedia (Auer et al., 2007) and typically test their approaches with the assumption that the target KB is complete with respect to the mentions being linked (e.g., De Cao et al., 2021;Deng et al., 2020;Hoffart et al., 2011;Tang et al., 2021a;Yamada et al., 2019).</p>
<p>There is a lack of high-quality datasets for table EL in the scientific domain with abundant outKB mentions. Recent work by Ruas and Couto (2022) provides a dataset that artificially mimics an incomplete KB for biomedical text by removing actual referent entities but linking concepts to the direct ancestor of the referent entities. In contrast, our work provides human-annotated labels of realistic missing entities for scientific tables, without relying on the target KB to contain ancestor relations. Our dataset offers two distinct advantages: first, it provides context from documents in addition to original table mentions, and second, it explicitly identifies mentions referring to outKB entities.</p>
<p>Scientific IE</p>
<p>The field of scientific information extraction (IE) aims to extract structured information from scientific documents. Various extraction tasks have been studied in this area, such as detecting and classifying semantic relations (Jain et al., 2020;Sahu et al., 2016), concept extraction (Fu et al., 2020), automatic leaderboard construction (Kardas et al., 2020;Hou et al., 2019), and citation analysis (Jurgens et al., 2018;.</p>
<p>Among these, Kardas et al., 2020;Hou et al., 2019;Yu et al., 2019 are the closest to ours. These works focus on identifying mentions that refer to tasks, datasets, and metrics in table cells to construct leaderboard records. However, they only identify entities as raw strings extracted from the set of papers they examine, without canonicalization, clustering, or actual linking to an external KB. For instance, in the dataset from Kardas et al., 2020, the ambiguous string Best Learner is identified as a model entity, and similarly Score is identified as a metric entity. Such incomplete and ambiguous entity identification makes it difficult for users to interpret the results and limits the practical applicability of the extracted information. In contrast, we propose a dataset and baselines for the end-to-end table EL task, beginning with a table in the context of a paper and ending with each cell linked to entities in the canonicalized ontology of the target KB (or classified as outKB).</p>
<p>Entity Linking in Scientific Tables</p>
<p>Our entity linking task takes as input a reference KB (the Papers with Code 2 taxonomy in our experiments), a table in a scientific paper, and the table's surrounding context. The goal is to output an entity from the KB for each table cell (or "outKB" if none). We decompose the task into several subtasks, discussed below. We then present S2abEL, the dataset we construct for scientific table EL.</p>
<p>Task Definition</p>
<p>Cell Type Classification (CTC) is the task of identifying types of entities contained in a table cell, based on the document in which the cell appears. This step is helpful to focus the later linking task on the correct type of entities from the target KB, and also excludes non-entity cells (e.g. those containing numeric values used to report experimental results) from later processing. Such exclusion removes a substantial fraction of table cells (74% in our dataset), reducing the computational cost.</p>
<p>One approach to CTC is to view it as a multilabel classification task since a cell may contain multiple entities of different types. However, our initial investigation found that only mentions of datasets and metrics co-appear to a notable degree (e.g., "QNLI (acc)" indicates the accuracy of some method evaluated on the Question-answering NLI dataset (Wang et al., 2018)). Therefore, we introduce a separate class for these instances, reducing CTC to a single-label classification task with 2 https://paperswithcode.com/ four positive classes: method, dataset, metric, and dataset&amp;metric.</p>
<p>Attributed Source Matching (ASM) is the task of identifying attributed source(s) for a table cell within the context of the document. The attributed source(s) for a concept in a document p is the reference paper mentioned in p to which the authors of p attribute the concept. ASM is a crucial step in distinguishing similar surface forms and finding the correct referent entities. For example in Figure 1, ASM can help clarify which entities "BlenderBot 1" and "R2C2 BlenderBot" refer to, as the first mention is attributed to Roller et al., 2021 while the second mention is attributed to Shuster et al., 2022. Identifying these attributions helps a system uniquely identify these two entities despite their very similar surface forms and the fact that their contexts in the document often overlap. In this work, we consider the documents listed in the reference section and the document itself as potential sources for attribution. The inclusion of the document itself is necessary since concepts may be introduced in the current document for the first time.</p>
<p>Candidate Entity Retrieval (CER) is the process of identifying a small set of entities from the target KB that are most likely to be the referent entity for a table cell within the context of the document. The purpose of this step is to exclude unlikely candidates and pass only a limited number of candidates to the next step, to reduce computational cost.</p>
<p>Entity Disambiguation (ED) with outKB Identification is the final stage. The objective is to determine the referent entity (or report outKB if none), given a table cell and its candidate entity set. The identification of outKB mentions significantly increases the complexity of the EL task, as it requires the method to differentiate between e.g. an unusual surface form of an inKB entity versus an outKB mention. However, distinguishing outKB mentions is a critical step in rapidly evolving domains like science, where existing KBs are highly incomplete.</p>
<p>Dataset Construction</p>
<p>Obtaining high-quality annotations for S2abEL is non-trivial. Identifying attributed sources and gold entities requires a global understanding of the text and tables in the full document. However, asking annotators to read every paper fully is prohibitively expensive. Presenting the full list of entities in the target KB to link from is also not feasible, while showing annotators short auto-populated candidate entity sets may introduce bias and miss gold entities. We address these challenges by designing a special-purpose annotation interface and pipeline, as detailed below.</p>
<p>In the construction process, we used two inhouse annotators with backgrounds in data analytics and data science, both having extensive experience in reading and annotating scientific papers. In addition, one author of the paper (author A) led and initial training phase with the annotators, and another author of the paper (author B) was responsible for evaluating the inter-annotator agreement (IAA) at the end of the annotation process. Bootstrapping existing resources -We began constructing our dataset by populating it with tables and cell type annotations from SegmentedTables 3 (Kardas et al., 2020), a CTC dataset where each cell is annotated according to whether it is a paper, metric, and so on. To gather data for the ASM task, we fine-tuned a T5-small (Raffel et al., 2022) model to extract the last name of the first author, year, and title for each paper that appears in the reference section of any papers in our dataset from the raw reference strings. We then used the extracted information to search for matching papers in Semantic Scholar (Kinney et al., 2023), to obtain their abstracts. Since the search APIs do not always return the matching paper at the top of the results, we manually verified the output for each query. Target KB -Papers with Code (PwC) 45 is a free and open knowledge base in the scientific domain with a total of 304,611 papers, 6,550 datasets, and 1,942 methods as of this writing. PwC includes basic relations between entities, such as relevant entities for a paper, the introducing paper for an entity, etc. Its data is collected from previously curated results and collaboratively edited by the community. While the KB has good precision, its coverage is not exhaustive -in our experiments, 42.8% of our entity mentions are outKB. Human Annotation -We developed a web interface using the Flask 6 library for the annotation process. It provides annotators with a link to the 3 https://github.com/paperswithcode/ axcell/releases 4 Our corpus is based on Papers with Code 2022/07 dump. 5 https://github.com/paperswithcode/ paperswithcode-data 6 flask.palletsprojects.com original paper, an indexed reference section, and annotation guidelines. For the CTC sub-task, we asked annotators to make necessary modifications to correct errors in SegmentedTables and accommodate the extra dataset&amp;metric class. During this phase, 15% of the original labels were changed. For the ASM subtask, annotators were asked to read relevant document sections for each cell and identify attributed sources, if any. This step can require a global understanding of the document, but candidate lists are relatively small since reference sections usually contain just tens of papers. For the EL sub-task, the web interface populates each cell with entity candidates that are 1) returned from PwC with the cell content as the search string, and/or 2) associated with the identified attributed paper(s) for this cell via the Paper-RelatesTo-Entity relation in PwC. Automatic candidate population is designed to be preliminary to prevent annotators from believing that gold entities should always come from the candidate set. Annotators were also asked to search against PwC using different surface forms of the cell content (e.g., full name, part of the cell content) before concluding that a cell refers to an outKB entity.</p>
<p>To ensure consistency and high quality, we conducted a training phase led by author A, where the two annotators were given four papers at a time to perform all annotation tasks. We then calculated the IAA between author A and each annotator for the four papers using Cohen's Kappa (McHugh, 2012), followed by disagreement discussion and guideline refinement. This process was repeated for three training rounds using different sets of papers until the IAA score was substantial (i.e., 100% for CTC, 82% for ASM, and 66% for EL). Afterward, the remaining set of papers was given to the annotators for annotation. The complete annotation instructions and web interface can be found in Appendix C.</p>
<p>Dataset and Annotation Statistics</p>
<p>Dataset Statistics - Table 1 </p>
<p>Method</p>
<p>In this section, we describe our approach for representing Paper Representation -For each referenced paper, we extract its index in the reference section, the last name of the first author, year, title, and abstract. Index, author name, and year are helpful for identifying inline citations (which frequently take the form of the index in brackets or the author and year in parens). Additionally, the title and abstract provide a summary of a paper which may contain information on new concepts it proposes. KB Entity Representation -To represent each entity in the target KB, we use its abbreviation, full name, and description from the KB, if available. The abbreviation and full name of an entity are crucial for capturing exact mentions in the text, while the description provides additional context for the entity (Logeswaran et al., 2019). Cell Type Classification -We concatenate features of cell representation (separated by special tokens) and input the resulting sequence to the pretrained language model SciBERT (Beltagy et al., 2019). For each token in the input sequence, we augment its word embedding vector with an additional trainable embedding vector from a separate embedding layer to differentiate whether a token is in the cell, from context sentences, etc. Subsequent mentions of SciBERT in the paper refers to this modified version. We pass the average of the output token embeddings at the last layer to a linear output layer and optimize for Cross Entropy loss. However, because the majority of cells in scientific tables pertain to experimental statistics, the distribution of cell types is highly imbalanced (as shown in Appendix A). To address this issue, we oversample the minority class data by randomly shuffling the context sentences.</p>
<p>Attributed Source Matching -To enable contextualization between cell context and a potential source, we combine the representations of each table cell and potential attributed source in the document as the input to a SciBERT followed by a linear output layer. We optimize for the Binary Cross Entropy loss, where all non-attributed sources in the document are used as negative examples for a cell. The model output measures the likelihood that a source should be attributed to given a table cell.</p>
<p>Candidate Entity Retrieval -We design a method that combines candidates retrieved by two strategies: (i) dense retrieval (DR) (Karpukhin et al., 2020) that leverages embeddings to represent latent semantics of table cells and entities, and (ii) attributed source retrieval (ASR) which uses the attributed source information to retrieve candidate entities.</p>
<p>For DR, we fine-tune a bi-encoder architecture (Reimers and Gurevych, 2019) with two separate SciBERT to optimize a triplet objective function. The model is only trained on cells whose gold referent entity exists in the KB. Top-ranked most similar entities based on the BM25F algorithm (Robertson and Zaragoza, 2009)  For ASR, we use the trained ASM model to obtain a list of papers ranked by their probabilities of being the attributed source estimated by the model. The candidate entity sequence O i asr is constructed by fetching entities associated with each potentially attributed paper in ranked order using the Paper-RelatesTo-Entity relations in PwC. Only entities of the same cell type as identified in CTC are retained. Note that including entities associated with lowerranked papers mitigates the errors propagated from the ASM model and the problem of imperfect entity and relation coverage that is common in real-world KBs.</p>
<p>We finally interleave O i dr and O i asr until we reach a pre-defined entity set size K.</p>
<p>Entity Disambiguation with outKB Identification -Given a table cell and its entity candidates, we fine-tune a cross-encoder architecture (Reimers and Gurevych, 2019) with a SciBERT that takes as input the fused cell representation and entity representation, followed by a linear output layer. We optimize for BCE loss using the same negative examples used in CER training. The trained model is used to estimate the probability that a table cell matches an entity. If the top-ranked entity for a cell has a matching likelihood lower than 0.5, which is a commonly used threshold for binary classification, then the cell is considered to be outKB.</p>
<p>Evaluations</p>
<p>As no existing baselines exist for the end-to-end table EL task with outKB mention identification, we compare our methods against appropriate recent work by evaluating their performance on subtasks of our dataset (Section 5.1). Additionally, we report the performance of the end-to-end system to provide baseline results for future work (Section 5.2). Finally, to understand the connection and impact of each sub-task on the final EL performance, we conducted a component-wise ablation study (Section 5.3). This study provides valuable insights into the difficulties and bottlenecks in model performance. The implementation is avail-able on GitHub 9 .</p>
<p>The experiments are designed to evaluate the performance of methods in a cross-domain setting (following the setup in Kardas et al., 2020), where training, validation, and test data come from different disjoint topics. This ensures that the methods are not overfitting to the particular characteristics of a topic and can generalize well to unseen data from different topics.</p>
<p>Evaluating Sub-tasks</p>
<p>Cell Type Classification</p>
<p>We compare our method against AxCell's cell type classification component (Kardas et al., 2020), which uses a ULMFiT architecture (Howard and Ruder, 2018) with LSTM layers pre-trained on arXiv papers. It takes as input the contents of table cells with a set of hand-crafted features to provide the context of cells in the paper. We use their publicly available implementation 10 with a slight modification to the output layer to suit our 5-class classification. Table 2 shows that our method outperforms Ax-Cell somewhat in terms of F1 scores. Although we do not claim our method on this particular sub-task is substantially better, we provide baseline results using state-of-the-art transformer models.</p>
<p>Candidate Entity Retrieval</p>
<p>Since the goal of CER is to generate a small list of potential entities for a table cell, we evaluate the performance of the CER method using recall@K. Figure 2 shows the results of evaluating dense retrieval (DR), attributed source retrieval (ASR), and a combination of both methods, with different candidate size limits K. We observe that seeding the candidate set with entities associated with attributed papers significantly outperforms DR, while interleaving candidates from ASR and DR produces the most promising results. These results demonstrate the effectiveness of utilizing information on attributed sources to generate high-quality candidates. It is worth noting that when K is sufficiently large, ASR considers all sources as attributed sources for a given cell, thus returning entities that are associated with any source. However, if the gold entity is not related to any cited source in the paper, it will still be missing from the candidate set. Increasing K further will not recover  Table 2: Results of cell type classification on our method and AxCell, with image classification papers fixed as the validation set and papers from each remaining category as the test set in turn. Each fold is run five times with different random seeds, and the reported numbers are averaged over 10 folds and 5 runs. this missing entity, as indicated by the saturation observed in Figure 2. Error Analysis -We examined the outputs of ASR and identified two main challenges. First, we observed that in 22.8% of the error cases when K = 100, authors did not cite papers for referred concepts. These cases typically involve well-known entities such as LSTM (Hochreiter and Schmidhuber, 1997). In the remaining error cases, the authors did cite papers; however, the gold entity was not retrieved due to incomplete Paper-RelatesTo-Entity relations in the target KB or because the authors cited the wrong paper. We additionally investigated the error cases from DR and found that a considerable fraction was caused by the use of generic words to refer to a specific entity. For instance, the validation set of a specific dataset entity was referred to as "val" in the table, the method proposed in the paper was referred to as "ours", and a subset of a dataset that represents data belonging to one of the classification categories was referred to as "window". Resolving the ambiguity of such references requires the model to have an understanding of the unique meaning of those words in the context. When using the combined candidate sets, missing gold entities were only observed when both DR and ASR failed, leading to superior performance compared to using either method alone.</p>
<p>Entity Disambiguation with inKB Mentions</p>
<p>The state-of-the-art method closest to our table EL task is TURL (Deng et al., 2020), designed for general-domain tables with inKB cells. It is a structure-aware Transformer encoder pre-trained on the general-purpose WikiTable corpus (Bhagavatula et al., 2015), which produces contextualized embeddings for table cells, rows, and columns that are suitable for a range of downstream applications, including table EL. We used TURL's public code 11 and fine-tuned it on the inKB cells of our dataset and compared it with our method using the same entity candidate set of size 50. Table 3 shows that our model achieves a substantial improvement in accuracy over TURL on nine out of ten paper folds. The examples in Table 6 (appendix) demonstrate that our model is more effective at recognizing the referent entity when the cell mention is ambiguous and looks similar to other entities in the KB. This is because TURL as a generic table embedding method focuses on just cell content and position while our approach combines cell with the full document. Our analysis further reveals that TURL made incorrect predictions for all cells whose mentions were shorter than four characters (likely an abbreviation or a pointer to a reference Micro avg 32.5 44.8 Table 3: Accuracy for end-to-end entity linking for cells that refer to an inKB entity with 10-fold-crossdomain evaluation using our approach and TURL. Our method is specialized for tables in scientific papers and outperforms the more general-purpose TURL method.  Table 4: End-to-end EL results with 10-fold-crossdomain evaluation of our method on learned DR + ASR candidate sets of size 50 with the inKB threshold set to 0.5. Although our model achieved reasonable overall accuracy, it is still far from perfect, leaving ample room for future improvements in the end-to-end table EL task.</p>
<p>paper). Meanwhile, our method correctly linked 39% of these cells.</p>
<p>End-to-end Evaluation</p>
<p>We now evaluate the end-to-end performance of our approach on the EL task with outKB identification. In addition to re-ranking candidate entities, the method needs to determine when cell mentions refer to entities that do not exist in the target KB. We report F 1 scores for outKB entities as the prediction is binary (precision and recall are reported in Appendix Table 8). For inKB mentions, we report the hit rate at top-1. Additionally, we eval- uate overall performance using accuracy 12 . For each topic of papers, we report the ratio of outKB mentions to inKB mentions. The top block of Table 4 shows the end-to-end EL performance of our method. Our analysis shows a positive Pearson correlation (Cohen et al., 2009) of 0.87 between O/I ratio and overall accuracy, indicating our method tends to predict a mention as outKB. Figure 3 shows the performance at various inKB thresholds. Table 7 (Appendix D). Our analysis reveals that a majority of incorrect inKB predictions are due to the use of generic words. For outKB mentions, the model tends to get confused when they are similar to existing entities in the target KB.</p>
<p>Error Analysis We sampled 100 examples of incorrect predictions for both outKB and inKB mentions and analyzed their causes of errors in</p>
<p>Component-wise Ablation Study</p>
<p>To investigate how much of the error in our end-toend three-step system was due to errors introduced in the first two stages (specifically, wrong cell type classifications from CTC or missing correct candidates from CER), we tried measuring system performance with these errors removed. Specifically, we tried replacing the CTC output with the gold cell labels, or adding the gold entity to the output CER candidate set, or both.</p>
<p>The results in the bottom block of Table 4 show that there is no significant difference in performance with gold inputs. This could be because CTC and CER are easier tasks compared to ED, and if the model fails those tasks, it is likely to still struggle to identify the correct referent entity, even if that is present in the candidate set or the correct cell type is given.</p>
<p>Conclusion</p>
<p>We introduced a high-quality dataset for training and evaluating methods for the table EL task with a significant fraction of outKB mentions. To the best of our knowledge, it is the first table EL dataset in the scientific domain. We presented an end-to-end baseline model for the task. Performance analysis of our method indicated future opportunities to achieve human-level results.</p>
<p>Limitations</p>
<p>In this section, we discuss some limitations of our work. First, our dataset only includes tables from English-language papers in the machine learning domain, linked to the Papers with Code KB, which may limit its generalizability to other domains, languages, and KBs. Second, we acknowledge that the creation of S2abEL required significant manual effort from domain experts, making it a resourceintensive process that may not be easily scalable. Third, our approach of using attributed papers to aid in identifying referent entities relies on the target KB containing relations that associate relevant papers and entities together. Fourth, we do not compare against GPT-series models due to budget limitations. Finally, while our experiments set one initial baseline for model performance on our task, substantially more exploration of different methods may improve performance on our task substantially.</p>
<p>A Detailed Dataset Statistics</p>
<p>S2abEL consists of 11 folds, each corresponding to a topic.   the number of papers, tables, and cells for each sub-task and topic. The class distribution for CTC is as follows: other (74%), dataset (8%), method (14%), metric (3%), and dataset&amp;metric (0.4%). For APM, 1,532 (16.6%) cells have missing attributed paper, 1,095 (11.9%) cells attribute to the paper itself, 6,598 (71.5%) cells attribute to an entry in the reference section of the paper. For EL, 3,610 (42.8%) cells refer to outKB entities and (57.2%) cells refer to inKB entities.</p>
<p>B Training Details</p>
<p>We trained all our models for two epochs with a batch size of 32, using the AdamW optimizer (Loshchilov and Hutter, 2019) with linear decay warm-up. The initial learning rate was 2e-5 and the warm-up ratio was 10%. All models were trained using a single 48Gb NVIDIA A6000 GPU. For the triplet loss function in DR, we used Euclidean as the distance function with a margin of 1. For the Candidate Entity Retrieval and Entity Disambiguation tasks, we used negative examples of size 50 at training time. Additionally for the ED task, we set candidate set size limitation as 50 when making predictions.</p>
<p>C Annotation Interface and Guidelines</p>
<p>Our annotation interface with annotation guidelines is at https://github.com/allenai/ s2abel/blob/main/common_utils/ Annotation%20Interface.pdf. Note that there might be cells that contain a subentity mentions consisting of an entity mention and a non-entity mention string, e.g., "Bert-large", "Bert with 6 layers frozen". For these cells, we asked the annotators to focus on the primary entity and our current model considers these mentions as mentions of the main entity. Thus those two mentions are labeled as method, and linked to https: paperswithcode.com/method/bert. We also specifically asked the annotators to mark cells that contain mentions of more than one primary entity or are confusing to understand, which are excluded from the dataset. We leave the tasks of linking subentities explicitly and cells to multiple entities for future work.     </p>
<p>D Error Case Study</p>
<p>Figure 1 :
1Part of a table in OPT: Open Pre-trained Transformer Language Models</p>
<p>Figure 2 :
2Evaluation of different candidate entity retrieval methods. The method in the parenthesis indicates whether a fine-tuned SciBert or BM25F is used.</p>
<p>Figure 3 :
3Entity linking results with varying inKB thresholds. Note that the inKB hit rate is low (44.8%) even when all mentions are predicted with an entity (i.e., threshold is 0).</p>
<p>provides a summary of the statistics for S2abEL, publicly available on GitHub 7 . ASM and EL annotations are only available for cells labeled positively in CTC. Metrics only are not linked to entities due to the lack of a controlled metric ontology in PwC. It is worth not-CTC </p>
<p>APM 
EL </p>
<h1>papers</h1>
<p>327 
316 
303 </p>
<h1>tables</h1>
<p>886 
790 
732 </p>
<h1>cells</h1>
<p>52,257 9,564 8,429 </p>
<p>Table 1 :
1Overall statistics of S2abEL. It consists of 52,257 data points for cell types, 9,564 for attributed source matching, and 8,429 for entity linking, with ground truth. Post-hoc IAA Evaluation -We conducted a posthoc evaluation to verify the quality of annotations, where author B, who is a researcher with a Ph.D. in Computer Science, independently annotated five random tables. The Cohen's Kappa scores show a substantial level of agreement(McHugh, 2012) between author B and the annotations (100% for CTC, 85.5% for ASM, and 60.6% for EL). These results demonstrate the quality and reliability of the annotations in S2abEL.ing that S2abEL contains 3,610 outKB mentions 
versus 4,819 inKB mentions, presenting a signifi-
cantly different challenge from prior datasets that 
mostly handle inKB mentions. More details are in 
Appendix A. </p>
<p>8 in Elasticsearch, are used as negative examples. For each table cell t i , the top-k nearest entities O i dr in the embedding space with ranks are returned as candidates.</p>
<p>Table 5
5provides detailed statistics onFold </p>
<p>CTC 
APM 
EL </p>
<h1>paper # table # cell</h1>
<h1>paper # table # cell</h1>
<h1>paper # table # cell</h1>
<p>Question ans. 
58 
139 
6,422 
57 
128 
1,320 
57 
127 
1,282 
Object det. 
54 
159 
17,020 
52 
141 
2,686 
51 
135 
2,527 
Image class. 
27 
94 
2,681 
27 
82 
608 
27 
81 
597 
Speech rec. 
22 
88 
3,516 
21 
76 
649 
21 
74 
612 
Image gen. 
25 
37 
1,184 
23 
34 
290 
23 
34 
288 
Machine trans. 
28 
48 
2,199 
25 
42 
412 
25 
41 
378 
Text class. 
21 
75 
4,085 
20 
55 
688 
19 
51 
600 
NLI 
32 
83 
3,385 
30 
68 
787 
30 
66 
697 
Pose estim. 
13 
47 
4,447 
11 
31 
550 
7 
18 
222 
Semantic seg, 
32 
82 
5,733 
30 
75 
927 
30 
75 
919 
Misc. 
15 
34 
1,585 
13 
30 
308 
13 
30 
307 </p>
<p>Table 5 :
5Detailed statistics of S2abEL. Global Conv. Net., End-to-End Mem. Net. Graph Conv. Net., Global Conv. Net. Graph Conv. Net.Cell Content Column header 
TURL result 
Our result 
Gold entity </p>
<p>InKB </p>
<p>"[33]" 
"" 
Cityscapes, PoseTrack, 
LAMBADA 
PointNet, GAN, CRF 
PointNet </p>
<p>"Text GCN" 
"Model" </p>
<p>Table 6 :
6Incorrect examples for end-to-end EL from TURL. The table includes the cell content and the column header in the first two columns, the top-3 ranked results from TURL and our approach in the third and fourth columns, respectively, and the gold entity in the last column.</p>
<p>Table 7 :
7Representative examples of erroneous end-to-end EL cases. The table includes the cause and the percentage for that cause in the first two columns, an example of cell content for that cause and our incorrect prediction in the third and fourth columns, and the gold entity in the last column.Test fold 
Precision Recall </p>
<p>Machine trans. 
46.4 
94.4 
Image gen. 
38.7 
95.7 
Misc. 
77.0 
95.1 
Speech rec. 
62.8 
89.3 
Question ans. 
76.9 
93.1 
NLI 
74.9 
87.2 
Text class. 
66.2 
92.2 
Object det. 
34.0 
91.1 
Semantic seg. 
61.4 
91.2 
Pose estim. 
63.8 
83.3 </p>
<p>Micro avg 
58.6 
91.4 
+ gold CTC 
59.5 
92.7 
+ gold Can. 
58.7 
91.4 
+ gold both 
59.5 
92.7 </p>
<p>Table 8 :
8Additional end-to-end Entity linking results for outKB cells.</p>
<p>Table 6
6presents examples where TURL made incorrect EL predictions while our approach made correct predictions.Table 7 summarizes the main causes of incorrect predictions made by our approach for both inKB and outKB mentions. relative location with reference to the top-left numeric cell in the table, i.e., top-left, top-right, bottom-left, and bottom-right context sentences top-ranked sentences in the full document (including table captions, section headers, etc.) regarding the cell content based on BM25 row context concatenated cell's row separated by special tokens column context concatenated cell's column separated by special tokens position cell's 2D position in the table in terms of distance from the top left corner reverse position cell's 2D position in the table in terms of distance from the bottom right corner. has reference whether cell has a referenceFeature 
Description </p>
<p>cell content 
cell's raw text 
region 
cell's </p>
<p>Table 9 :
9Features for cell representation.
https://github.com/allenai/S2abEL/ blob/main/data/release_data.tar.gz
We chose to use BM25F instead of BM25 because it can take into account entity data with several fields, including name, full name, and description.
https://github.com/allenai/S2abEL 10 https://github.com/paperswithcode/ axcell
https://github.com/sunlab-osu/TURL
A cell is considered a correct prediction if it is an outKB mention and predicted as such, or if it is an inKB mention and predicted as inKB with the gold entity being ranked at top 1.</p>
<p>Dbpedia: A nucleus for a web of open data. Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, Zachary Ives, Proceedings of the 6th International The Semantic Web and 2nd Asian Conference on Asian Semantic Web Conference, ISWC'07/ASWC'07. the 6th International The Semantic Web and 2nd Asian Conference on Asian Semantic Web Conference, ISWC'07/ASWC'07Berlin, HeidelbergSpringer-VerlagSören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. 2007. Dbpedia: A nucleus for a web of open data. In Proceedings of the 6th International The Seman- tic Web and 2nd Asian Conference on Asian Se- mantic Web Conference, ISWC'07/ASWC'07, page 722-735, Berlin, Heidelberg. Springer-Verlag.</p>
<p>SciB-ERT: A pretrained language model for scientific text. Iz Beltagy, Kyle Lo, Arman Cohan, 10.18653/v1/D19-1371Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB- ERT: A pretrained language model for scientific text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan- guage Processing (EMNLP-IJCNLP), pages 3615- 3620, Hong Kong, China. Association for Computa- tional Linguistics.</p>
<p>Tabel: Entity linking in web tables. Chandra Sekhar Bhagavatula, Thanapon Noraset, Doug Downey, The Semantic Web-ISWC 2015: 14th International Semantic Web Conference. Bethlehem, PA, USASpringerProceedings, Part IChandra Sekhar Bhagavatula, Thanapon Noraset, and Doug Downey. 2015. Tabel: Entity linking in web tables. In The Semantic Web-ISWC 2015: 14th In- ternational Semantic Web Conference, Bethlehem, PA, USA, October 11-15, 2015, Proceedings, Part I, pages 425-441. Springer.</p>
<p>Fast and space-efficient entity linking for queries. Roi Blanco, Giuseppe Ottaviano, Edgar Meij, 10.1145/2684822.2685317Proceedings of the Eighth ACM International Conference on Web Search and Data Mining, WSDM '15. the Eighth ACM International Conference on Web Search and Data Mining, WSDM '15New York, NY, USAAssociation for Computing MachineryRoi Blanco, Giuseppe Ottaviano, and Edgar Meij. 2015. Fast and space-efficient entity linking for queries. In Proceedings of the Eighth ACM International Con- ference on Web Search and Data Mining, WSDM '15, page 179-188, New York, NY, USA. Associa- tion for Computing Machinery.</p>
<p>Structural scaffolds for citation intent classification in scientific publications. Arman Cohan, Waleed Ammar, Madeleine Van Zuylen, Field Cady, 10.18653/v1/N19-1361Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics1Arman Cohan, Waleed Ammar, Madeleine van Zuylen, and Field Cady. 2019. Structural scaffolds for ci- tation intent classification in scientific publications. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Compu- tational Linguistics: Human Language Technolo- gies, Volume 1 (Long and Short Papers), pages 3586-3596, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Pearson correlation coefficient. Noise reduction in speech processing. Israel Cohen, Yiteng Huang, Jingdong Chen, Jacob Benesty, Jacob Benesty, Jingdong Chen, Yiteng Huang, Israel Cohen, Israel Cohen, Yiteng Huang, Jingdong Chen, Jacob Benesty, Jacob Benesty, Jingdong Chen, Yiteng Huang, and Israel Cohen. 2009. Pearson correlation coefficient. Noise reduction in speech processing, pages 1-4.</p>
<p>Large-scale named entity disambiguation based on Wikipedia data. Silviu Cucerzan, Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL). the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)Prague, Czech RepublicAssociation for Computational LinguisticsSilviu Cucerzan. 2007. Large-scale named entity dis- ambiguation based on Wikipedia data. In Proceed- ings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Com- putational Natural Language Learning (EMNLP- CoNLL), pages 708-716, Prague, Czech Republic. Association for Computational Linguistics.</p>
<p>Autoregressive entity retrieval. Nicola De Cao, Gautier Izacard, Sebastian Riedel, Fabio Petroni, 9th International Conference on Learning Representations. AustriaICLR 2021, Virtual Event. OpenReview.netNicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. 2021. Autoregressive entity retrieval. In 9th International Conference on Learning Repre- sentations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.</p>
<p>Turl: table understanding through representation learning. Xiang Deng, Huan Sun, Alyssa Lees, You Wu, Cong Yu, Proceedings of the VLDB Endowment. the VLDB Endowment14Xiang Deng, Huan Sun, Alyssa Lees, You Wu, and Cong Yu. 2020. Turl: table understanding through representation learning. Proceedings of the VLDB Endowment, 14(3):307-319.</p>
<p>Earl: joint entity and relation linking for question answering over knowledge graphs. Mohnish Dubey, Debayan Banerjee, Debanjan Chaudhuri, Jens Lehmann, The Semantic Web-ISWC 2018: 17th International Semantic Web Conference. Monterey, CA, USASpringerProceedings, Part I 17Mohnish Dubey, Debayan Banerjee, Debanjan Chaud- huri, and Jens Lehmann. 2018. Earl: joint entity and relation linking for question answering over knowl- edge graphs. In The Semantic Web-ISWC 2018: 17th International Semantic Web Conference, Mon- terey, CA, USA, October 8-12, 2018, Proceedings, Part I 17, pages 108-126. Springer.</p>
<p>. Sunyang Fu, David Chen, Huan He, Sijia Liu, Sungrim Moon, Kevin J. Peterson, Feichen Shen, LiweiSunyang Fu, David Chen, Huan He, Sijia Liu, Sun- grim Moon, Kevin J. Peterson, Feichen Shen, Liwei</p>
<p>Clinical concept extraction: A methodology review. Yanshan Wang, Andrew Wang, Yiqing Wen, Sunghwan Zhao, Hongfang Sohn, Liu, 10.1016/j.jbi.2020.103526Journal of Biomedical Informatics. 109103526Wang, Yanshan Wang, Andrew Wen, Yiqing Zhao, Sunghwan Sohn, and Hongfang Liu. 2020. Clinical concept extraction: A methodology review. Journal of Biomedical Informatics, 109:103526.</p>
<p>Read, retrospect, select: An mrc framework to short text entity linking. Yingjie Gu, Xiaoye Qu, Zhefeng Wang, Baoxing Huai, Nicholas Jing Yuan, Xiaolin Gui, 10.1609/aaai.v35i14.17528Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence35Yingjie Gu, Xiaoye Qu, Zhefeng Wang, Baoxing Huai, Nicholas Jing Yuan, and Xiaolin Gui. 2021. Read, retrospect, select: An mrc framework to short text entity linking. Proceedings of the AAAI Conference on Artificial Intelligence, 35(14):12920-12928.</p>
<p>Augmenting scientific papers with just-in-time, position-sensitive definitions of terms and symbols. Andrew Head, Kyle Lo, Dongyeop Kang, Raymond Fok, Sam Skjonsberg, S Daniel, Marti A Weld, Hearst, Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. the 2021 CHI Conference on Human Factors in Computing SystemsAndrew Head, Kyle Lo, Dongyeop Kang, Raymond Fok, Sam Skjonsberg, Daniel S Weld, and Marti A Hearst. 2021. Augmenting scientific papers with just-in-time, position-sensitive definitions of terms and symbols. In Proceedings of the 2021 CHI Con- ference on Human Factors in Computing Systems, pages 1-18.</p>
<p>Long short-term memory. Sepp Hochreiter, Jürgen Schmidhuber, Neural computation. 98Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735-1780.</p>
<p>Robust disambiguation of named entities in text. Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen Fürstenau, Manfred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater, Gerhard Weikum, Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing. the 2011 Conference on Empirical Methods in Natural Language ProcessingEdinburgh, Scotland, UK. Association for Computational LinguisticsJohannes Hoffart, Mohamed Amir Yosef, Ilaria Bor- dino, Hagen Fürstenau, Manfred Pinkal, Marc Span- iol, Bilyana Taneva, Stefan Thater, and Gerhard Weikum. 2011. Robust disambiguation of named en- tities in text. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Process- ing, pages 782-792, Edinburgh, Scotland, UK. Asso- ciation for Computational Linguistics.</p>
<p>Tom Hope, Doug Downey, Oren Etzioni, S Daniel, Eric Weld, Horvitz, arXiv:2205.020072022. A computational inflection for scientific discovery. arXiv preprintTom Hope, Doug Downey, Oren Etzioni, Daniel S Weld, and Eric Horvitz. 2022. A computational inflection for scientific discovery. arXiv preprint arXiv:2205.02007.</p>
<p>Identification of tasks, datasets, evaluation metrics, and numeric scores for scientific leaderboards construction. Yufang Hou, Charles Jochim, Martin Gleize, Francesca Bonin, Debasis Ganguly, 10.18653/v1/P19-1513Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsYufang Hou, Charles Jochim, Martin Gleize, Francesca Bonin, and Debasis Ganguly. 2019. Identifica- tion of tasks, datasets, evaluation metrics, and nu- meric scores for scientific leaderboards construction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5203-5213, Florence, Italy. Association for Compu- tational Linguistics.</p>
<p>Universal language model fine-tuning for text classification. Jeremy Howard, Sebastian Ruder, 10.18653/v1/P18-1031Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics1Jeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. In Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers), pages 328-339, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>TABBIE: Pretrained representations of tabular data. Hiroshi Iida, Dung Thai, Varun Manjunatha, Mohit Iyyer, 10.18653/v1/2021.naacl-main.270Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational LinguisticsHiroshi Iida, Dung Thai, Varun Manjunatha, and Mo- hit Iyyer. 2021. TABBIE: Pretrained representations of tabular data. In Proceedings of the 2021 Confer- ence of the North American Chapter of the Associ- ation for Computational Linguistics: Human Lan- guage Technologies, pages 3446-3456, Online. As- sociation for Computational Linguistics.</p>
<p>SciREX: A challenge dataset for document-level information extraction. Sarthak Jain, Madeleine Van Zuylen, Hannaneh Hajishirzi, Iz Beltagy, 10.18653/v1/2020.acl-main.670Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsSarthak Jain, Madeleine van Zuylen, Hannaneh Ha- jishirzi, and Iz Beltagy. 2020. SciREX: A chal- lenge dataset for document-level information extrac- tion. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7506-7516, Online. Association for Compu- tational Linguistics.</p>
<p>Measuring the evolution of a scientific field through citation frames. David Jurgens, Srijan Kumar, Raine Hoover, Dan Mc-Farland, Dan Jurafsky, Transactions of the Association for Computational Linguistics. 6David Jurgens, Srijan Kumar, Raine Hoover, Dan Mc- Farland, and Dan Jurafsky. 2018. Measuring the evo- lution of a scientific field through citation frames. Transactions of the Association for Computational Linguistics, 6:391-406.</p>
<p>AxCell: Automatic extraction of results from machine learning papers. Marcin Kardas, Piotr Czapla, Pontus Stenetorp, Sebastian Ruder, Sebastian Riedel, Ross Taylor, Robert Stojnic, 10.18653/v1/2020.emnlp-main.692Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational LinguisticsMarcin Kardas, Piotr Czapla, Pontus Stenetorp, Se- bastian Ruder, Sebastian Riedel, Ross Taylor, and Robert Stojnic. 2020. AxCell: Automatic extraction of results from machine learning papers. In Proceed- ings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8580-8594, Online. Association for Computational Linguistics.</p>
<p>Dense passage retrieval for open-domain question answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-Tau Yih, 10.18653/v1/2020.emnlp-main.550Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Nat- ural Language Processing (EMNLP), pages 6769- 6781, Online. Association for Computational Lin- guistics.</p>
<p>. Rodney Michael Kinney, Chloe Anastasiades, Russell Authur, Iz Beltagy, Jonathan Bragg, Alexandra Buraczynski, Isabel Cachola, Stefan Candra, Yoganand Chandrasekhar, Arman Cohan, Miles Crawford, Doug Downey, Jason Dunkelberger, Oren Etzioni, Robert Evans, Sergey Feldman, Joseph Gorney, David W Graham, F Q Hu, Regan Huff, Daniel King, Sebastian Kohlmeier, Bailey Kuehl, Michael Langan, Daniel Lin, Haokun Liu, Kyle Lo, Jaron Lochner, Kelsey Macmillan, Tyler Murray, Christopher Newell, Smita Rao, Shaurya Rohatgi, L Paul, Zejiang Sayre, Amanpreet Shen, Luca Singh, Shivashankar Soldaini, A Subramanian, Alex D Tanaka, Linda M Wade, Lucy Lu Wagner, Christopher Wang, Caroline Wilhelm, Jiangjiang Wu, Angele Yang, Zamarron, Madeleine van Zuylen, and Daniel S. Weld. 2023. The semantic scholar open data platform. ArXiv, abs/2301.10140Rodney Michael Kinney, Chloe Anastasiades, Rus- sell Authur, Iz Beltagy, Jonathan Bragg, Alexan- dra Buraczynski, Isabel Cachola, Stefan Candra, Yo- ganand Chandrasekhar, Arman Cohan, Miles Craw- ford, Doug Downey, Jason Dunkelberger, Oren Et- zioni, Robert Evans, Sergey Feldman, Joseph Gor- ney, David W. Graham, F.Q. Hu, Regan Huff, Daniel King, Sebastian Kohlmeier, Bailey Kuehl, Michael Langan, Daniel Lin, Haokun Liu, Kyle Lo, Jaron Lochner, Kelsey MacMillan, Tyler Murray, Christopher Newell, Smita Rao, Shaurya Rohatgi, Paul L Sayre, Zejiang Shen, Amanpreet Singh, Luca Soldaini, Shivashankar Subramanian, A. Tanaka, Alex D Wade, Linda M. Wagner, Lucy Lu Wang, Christopher Wilhelm, Caroline Wu, Jiangjiang Yang, Angele Zamarron, Madeleine van Zuylen, and Daniel S. Weld. 2023. The semantic scholar open data platform. ArXiv, abs/2301.10140.</p>
<p>Efficient one-pass end-to-end entity linking for questions. Belinda Z Li, Sewon Min, Srinivasan Iyer, Yashar Mehdad, Wen-Tau Yih, 10.18653/v1/2020.emnlp-main.522Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational LinguisticsBelinda Z. Li, Sewon Min, Srinivasan Iyer, Yashar Mehdad, and Wen-tau Yih. 2020. Efficient one-pass end-to-end entity linking for questions. In Proceed- ings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6433-6441, Online. Association for Computational Linguistics.</p>
<p>Zero-shot entity linking by reading entity descriptions. Lajanugen Logeswaran, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Jacob Devlin, Honglak Lee, 10.18653/v1/P19-1335Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsLajanugen Logeswaran, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Jacob Devlin, and Honglak Lee. 2019. Zero-shot entity linking by reading entity de- scriptions. In Proceedings of the 57th Annual Meet- ing of the Association for Computational Linguistics, pages 3449-3460, Florence, Italy. Association for Computational Linguistics.</p>
<p>Decoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, International Conference on Learning Representations. Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In International Con- ference on Learning Representations.</p>
<p>Interrater reliability: the kappa statistic. L Mary, Mchugh, Biochemia medica. 223Mary L McHugh. 2012. Interrater reliability: the kappa statistic. Biochemia medica, 22(3):276-282.</p>
<p>A practical entity linking system for tables in scientific literature. Varish Mulwad, Tim Finin, S Vijay, Jenny Weisenberg Kumar, Sharad Williams, Anupam Dixit, Joshi, 3rd Workshop on Scientific Document Understanding at AAAI-2023. Varish Mulwad, Tim Finin, Vijay S Kumar, Jenny Weisenberg Williams, Sharad Dixit, Anupam Joshi, et al. 2023. A practical entity linking system for tables in scientific literature. In 3rd Workshop on Scientific Document Understanding at AAAI-2023.</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, J. Mach. Learn. Res. 211Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2022. Exploring the limits of transfer learning with a unified text-to-text trans- former. J. Mach. Learn. Res., 21(1).</p>
<p>Sentence-BERT: Sentence embeddings using Siamese BERTnetworks. Nils Reimers, Iryna Gurevych, 10.18653/v1/D19-1410Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsNils Reimers and Iryna Gurevych. 2019. Sentence- BERT: Sentence embeddings using Siamese BERT- networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu- ral Language Processing (EMNLP-IJCNLP), pages 3982-3992, Hong Kong, China. Association for Computational Linguistics.</p>
<p>The probabilistic relevance framework: Bm25 and beyond. Stephen Robertson, Hugo Zaragoza, 10.1561/1500000019Found. Trends Inf. Retr. 34Stephen Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: Bm25 and be- yond. Found. Trends Inf. Retr., 3(4):333-389.</p>
<p>Recipes for building an open-domain chatbot. Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Eric Michael Smith, Y-Lan Boureau, Jason Weston, 10.18653/v1/2021.eacl-main.24Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumeOnlineAssociation for Computational LinguisticsStephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Eric Michael Smith, Y-Lan Boureau, and Jason We- ston. 2021. Recipes for building an open-domain chatbot. In Proceedings of the 16th Conference of the European Chapter of the Association for Compu- tational Linguistics: Main Volume, pages 300-325, Online. Association for Computational Linguistics.</p>
<p>Nilinker: Attention-based approach to nil entity linking. Pedro Ruas, Francisco M Couto, 10.1016/j.jbi.2022.104137Journal of Biomedical Informatics. 132104137Pedro Ruas and Francisco M. Couto. 2022. Nilinker: Attention-based approach to nil entity linking. Jour- nal of Biomedical Informatics, 132:104137.</p>
<p>Relation extraction from clinical texts using domain invariant convolutional neural network. Sunil Sahu, Ashish Anand, Krishnadev Oruganty, Mahanandeeshwar Gattu, 10.18653/v1/W16-2928Proceedings of the 15th Workshop on Biomedical Natural Language Processing. the 15th Workshop on Biomedical Natural Language ProcessingBerlin, GermanyAssociation for Computational LinguisticsSunil Sahu, Ashish Anand, Krishnadev Oruganty, and Mahanandeeshwar Gattu. 2016. Relation extraction from clinical texts using domain invariant convolu- tional neural network. In Proceedings of the 15th Workshop on Biomedical Natural Language Process- ing, pages 206-215, Berlin, Germany. Association for Computational Linguistics.</p>
<p>Language models that seek for knowledge: Modular search &amp; generation for dialogue and prompt completion. Kurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, Jason Weston, 10.48550/ARXIV.2203.13224Kurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, and Jason Weston. 2022. Language models that seek for knowl- edge: Modular search &amp; generation for dialogue and prompt completion.</p>
<p>A bidirectional multi-paragraph reading model for zero-shot entity linking. Hongyin Tang, Xingwu Sun, Jin Beihong, Fuzheng Zhang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence35Hongyin Tang, Xingwu Sun, Beihong Jin, and Fuzheng Zhang. 2021a. A bidirectional multi-paragraph read- ing model for zero-shot entity linking. In Proceed- ings of the AAAI Conference on Artificial Intelli- gence, volume 35, pages 13889-13897.</p>
<p>Rpt: relational pre-trained transformer is almost all you need towards democratizing data preparation. Nan Tang, Ju Fan, Fangyi Li, Jianhong Tu, Xiaoyong Du, Guoliang Li, Sam Madden, Mourad Ouzzani, arXiv:2012.02469arXiv preprintNan Tang, Ju Fan, Fangyi Li, Jianhong Tu, Xiaoyong Du, Guoliang Li, Sam Madden, and Mourad Ouz- zani. 2020. Rpt: relational pre-trained transformer is almost all you need towards democratizing data preparation. arXiv preprint arXiv:2012.02469.</p>
<p>Rpt: Relational pre-trained transformer is almost all you need towards democratizing data preparation. Nan Tang, Ju Fan, Fangyi Li, Jianhong Tu, Xiaoyong Du, Guoliang Li, Sam Madden, Mourad Ouzzani, 10.14778/3457390.3457391Proc. VLDB Endow. 148Nan Tang, Ju Fan, Fangyi Li, Jianhong Tu, Xiaoyong Du, Guoliang Li, Sam Madden, and Mourad Ouz- zani. 2021b. Rpt: Relational pre-trained transformer is almost all you need towards democratizing data preparation. Proc. VLDB Endow., 14(8):1254-1261.</p>
<p>Wikidata: A free collaborative knowledgebase. Denny Vrandečić, Markus Krötzsch, 10.1145/2629489Commun. ACM. 5710Denny Vrandečić and Markus Krötzsch. 2014. Wiki- data: A free collaborative knowledgebase. Commun. ACM, 57(10):78-85.</p>
<p>GLUE: A multi-task benchmark and analysis platform for natural language understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, 10.18653/v1/W18-5446Proceedings of the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP. the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLPBrussels, BelgiumAssociation for Computational LinguisticsAlex Wang, Amanpreet Singh, Julian Michael, Fe- lix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis plat- form for natural language understanding. In Pro- ceedings of the 2018 EMNLP Workshop Black- boxNLP: Analyzing and Interpreting Neural Net- works for NLP, pages 353-355, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Global entity disambiguation with pretrained contextualized embeddings of words and entities. Ikuya Yamada, Koki Washio, Hiroyuki Shindo, Yuji Matsumoto, arXiv:1909.00426arXiv preprintIkuya Yamada, Koki Washio, Hiroyuki Shindo, and Yuji Matsumoto. 2019. Global entity disam- biguation with pretrained contextualized embed- dings of words and entities. arXiv preprint arXiv:1909.00426.</p>
<p>Global entity disambiguation with BERT. Ikuya Yamada, Koki Washio, Hiroyuki Shindo, Yuji Matsumoto, 10.18653/v1/2022.naacl-main.238Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational LinguisticsIkuya Yamada, Koki Washio, Hiroyuki Shindo, and Yuji Matsumoto. 2022. Global entity disambigua- tion with BERT. In Proceedings of the 2022 Con- ference of the North American Chapter of the As- sociation for Computational Linguistics: Human Language Technologies, pages 3264-3271, Seattle, United States. Association for Computational Lin- guistics.</p>
<p>Tablepedia: Automating pdf table reading in an experimental evidence exploration and analytic system. Wenhao Yu, Zongze Li, Qingkai Zeng, Meng Jiang, 10.1145/3308558.3314118The World Wide Web Conference, WWW '19. New York, NY, USAAssociation for Computing MachineryWenhao Yu, Zongze Li, Qingkai Zeng, and Meng Jiang. 2019. Tablepedia: Automating pdf table reading in an experimental evidence exploration and analytic system. In The World Wide Web Conference, WWW '19, page 3615-3619, New York, NY, USA. Associ- ation for Computing Machinery.</p>
<p>Experimental evidence extraction system in data science with hybrid table features and ensemble learning. Wenhao Yu, Wei Peng, Yu Shu, Qingkai Zeng, Meng Jiang, 10.1145/3366423.3380174Proceedings of The Web Conference 2020, WWW '20. The Web Conference 2020, WWW '20New York, NY, USAAssociation for Computing MachineryWenhao Yu, Wei Peng, Yu Shu, Qingkai Zeng, and Meng Jiang. 2020. Experimental evidence extrac- tion system in data science with hybrid table features and ensemble learning. In Proceedings of The Web Conference 2020, WWW '20, page 951-961, New York, NY, USA. Association for Computing Machin- ery.</p>
<p>Novel entity discovery from web tables. Shuo Zhang, Edgar Meij, Krisztian Balog, Ridho Reinanda, 10.1145/3366423.3380205Proceedings of The Web Conference 2020, WWW '20. The Web Conference 2020, WWW '20New York, NY, USAAssociation for Computing MachineryShuo Zhang, Edgar Meij, Krisztian Balog, and Ridho Reinanda. 2020. Novel entity discovery from web tables. In Proceedings of The Web Conference 2020, WWW '20, page 1298-1308, New York, NY, USA. Association for Computing Machinery.</p>
<p>. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, arXiv:2205.01068Mona Diab, Xian Li, Xi Victoria LinarXiv preprintet al. 2022. Opt: Open pre-trained transformer language modelsSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher De- wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.</p>            </div>
        </div>

    </div>
</body>
</html>
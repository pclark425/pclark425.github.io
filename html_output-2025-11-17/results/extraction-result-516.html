<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-516 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-516</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-516</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-5a9d4bcffa9989cac4139b2844358884ae023e8d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5a9d4bcffa9989cac4139b2844358884ae023e8d" target="_blank">SayNav: Grounding Large Language Models for Dynamic Planning to Navigation in New Environments</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Automated Planning and Scheduling</p>
                <p><strong>Paper TL;DR:</strong> SayNav achieves state-of-the-art results and even outperforms an oracle based baseline with strong ground-truth assumptions by more than 8% in terms of success rate, highlighting its ability to generate dynamic plans for successfully locating objects in large-scale new environments.</p>
                <p><strong>Paper Abstract:</strong> Semantic reasoning and dynamic planning capabilities are crucial for an autonomous agent to perform complex navigation tasks in unknown environments. It requires a large amount of common-sense knowledge, that humans possess, to succeed in these tasks. We present SayNav, a new approach that leverages human knowledge from Large Language Models (LLMs) for efficient generalization to complex navigation tasks in unknown large-scale environments. SayNav uses a novel grounding mechanism, that incrementally builds a 3D scene graph of the explored environment as inputs to LLMs, for generating feasible and contextually appropriate high-level plans for navigation. The LLM-generated plan is then executed by a pre-trained low-level planner, that treats each planned step as a short-distance point-goal navigation sub-task. SayNav dynamically generates step-by-step instructions during navigation and continuously refines future steps based on newly perceived information. We evaluate SayNav on multi-object navigation (MultiON) task, that requires the agent to utilize a massive amount of human knowledge to efficiently search multiple different objects in an unknown environment. We also introduce a benchmark dataset for MultiON task employing ProcTHOR framework that provides large photo-realistic indoor environments with variety of objects. SayNav achieves state-of-the-art results and even outperforms an oracle based baseline with strong ground-truth assumptions by more than 8% in terms of success rate, highlighting its ability to generate dynamic plans for successfully locating objects in large-scale new environments. The code, benchmark dataset and demonstration videos are accessible at https://www.sri.com/ics/computer-vision/saynav.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e516.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e516.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SayNav-LLM(gpt-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SayNav High-Level Planner using gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based high-level dynamic planner inside the SayNav system that consumes textualized 3D scene-graph subgraphs and outputs short-term stepwise navigation instructions (navigate/look) via few-shot in-context prompts to support multi-object navigation in unknown houses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large instruction-tuned transformer language model (OpenAI GPT-3.5 family) used off-the-shelf; no model fine-tuning performed in paper — instead configured via few-shot in-context prompting and programming-style prompts to emit pseudo-code plans and room/type feasibility judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-Object Navigation (MultiON) in ProcTHOR</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Agent must explore an unknown multi-room house (ProcTHOR procedural houses) and locate instances of three target object categories. During episodes the agent receives RGBD, semantic segmentation, and pose; an incremental 3D scene graph is built from perception and converted to text subgraphs that are fed to the LLM to produce short-horizon plans (navigate to object/point, look for objects) with conditional fallbacks and room-feasibility reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on large text corpora (LLM priors) + in-context examples (few-shot prompts) + explicit symbolic grounding from the incremental 3D scene graph (textualized subgraph)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>few-shot in-context prompting (programming-style prompts), dedicated prompts for room identification and feasibility, and optional conversational-chain memory (LangChain) for tracking</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>external symbolic 3D scene graph (four-layer: small objects, large objects, rooms, house) converted into textual prompts; LLM outputs procedural pseudo-code (navigate/look steps with comments); LLM also annotates/augments scene graph (e.g., infer room type) and writes back decisions (come back later/skip). Knowledge is therefore represented externally as explicit symbolic scene graph + natural language pseudo-code plans.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success Rate (SR), Success weighted by Path Length (SPL), Kendall Tau ordering similarity</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Representative numbers (from paper Table 1): GT scene graph + Oracle low-level (OrNav): SR = 95.35%, SPL = 0.43; VO scene graph + learned low-level (PNav): SR = 60.32%, SPL = 0.34. Baseline (privileged PointNav with optimal order): SR = 56.06%, SPL = 0.49.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Successfully uses commonsense spatial priors (room-type → likely object locations), object-relational cues (search small objects near large landmarks like 'table'), procedural reasoning (generates ordered multi-step plans, conditional fallbacks such as 'if not found on desk, check bed'), and short-horizon decomposition that matches low-level point-goal capabilities; generates non-redundant, contextually appropriate next-steps when grounded by the scene graph.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Failures mainly caused by imperfect perception and grounding: inaccurate 3D coordinates from RGBD/segmentation (missing small objects), depth errors with transparent/glass doors, inability to detect door open/closed status, and occasional LLM planning errors when low-level execution fails; LLM-memory (conversational chain) with gpt-3.5 performed worse, indicating limitations in robust plan-tracking with this model.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Baseline: IL-trained PointNav agent given ground-truth object coordinates and optimal order — SR = 56.06%, SPL = 0.49. SayNav (gpt-3.5) outperforms this baseline in SR when using grounded scene graphs (e.g., 95.35% SR with GT+OrNav).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Key ablations: (1) Scene graph source: GT (ground-truth) vs VO (visual observation). Replacing VO with GT dramatically improves SR (e.g., gpt-3.5 GT+OrNav 95.35% SR vs VO+OrNav 71.32% SR). (2) Low-level planner: OrNav (A* with map) vs PNav (IL-trained point-goal agent); OrNav consistently increases SR and SPL (e.g., gpt-3.5 GT OrNav 95.35% vs GT PNav 80.62%). (3) LLM memory via LangChain: using LLM-based plan-tracker reduces SR with gpt-3.5 but gpt-4 mitigates this (see separate entry).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs can supply rich commonsense spatial, object-relational, and procedural priors for high-level planning when strictly grounded by an explicit symbolic 3D scene graph that encodes 'near/in/door' relations and object coordinates; the LLM itself does not need direct sensory inputs — it operates on textualized scene graphs and in-context examples — and produces effective dynamic short-horizon plans and conditional fallbacks, but overall system performance is highly dependent on the fidelity of perceptual grounding and the reliability of the low-level motion executor.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SayNav: Grounding Large Language Models for Dynamic Planning to Navigation in New Environments', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e516.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e516.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SayNav-LLM(gpt-4)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SayNav High-Level Planner using GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The same SayNav LLM-based high-level dynamic planner evaluated using GPT-4, which demonstrates improved commonsense ordering and feasibility reasoning when given the same textualized 3D scene-graph prompts and in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4 instruction-tuned transformer used off-the-shelf; no fine-tuning in the paper. Configured via few-shot in-context programming-style prompts and specialized prompts for room identification and feasibility checks; also evaluated in a dual-instance LangChain memory configuration for plan tracking.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-Object Navigation (MultiON) in ProcTHOR</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same MultiON task: locate three target object categories in unknown procedural houses; LLM receives textualized subgraphs of the incremental 3D scene graph (object nodes with coordinates, room nodes and door edges) and outputs short-term navigate/look pseudo-code steps with comments and conditional fallbacks; the low-level planner executes point-goal sub-tasks from assigned object coordinates.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on broad text corpora (LLM priors) + few-shot in-context examples provided in prompts + explicit scene-graph grounding (textualized subgraph) generated from sensory perception</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>few-shot in-context prompting (programmatic prompt templates), separate prompts for room identification and feasibility (yes/no), and optional conversational-chain memory (LangChain) for plan tracking</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>External explicit 3D scene graph (nodes: small objects, large objects, rooms, house; edges: 'near', 'in', door links) converted into text; LLM produces procedural pseudo-code (navigate(x,y); look(objects)) plus descriptive comments; LLM also annotates room types and feasibility flags that are written back into the scene graph.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success Rate (SR), Success weighted by Path Length (SPL), Kendall Tau for ordering similarity</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Representative numbers (from paper Table 1): GT scene graph + OrNav: SR = 93.93%, SPL = 0.46; VO scene graph + PNav: SR = 64.34%, SPL = 0.33. With LLM memory (LangChain) and gpt-4: GT+OrNav SR = 93.93% (Table 2); VO+PNav SR ≈ 61.60% in some reported experiments (Table 2). Baseline SR = 56.06%.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Improved object-ordering and feasibility judgments relative to gpt-3.5 (higher Kendall Tau), better selection of likely search targets inside rooms, more optimal short-horizon plans, and more robust performance when using LLM-based conversational memory for plan tracking.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Same perception-grounding weaknesses remain (inaccurate object localization from RGBD/segmentation, missing small objects, glass-door depth errors, failure to detect open doors), and low-level navigation failures (PNav) reduce SPL; LLM planning can still be sensitive to corrupted or incomplete scene-graph textualization leading to suboptimal steps.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to the privileged baseline PointNav with optimal order (SR = 56.06%), SayNav with GPT-4 and full ground-truth grounding (GT+OrNav) attains much higher SR (93.93%); yet SPL is lower than baseline when VO or learned low-level planner is used, due to exploration overhead and low-level inefficiencies.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Key ablations reported: (1) Scene graph: GT vs VO — GT yields large SR increases (e.g., GPT-4 GT+OrNav 93.93% vs VO+OrNav 69.80% SR). (2) Low-level planner: OrNav vs PNav — replacing OrNav with PNav reduces SR and SPL (e.g., GPT-4 GT OrNav 93.93% vs GT PNav 84.09% SR). (3) LLM-memory (LangChain): GPT-4 retained high performance with LLM-memory, while GPT-3.5 degraded, showing that higher-capacity LLMs can take over plan-tracking effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Higher-capacity LLMs (GPT-4) better exploit textualized spatial and object-relational scene-graph information to produce more optimal object-orderings and feasibility judgments; nevertheless, the LLM must be tightly grounded by an accurate explicit scene representation and coupled with a reliable low-level executor — otherwise perceptual and executional errors dominate overall system failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SayNav: Grounding Large Language Models for Dynamic Planning to Navigation in New Environments', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Task Planning <em>(Rating: 2)</em></li>
                <li>Leveraging Large Language Models for Visual Target Navigation <em>(Rating: 2)</em></li>
                <li>LLM-planner: Few-shot grounded planning for embodied agents with large language models <em>(Rating: 2)</em></li>
                <li>Do as i can, not as i say: Grounding language in robotic affordances <em>(Rating: 1)</em></li>
                <li>Inner monologue: Embodied reasoning through planning with language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-516",
    "paper_id": "paper-5a9d4bcffa9989cac4139b2844358884ae023e8d",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "SayNav-LLM(gpt-3.5)",
            "name_full": "SayNav High-Level Planner using gpt-3.5-turbo",
            "brief_description": "An LLM-based high-level dynamic planner inside the SayNav system that consumes textualized 3D scene-graph subgraphs and outputs short-term stepwise navigation instructions (navigate/look) via few-shot in-context prompts to support multi-object navigation in unknown houses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo",
            "model_size": null,
            "model_description": "A large instruction-tuned transformer language model (OpenAI GPT-3.5 family) used off-the-shelf; no model fine-tuning performed in paper — instead configured via few-shot in-context prompting and programming-style prompts to emit pseudo-code plans and room/type feasibility judgments.",
            "task_name": "Multi-Object Navigation (MultiON) in ProcTHOR",
            "task_description": "Agent must explore an unknown multi-room house (ProcTHOR procedural houses) and locate instances of three target object categories. During episodes the agent receives RGBD, semantic segmentation, and pose; an incremental 3D scene graph is built from perception and converted to text subgraphs that are fed to the LLM to produce short-horizon plans (navigate to object/point, look for objects) with conditional fallbacks and room-feasibility reasoning.",
            "task_type": "navigation",
            "knowledge_type": "spatial+procedural+object-relational",
            "knowledge_source": "pre-training on large text corpora (LLM priors) + in-context examples (few-shot prompts) + explicit symbolic grounding from the incremental 3D scene graph (textualized subgraph)",
            "has_direct_sensory_input": false,
            "elicitation_method": "few-shot in-context prompting (programming-style prompts), dedicated prompts for room identification and feasibility, and optional conversational-chain memory (LangChain) for tracking",
            "knowledge_representation": "external symbolic 3D scene graph (four-layer: small objects, large objects, rooms, house) converted into textual prompts; LLM outputs procedural pseudo-code (navigate/look steps with comments); LLM also annotates/augments scene graph (e.g., infer room type) and writes back decisions (come back later/skip). Knowledge is therefore represented externally as explicit symbolic scene graph + natural language pseudo-code plans.",
            "performance_metric": "Success Rate (SR), Success weighted by Path Length (SPL), Kendall Tau ordering similarity",
            "performance_result": "Representative numbers (from paper Table 1): GT scene graph + Oracle low-level (OrNav): SR = 95.35%, SPL = 0.43; VO scene graph + learned low-level (PNav): SR = 60.32%, SPL = 0.34. Baseline (privileged PointNav with optimal order): SR = 56.06%, SPL = 0.49.",
            "success_patterns": "Successfully uses commonsense spatial priors (room-type → likely object locations), object-relational cues (search small objects near large landmarks like 'table'), procedural reasoning (generates ordered multi-step plans, conditional fallbacks such as 'if not found on desk, check bed'), and short-horizon decomposition that matches low-level point-goal capabilities; generates non-redundant, contextually appropriate next-steps when grounded by the scene graph.",
            "failure_patterns": "Failures mainly caused by imperfect perception and grounding: inaccurate 3D coordinates from RGBD/segmentation (missing small objects), depth errors with transparent/glass doors, inability to detect door open/closed status, and occasional LLM planning errors when low-level execution fails; LLM-memory (conversational chain) with gpt-3.5 performed worse, indicating limitations in robust plan-tracking with this model.",
            "baseline_comparison": "Baseline: IL-trained PointNav agent given ground-truth object coordinates and optimal order — SR = 56.06%, SPL = 0.49. SayNav (gpt-3.5) outperforms this baseline in SR when using grounded scene graphs (e.g., 95.35% SR with GT+OrNav).",
            "ablation_results": "Key ablations: (1) Scene graph source: GT (ground-truth) vs VO (visual observation). Replacing VO with GT dramatically improves SR (e.g., gpt-3.5 GT+OrNav 95.35% SR vs VO+OrNav 71.32% SR). (2) Low-level planner: OrNav (A* with map) vs PNav (IL-trained point-goal agent); OrNav consistently increases SR and SPL (e.g., gpt-3.5 GT OrNav 95.35% vs GT PNav 80.62%). (3) LLM memory via LangChain: using LLM-based plan-tracker reduces SR with gpt-3.5 but gpt-4 mitigates this (see separate entry).",
            "key_findings": "LLMs can supply rich commonsense spatial, object-relational, and procedural priors for high-level planning when strictly grounded by an explicit symbolic 3D scene graph that encodes 'near/in/door' relations and object coordinates; the LLM itself does not need direct sensory inputs — it operates on textualized scene graphs and in-context examples — and produces effective dynamic short-horizon plans and conditional fallbacks, but overall system performance is highly dependent on the fidelity of perceptual grounding and the reliability of the low-level motion executor.",
            "uuid": "e516.0",
            "source_info": {
                "paper_title": "SayNav: Grounding Large Language Models for Dynamic Planning to Navigation in New Environments",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "SayNav-LLM(gpt-4)",
            "name_full": "SayNav High-Level Planner using GPT-4",
            "brief_description": "The same SayNav LLM-based high-level dynamic planner evaluated using GPT-4, which demonstrates improved commonsense ordering and feasibility reasoning when given the same textualized 3D scene-graph prompts and in-context examples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-4",
            "model_size": null,
            "model_description": "OpenAI GPT-4 instruction-tuned transformer used off-the-shelf; no fine-tuning in the paper. Configured via few-shot in-context programming-style prompts and specialized prompts for room identification and feasibility checks; also evaluated in a dual-instance LangChain memory configuration for plan tracking.",
            "task_name": "Multi-Object Navigation (MultiON) in ProcTHOR",
            "task_description": "Same MultiON task: locate three target object categories in unknown procedural houses; LLM receives textualized subgraphs of the incremental 3D scene graph (object nodes with coordinates, room nodes and door edges) and outputs short-term navigate/look pseudo-code steps with comments and conditional fallbacks; the low-level planner executes point-goal sub-tasks from assigned object coordinates.",
            "task_type": "navigation",
            "knowledge_type": "spatial+procedural+object-relational",
            "knowledge_source": "pre-training on broad text corpora (LLM priors) + few-shot in-context examples provided in prompts + explicit scene-graph grounding (textualized subgraph) generated from sensory perception",
            "has_direct_sensory_input": false,
            "elicitation_method": "few-shot in-context prompting (programmatic prompt templates), separate prompts for room identification and feasibility (yes/no), and optional conversational-chain memory (LangChain) for plan tracking",
            "knowledge_representation": "External explicit 3D scene graph (nodes: small objects, large objects, rooms, house; edges: 'near', 'in', door links) converted into text; LLM produces procedural pseudo-code (navigate(x,y); look(objects)) plus descriptive comments; LLM also annotates room types and feasibility flags that are written back into the scene graph.",
            "performance_metric": "Success Rate (SR), Success weighted by Path Length (SPL), Kendall Tau for ordering similarity",
            "performance_result": "Representative numbers (from paper Table 1): GT scene graph + OrNav: SR = 93.93%, SPL = 0.46; VO scene graph + PNav: SR = 64.34%, SPL = 0.33. With LLM memory (LangChain) and gpt-4: GT+OrNav SR = 93.93% (Table 2); VO+PNav SR ≈ 61.60% in some reported experiments (Table 2). Baseline SR = 56.06%.",
            "success_patterns": "Improved object-ordering and feasibility judgments relative to gpt-3.5 (higher Kendall Tau), better selection of likely search targets inside rooms, more optimal short-horizon plans, and more robust performance when using LLM-based conversational memory for plan tracking.",
            "failure_patterns": "Same perception-grounding weaknesses remain (inaccurate object localization from RGBD/segmentation, missing small objects, glass-door depth errors, failure to detect open doors), and low-level navigation failures (PNav) reduce SPL; LLM planning can still be sensitive to corrupted or incomplete scene-graph textualization leading to suboptimal steps.",
            "baseline_comparison": "Compared to the privileged baseline PointNav with optimal order (SR = 56.06%), SayNav with GPT-4 and full ground-truth grounding (GT+OrNav) attains much higher SR (93.93%); yet SPL is lower than baseline when VO or learned low-level planner is used, due to exploration overhead and low-level inefficiencies.",
            "ablation_results": "Key ablations reported: (1) Scene graph: GT vs VO — GT yields large SR increases (e.g., GPT-4 GT+OrNav 93.93% vs VO+OrNav 69.80% SR). (2) Low-level planner: OrNav vs PNav — replacing OrNav with PNav reduces SR and SPL (e.g., GPT-4 GT OrNav 93.93% vs GT PNav 84.09% SR). (3) LLM-memory (LangChain): GPT-4 retained high performance with LLM-memory, while GPT-3.5 degraded, showing that higher-capacity LLMs can take over plan-tracking effectively.",
            "key_findings": "Higher-capacity LLMs (GPT-4) better exploit textualized spatial and object-relational scene-graph information to produce more optimal object-orderings and feasibility judgments; nevertheless, the LLM must be tightly grounded by an accurate explicit scene representation and coupled with a reliable low-level executor — otherwise perceptual and executional errors dominate overall system failure modes.",
            "uuid": "e516.1",
            "source_info": {
                "paper_title": "SayNav: Grounding Large Language Models for Dynamic Planning to Navigation in New Environments",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Task Planning",
            "rating": 2
        },
        {
            "paper_title": "Leveraging Large Language Models for Visual Target Navigation",
            "rating": 2
        },
        {
            "paper_title": "LLM-planner: Few-shot grounded planning for embodied agents with large language models",
            "rating": 2
        },
        {
            "paper_title": "Do as i can, not as i say: Grounding language in robotic affordances",
            "rating": 1
        },
        {
            "paper_title": "Inner monologue: Embodied reasoning through planning with language models",
            "rating": 1
        }
    ],
    "cost": 0.01228475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>SayNav: Grounding Large Language Models for Dynamic Planning to Navigation in New Environments</h1>
<p>Abhinav Rajvanshi ${ }^{1}$, Karan Sikka ${ }^{1}$, Xiao Lin ${ }^{1}$, Bhoram Lee ${ }^{1}$, Han-Pang Chiu ${ }^{1}$, Alvaro Velasquez ${ }^{2}$<br>${ }^{1}$ {first_name}:{last_name}@sri.com, SRI International, 201 Washington Rd, Princeton, NJ 08540, USA<br>${ }^{2}$ alvaro.velasquez@colorado.edu, University of Colorado Boulder, Boulder, CO 80309, USA</p>
<h4>Abstract</h4>
<p>Semantic reasoning and dynamic planning capabilities are crucial for an autonomous agent to perform complex navigation tasks in unknown environments. It requires a large amount of common-sense knowledge, that humans possess, to succeed in these tasks. We present SayNav, a new approach that leverages human knowledge from Large Language Models (LLMs) for efficient generalization to complex navigation tasks in unknown large-scale environments. SayNav uses a novel grounding mechanism, that incrementally builds a 3D scene graph of the explored environment as inputs to LLMs, for generating feasible and contextually appropriate high-level plans for navigation. The LLM-generated plan is then executed by a pre-trained low-level planner, that treats each planned step as a short-distance point-goal navigation sub-task. SayNav dynamically generates step-by-step instructions during navigation and continuously refines future steps based on newly perceived information. We evaluate SayNav on multi-object navigation (MultiON) task, that requires the agent to utilize a massive amount of human knowledge to efficiently search multiple different objects in an unknown environment. We also introduce a benchmark dataset for MultiON task employing ProcTHOR framework that provides large photo-realistic indoor environments with variety of objects. SayNav achieves state-of-the-art results and even outperforms an oracle based baseline with strong ground-truth assumptions by more than $8 \%$ in terms of success rate, highlighting its ability to generate dynamic plans for successfully locating objects in large-scale new environments. The code, benchmark dataset and demonstration videos are accessible at https://www.sri.com/ics/computer-vision/saynav.</p>
<h2>1 Introduction</h2>
<p>Finding multiple target objects in a novel environment is a relatively easy task for a human but a daunting task for an autonomous agent. Given such a task, humans are able to leverage common-sense priors like room layouts and plausible object placement to infer likely locations of objects. For example, there are higher chances of finding a pillow on the bed in the bedroom and a spoon on the dining table or in the kitchen. Humans are also capable of dynamically planning and adjusting their search strategies and actions based</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>on new visual observations during exploration in a new environment. For example, a human would search for a spoon first instead of a pillow if entering a kitchen.</p>
<p>Such reasoning and dynamic planning capabilities are essential for an autonomous agent to accomplish complex navigation tasks in novel settings, such as searching and locating specific objects in new houses. However, current learning-based methods, with the most popular being deep reinforcement learning (DRL) (Anderson et al. 2018; Chaplot et al. 2020; Khandelwal et al. 2022), require massive amounts of training for the agent to achieve reasonable performance even for simpler navigation tasks, such as finding a single object (object-goal navigation) or reaching a single target point (point-goal navigation) (Anderson et al. 2018). Moreover, significant computational resources are needed to replicate human ability to generalize to new environments. Such computational demands impede the development of an autonomous agent to efficiently conduct complex tasks at unknown places.</p>
<p>In this paper, we propose SayNav - a new approach to leverage common-sense knowledge from Large Language Models (LLMs) for efficient generalization to complicated navigation tasks in unknown large-scale environments. Recently, agents equipped with LLM-based planners have shown remarkable capabilities to conduct complex manipulation tasks with only a few training samples (Ahn et al. 2022; Song et al. 2022). SayNav follows this trend of utilizing LLMs in developing generalist planning agents specifically for navigation tasks. To fully demonstrate and validate SayNav's capabilities, we choose a complex navigation task, multi-object navigation (MultiON). For this task, the agent needs to efficiently explore a new 3D environment to locate multiple different objects given the names of these objects. This task requires a large amount of prior knowledge and dynamic planning capabilities (similar to humans) for success.</p>
<p>MultiON task has emerged recently as a generalization of the Object-goal Navigation task. It was introduced by (Wani et al. 2020) as a task of "navigation to an ordered sequence of objects" and most of the other works on MultiON follow the same definition. (Gireesh et al. 2023) dropped the constraint of having a ordered sequence of the given objects and defined the goal as to localize certain number of objects in any order. This is how we also define our MultiON task</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />Figure 1: A SayNav example: The robot uses LLM-based planner to efficiently find one target object (laptop) in a new house.</p>
<p>as it poses much larger planning challenges and task complexities than the previous definition. Following the trend in MultiON task, we will be using three different objects as the targets for each episode for experiments. Note that SayNav is capable of searching for any number of objects in the environment.</p>
<p>The key innovation of SayNav is to incrementally build and expand a 3D scene graph of the new environment using perceived information during exploration. It then grounds feasible and contextually appropriate knowledge from LLMs which is used by the agent for navigation. This new grounding mechanism ensures that LLMs adhere to the physical constraints in the new environment, including the spatial layouts and geometric relationships among perceived entities. 3D scene graphs <em>Armeni et al. (2019); Kim et al. (2019); Rosinol et al. (2021); Hughes et al. (2022); Wald et al. (2020); Wu et al. (2021)</em> have recently emerged as powerful high-level representations of 3D large-scale environments to support real-time decisions in robotics. A 3D scene graph is a layered graph which represents spatial concepts (nodes) at multiple levels of abstraction (such as objects, places, rooms, and buildings) with their relations (edges). We utilize this 3D scene graph representation to ground LLMs in the current environment, which is used to continuously build and refine the search plan for the agent during navigation.</p>
<p>Specifically, SayNav utilizes LLMs to generate step-by-step instructions on the fly, for locating target objects during navigation. To ensure feasible and effective planning in a dynamic manner, SayNav continuously extracts and converts a subgraph (from the current 3D scene graph) into a textual prompt to be fed to the LLMs. This extracted subgraph includes spatial concepts in the local region centered around the current position of the agent. The LLM then plans next steps based on this subgraph, such as inferring likely locations for the target object and prioritizing them. This plan also includes conditional statements and fallback options when any of the steps is unable to achieve the goal. For example, if the agent is not able to find the laptop on the desk, it will go to a next likely location (bed) in a bedroom.</p>
<p>SayNav also leverages LLMs to augment and refine the scene graph during navigation, such as annotating the room type based on current perceived objects. This improves the hierarchical organization of semantic information in the scene graph, that can support better planning. SayNav computes the feasibility of completing the current goal based on the room type which helps in better optimization of plan. For example, it can skip the restroom when looking for a spoon, but can come back later if needed.</p>
<p>SayNav only requires a few examples via in-context learning <em>Brown et al. (2020); Ouyang et al. (2022)</em> for configuring LLMs to conduct high-level dynamic planning to complicated MultiON tasks in new environments. The LLM-generated plan is then executed by a pre-trained low-level planner that treats each planned step as a short-distance point-goal navigation sub-task (such as moving to a perceived object). This decomposition reduces the planning complexity of the navigation task, because the sub-tasks planned by LLMs are simple enough for low-level planners to execute successfully.</p>
<p>Figure 1 illustrates an example of SayNav utilizing LLMs to efficiently explore a new environment and locate one (laptop) of the three target objects. The agent first looks around (i.e., observes to build the scene graph) and identifies what type of room it starts from. After checking potential locations of the target objects in the room, the agent does not find any target. Then it decides to go through the door to move to another room. The agent continuously expands the scene graph during exploration and realizes that the neighbor room is a living room. There, it finds one target on the table and continues searching for other two objects.</p>
<p>The main contributions are summarized as follows.</p>
<ol>
<li>We present, to the best of our knowledge, the first LLM-based high-level planner specifically for navigation tasks in large-scale unknown photo-realistic environments. The proposed LLM planner incrementally generates step-by-step instructions in a dynamic manner during navigation. The instructions generated from LLMs during navigation are consistent and non-redundant.</li>
<li>We propose a novel grounding mechanism to LLMs for navigation in new large-scale environments. SayNav incrementally builds and expands a 3D scene graph during exploration. Next-step plans are generated from LLMs, by utilizing text prompts based on a selected portion</li>
</ol>
<p>(subgraph) of the scene graph. Parts of the scene graph are also continuously refined and updated by LLMs.
3. We introduce a benchmark dataset for MultiON task across different houses for our evaluation and future use by researchers.</p>
<h2>2 Related Work</h2>
<p>In this section, we provide a brief review on related works in visual navigation, high-level planning with LLMs for autonomous agents and MultiON.</p>
<p>Visual Navigation in New Environments is a fundamental capability for many applications for autonomous agents. Recent learning-based approaches with DRL methods have shown great potential to outperform classical approaches based on SLAM (simultaneous localization and mapping) and path planning techniques, on different visual navigation tasks Mishkin2019. These navigation tasks include point-goal navigation Wijmans2019, imagegoal navigation Zhu2017, and object-goal navigation Chaplot2020.</p>
<p>However, these methods generally require at least hundreds of millions of iterations Wijmans2019 for training agents to generalize in new environments. This entails high cost in terms of both data collection and computation. In addition, it hinders the development of autonomous agents that can conduct more complex navigation tasks, such as multi-object navigation and cordon and search, that requires the ability to exploit common-sense knowledge and plan dynamically in novel environments.</p>
<p>Leveraging common-sense knowledge from LLMs allows us to avoid the high cost of training as in the previous learning-based methods. By effectively grounding LLMs (such as ChatGPT) via text prompting, our approach enables efficient high-level planning for visual navigation in unknown environments. To better demonstrate and evaluate our proposed method, we use multi-object navigation, which is more complex than previous navigation tasks such as objectgoal navigation. The MultiON task demands common-sense knowledge, as humans do, to efficiently search for multiple different objects in large-scale unknown environments.</p>
<p>High-Level Planning with LLMs has become an emergent trend in the robotics field. LLMs by virtue of both training on internet scale data and instruction tuning have demonstrated excellent capabilities to perform zero/few shot learning for unseen tasks Zhao2023, Brown2020. Recent instruction tuned models such as ChatGPT have further shown strong capabilities to follow natural instructions expressed as prompts Chung2022, Peng2023.</p>
<p>Recent works in autonomy have used LLMs and demonstrated significant progress Ahn2022, Song2022, Huang2022, Liu2023, Driess2023, Brown2020, Ouyang2022 in incorporating human knowledge, that enables efficient training of autonomous agents for tasks such as mobile manipulation. These works reduce the learning complexity by using a twolevel planning architecture. For each assigned task, they utilize LLMs to generate a high-level step-by-step plan. Each planned step, formulated as a much simpler sub-task, can be
executed by an oracle (ground truth) or a pre-trained lowlevel planner that maps one step into a sequence of primitive actions. Agents with these LLM-based planners are able to perform a new task with only a few training examples via in-context learning Brown2020, Ouyang2022.</p>
<p>However, these LLM-based planners have two major limitations for visual navigation tasks in new large-scale environments. First, the grounding mechanisms in these methods Ahn2022, Song2022, Huang2022, Liu2023 are designed for small-scale environments. For example, works such as Song2022, Singh2023 have focused on the AI-THOR based environment that consists of only a single room. Moreover, these methods only rely on detection of specific objects. They do not consider room layout and the topological arrangement of perceived entities inside the room, which are important to ground LLMs in the physical environment for visual navigation tasks. Therefore, knowledge extracted from LLMs using these methods might not be contextually appropriate to an agent for navigation in large-scale settings, such as multiroom houses.</p>
<p>Second, some of these LLM-based planners typically generate a multi-step long-horizon plan in the beginning for the assigned task, which is not feasible for navigating in unknown environments. They also lack the capability to change the plan during task execution. In contrast, an effective search plan for navigation in new places is required to be incrementally generated and updated during exploration. Future actions are decided based on current perceived scenes with the memory of previously-visited regions.</p>
<p>SayPlan Rana2023 addresses the first issue by using a pre-built ground-truth 3D scene graph of a known large-scale place, to ground LLMs for high-level task planning. However, the planning complexity for SayPlan is simplified due to the availability of the entire ground-truth scene graph prior to task execution. In other words, SayPlan cannot be used for task planning in unknown environments.</p>
<p>Our approach, SayNav, is designed to leverage LLMs specifically for visual navigation in unknown large-scale environments. We propose a new grounding mechanism that incrementally builds a 3D scene graph of the explored environment as inputs to LLMs, for generating the high-level plans. SayNav also dynamically generates step-by-step instructions during navigation. It continuously refines future steps based on newly perceived information via LLMs.</p>
<p>The only work we found to leverage LLMs specifically for navigation tasks in unknown environments is L3MVN Yu2023. It uses LLMs to find the nearby semantic frontier based on detected objects, for expanding the exploration area to eventually find the target object. For example, moving to the (sofa, table) region which is more likely to have TV. In other words, it utilizes LLMs to hint to the next exploration direction. It does not use LLMs as a full high-level planner, that generates step-by-step instructions. In contrast, our SayNav uses the 3D scene graph to ground LLMs as a high-level planner. Our LLM-based planner generates the instructions in a dynamic manner, and considers its prior planned steps to generate better future plans.</p>
<p>MultiON task has attracted attention of researchers in the</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />Figure 2: The overview of our SayNav framework.</p>
<p>last few years. As already mentioned, most of them (Chen et al. 2022; Zeng et al. 2023; Marza et al. 2022, 2023) have considered pre-defined sequence of objects to be localized which simplifies the task by a great extent. Moreover, they employ pure DRL-based approaches and hence suffer from issues discussed before. (Gireesh et al. 2023) is the only work to consider MultiON without any sequence of objects but again makes use of a DRL-based approach.</p>
<h2>3 SayNav</h2>
<p>We now describe SayNav’s framework as well as the multiobject navigation task.</p>
<h3>3.1 Task Definition</h3>
<p>We choose Multi-Object Navigation task, to validate SayNav. The goal of this task is to navigate the agent in a large-scale unknown environment in order to find an instance for each of three predefined object categories (such as ”laptop”, ”tomato”, and ”bread”). The agent is initialized at a random location in the environment and receives the goal object categories (o_{i}, o_{j}, o_{k}) as input. At each time step $t$ during navigation, the agent receives environment observations $e_{t}$ and takes control actions $a_{t}$. The observations include RGBD images, semantic segmentation maps, and the agent’s pose (location and orientation). The action space includes five control commands: turn-left, turn-right, move-forward, stop, and look-around. Both turn-left and turn-right actions rotate the agent by 90 degrees. The move-forward action moves the agent by 25 centimeters. The task execution is successful if the agent locates (by detection) all three objects within a time period.</p>
<p>Note that MultiON task poses much larger planning challenges and task complexities than previous navigation tasks, which either look for a single object (Chaplot et al. 2020) or reach a single target point (Wijmans et al. 2019). For example, the agent needs to dynamically set up the search plan based on the prioritized order among three different objects. This plan can also be changed during exploration in a new house with unknown layouts. As shown in Figure 1, the agent first realizes that it is in the bedroom and then decides to prioritize places (such as the table) to locate the laptop inside this room. On the other hand, if the agent had started in the kitchen, it would have been more efficient to search for the fork and spoon first. Therefore, this new task requires extensive semantic reasoning and dynamic planning capabilities, as what humans possess, for an autonomous agent to explore in large-scale unknown environments.</p>
<h3>3.2 Overview</h3>
<p>SayNav’s framework is illustrated in Figure 2. The corresponding pseudo-code is in Algorithm 1. It includes three modules: (1) Incremental Scene Graph Generation, (2) High-Level LLM-based Dynamic Planner, and (3) Low-Level Planner. The Incremental Scene Graph Generation module accumulates observations received by the agent to build and expand a scene graph, which encodes semantic entities (such as objects and furniture) from the areas the agent has explored. The High-Level LLM-based Dynamic Planner continuously converts relevant information from the scene graph into text prompts to a pre-trained LLM, for dynamically generating short-term high-level plans. Each LLM-planned step is executed by the Low-Level Planner to generate a series of control commands for execution.</p>
<h3>3.3 Incremental Scene Graph Generation</h3>
<p>This module continuously builds and expands a 3D scene graph of the environment being explored. A 3D scene graph is a layered graph which represents spatial concepts (nodes) at multiple levels of abstraction with their relations (edges). This representation has recently emerged as a powerful high-level abstraction for 3D large-scale environments in robotics. Here we define four levels in the 3D scene graph: small objects, large objects, rooms, and house. Each object node is associated with its 3D coordinate and room node is associated with its bounds. Every door is treated as an edge between two rooms, which also has an associated 3D coordinate. All other edges reveal the topological relationships among semantic concepts across different levels. Figure 3 shows one example of our scene graph. Mathematically, our scene graph can be represented as a set of 4 kinds of triplets:</p>
<p>${(s_{h},\text{<code>near', }l_{i}),(l_{i},\text{</code>in', }r_{j}),(r_{j},d_{jk},r_{k}),(r_{j},\text{`in', }H)}$</p>
<p>where, $s_{h}$: small object, $l_{i}$: large object, $r_{j},r_{k}$: rooms ( $i \neq$ $j), d_{j k}$ : door between $r_{j} \&amp; r_{k}, H$ : house (root node).</p>
<p>Large objects act as landmarks which are visible from far away distance. LLM uses these large objects to reason if a small object can be found near them. For example, it may be logical to walk towards a dining table to find a knife. The decision of small vs. large objects is made based on its dimensions and LLM’s understanding about the object class.</p>
<p>The scene graph is built using environmental observations received by the agent during exploration. The depth of each segmented object can be obtained based on RGBD images and semantic segmentation images. The 3D coordinate of each perceived object can then be estimated by combining its depth information at multiple timestamps and the corresponding agent’s poses.</p>
<p>We also utilize LLMs to augment and refine high-level abstractions of the scene graph. For example, we use LLMs</p>
<table>
<thead>
<tr>
<th>Algorithm 1: SayNav</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Input : Start location of robot start_location</td>
<td></td>
</tr>
<tr>
<td>house ID house_id</td>
<td></td>
</tr>
<tr>
<td>Target Objects target_objects</td>
<td></td>
</tr>
<tr>
<td>1 unfound_objects $\leftarrow$ target_objects</td>
<td></td>
</tr>
<tr>
<td>2 spawn_robot (start_location)</td>
<td></td>
</tr>
<tr>
<td>3 SceneGraph $\leftarrow$</td>
<td></td>
</tr>
<tr>
<td>create_scene_graph (house_id)</td>
<td></td>
</tr>
<tr>
<td>4 while len(unfound_objects) $&gt;0$ do</td>
<td></td>
</tr>
<tr>
<td>5</td>
<td></td>
</tr>
<tr>
<td>6</td>
<td></td>
</tr>
<tr>
<td>update_unfound_objects (objs_found)</td>
<td></td>
</tr>
<tr>
<td>7</td>
<td></td>
</tr>
<tr>
<td>room_type $\leftarrow$</td>
<td></td>
</tr>
<tr>
<td>identify_room_type (observations)</td>
<td></td>
</tr>
<tr>
<td>8</td>
<td></td>
</tr>
<tr>
<td>SceneGraph.update(room_type, observations)</td>
<td></td>
</tr>
<tr>
<td>9</td>
<td></td>
</tr>
<tr>
<td>plan_needed $\leftarrow$ is_feasible (room_type)</td>
<td></td>
</tr>
<tr>
<td>10</td>
<td></td>
</tr>
<tr>
<td>if plan_needed then</td>
<td></td>
</tr>
<tr>
<td>11</td>
<td></td>
</tr>
<tr>
<td>subgraph $\leftarrow$ SceneGraph.</td>
<td></td>
</tr>
<tr>
<td>12</td>
<td></td>
</tr>
<tr>
<td>extract_subgraph(room_type)</td>
<td></td>
</tr>
<tr>
<td>13</td>
<td></td>
</tr>
<tr>
<td>plan $\leftarrow$</td>
<td></td>
</tr>
<tr>
<td>query_llm_for_plan (subgraph,</td>
<td></td>
</tr>
<tr>
<td>unfound_objects)</td>
<td></td>
</tr>
<tr>
<td>14</td>
<td></td>
</tr>
<tr>
<td>for action in plan do</td>
<td></td>
</tr>
<tr>
<td>15</td>
<td></td>
</tr>
<tr>
<td>if action_type $=$ 'navigate' then</td>
<td></td>
</tr>
<tr>
<td>16</td>
<td></td>
</tr>
<tr>
<td>navigate_to (action_target_location)</td>
<td></td>
</tr>
<tr>
<td>17</td>
<td></td>
</tr>
<tr>
<td>end</td>
<td></td>
</tr>
<tr>
<td>18</td>
<td></td>
</tr>
<tr>
<td>else if action_type $=$ 'look' then</td>
<td></td>
</tr>
<tr>
<td>19</td>
<td></td>
</tr>
<tr>
<td>objs_found, observations $\leftarrow$</td>
<td></td>
</tr>
<tr>
<td>look_around()</td>
<td></td>
</tr>
<tr>
<td>SceneGraph.update(observations)</td>
<td></td>
</tr>
<tr>
<td>20</td>
<td></td>
</tr>
<tr>
<td>update_unfound_objects (objs_found)</td>
<td></td>
</tr>
<tr>
<td>21</td>
<td></td>
</tr>
<tr>
<td>end</td>
<td></td>
</tr>
<tr>
<td>22</td>
<td></td>
</tr>
<tr>
<td>end</td>
<td></td>
</tr>
<tr>
<td>23</td>
<td></td>
</tr>
<tr>
<td>if len(unfound_objects) $&gt;0$ then</td>
<td></td>
</tr>
<tr>
<td>24</td>
<td></td>
</tr>
<tr>
<td>if SceneGraph.all_doors_explored() then</td>
<td></td>
</tr>
<tr>
<td>25</td>
<td></td>
</tr>
<tr>
<td>return 'Task Failed'</td>
<td></td>
</tr>
<tr>
<td>26</td>
<td></td>
</tr>
<tr>
<td>end</td>
<td></td>
</tr>
<tr>
<td>27</td>
<td></td>
</tr>
<tr>
<td>door $\leftarrow$</td>
<td></td>
</tr>
<tr>
<td>find_next_unexplored_door()</td>
<td></td>
</tr>
<tr>
<td>navigate_to (door)</td>
<td></td>
</tr>
<tr>
<td>28</td>
<td></td>
</tr>
<tr>
<td>end</td>
<td></td>
</tr>
<tr>
<td>29</td>
<td></td>
</tr>
<tr>
<td>end</td>
<td></td>
</tr>
<tr>
<td>30</td>
<td></td>
</tr>
<tr>
<td>return 'Success'</td>
<td></td>
</tr>
</tbody>
</table>
<p>to annotate and identify the spatial entity (room type) at the room level of the graph based on its connected objects at lower levels. For instance, a room is probably a bedroom if it includes a bed. The bounds of a room are calculated based on the detection of walls and floor of the room.</p>
<p>SayNav also uses the 3D scene graph to support memory for future planning. For example, it automatically annotates the room nodes that have been investigated. Therefore, it will not generate repeated plans when the agent revisits the same room during exploration.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: An example of our scene graph.</p>
<h3>3.4 High-Level LLM-based Dynamic Planner</h3>
<p>Similar to previous works in LLM-based planning, SayNav utilizes a two-level planning architecture to reduce the learning complexity of the assigned task. However, instead of generating a complete high-level plan for the entire task in the beginning, SayNav utilizes LLMs to incrementally generate a short-term plan regularly, based on current observations and the memory of previously-visited regions. This high-level planner can be set-up using only a few training examples via typical in-context learning procedures (Brown et al. 2020; Ouyang et al. 2022) (as shown in Figure 4).</p>
<p>Our high-level LLM-based dynamic planner extracts a subgraph from the full 3D scene graph and converts it into text prompts, which are fed to an LLM. The extracted subgraph includes spatial concepts in the local region centered around the current position of the agent. We implemented the LLM prompts similar to (Singh et al. 2023), which constructs programming language prompts based on the text labels in the extracted subgraph. Once prompts are received, the LLM planner outputs short-term step-by-step instructions, as pseudo code. The generated plan provides an efficient search strategy within the current perceived area based on human knowledge, prioritizing locations to visit inside the room based on the likelihoods of target objects being discovered. For example, LLM may provide a plan to first check the desk and then the bed to find the laptop in the bedroom. Figure 4 shows the prompt structure used to generate the plan. We provide two in-context examples inside the prompt to constrain the LLM-generated plans. For instance, we constrain each step to generate a navigate or look function call with arguments and a high-level comment. Note that we also make LLM output a descriptive comment associated with each step in the generated plan, for minimizing the prevailing issue of hallucinations in LLMs.</p>
<p>The LLM-based planner also extends and updates the plan when the previous plan fails or the task goal (finding three objects) is not achieved after the execution of previous shortterm plan. It attempts to update the plan either by (i) generating a plan based on new information received from the environment, (ii) putting the low-level planner into an exploratory mode (ex: try to go to next room), or (iii) refining the scene graph by instructing the low-level planner to randomly move around and collect more observations.</p>
<p>Before generating the high-level plan for a local region (room), LLM also reasons about the feasibility of locating the target object(s) in the detected room-type. It may decide</p>
<table>
<thead>
<tr>
<th>System</th>
<th>Search Plan prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td>Assume you are provided with a text-based description of a room in a house with objects and their 2D coordinates</td>
<td></td>
</tr>
<tr>
<td>Task: I am at a base location. Suggest me a step-wise high-level plan to achieve the goal below. Here are some rules:</td>
<td></td>
</tr>
<tr>
<td>1. I can only perform the actions- (navigate, (x, y)), (look, objects)</td>
<td></td>
</tr>
<tr>
<td>2. I can only localize objects around other objects and not room e.g. apple should be looked for on the table and not kitchen.</td>
<td></td>
</tr>
<tr>
<td>3. Provide the plan in a csv format. I have shown two examples below:</td>
<td></td>
</tr>
<tr>
<td>a) Goal = ‘Find a laptop’, Base Location = ‘LivingRoom’</td>
<td></td>
</tr>
<tr>
<td>- navigate; (3.4, 2.6); Go to table</td>
<td></td>
</tr>
<tr>
<td>- look; (laptop); Look for laptop near the table</td>
<td></td>
</tr>
<tr>
<td>b) Goal = ‘Find apple and bowl’, Base Location = ‘Kitchen’</td>
<td></td>
</tr>
<tr>
<td>- navigate; (3.4, 2.6); Go to DiningTable</td>
<td></td>
</tr>
<tr>
<td>- look; (apple, bowl); Look for apple and bowl near the DiningTable</td>
<td></td>
</tr>
<tr>
<td>- navigate; (8.32, 0.63); Go to CounterTop</td>
<td></td>
</tr>
<tr>
<td>- look; (apple, bowl); Look for apple and bowl near the CounterTop</td>
<td></td>
</tr>
<tr>
<td>4. Start the line with - and no numbers</td>
<td></td>
</tr>
<tr>
<td>5. Provide the plan as human would by e.g. based on contextual relationships between objects</td>
<td></td>
</tr>
<tr>
<td>User</td>
<td></td>
</tr>
<tr>
<td>Room description</td>
<td></td>
</tr>
<tr>
<td>(room_graph)</td>
<td></td>
</tr>
<tr>
<td>Goal: (goal)</td>
<td></td>
</tr>
<tr>
<td>Base location: (base_location)</td>
<td></td>
</tr>
</tbody>
</table>
<p>Figure 4: Prompt used to create the search plan for a particular room
to skip searching a specific room and come back later if the feasibility is low. In this case, it would mark the corresponding room-node in the scene graph to come back later. Figure 5 shows the structure of prompts used to identify the room type and determine the feasibility of locating target object in a room.</p>
<table>
<thead>
<tr>
<th>Search Feasibility Prompt</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>System</td>
<td></td>
</tr>
<tr>
<td>Answer the question as ‘Yes’ or ‘No’</td>
<td></td>
</tr>
<tr>
<td>User</td>
<td></td>
</tr>
<tr>
<td>Is it likely to find (object) in a (room_type)</td>
<td></td>
</tr>
<tr>
<td>Room Identification Prompt</td>
<td></td>
</tr>
<tr>
<td>System</td>
<td></td>
</tr>
<tr>
<td>Identify the room based on the list of seen objects from the list below</td>
<td></td>
</tr>
<tr>
<td>- Kitchen</td>
<td></td>
</tr>
<tr>
<td>- LivingRoom</td>
<td></td>
</tr>
<tr>
<td>- Bathroom</td>
<td></td>
</tr>
<tr>
<td>- Bedroom</td>
<td></td>
</tr>
<tr>
<td>Output should be the room name as a single word without the -</td>
<td></td>
</tr>
<tr>
<td>User</td>
<td></td>
</tr>
<tr>
<td>(object_list)</td>
<td></td>
</tr>
</tbody>
</table>
<p>Figure 5: Prompts used to compute the feasibility of finding an object in a room-type and to identify the room-type.</p>
<h3>3.5 Low-Level Planner</h3>
<p>The Low-Level Planner converts each LLM-planned step into a series of control commands for execution. To integrate two planners, SayNav formulates each LLM-planned step as a short-distance point-goal navigation (POINTNAV) sub-task for the low-level planner. The target point for each sub-task, such as moving from the current position to the table in the current room, is assigned by the 3D coordinate of the object (e.g. table) described in each planned step.</p>
<p>SayNav’s low-level planner takes the RGBD images (res- olution $320 \times 240$ ) and the agent’s pose (location and orientation) as inputs, and it outputs move_forward, turn_left and turn_right actions to control the robot following standard POINTNAV settings. Note that largescale DRL approaches typically take $10^{8}$ to $10^{9}$ simulation steps during training to solve POINTNAV tasks in simulation environments (Wijmans et al. 2019; Weihs et al. 2020), which poses serious training requirements and computation demands. In SayNav, however, the two-level planning architecture simplifies the job of the low-level planner. The low-level planner mostly outputs control commands for short-range movements. The LLM-based high-level planner is also robust to failures in the low-level planner, by making regular plan updates. In this way, the training load required on the low-level planner can be greatly reduced.</p>
<p>Encouraged by the success of imitation learning (IL) on navigation tasks under resource constraints (Ramrakhya et al. 2022, 2023; Shah et al. 2023), we investigate a sample efficient IL-based method to train the low-level planner for the agent in SayNav. This low-level planner is trained from scratch (without pretraining) on only 2800 episodes or $7 \times 10^{5}$ simulation steps. Specifically, the low-level planner is trained using the DAGGER algorithm (Ross et al. 2011) to follow a shortest path oracle as the expert. Despite the fact that the shortest path oracle lacks the exploration behavior required to solve the POINTNAV task in complex environments (e.g. multiple rooms), we find that it helps the agent to learn short-distance navigation skills very quickly, without human-in-the-loop.</p>
<p>We first implement a shortest path oracle using A<em> algorithm on the grid of reachable positions in the house, and then train a POINTNAV agent using the DAGGER (Ross et al. 2011) algorithm with the A</em> oracle as the expert. We perform DAGGER dataset aggregation over two rounds. In the first round, 2,000 episodes were collected using a random agent. In the second round, 800 additional episodes were collected using an agent trained using episodes from the first round. The aggregated dataset contains a total of 2,800 episodes or $7 \times 10^{5}$ simulator steps for IL. For each episode, we choose a scene from the ProcThor-10k train split, randomly sample a start location and sample a random object from the scene as the goal location. The robot then performs the task by taking expert action with $p=0.2$ and agent action from $p=0.8$. The robot’s observations and expert actions are stored for behavior cloning.</p>
<p>For behavior cloning, the objective function is to minimize cross entropy loss of predicted action against expert action at every step. For agent architecture, following (Wijmans et al. 2019), we use a standard architecture shown in Figure 6. As mentioned, our agent receives an RGBD image and the agent’s pose with respect to the goal location as the input. A GroupNorm (Wu et al. 2018) ResNet18 (He et al. 2016) encodes the input RGBD image, and a 2-layer 512 hidden size gated recurrent unit (GRU) (Cho et al. 2014) combines history and sensor inputs to predict the next action. We use the Adamax optimizer with lr $10^{-4}$ and weight decay $10^{-4}$. We optimize the objective function with batch size of 16 episodes for 50 epochs.</p>
<p>We evaluate the POINTNAV performance of the agent</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" />Figure 6: Low-level POINTNAV architecture.</p>
<p>on the publicly available AI2Thor OBJECTNAV dataset ProcThor-10k-val split, using the ground truth location of the object as the goal for POINTNAV evaluation. The evaluation split contains 1550 episodes. When multiple instances of the target object are available, we arbitrarily select the first instance as the POINTNAV goal. Our low-level planner achieves $84.5\%$ SR (success radius 1.5m, max 300 steps) and 0.782 SPL. On the subset of episodes where the starting position and the goal position are in the same room, performance increases to $98.5\%$ SR and 0.930 SPL.</p>
<h2>4 Experimental Results</h2>
<h3>4.1 Multi-Object Navigation Dataset</h3>
<p>Most prior Embodied AI simulators such as AI2-THOR <em>Kolve et al. (2017)</em> or Habitat <em>Szot et al. (2021)</em> are either based on environments with single rooms or lack the natural placement of objects within the environment or lack the ability to interact with the objects. For our experiments, we opted for the recently introduced ProcTHOR framework <em>Deitke et al. (2022)</em>, which is built on top of the AI2-THOR simulator. ProcTHOR is capable of procedurally generating full floor plans of a house given a room specification (ex: a house with 2 bedrooms, 1 kitchen and 1 bathroom). It also populates each floorplan with 108 object types, with realistic, physically plausible, and natural placements. The scenes in ProcTHOR are interactive which allows to change the state, lighting and texture of objects, posing a bigger challenge for perception and a broader scope for future work. We build a benchmark dataset of 132 episodes using 132 different houses with 3-10 rooms each and select 3 objects for each house to conduct MultiON task. Each episode in the dataset is described using the following properties:</p>
<ol>
<li>data_type : This corresponds to either ‘val’ or ‘test’ based on which set of data was used from ProcThor-10K to construct the episode.</li>
<li>house_idx : This corresponds to index of the specific house in the ProcThor-10K dataset.</li>
<li>num_rooms : Number of rooms in the house</li>
<li>num_targets : Number of targets in the house (Currently we have limited the dataset to 3 targets)</li>
<li>targets : List of unique targets in the house along with their ground truth locations</li>
</ol>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>6. start_position : Start position randomly sampled from all reachable positions in the house
7. start_heading : Start heading randomly sampled from 0, 90, 180 and 270 degrees
8. shortest_path_targets_order : The order of targets that results in shortest path using A<em> planner. This is evaluated by running A</em> planner on all possible target orders.
9. shortest_path_length : Length of the shortest path computed via A* planner along the shortest_path_targets_order</p>
<h3>4.2 Metrics</h3>
<p>We report two standard metrics that are used for evaluating navigation tasks: Success Rate (SR) and Success Weighted by Path Length (SPL) <em>Anderson et al. (2018)</em>. SR measures the percentage of cases where the agent is able to find all the three objects successfully, while SPL normalizes the success by ratio of the shortest path to actual path taken. We use the minimum of the shortest path from the starting point to permutations of all the target objects. In addition to these two metrics, we measure the similarity between the object ordering obtained by the agent and that by the ground-truth. The ground-truth object ordering gives an idea of how a perfect agent would have explored the space by first identifying objects that are highly probably in current room/scene-graph and then exploring other rooms. We use the Kendall distance metric <em>Lapata (2006)</em>, which computes the distance between two rankings based on the number of disagreeing pairs. We use the Kendall Tau that converts this distance into a correlation coefficient, and report it over the successful episodes (all three targets are located).</p>
<h3>4.3 Implementation Details</h3>
<p>We use the default robot with head-mounted RGBD camera in the AI2-Thor simulator. The camera has $320\times 240$ resolution with 90°field-of-view (FoV). The details of the robot observations and actions can be referred to the section 3.1.</p>
<p>We conduct experiments with 2 different LLMs: gpt-3.5-turbo and gpt-4. For training the low-level planner, we used the IL method, described in the section 3.5. It achieves $84.5\%$ SR (success radius 1.5m, max 300 steps) and 0.782 SPL in unseen ProcThor-10k-val scenes with random start and goal locations. For short-range movements within a single room, performance increases to $98.5\%$ SR and 0.930 SPL.</p>
<p>Note that SayNav consists of three modules – incremental scene graph generator, LLM-based planner, and a low-level planner. The major goal of our experiments is to fully validate and verify the LLM-based planning capabilities in SayNav for MultiON. Therefore, for each of the other two modules, we implemented an alternative option which uses ground truth information to avoid any error within that module. This allows us to conduct ablation study for determining the impact of each module on the overall performance.</p>
<p>First, we allow the scene graph to be generated using ground truth (GT) instead of visual observations (VO). We have described the generation of scene graph using VO in the section 3.3. The GT option directly uses the ground truth information of surrounding objects, including 3D coordinates and geometric relationships among objects, to incrementally</p>
<table>
<thead>
<tr>
<th></th>
<th>Scene</th>
<th>LL</th>
<th>SR</th>
<th>SPL</th>
<th>Kendall</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Graph</td>
<td>Planner</td>
<td>(%)</td>
<td></td>
<td>Tau</td>
</tr>
<tr>
<td>Baseline</td>
<td></td>
<td></td>
<td>56.06</td>
<td>0.49</td>
<td></td>
</tr>
<tr>
<td>SayNav</td>
<td>GT</td>
<td>OrNav</td>
<td>95.35</td>
<td>0.43</td>
<td>0.70</td>
</tr>
<tr>
<td>(gpt-3.5)</td>
<td>GT</td>
<td>PNav</td>
<td>80.62</td>
<td>0.32</td>
<td>0.72</td>
</tr>
<tr>
<td></td>
<td>VO</td>
<td>OrNav</td>
<td>71.32</td>
<td>0.48</td>
<td>0.56</td>
</tr>
<tr>
<td></td>
<td>VO</td>
<td>PNav</td>
<td>60.32</td>
<td>0.34</td>
<td>0.62</td>
</tr>
<tr>
<td>SayNav</td>
<td>GT</td>
<td>OrNav</td>
<td>93.93</td>
<td>0.46</td>
<td>0.76</td>
</tr>
<tr>
<td>(gpt-4)</td>
<td>GT</td>
<td>PNav</td>
<td>84.09</td>
<td>0.36</td>
<td>0.78</td>
</tr>
<tr>
<td></td>
<td>VO</td>
<td>OrNav</td>
<td>69.80</td>
<td>0.47</td>
<td>0.76</td>
</tr>
<tr>
<td></td>
<td>VO</td>
<td>PNav</td>
<td>64.34</td>
<td>0.33</td>
<td>0.78</td>
</tr>
</tbody>
</table>
<p>Table 1: Results of SayNav on multi-object navigation task. Baseline uses a PNav agent to navigate along the shortest route among targets based on ground-truth positions; GT and VO build the scene graph from ground-truth object/room locations and visual observations provided by the simulator respectively; OrNav and PNav use oracle and IL-learned low-level (LL) planner respectively for navigating between the points assigned by the high-level planner.</p>
<p>build the scene graph during exploration. This option avoids any association and computation ambiguity from processing on visual observations, such as computing 3D coordinates for each object based on RGBD image and its segmentation.</p>
<p>Second, we use an oracle planner (OrNav) as the low-level planner instead of our efficiently-trained IL agent (PNav). We have described the implementation of PNav in the section 3.5. For OrNav, we use an A* planner which has access to the map of the environment. Given a target location, it can plan the shortest path from the agent’s current location to the target.</p>
<h3>4.4 Baseline</h3>
<p>Most existing LLM-based approaches for robot navigation operate in known and/or small-scale environments. However, there do exist RL / IL based approaches (ex: <em>Gireesh et al. (2023)</em>) and traditional SLAM-based approaches that can work in our problem setting. Many previous works (ex: <em>Savva et al. (2019)</em>) have shown that RL/IL based Point-goal Navigation (PointNav) outperforms SLAM-based approaches in unknown environments by a large extent (80% vs 62% in the mentioned example). Due to lack of open-source datasets/code from existing works, we chose to implement the strongest possible baseline method which employs IL-based PointNav policy that could potentially reflect the upper bound of the performance of a learning-based agent using the same amount of training data as ours. The baseline agent uses two privileged information that SayNav doesn’t have access to. First, it has access to the optimal order of target objects which achieves the shortest path. This simplifies the MultiON task to become a series of ObjectNav tasks. Second, as a PointNav policy typically performs better than an ObjectNav policy, we also provide the baseline agent access to the ground truth coordinate of each target object, which then simplifies the task to a series of PointNav tasks. In other words, we implement a PointNav agent to navigate through ground truth points of the objects in the optimal order.</p>
<h3>4.5 Quantitative Results</h3>
<p>Table 1 shows the results of the baseline along with different implementation choices in SayNav. Note for the baseline method, even after using ground-truth object locations in optimal order, SR is only 56.06%, which indicates the difficulty of MultiON task. It is because of the fact that PointNav policy observes a loss in performance in large-scale environments. Although, the baseline has access to the locations of the goal objects, finding each object still requires it to plan a long-range path across multiple rooms. In comparison, SayNav, without using any ground truth, achieves a higher SR (60.32% and 64.34% with gpt-3.5-turbo and gpt-4 respectively). This improvement highlights the superiority of SayNav in navigating in large-scale unknown environments.</p>
<p>Goal: Find alarmclock, laptop, cellphone</p>
<p>Starting Location: Kitchen</p>
<p>SayNav determines it is a kitchen based on perceived exertion.</p>
<p>Look around &amp; locate laptop</p>
<p>SayNav determines it is a living room.</p>
<p>Explores &amp; goes to next room</p>
<p>Explore determines it is a living room.</p>
<p>Explores &amp; goes to next room</p>
<p>Locate alarmclock</p>
<p>Figure 7: Visualization of an episode with SayNav (OrNav + GT) for multi-object object navigation task.</p>
<p>SR: With SayNav, we observe that the best performance is achieved when using scene graph generated by ground-truth object/room location from the simulator (GT) along with OrNav. Note that SR of 95.35% with gpt-3.5 and 93.93% with gpt-4 in this setting reflects the strength of our LLM-based high-level planner which is the only imperfect module in the experiment. When we replaced GT with VO, we do observe a loss in performance. We found that the drop in SR can be associated with various challenges encountered in any perception based algorithms. The inaccurate estimation of objects’ 3D positions due to partial observations can lead to failures in detecting targets and navigation. In addition,</p>
<p>we remove objects less than 20 pixels on semantic segmentation observations for more practical behavior. Therefore, very small objects can also be missed-out while building the scene graph from visual observations. We also observed a specific challenge in VO associated with estimating the location of glass doors. In the depth map, the depths of visible objects behind the glass door represent the depths of the actual physical door, which fails the estimation of the location of the door. A similar trend can be found from results with GT \&amp; PNav and with VO \&amp; PNav. When replacing OrNav with PNav, we also observe a fall in performance. This is obvious as PNav doesn't access any ground truth information, as compared to OrNav.</p>
<p>However, even with all these challenges, SayNav outperforms the oracle-based baseline and succeeds in MultiON tasks. We believe it is due to LLM-based dynamic planning capabilities, with the grounding mechanism based on incremental scene graph generation. It leverages common-sense knowledge, as humans do, to efficiently search multiple different objects in an unknown environment. It also refines or corrects the plan in case of any failure in a planned step.</p>
<p>SPL: Looking at the SPL metric, we see a drop in SayNav as compared to the baseline. Note that SPL reflects the length of the path taken by the agent as compared to the shortest possible path. For example, SPL=1 for an episode would mean that the agent, starting from initial position, goes straight to the targets along the shortest path in the optimal order with zero exploration which is practically impossible in an unknown environment. As a result of access to the ground-truth object locations in optimal order, it becomes obvious for the baseline to have higher SPL. From the results, we also observe that the low-level planner has the major impact on SPL. The system achieves higher SPL with OrNav as compared to PNav.</p>
<p>Kendall-Tau: The Kendall-Tau $(\tau)$ metric measures the similarity between the order of objects as located by the agent and the optimal ordering based on the ground-truth. It shows the importance of the knowledge provided by the LLMs, for finding the optimal plan. We observe that the ordering of the objects is not affected much by the low-level planner. This is reasonable since the ordering should majorly depend on the plans generated by the high-level planner. As expected, the score drops when we replace GT with VO since LLM uses scene-graph to generate the plan. The considerable improvement in the score with gpt-4 (vs gtp-3.5-turbo) shows that using a better LLM enables an improvement in use of common-sense priors and yields more optimal ordering. Also, note that Kendall Tau metric doesn't apply to the baseline since it already has access to the optimal order of targets.</p>
<h3>4.6 Qualitative Results</h3>
<p>We show an example of a typical episode in Figure 7 where the agent is asked to locate an alarm-clock, a laptop, and a cellphone in an unknown house. The agent happens to start in the kitchen (determined by LLM based on perceived objects). The planner reasons that it is unlikely to find either of the objects there, so it decides to go to another room through a door. Then, it comes to a living-room where it is able to lo-</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Scene <br> Graph</th>
<th style="text-align: center;">LL <br> Planner</th>
<th style="text-align: center;">SR <br> (\%)</th>
<th style="text-align: center;">SPL</th>
<th style="text-align: center;">Kendall <br> Tau</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SayNav <br> (gpt-3.5)</td>
<td style="text-align: center;">GT</td>
<td style="text-align: center;">OrNav</td>
<td style="text-align: center;">77.86</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.72</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GT</td>
<td style="text-align: center;">PNav</td>
<td style="text-align: center;">61.83</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.76</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VO</td>
<td style="text-align: center;">OrNav</td>
<td style="text-align: center;">58.73</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.72</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VO</td>
<td style="text-align: center;">PNav</td>
<td style="text-align: center;">46.77</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.82</td>
</tr>
<tr>
<td style="text-align: center;">SayNav <br> (gpt-4)</td>
<td style="text-align: center;">GT</td>
<td style="text-align: center;">OrNav</td>
<td style="text-align: center;">93.93</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.69</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GT</td>
<td style="text-align: center;">PNav</td>
<td style="text-align: center;">86.36</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">0.77</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VO</td>
<td style="text-align: center;">OrNav</td>
<td style="text-align: center;">72.09</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.73</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VO</td>
<td style="text-align: center;">PNav</td>
<td style="text-align: center;">61.60</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">0.77</td>
</tr>
</tbody>
</table>
<p>Table 2: Results of SayNav on MultiON task using LLM Memory
cate the laptop and cellphone. The third object still remains unfound, so it again decides to go to another room via a door. Eventually, it locates the alarm-clock in the final room. The complete demo-video for this example can be found at our project link.</p>
<h3>4.7 Memory via LLM</h3>
<p>As mentioned in the section 3.3, SayNav uses the 3D scene graph to support memory for future planning. For instance, it automatically annotates the room nodes that have already been explored and won't plan for the room if the agent happens to visit the same room again. This type of implementation to support memory works perfectly for our chosen task. However, we also wanted to explore the possibility of making the LLM track its own plans. Hence, we also implemented SayNav's memory via LLM by using Conversational Chains module of the LangChain framework ${ }^{2}$. Here we use two separate instances of identical LLM models, one to generate the high level plans and another to track the generated plans. The LLM tracking the generated plan, uses the Room Tracking Prompt in Figure 8 and is also equipped with a conversational memory. The LLM responsible for generating the plans receives detailed description of the surrounding environment while the other LLM instance only receives the minimal information necessary to do the tracking. A key advantage of this framework is that it avoids hitting the maximum token limit on the LLM by not relying on the entire conversational history (as is usually done). We believe that LLM-based tracking might be able to generalize better to other tasks since it eliminates the need of a module for tracking plan history, which would require different implementations for different task (we will test this in our future work).</p>
<p>Table 2 shows the performance of SayNav using LLM Memory via gpt-3.5-turbo and gpt-4. Note that we use the same model for both the instances of LLM in our experiments. We do observe substantial drop in SR and SPL metrics by using LLM Memory in the case of gpt-3.5-turbo as compared to the results reported in Table 1. However, with gpt-4, the LLM Memory is able to achieve similar results as compared to Table 1. This shows that it is feasible to handover the task of tracking to the better LLM models.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>Room Tracking Prompt</h2>
<p>System
An agent is looking for certain objects in a house. It goes from one room to another room to search the objects.
Given a room name, output one of the following three options:</p>
<p>1) - Search this room;
2) - Come back later; Reason: It is unlikely to find either of the given objects in this room
3) - Skip this room; Reason: It has already been searched</p>
<p>Follow these rules while selecting an option:
a. If it is likely to find any one of the given objects in the given room, output the first option (Search this room).
b. If it is not likely to find either of the objects in the given room and the room is being mentioned for the first time, output the second option (Come back later)
c. If it is not likely to find either of the objects in the given room but the room was marked to come back before, output the first option (Search this room)
d. If the agent has already searched the room before, output the third option (Skip this room)</p>
<p>Notes:
a. 'Bedroom 1' and 'Bedroom 2' are different rooms.
b. 'Bathroom 1' and 'Bathroom 2' are different rooms.</p>
<p>Here is an example:
Objects: Apple, Laptop
Room: LivingRoom</p>
<ul>
<li>Search this room</li>
</ul>
<p>Objects: Apple, Laptop
Room: Bathroom</p>
<ul>
<li>Come back later; Reason: It is unlikely to find either of the given objects in this room Objects: Apple
Room: LivingRoom</li>
<li>Skip this room; Reason: It has already been searched</li>
</ul>
<p>Objects: Apple
Room: Bathroom</p>
<ul>
<li>Search this room</li>
</ul>
<p>User
Objects: (objects)
Room: (room)
Figure 8: Prompt used for room tracking via LLM.</p>
<h3>4.8 Real-World Demonstration</h3>
<p>We also ported SayNav to a real robot and tested under various settings, for showcasing the efficient generalization of SayNav to real environments. The demonstration video for a cafeteria environment is available at our project link.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 9: SayNav on a real robot</p>
<h2>5 Limitations and Discussion</h2>
<p>This section discusses the limitations of our work and some ideas to improve SayNav in the future. As mentioned before, our scene graph generation faces various challenges encountered in any perception-based algorithms. In addition
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 10: A failure example: The left picture shows the RGB image from the camera mounted on the agent and the right picture shows the top-down view of the house. Due to geometry of the room, agent is unable to observe that the door is open and hence, unable to navigate through the door (marked with yellow rectangle).
to the glass door issue that we described earlier, Figure 10 shows a failure case for another issue due to visual observations. Note, in our experiments, the agent is not equipped with arms to open/close the door. Therefore, it only can go through open doors to move to other rooms. In this episode, the agent from most of the positions in the room cannot observe that the door is open (which connects to the other room that has the target object). The robot repeatedly tries to go towards the center of the current room and refine the scene graph. However, it is still not able to identify the "open" status for the door, and therefore, fails to achieve the goal.</p>
<p>A better mechanism to verify attributes (open/close) associated with the object node (door) in the scene graph can help to alleviate this case. For example, the agent can move closer to the door, verify visual observations from all possible angles, and compare the depth information of the door to that of the wall (closed doors shall have nearly identical depths as the connected wall). In the future, we would like to develop verification and feedback mechanisms to validate the plans generated by LLMs for improving SayNav's performance. We also plan to explore smaller LLMs (instead of GPT-3.5 and GPT-4), which can run locally on a real robot.</p>
<h2>6 Conclusion</h2>
<p>We present SayNav, a new approach for efficient generalization to complex navigation tasks in unknown large-scale environments. SayNav incrementally builds and converts a 3D scene graph of the explored environment to LLMs, for generating dynamic and contextually appropriate high-level plans for navigation. We evaluate SayNav on the MultiON task, that requires the agent to efficiently search multiple different objects in an unknown environment. Our results demonstrate that SayNav even outperforms a strong oracle based Point-nav baseline ( $64.34 \%$ vs $56.06 \%$ ), for successfully locating objects in large-scale new environments.</p>
<h2>7 Acknowledgement</h2>
<p>This material is, in part, based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No. HR001123C0089. Any opinions, findings, conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the</p>
<p>views of the DARPA. We would like to thank Tixiao Shan for experiments and demonstrations in the real world. We also thank Rakesh "Teddy" Kumar, Ajay Divakaran, and Supun Samarasekera for their valuable feedback to this material.</p>
<h2>References</h2>
<p>Ahn, M.; et al. 2022. Do as i can, not as i say: Grounding language in robotic affordances. In arXiv:2204.01691.
Anderson, P.; et al. 2018. On evaluation of embodied navigation agents. arXiv:1807.06757.
Armeni, I.; et al. 2019. 3d scene graph: A structure for unified semantics, 3d space, and camera. In CVPR.
Brown, T.; et al. 2020. Language models are few-shot learners. NeurIPS, 33: 1877-1901.
Chaplot, D.; et al. 2020. Object goal navigation using goaloriented semantic exploration. NeurIPS, 33: 4247-4258.
Chen, P.; et al. 2022. Learning Active Camera for MultiObject Navigation. In NeurIPS.
Cho, K.; et al. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In arXiv:1406.1078.
Chung, H. W.; et al. 2022. Scaling instruction-finetuned language models. arXiv:2210.11416.
Deitke, M.; et al. 2022. ProcTHOR: Large-Scale Embodied AI Using Procedural Generation. NeurIPS, 35: 5982-5994.
Driess, D.; et al. 2023. Palm-e: An embodied multimodal language model. In arXiv:2303.03378.
Gireesh, N.; et al. 2023. Sequence-Agnostic Multi-Object Navigation. arXiv:2305.06178.
He, K.; et al. 2016. Deep residual learning for image recognition. In CVPR.
Huang, W.; et al. 2022. Inner monologue: Embodied reasoning through planning with language models. In arXiv:2207.05608.
Hughes, N.; et al. 2022. Hydra: A real-time spatial perception engine for 3d scene graph construction and optimization. In Robotics: Science and Systems.
Khandelwal, A.; et al. 2022. Simple but effective: Clip embeddings for embodied ai. In CVPR, 14829-14838.
Kim, U.; et al. 2019. 3-D scene graph: A sparse and semantic representation of physical environments for intelligent agents. IEEE Transactions on Cybernetics, 50(12): 49214933.</p>
<p>Kolve, E.; et al. 2017. Ai2-thor: An interactive 3d environment for visual ai. arXiv preprint arXiv:1712.05474.
Lapata, M. 2006. Automatic evaluation of information ordering: Kendall's tau. Computational Linguistics, 32(4): $471-484$.
Liu, B.; et al. 2023. LLM+P: Empowering large language models with optimal planning proficiency. In arXiv:2304.11477.
Marza, P.; et al. 2022. Teaching Agents how to Map: Spatial Reasoning for Multi-Object Navigation. In IROS.</p>
<p>Marza, P.; et al. 2023. Multi-Object Navigation with dynamically learned neural implicit representations. In ICCV.
Mishkin, D.; et al. 2019. Benchmarking classic and learned navigation in complex 3d environments. In arXiv:1901.10915.
Ouyang, L.; et al. 2022. Training language models to follow instructions with human feedback. In arXiv:2203.02155.
Peng, B.; et al. 2023. Instruction tuning with gpt-4. arXiv:2304.03277.
Ramrakhya, R.; et al. 2022. Habitat-web: Learning embodied object-search strategies from human demonstrations at scale. In CVPR, 5173-5183.
Ramrakhya, R.; et al. 2023. Pirlnav: Pretraining with imitation and rl finetuning for objectnav. In CVPR, 17896-17906.
Rana, K.; et al. 2023. SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Task Planning. In arXiv:2307.06135.
Rosinol, A.; et al. 2021. Kimera: From slam to spatial perception with 3d dynamic scene graphs. The International Journal of Robotics Research, 40(12-14): 1510-1546.
Ross, S.; et al. 2011. A reduction of imitation learning and structured prediction to no-regret online learning. In AISTATS. JMLR Workshop and Conference Proceedings.
Savva, M.; et al. 2019. Habitat: A Platform for Embodied AI Research. In ICCV.
Shah, D.; et al. 2023. ViNT: A Foundation Model for Visual Navigation. arXiv:2306.14846.
Singh, I.; et al. 2023. Progprompt: Generating situated robot task plans using large language models. In ICRA.
Song, C. H.; et al. 2022. Llm-planner: Few-shot grounded planning for embodied agents with large language models. In arXiv:2212.04088.
Szot, A.; et al. 2021. Habitat 2.0: Training home assistants to rearrange their habitat. NeurIPS, 34: 251-266.
Wald, J.; et al. 2020. Learning 3D semantic scene graphs from 3D indoor reconstructions. In CVPR.
Wani, S.; et al. 2020. MultiON: Benchmarking Semantic Map Memory using Multi-Object Navigation. In NeurIPS.
Weihs, L.; et al. 2020. Allenact: A framework for embodied ai research. arXiv preprint arXiv:2008.12760.
Wijmans, E.; et al. 2019. Dd-ppo: Learning nearperfect pointgoal navigators from 2.5 billion frames. In arXiv:1911.00357.
Wu, S.; et al. 2021. SceneGraphFusion: Incremental 3D scene graph prediction from RGB-D sequences. In CVPR.
Wu, Y.; et al. 2018. Group normalization. In ECCV, 3-19.
Yu, B.; et al. 2023. Leveraging Large Language Models for Visual Target Navigation. In arXiv:2304.05501.
Zeng, H.; et al. 2023. Multi-Object Navigation Using Potential Target Position Policy Function. IEEE Transactions on Image Processing, 32: 2608 - 2619.
Zhao, W. X.; et al. 2023. A survey of large language models. arXiv:2303.18223.
Zhu, Y.; et al. 2017. Target-driven visual navigation in indoor scenes using deep reinforcement learning. In ICRA.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://python.langchain.com/docs/modules/chains&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
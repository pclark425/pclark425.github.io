<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3504 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3504</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3504</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-78.html">extraction-schema-78</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-a7fae05c72d371cad8aa95ed5f6b2864a11e3862</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a7fae05c72d371cad8aa95ed5f6b2864a11e3862" target="_blank">DAGN: Discourse-Aware Graph Network for Logical Reasoning</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a discourse-aware graph network (DAGN) that reasons relying on the discourse structure of the texts, and encodes discourse information as a graph with elementary discourse units (EDUs) and discourse relations, and learns the discourse- aware features via a graph network for downstream QA tasks.</p>
                <p><strong>Paper Abstract:</strong> Recent QA with logical reasoning questions requires passage-level relations among the sentences. However, current approaches still focus on sentence-level relations interacting among tokens. In this work, we explore aggregating passage-level clues for solving logical reasoning QA by using discourse-based information. We propose a discourse-aware graph network (DAGN) that reasons relying on the discourse structure of the texts. The model encodes discourse information as a graph with elementary discourse units (EDUs) and discourse relations, and learns the discourse-aware features via a graph network for downstream QA tasks. Experiments are conducted on two logical reasoning QA datasets, ReClor and LogiQA, and our proposed DAGN achieves competitive results. The source code is available at https://github.com/Eleanor-H/DAGN.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3504.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3504.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DAGN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Discourse-Aware Graph Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-based model that constructs discourse-based logic graphs (EDUs as nodes, explicit connectives and punctuation as edge types) and uses a graph neural network to produce discourse features which are fused with contextual token embeddings (RoBERTa-Large backbone) for multiple-choice logical reasoning QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DAGN (with RoBERTa-Large backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A hybrid system: RoBERTa-Large produces contextual token embeddings; texts are segmented into elementary discourse units (EDUs) using PDTB explicit connectives and punctuation, a two-type-edge discourse graph is built, node embeddings (sum of EDU token embeddings) are updated via message passing with edge-type-specific adjacency transforms and gating weights, and updated node features are added back to token embeddings; final answer classification uses layer norm, Bi-GRU, residual connections and an MLP.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor; LogiQA (logical reasoning multiple-choice QA)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Passage-level logical reasoning multiple-choice QA derived from standardized exams (ReClor) and civil service exams (LogiQA); questions require identifying assumptions, weaknesses, causal/hypothetical relations, and other logical relations across sentences/clauses.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Construct a discourse-based logic graph using EDUs (delimited by explicit PDTB connectives and punctuation) as nodes and two edge types (explicit connectives and punctuation). Apply a graph reasoning module (message passing with learned node gating and edge-type adjacency matrices) to produce discourse-aware node representations, then enhance token embeddings by adding corresponding node representations before answer prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>ReClor — Dev: 65.20% ; Test: 58.20% ; Test-EASY: 76.14% ; Test-HARD: 44.11%. LogiQA — Dev: 35.48% ; Test: 38.71%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>RoBERTa-Large (baseline reported in paper): ReClor — Dev: 62.60% ; Test: 55.60% ; Test-EASY: 75.50% ; Test-HARD: 40.00%. LogiQA — Dev: 35.02% ; Test: 35.33%.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>On ReClor, DAGN improves over RoBERTa-Large by +2.6 percentage points on Dev and +2.6 on Test; Test-EASY +0.64; Test-HARD +4.11, indicating a larger gain on harder questions. On LogiQA, DAGN improves over RoBERTa-Large by +0.46 (Dev) and +3.38 (Test).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Paper notes that logical structures are often hidden and datasets lack annotated discourse/logic structure; performance on the HARD subset remains substantially lower than EASY (absolute Test-HARD 44.11%), indicating remaining difficulty. Method depends on explicit connectives / punctuation for EDU delimitation (PDTB-style), so cases lacking explicit discourse markers or with noisy segmentation may degrade performance. No detailed per-type failure cases given beyond lower HARD subset performance and general difficulty of extracting hidden logical structure.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Ablations (ReClor dev): removing/disabling components degrades performance — replacing EDUs with clause nodes or sentence nodes drops accuracy from 65.20% to 64.40%; collapsing edge types into a single type gives 64.80%; making the graph fully connected drops to 61.60%; removing the graph module entirely yields 64.00%. These show fine-grained EDU nodes and the specific discourse-linked edges are important for gains, and that graph reasoning contributes beyond an extra prediction module.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DAGN: Discourse-Aware Graph Network for Logical Reasoning', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3504.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3504.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DAGN (Aug)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DAGN (augmented graph features variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of DAGN that augments graph features (implementation details in paper) to further improve performance on logical reasoning QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DAGN (Aug) (with RoBERTa-Large backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same architecture as DAGN; a variant that augments the graph features (described as DAGN (Aug) in experiments). The paper does not provide extra architectural detail beyond being an augmented variant.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor; LogiQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same logical reasoning multiple-choice QA tasks as DAGN.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Augmentation of the graph features (paper reports an 'Aug' variant but does not fully specify augmentation details in main text; used as an experimental variant to test effect of augmented graph features).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>ReClor — Dev: 65.80% ; Test: 58.30% ; Test-EASY: 75.91% ; Test-HARD: 44.46%. LogiQA — Dev: 36.87% ; Test: 39.32%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>RoBERTa-Large: ReClor Dev 62.60% / Test 55.60% ; LogiQA Dev 35.02% / Test 35.33%.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Marginal improvement over DAGN and RoBERTa-Large: ReClor Test +2.7 over RoBERTa-Large (58.30 vs 55.60); Test-HARD +4.46 over RoBERTa-Large (44.46 vs 40.00). LogiQA Test +3.99 over RoBERTa-Large (39.32 vs 35.33).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Same general limitations as DAGN; paper does not detail what specific augmentations are applied or failure modes unique to this variant.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Reported alongside DAGN; improvements are small compared to base DAGN, suggesting modest benefit from the unspecified augmentation. Main ablation studies in paper focus on graph node/edge design and presence of graph module (see DAGN entry).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DAGN: Discourse-Aware Graph Network for Logical Reasoning', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3504.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3504.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa-Large</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa-Large (backbone pre-trained language model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large pre-trained transformer-based language model used as the backbone for DAGN to produce contextual token embeddings; in this paper it is fine-tuned within the end-to-end DAGN system and also reported as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RoBERTa: A robustly optimized bert pretraining approach</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-Large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A pre-trained Transformer (BERT-family) model; the paper states the RoBERTa-Large backbone used has 24 hidden layers and hidden size 1024 and is fine-tuned end-to-end as part of DAGN.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor; LogiQA (baselines reported)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multiple-choice logical reasoning QA datasets used as benchmarks for strict logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Vanilla fine-tuning of RoBERTa-Large on task (baseline); in DAGN it is combined with the discourse-aware graph module to improve reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported baseline performance (from tables): ReClor — Dev: 62.60% ; Test: 55.60% ; Test-EASY: 75.50% ; Test-HARD: 40.00%. LogiQA — Dev: 35.02% ; Test: 35.33%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Not applicable (this is the baseline used for comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Baseline performance shows substantial gap on HARD subset (40.00% Test-HARD on ReClor) indicating RoBERTa-Large alone struggles with harder logical reasoning items; paper motivates discourse-based augmentation to address these shortcomings.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Used as backbone for DAGN; ablating the graph module shows that adding graph reasoning atop RoBERTa-Large yields improved performance (DAGN vs. RoBERTa-Large numbers in main results).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DAGN: Discourse-Aware Graph Network for Logical Reasoning', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3504.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3504.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT-Large</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT-Large</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large pre-trained bidirectional Transformer (BERT) model reported as a baseline in the paper's experiments on ReClor and LogiQA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-Large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>BERT-Large is a deep bidirectional Transformer pre-trained with masked language modeling and next sentence prediction; the paper lists reported baseline accuracy numbers but does not describe running or modifying BERT-Large within their method.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor; LogiQA (baseline numbers reported)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multiple-choice logical reasoning QA datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Vanilla fine-tuning as baseline (reported numbers in tables, taken from prior works / baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>ReClor — Dev: 53.80% ; Test: 49.80% ; Test-EASY: 72.00% ; Test-HARD: 32.30%. LogiQA — Dev: 34.10% ; Test: 31.03%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>DAGN substantially outperforms BERT-Large on both datasets (e.g., ReClor Test 58.20% vs BERT-Large 49.80%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>As a baseline, BERT-Large shows notably lower performance on HARD items (e.g., 32.30% on ReClor Test-HARD) indicating limited capability on strict logical reasoning compared to larger models / discourse-augmented methods.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>No ablations specific to BERT-Large in this paper; used only as a baseline comparator in tables.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DAGN: Discourse-Aware Graph Network for Logical Reasoning', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3504.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3504.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XLNet-Large</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>XLNet-Large</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large pre-trained Transformer model based on permutation language modeling used as a baseline in the ReClor comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>XLNet-Large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>XLNet-Large is a pre-trained Transformer using permutation-based language modeling; the paper reports baseline accuracy numbers from prior work / leaderboards.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ReClor (baseline numbers reported)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multiple-choice logical reasoning QA (ReClor).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Vanilla fine-tuning as baseline (reported numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>ReClor — Dev: 62.00% ; Test: 56.00% ; Test-EASY: 75.70% ; Test-HARD: 40.50%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>DAGN improves over XLNet-Large on Dev and Test (e.g., Dev 65.20% vs 62.00%; Test 58.20% vs 56.00%) and shows a larger gain on HARD subset (DAGN Test-HARD 44.11% vs XLNet-Large 40.50%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>As a baseline, XLNet-Large performs similarly to RoBERTa-Large and demonstrates limited abilities on HARD subset; no further failure analysis provided for XLNet in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>No XLNet-specific ablations in this paper; included solely as a comparative baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DAGN: Discourse-Aware Graph Network for Logical Reasoning', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ReClor: A reading comprehension dataset requiring logical reasoning <em>(Rating: 2)</em></li>
                <li>LogiQA: A challenge dataset for machine reading comprehension with logical reasoning <em>(Rating: 2)</em></li>
                <li>The penn discourse treebank 2.0 <em>(Rating: 2)</em></li>
                <li>RoBERTa: A robustly optimized bert pretraining approach <em>(Rating: 1)</em></li>
                <li>BERT: Pre-training of deep bidirectional transformers for language understanding <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3504",
    "paper_id": "paper-a7fae05c72d371cad8aa95ed5f6b2864a11e3862",
    "extraction_schema_id": "extraction-schema-78",
    "extracted_data": [
        {
            "name_short": "DAGN",
            "name_full": "Discourse-Aware Graph Network",
            "brief_description": "A graph-based model that constructs discourse-based logic graphs (EDUs as nodes, explicit connectives and punctuation as edge types) and uses a graph neural network to produce discourse features which are fused with contextual token embeddings (RoBERTa-Large backbone) for multiple-choice logical reasoning QA.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DAGN (with RoBERTa-Large backbone)",
            "model_description": "A hybrid system: RoBERTa-Large produces contextual token embeddings; texts are segmented into elementary discourse units (EDUs) using PDTB explicit connectives and punctuation, a two-type-edge discourse graph is built, node embeddings (sum of EDU token embeddings) are updated via message passing with edge-type-specific adjacency transforms and gating weights, and updated node features are added back to token embeddings; final answer classification uses layer norm, Bi-GRU, residual connections and an MLP.",
            "model_size": null,
            "reasoning_task_name": "ReClor; LogiQA (logical reasoning multiple-choice QA)",
            "reasoning_task_description": "Passage-level logical reasoning multiple-choice QA derived from standardized exams (ReClor) and civil service exams (LogiQA); questions require identifying assumptions, weaknesses, causal/hypothetical relations, and other logical relations across sentences/clauses.",
            "method_or_intervention": "Construct a discourse-based logic graph using EDUs (delimited by explicit PDTB connectives and punctuation) as nodes and two edge types (explicit connectives and punctuation). Apply a graph reasoning module (message passing with learned node gating and edge-type adjacency matrices) to produce discourse-aware node representations, then enhance token embeddings by adding corresponding node representations before answer prediction.",
            "performance": "ReClor — Dev: 65.20% ; Test: 58.20% ; Test-EASY: 76.14% ; Test-HARD: 44.11%. LogiQA — Dev: 35.48% ; Test: 38.71%.",
            "baseline_performance": "RoBERTa-Large (baseline reported in paper): ReClor — Dev: 62.60% ; Test: 55.60% ; Test-EASY: 75.50% ; Test-HARD: 40.00%. LogiQA — Dev: 35.02% ; Test: 35.33%.",
            "improvement_over_baseline": "On ReClor, DAGN improves over RoBERTa-Large by +2.6 percentage points on Dev and +2.6 on Test; Test-EASY +0.64; Test-HARD +4.11, indicating a larger gain on harder questions. On LogiQA, DAGN improves over RoBERTa-Large by +0.46 (Dev) and +3.38 (Test).",
            "limitations_or_failures": "Paper notes that logical structures are often hidden and datasets lack annotated discourse/logic structure; performance on the HARD subset remains substantially lower than EASY (absolute Test-HARD 44.11%), indicating remaining difficulty. Method depends on explicit connectives / punctuation for EDU delimitation (PDTB-style), so cases lacking explicit discourse markers or with noisy segmentation may degrade performance. No detailed per-type failure cases given beyond lower HARD subset performance and general difficulty of extracting hidden logical structure.",
            "ablation_or_analysis": "Ablations (ReClor dev): removing/disabling components degrades performance — replacing EDUs with clause nodes or sentence nodes drops accuracy from 65.20% to 64.40%; collapsing edge types into a single type gives 64.80%; making the graph fully connected drops to 61.60%; removing the graph module entirely yields 64.00%. These show fine-grained EDU nodes and the specific discourse-linked edges are important for gains, and that graph reasoning contributes beyond an extra prediction module.",
            "uuid": "e3504.0",
            "source_info": {
                "paper_title": "DAGN: Discourse-Aware Graph Network for Logical Reasoning",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "DAGN (Aug)",
            "name_full": "DAGN (augmented graph features variant)",
            "brief_description": "A variant of DAGN that augments graph features (implementation details in paper) to further improve performance on logical reasoning QA.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DAGN (Aug) (with RoBERTa-Large backbone)",
            "model_description": "Same architecture as DAGN; a variant that augments the graph features (described as DAGN (Aug) in experiments). The paper does not provide extra architectural detail beyond being an augmented variant.",
            "model_size": null,
            "reasoning_task_name": "ReClor; LogiQA",
            "reasoning_task_description": "Same logical reasoning multiple-choice QA tasks as DAGN.",
            "method_or_intervention": "Augmentation of the graph features (paper reports an 'Aug' variant but does not fully specify augmentation details in main text; used as an experimental variant to test effect of augmented graph features).",
            "performance": "ReClor — Dev: 65.80% ; Test: 58.30% ; Test-EASY: 75.91% ; Test-HARD: 44.46%. LogiQA — Dev: 36.87% ; Test: 39.32%.",
            "baseline_performance": "RoBERTa-Large: ReClor Dev 62.60% / Test 55.60% ; LogiQA Dev 35.02% / Test 35.33%.",
            "improvement_over_baseline": "Marginal improvement over DAGN and RoBERTa-Large: ReClor Test +2.7 over RoBERTa-Large (58.30 vs 55.60); Test-HARD +4.46 over RoBERTa-Large (44.46 vs 40.00). LogiQA Test +3.99 over RoBERTa-Large (39.32 vs 35.33).",
            "limitations_or_failures": "Same general limitations as DAGN; paper does not detail what specific augmentations are applied or failure modes unique to this variant.",
            "ablation_or_analysis": "Reported alongside DAGN; improvements are small compared to base DAGN, suggesting modest benefit from the unspecified augmentation. Main ablation studies in paper focus on graph node/edge design and presence of graph module (see DAGN entry).",
            "uuid": "e3504.1",
            "source_info": {
                "paper_title": "DAGN: Discourse-Aware Graph Network for Logical Reasoning",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "RoBERTa-Large",
            "name_full": "RoBERTa-Large (backbone pre-trained language model)",
            "brief_description": "A large pre-trained transformer-based language model used as the backbone for DAGN to produce contextual token embeddings; in this paper it is fine-tuned within the end-to-end DAGN system and also reported as a baseline.",
            "citation_title": "RoBERTa: A robustly optimized bert pretraining approach",
            "mention_or_use": "use",
            "model_name": "RoBERTa-Large",
            "model_description": "A pre-trained Transformer (BERT-family) model; the paper states the RoBERTa-Large backbone used has 24 hidden layers and hidden size 1024 and is fine-tuned end-to-end as part of DAGN.",
            "model_size": null,
            "reasoning_task_name": "ReClor; LogiQA (baselines reported)",
            "reasoning_task_description": "Multiple-choice logical reasoning QA datasets used as benchmarks for strict logical reasoning.",
            "method_or_intervention": "Vanilla fine-tuning of RoBERTa-Large on task (baseline); in DAGN it is combined with the discourse-aware graph module to improve reasoning.",
            "performance": "Reported baseline performance (from tables): ReClor — Dev: 62.60% ; Test: 55.60% ; Test-EASY: 75.50% ; Test-HARD: 40.00%. LogiQA — Dev: 35.02% ; Test: 35.33%.",
            "baseline_performance": null,
            "improvement_over_baseline": "Not applicable (this is the baseline used for comparison).",
            "limitations_or_failures": "Baseline performance shows substantial gap on HARD subset (40.00% Test-HARD on ReClor) indicating RoBERTa-Large alone struggles with harder logical reasoning items; paper motivates discourse-based augmentation to address these shortcomings.",
            "ablation_or_analysis": "Used as backbone for DAGN; ablating the graph module shows that adding graph reasoning atop RoBERTa-Large yields improved performance (DAGN vs. RoBERTa-Large numbers in main results).",
            "uuid": "e3504.2",
            "source_info": {
                "paper_title": "DAGN: Discourse-Aware Graph Network for Logical Reasoning",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "BERT-Large",
            "name_full": "BERT-Large",
            "brief_description": "A large pre-trained bidirectional Transformer (BERT) model reported as a baseline in the paper's experiments on ReClor and LogiQA.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "BERT-Large",
            "model_description": "BERT-Large is a deep bidirectional Transformer pre-trained with masked language modeling and next sentence prediction; the paper lists reported baseline accuracy numbers but does not describe running or modifying BERT-Large within their method.",
            "model_size": null,
            "reasoning_task_name": "ReClor; LogiQA (baseline numbers reported)",
            "reasoning_task_description": "Multiple-choice logical reasoning QA datasets.",
            "method_or_intervention": "Vanilla fine-tuning as baseline (reported numbers in tables, taken from prior works / baselines).",
            "performance": "ReClor — Dev: 53.80% ; Test: 49.80% ; Test-EASY: 72.00% ; Test-HARD: 32.30%. LogiQA — Dev: 34.10% ; Test: 31.03%.",
            "baseline_performance": null,
            "improvement_over_baseline": "DAGN substantially outperforms BERT-Large on both datasets (e.g., ReClor Test 58.20% vs BERT-Large 49.80%).",
            "limitations_or_failures": "As a baseline, BERT-Large shows notably lower performance on HARD items (e.g., 32.30% on ReClor Test-HARD) indicating limited capability on strict logical reasoning compared to larger models / discourse-augmented methods.",
            "ablation_or_analysis": "No ablations specific to BERT-Large in this paper; used only as a baseline comparator in tables.",
            "uuid": "e3504.3",
            "source_info": {
                "paper_title": "DAGN: Discourse-Aware Graph Network for Logical Reasoning",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "XLNet-Large",
            "name_full": "XLNet-Large",
            "brief_description": "A large pre-trained Transformer model based on permutation language modeling used as a baseline in the ReClor comparisons.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "XLNet-Large",
            "model_description": "XLNet-Large is a pre-trained Transformer using permutation-based language modeling; the paper reports baseline accuracy numbers from prior work / leaderboards.",
            "model_size": null,
            "reasoning_task_name": "ReClor (baseline numbers reported)",
            "reasoning_task_description": "Multiple-choice logical reasoning QA (ReClor).",
            "method_or_intervention": "Vanilla fine-tuning as baseline (reported numbers).",
            "performance": "ReClor — Dev: 62.00% ; Test: 56.00% ; Test-EASY: 75.70% ; Test-HARD: 40.50%.",
            "baseline_performance": null,
            "improvement_over_baseline": "DAGN improves over XLNet-Large on Dev and Test (e.g., Dev 65.20% vs 62.00%; Test 58.20% vs 56.00%) and shows a larger gain on HARD subset (DAGN Test-HARD 44.11% vs XLNet-Large 40.50%).",
            "limitations_or_failures": "As a baseline, XLNet-Large performs similarly to RoBERTa-Large and demonstrates limited abilities on HARD subset; no further failure analysis provided for XLNet in the paper.",
            "ablation_or_analysis": "No XLNet-specific ablations in this paper; included solely as a comparative baseline.",
            "uuid": "e3504.4",
            "source_info": {
                "paper_title": "DAGN: Discourse-Aware Graph Network for Logical Reasoning",
                "publication_date_yy_mm": "2021-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ReClor: A reading comprehension dataset requiring logical reasoning",
            "rating": 2
        },
        {
            "paper_title": "LogiQA: A challenge dataset for machine reading comprehension with logical reasoning",
            "rating": 2
        },
        {
            "paper_title": "The penn discourse treebank 2.0",
            "rating": 2
        },
        {
            "paper_title": "RoBERTa: A robustly optimized bert pretraining approach",
            "rating": 1
        },
        {
            "paper_title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "rating": 1
        }
    ],
    "cost": 0.012097499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>DAGN: Discourse-Aware Graph Network for Logical Reasoning</h1>
<p>Yinya Huang ${ }^{1 *}$ Meng Fang ${ }^{2}$ Yu Cao ${ }^{3}$ Liwei Wang ${ }^{4}$ Xiaodan Liang ${ }^{1 \dagger}$<br>${ }^{1}$ Shenzhen Campus of Sun Yat-sen University<br>${ }^{2}$ Tencent Robotics X<br>${ }^{3}$ School of Computer Science, The University of Sydney<br>${ }^{4}$ The Chinese University of Hong Kong<br>yinya.huang@hotmail, mfang@tencent.com, ycao8647@uni.sydney.edu.au,<br>lwwang@cse.cuhk.edu.hk, xdliang328@gmail.com</p>
<h4>Abstract</h4>
<p>Recent QA with logical reasoning questions requires passage-level relations among the sentences. However, current approaches still focus on sentence-level relations interacting among tokens. In this work, we explore aggregating passage-level clues for solving logical reasoning QA by using discourse-based information. We propose a discourse-aware graph network (DAGN) that reasons relying on the discourse structure of the texts. The model encodes discourse information as a graph with elementary discourse units (EDUs) and discourse relations, and learns the discourseaware features via a graph network for downstream QA tasks. Experiments are conducted on two logical reasoning QA datasets, ReClor and LogiQA, and our proposed DAGN achieves competitive results. The source code is available at https://github.com/EleanorH/DAGN.</p>
<h2>1 Introduction</h2>
<p>A variety of QA datasets have promoted the development of reading comprehensions, for instance, SQuAD (Rajpurkar et al., 2016), HotpotQA (Yang et al., 2018), DROP (Dua et al., 2019), and so on. Recently, QA datasets with more complicated reasoning types, i.e., logical reasoning, are also introduced, such as ReClor (Yu et al., 2020) and LogiQA (Liu et al., 2020). The logical questions are taken from standardized exams such as GMAT and LSAT, and require QA models to read complicated argument passages and identify logical relationships therein. For example, selecting a correct assumption that supports an argument, or finding out a claim that weakens an argument in a passage. Such logical reasoning is beyond the capability of most of the previous QA models which focus on reasoning with entities or numerical keywords.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>A main challenge for the QA models is to uncover the logical structures under passages, such as identifying claims or hypotheses, or pointing out flaws in arguments. To achieve this, the QA models should first be aware of logical units, which can be sentences or clauses or other meaningful text spans, then identify the logical relationships between the units. However, the logical structures are usually hidden and difficult to be extracted, and most datasets do not provide such logical structure annotations.</p>
<p>An intuitive idea for unwrapping such logical information is using discourse relations. For instance, as a conjunction, "because" indicates a causal relationship, whereas "if" indicates a hypothetical relationship. However, such discourse-based information is seldom considered in logical reasoning tasks. Modeling logical structures is still lacking in logical reasoning tasks, while current opened methods use contextual pre-trained models (Yu et al., 2020). Besides, previous graph-based methods (Ran et al., 2019; Chen et al., 2020a) that construct entity-based graphs are not suitable for logical reasoning tasks because of different reasoning units.</p>
<p>In this paper, we propose a new approach to solve logical reasoning QA tasks by incorporating discourse-based information. First, we construct discourse structures. We use discourse relations from the Penn Discourse TreeBank 2.0 (PDTB 2.0) (Prasad et al., 2008) as delimiters to split texts into elementary discourse units (EDUs). A logic graph is constructed in which EDUs are nodes and discourse relations are edges. Then, we propose a Discourse-Aware Graph Network (DAGN) for learning high-level discourse features to represent passages. The discourse features are incorporated with the contextual token features from pre-trained language models. With the enhanced features, DAGN predicts answers to logical questions. Our experiments show that DAGN surpasses current opened methods on two recent logical rea-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The architecture of our proposed method with an example below.
soning QA datasets, ReClor and LogiQA.
Our main contributions are three-fold:</p>
<ul>
<li>We propose to construct logic graphs from texts by using discourse relations as edges and elementary discourse units as nodes.</li>
<li>We obtain discourse features via graph neural networks to facilitate logical reasoning in QA models.</li>
<li>We show the effectiveness of using logic graph and feature enhancement by noticeable improvements on two datasets, ReClor and LogiQA.</li>
</ul>
<h2>2 Method</h2>
<p>Our intuition is to explicitly use discourse-based information to mimic the human reasoning process for logical reasoning questions. The questions are in multiple choices format, which means given a triplet (context, question, answer options), models answer the question by selecting the correct answer option. Our framework is shown in Figure 1. We first construct a discourse-based logic graph from the raw text. Then we conduct reasoning via graph networks to learn and update the discoursebased features, which are incorporated with the contextual token embeddings for downstream answer prediction.</p>
<h3>2.1 Graph Construction</h3>
<p>Our discourse-based logic graph is constructed via two steps: delimiting text into elementary discourse units (EDUs) and forming the graph using their relations as edges, as illustrated in Figure 1(1).</p>
<p>Discourse Units Delimitation It is studied that clause-like text spans delimited by discourse relations can be discourse units that reveal the rhetorical structure of texts (Mann and Thompson, 1988; Prasad et al., 2008). We further observe that such discourse units are essential units in logical reasoning, such as being assumptions or opinions. As the example shown in Figure 1, the "while" in the context indicates a comparison between the attributes of "pure analog system" and that of "digital systems". The "because" in the option provides evidence "error cannot occur in the emission of digital signals" to the claim "digital systems are the best information systems".</p>
<p>We use PDTB 2.0 (Prasad et al., 2008) to help drawing discourse relations. PDTB 2.0 contains discourse relations that are manually annotated on the 1 million Wall Street Journal (WSJ) corpus and are broadly characterized into "Explicit" and "Implicit" connectives. The former apparently presents in sentences such as discourse adverbial "instead" or subordinating conjunction "because", whereas the latter are inferred by annotators between successive pairs of text spans split by punctuation marks</p>
<p>such as "." or ";". We simply take all the "Explicit" connectives as well as common punctuation marks to form our discourse delimiter library (details are given in Appendix A), with which we delimit the texts into EDUs. For each data sample, we segment the context and options, ignoring the question since the question usually does not carry logical content.</p>
<p>Discourse Graph Construction We define the discourse-based graphs with EDUs as nodes, the "Explicit" connectives as well as the punctuation marks as two types of edges. We assume that each connective or punctuation mark connects the EDUs before and after it. For example, the option sentence in Figure 1 is delimited into two EDUs, $\mathrm{EDU}<em 8="8">{7}=$ "digital systems are the best information systems" and $\mathrm{EDU}</em>}=$ "error cannot occur in the emission of digital signals" by the connective $r=$ "because". Then the returned triplets are $\left(\mathrm{EDU<em 8="8">{7}, r, \mathrm{EDU}</em>}\right)$ and $\left(\mathrm{EDU<em 7="7">{8}, r, \mathrm{EDU}</em>\right)$.}\right)$. For each data sample with the context and multiple answer options, we separately construct graphs corresponding to each option, with EDUs in the same context and every single option. The graph for the single option $k$ is denoted by $\mathcal{G}^{k}=\left(\mathcal{V}^{k}, \mathcal{E}^{k</p>
<h3>2.2 Discourse-Aware Graph Network</h3>
<p>We present the Discourse-Aware Graph Network (DAGN) that uses the constructed graph to exploit discourse-based information for answering logical questions. It consists of three main components: an EDU encoding module, a graph reasoning module, and an answer prediction module. The former two are demonstrated in Figure 1(2), whereas the final component is in Figure 1(3).</p>
<p>EDU Encoding An EDU span embedding is obtained from its token embeddings. There are two steps. First, similar to previous works (Yu et al., 2020; Liu et al., 2020), we encode such input sequence "<s> context </s> question || option </s>" into contextual token embeddings with pre-trained language models, where <s> and </s> are the special tokens for RoBERTa (Liu et al., 2019) model, and || denotes concatenation. Second, given the token embedding sequence $\left{\mathbf{t}<em 2="2">{1}, \mathbf{t}</em>}, \ldots, \mathbf{t<em n="n">{L}\right}$, the $n$-th EDU embedding is obtained by $\mathbf{e}</em>}=\sum_{l \in S_{n}} \mathbf{t<em n="n">{l}$, where $S</em>$ is the set of token indices belonging to $n$-th EDU.</p>
<p>Graph Reasoning After EDU encoding, DAGN performs reasoning over the discourse graph. Inspired by previous graph-based models (Ran et al.,
2019; Chen et al., 2020a), we also learn graph node representations to obtain higher-level features. However, we consider different graph construction and encoding. Specifically, let $\mathcal{G}^{k}=\left(\mathcal{V}^{k}, \mathcal{E}^{k}\right)$ denote a graph corresponding to the $k$-th option in answer choices. For each node $v_{i} \in \mathbf{V}$, the node embedding $\mathbf{v}<em i="i">{i}$ is initialized with the corresponding EDU embedding $\mathbf{e}</em>} . \mathcal{N<em j="j">{i}=\left{j \mid\left(v</em>$ indicates graph edges corresponding to punctuation marks.}, v_{i}\right) \in \mathcal{E}^{k}\right}$ indicates the neighbors of node $v_{i}$. $\mathbf{W}^{r_{j i}}$ is the adjacency matrix for one of the two edge types, where $r_{E}$ indicates graph edges corresponding to the explicit connectives, and $r_{I</p>
<p>The model first calculates weight $\alpha_{i}$ for each node with a linear transformation and a sigmoid function $\alpha_{i}=\sigma\left(\mathbf{W}^{\alpha}\left(\mathbf{v}_{i}\right)+b^{\alpha}\right)$, then conducts message propagation with the weights:</p>
<p>$$
\tilde{\mathbf{v}}<em i="i">{i}=\frac{1}{\left|\mathcal{N}</em>}\right|}\left(\sum_{j \in \mathcal{N<em j="j">{i}} \alpha</em>} \mathbf{W}^{r_{j i}} \mathbf{v<em i="i" j="j">{j}\right), r</em>\right}
$$} \in\left{r_{E}, r_{I</p>
<p>where $\tilde{\mathbf{v}}<em i="i">{i}$ is the message representation of node $v</em>}$. $\alpha_{j}$ and $\mathbf{v<em j="j">{j}$ are the weight and the node embedding of $v</em>$ respectively.</p>
<p>After the message propagation, the node representations are updated with the initial node embeddings and the message representations by</p>
<p>$$
\mathbf{v}<em i="i">{i}^{\prime}=\operatorname{ReLU}\left(\mathbf{W}^{u} \mathbf{v}</em>\right)
$$}+\tilde{\mathbf{v}}_{i}+\mathbf{b}^{u</p>
<p>where $\mathbf{W}^{u}$ and $\mathbf{b}^{u}$ are weight and bias respectively. The updated node representations $\mathbf{v}<em l="l">{i}^{\prime}$ will be used to enhance the contextual token embedding via summation in corresponding positions. Thus $\mathbf{t}</em>}^{\prime}=$ $\mathbf{t<em n="n">{l}+\mathbf{v}</em>$ is the corresponding token indices set for $n$-th EDU.}^{\prime}$, where $l \in S_{n}$ and $S_{n</p>
<p>Answer Prediction The probabilities of options are obtained by feeding the discourse-enhanced token embeddings into the answer prediction module. The model is end-to-end trained using cross entropy loss. Specifically, the embedding sequence first goes through a layer normalization (Ba et al., 2016), then a bidirectional GRU (Cho et al., 2014). The output embeddings are then added to the input ones as the residual structure (He et al., 2016). We finally obtain the encoded sequence after another layer normalization on the added embeddings.</p>
<p>We then merge the high-level discourse features and the low-level token features. Specifically, the variant-length encoded context sequence, question-and-option sequence are pooled via weighted summation wherein the weights are softmax results of</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">Dev</th>
<th style="text-align: center;">Test</th>
<th style="text-align: center;">Test-E</th>
<th style="text-align: center;">Test-H</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BERT-Large</td>
<td style="text-align: center;">53.80</td>
<td style="text-align: center;">49.80</td>
<td style="text-align: center;">72.00</td>
<td style="text-align: center;">32.30</td>
</tr>
<tr>
<td style="text-align: left;">XLNet-Large</td>
<td style="text-align: center;">62.00</td>
<td style="text-align: center;">56.00</td>
<td style="text-align: center;">75.70</td>
<td style="text-align: center;">40.50</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa-Large</td>
<td style="text-align: center;">62.60</td>
<td style="text-align: center;">55.60</td>
<td style="text-align: center;">75.50</td>
<td style="text-align: center;">40.00</td>
</tr>
<tr>
<td style="text-align: left;">DAGN</td>
<td style="text-align: center;">$\mathbf{6 5 . 2 0}$</td>
<td style="text-align: center;">$\mathbf{5 8 . 2 0}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 1 4}$</td>
<td style="text-align: center;">$\mathbf{4 4 . 1 1}$</td>
</tr>
<tr>
<td style="text-align: left;">DAGN (Aug)</td>
<td style="text-align: center;">$\mathbf{6 5 . 8 0}$</td>
<td style="text-align: center;">$\mathbf{5 8 . 3 0}$</td>
<td style="text-align: center;">$\mathbf{7 5 . 9 1}$</td>
<td style="text-align: center;">$\mathbf{4 4 . 4 6}$</td>
</tr>
</tbody>
</table>
<p>${ }^{<em>}$ The results are taken from the ReClor paper.
${ }^{</em>}$ DAGN ranks the 1st on the public ReClor leaderboard'until 17th Nov., 2020 before submitting it to NAACL. Until now, we find that several better results appeared in the leaderboard and they are not opened.</p>
<p>Table 1: Experimental results (accuracy \%) of DAGN compared with baseline models on ReClor dataset. Test-E $=$ Test-EASY, Test-H $=$ Test-HARD.
a linear transformation of the sequence, resulting in single feature vectors separately. We concatenate them with " $<s>$ " embedding from the backbone pre-trained model, and feed the new vector into a two-layer perceptron with a GELU activation (Hendrycks and Gimpel, 2016) to get the output features for classification.</p>
<h2>3 Experiments</h2>
<p>We evaluate the performance of DAGN on two logical reasoning datasets, ReClor (Yu et al., 2020) and LogiQA (Liu et al., 2020), and conduct ablation study on graph construction and graph network. The implementation details are shown in Appendix B.</p>
<h3>3.1 Datasets</h3>
<p>ReClor contains 6,138 questions modified from standardized tests such as GMAT and LSAT, which are split into train / dev / test sets with 4,638 / 500 / 1,000 samples respectively. The training set and the development set are available. The test set is blind and hold-out, and split into an EASY subset and a HARD subset according to the performance of BERT-base model (Devlin et al., 2019). The test results are obtained by submitting the test predictions to the leaderboard. LogiQA consists of 8,678 questions that are collected from National Civil Servants Examinations of China and manually translated into English by professionals. The dataset is randomly split into train / dev / test sets with 7,376 / 651 / 651 samples respectively. Both datasets contain multiple logical reasoning types.</p>
<h3>3.2 Results</h3>
<p>The experimental results are shown in Tables 1 and 2. Since there is no public method for both datasets, we compare DAGN with the baseline</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">Dev</th>
<th style="text-align: center;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BERT-Large</td>
<td style="text-align: center;">34.10</td>
<td style="text-align: center;">31.03</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa-Large</td>
<td style="text-align: center;">35.02</td>
<td style="text-align: center;">35.33</td>
</tr>
<tr>
<td style="text-align: left;">DAGN</td>
<td style="text-align: center;">$\mathbf{3 5 . 4 8}$</td>
<td style="text-align: center;">$\mathbf{3 8 . 7 1}$</td>
</tr>
<tr>
<td style="text-align: left;">DAGN (Aug)</td>
<td style="text-align: center;">$\mathbf{3 6 . 8 7}$</td>
<td style="text-align: center;">$\mathbf{3 9 . 3 2}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Experimental results (accuracy \%) of DAGN compared with baseline models on LogiQA dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">Dev</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DAGN</td>
<td style="text-align: center;">$\mathbf{6 5 . 2 0}$</td>
</tr>
<tr>
<td style="text-align: left;">ablation on nodes</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">DAGN - clause nodes</td>
<td style="text-align: center;">64.40</td>
</tr>
<tr>
<td style="text-align: left;">DAGN - sentence nodes</td>
<td style="text-align: center;">64.40</td>
</tr>
<tr>
<td style="text-align: left;">ablation on edges</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">DAGN - single edge type</td>
<td style="text-align: center;">64.80</td>
</tr>
<tr>
<td style="text-align: left;">DAGN - fully connected edges</td>
<td style="text-align: center;">61.60</td>
</tr>
<tr>
<td style="text-align: left;">ablation on graph reasoning</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">DAGN w/o graph module</td>
<td style="text-align: center;">64.00</td>
</tr>
</tbody>
</table>
<p>Table 3: Ablation study results (accurcy \%) on ReClor development set.
models. As for DAGN, we fine-tune RoBERTaLarge as the backbone. DAGN (Aug) is a variant that augments the graph features.</p>
<p>DAGN reaches $58.20 \%$ of test accuracy on ReClor. DAGN (Aug) reaches $58.30 \%$, therein $75.91 \%$ on EASY subset, and $44.46 \%$ on HARD subset. Compared with RoBERTa-Large, the improvement on the HARD subset is remarkably $4.46 \%$. This indicates that the incorporated discourse-based information supplements the shortcoming of the baseline model, and that the discourse features are beneficial for such logical reasoning. Besides, DAGN and DAGN (Aug) also outperform the baseline models on LogiQA, especially showing $4.01 \%$ improvement over RoBERTaLarge on the test set.</p>
<h3>3.3 Ablation Study</h3>
<p>We conduct ablation study on graph construction details as well as the graph reasoning module. The results are reported in Table 3.</p>
<p>Varied Graph Nodes We first use clauses or sentences in substitution for EDUs as graph nodes. For clause nodes, we simply remove "Explicit" connectives during discourse unit delimitation. So that the texts are just delimited by punctuation marks. For sentence nodes, we further reduce the delimiter library to solely period ("."). Using the modified graphs with clause nodes or coarser sentence nodes, the accuracy of DAGN drops to $64.40 \%$. This indicates that clause or sentence nodes carry</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>less discourse information and act poorly as logical reasoning units.</p>
<p>Varied Graph Edges We make two changes of the edges: (1) modifying the edge type, (2) modifying the edge linking. For edge type, all edges are regarded as a single type. For edge linking, we ignore discourse relations and connect every pair of nodes, turning the graph into fully-connected. The resulting accuracies drop to $64.80 \%$ and $61.60 \%$ respectively. It is proved that in the graph we built, edges link EDUs in reasonable manners, which properly indicates the logical relations.</p>
<p>Ablation on Graph Reasoning We remove the graph module from DAGN and give a comparison. This model solely contains an extra prediction module than the baseline. The performance on ReClor dev set is between the baseline model and DAGN. Therefore, despite the prediction module benefits the accuracy, the lack of graph reasoning leads to the absence of discourse features and degenerates the performance. It demonstrates the necessity of discourse-based structure in logical reasoning.</p>
<h2>4 Related Works</h2>
<p>Recent datasets for reading comprehension tend to be more complicated and require models' capability of reasoning. For instance, HotpotQA (Yang et al., 2018), WikiHop (Welbl et al., 2018), OpenBookQA (Mihaylov et al., 2018), and MultiRC (Khashabi et al., 2018) require the models to have multi-hop reasoning. DROP (Dua et al., 2019) and MA-TACO (Zhou et al., 2019) need the models to have numerical reasoning. WIQA (Tandon et al., 2019) and CosmosQA (Huang et al., 2019) require causal reasoning that the models can understand the counterfactual hypothesis or find out the causeeffect relationships in events. However, the logical reasoning datasets (Yu et al., 2020; Liu et al., 2020) require the models to have the logical reasoning capability of uncovering the inner logic of texts.</p>
<p>Deep neural networks are used for reasoningdriven RC. Evidence-based methods (Madaan et al., 2020; Huang et al., 2020; Rajagopal et al., 2020) generate explainable evidence from a given context as the backup of reasoning. Graph-based methods (Qiu et al., 2019; De Cao et al., 2019; Cao et al., 2019; Ran et al., 2019; Chen et al., 2020b; Xu et al., 2020b; Zhang et al., 2020) explicitly model the reasoning process with constructed graphs, then learn and update features through message passing
based on graphs. There are also other methods such as neuro-symbolic models (Saha et al., 2021) and adversarial training (Pereira et al., 2020). Our paper uses a graph-based model. However, for uncovering logical relations, graph nodes and edges are customized with discourse information.</p>
<p>Discourse information provides a high-level understanding of texts and hence is beneficial for many of the natural language tasks, for instance, text summarization (Cohan et al., 2018; Joty et al., 2019; Xu et al., 2020a; Feng et al., 2020), neural machine translation (Voita et al., 2018), and coherent text generation (Wang et al., 2020; Bosselut et al., 2018). There are also discourse-based applications for reading comprehension. DISCERN (Gao et al., 2020) segments texts into EDUs and learns interactive EDU features. Mihaylov and Frank (2019) provide additional discourse-based annotations and encodes them with discourseaware self-attention models. Unlike previous works, DAGN first uses discourse relations as graph edges connecting EDUs for texts, then learns the discourse features via message passing with graph neural networks.</p>
<h2>5 Conclusion</h2>
<p>In this paper, we introduce a Discourse-Aware Graph Network (DAGN) to addressing logical reasoning QA tasks. We first treat elementary discourse units (EDUs) that are split by discourse relations as basic reasoning units. We then build discourse-based logic graphs with EDUs as nodes and discourse relations as edges. DAGN then learns the discourse-based features and enhances them with contextual token embeddings. DAGN reaches competitive performances on two recent logical reasoning datasets ReClor and LogiQA.</p>
<h2>Acknowledgements</h2>
<p>The authors would like to thank Wenge Liu, Jianheng Tang, Guanlin Li and Wei Wang for their support and useful discussions. This work was supported in part by National Natural Science Foundation of China (NSFC) under Grant No.U19A2073 and No.61976233, Guangdong Province Basic and Applied Basic Research (Regional Joint Fund-Key) Grant No.2019B1515120039, Shenzhen Basic Research Project (Project No. JCYJ20190807154211365), Zhijiang Lab's Open Fund (No. 2020AA3AB14) and CSIG Young Fellow Support Fund.</p>
<h2>References</h2>
<p>Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. stat, 1050:21.</p>
<p>Antoine Bosselut, Asli Celikyilmaz, Xiaodong He, Jianfeng Gao, Po-Sen Huang, and Yejin Choi. 2018. Discourse-aware neural rewards for coherent text generation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 173184 .</p>
<p>Yu Cao, Meng Fang, and Dacheng Tao. 2019. Bag: Bi-directional attention entity graph convolutional network for multi-hop reasoning question answering. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 357-362.</p>
<p>Kunlong Chen, Weidi Xu, Xingyi Cheng, Zou Xiaochuan, Yuyu Zhang, Le Song, Taifeng Wang, Yuan Qi, and Wei Chu. 2020a. Question directed graph attention network for numerical reasoning over text. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6759-6768.</p>
<p>Kunlong Chen, Weidi Xu, Xingyi Cheng, Zou Xiaochuan, Yuyu Zhang, Le Song, Taifeng Wang, Yuan Qi, and Wei Chu. 2020b. Question directed graph attention network for numerical reasoning over text. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6759-6768, Online. Association for Computational Linguistics.</p>
<p>Kyunghyun Cho, Bart van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder-decoder approaches. In Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 103-111.</p>
<p>Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. 2018. A discourse-aware attention model for abstractive summarization of long documents. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 615-621, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Nicola De Cao, Wilker Aziz, and Ivan Titov. 2019. Question answering by reasoning across documents with graph convolutional networks. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2306-2317.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages $4171-4186$.</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2368-2378.</p>
<p>Xiachong Feng, Xiaocheng Feng, Bing Qin, Xinwei Geng, and Ting Liu. 2020. Dialogue discourse-aware graph convolutional networks for abstractive meeting summarization. arXiv preprint arXiv:2012.03502.</p>
<p>Yifan Gao, Chien-Sheng Wu, Jingjing Li, Shafiq Joty, Steven CH Hoi, Caiming Xiong, Irwin King, and Michael Lyu. 2020. Discern: Discourse-aware entailment reasoning network for conversational machine reading. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2439-2449.</p>
<p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778 .</p>
<p>Dan Hendrycks and Kevin Gimpel. 2016. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415.</p>
<p>Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019. Cosmos qa: Machine reading comprehension with contextual commonsense reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2391-2401.</p>
<p>Yinya Huang, Meng Fang, Xunlin Zhan, Qingxing Cao, Xiaodan Liang, and Liang Lin. 2020. Remnet: Recursive erasure memory network for commonsense evidence refinement. arXiv preprint arXiv:2012.13185.</p>
<p>Shafiq Joty, Giuseppe Carenini, Raymond Ng, and Gabriel Murray. 2019. Discourse analysis and its applications. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 12-17, Florence, Italy. Association for Computational Linguistics.</p>
<p>Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. 2018. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 252-262.</p>
<p>Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In $I C L R$ (Poster).</p>
<p>Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. 2020. Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. IJCAI 2020.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Aman Madaan, Dheeraj Rajagopal, Yiming Yang, Abhilasha Ravichander, Eduard Hovy, and Shrimai Prabhumoye. 2020. Eigen: Event influence generation using pre-trained language models. arXiv preprint arXiv:2010.11764.</p>
<p>William C Mann and Sandra A Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3):243-281.</p>
<p>Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct electricity? a new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2381-2391.</p>
<p>Todor Mihaylov and Anette Frank. 2019. Discourseaware semantic self-attention for narrative reading comprehension. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 2541-2552, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Lis Pereira, Xiaodong Liu, Fei Cheng, Masayuki Asahara, and Ichiro Kobayashi. 2020. Adversarial training for commonsense inference. In Proceedings of the 5th Workshop on Representation Learning for NLP, pages 55-60, Online. Association for Computational Linguistics.</p>
<p>Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind K Joshi, and Bonnie L Webber. 2008. The penn discourse treebank 2.0. In LREC. Citeseer.</p>
<p>Lin Qiu, Yunxuan Xiao, Yanru Qu, Hao Zhou, Lei Li, Weinan Zhang, and Yong Yu. 2019. Dynamically fused graph network for multi-hop reasoning. In Proceedings of the 57th Annual Meeting of the</p>
<p>Association for Computational Linguistics, pages 6140-6150, Florence, Italy. Association for Computational Linguistics.</p>
<p>Dheeraj Rajagopal, Niket Tandon, Peter Clark, Bhavana Dalvi, and Eduard Hovy. 2020. What-if i ask you to explain: Explaining the effects of perturbations in procedural text. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pages 3345-3355.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392.</p>
<p>Qiu Ran, Yankai Lin, Peng Li, Jie Zhou, and Zhiyuan Liu. 2019. Numnet: Machine reading comprehension with numerical reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2474-2484.</p>
<p>Amrita Saha, Shafiq Joty, and Steven CH Hoi. 2021. Weakly supervised neuro-symbolic module networks for numerical reasoning. arXiv preprint arXiv:2101.11802.</p>
<p>Niket Tandon, Bhavana Dalvi, Keisuke Sakaguchi, Peter Clark, and Antoine Bosselut. 2019. Wiqa: A dataset for "what if..." reasoning over procedural text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 6078-6087.</p>
<p>Elena Voita, Pavel Serdyukov, Rico Sennrich, and Ivan Titov. 2018. Context-aware neural machine translation learns anaphora resolution. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1264-1274.</p>
<p>Wei Wang, Piji Li, and Hai-Tao Zheng. 2020. Consistency and coherency enhanced story generation. arXiv preprint arXiv:2010.08822.</p>
<p>Johannes Welbl, Pontus Stenetop, and Sebastian Riedel. 2018. Constructing datasets for multi-hop reading comprehension across documents. Transactions of the Association for Computational Linguistics, 6:287-302.</p>
<p>Jiacheng Xu, Zhe Gan, Yu Cheng, and Jingjing Liu. 2020a. Discourse-aware neural extractive text summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5021-5031, Online. Association for Computational Linguistics.</p>
<p>Yunqiu Xu, Meng Fang, Ling Chen, Yali Du, Joey Tianyi Zhou, and Chengqi Zhang. 2020b. Deep</p>
<p>reinforcement learning with stacked hierarchical attention for text-based games. In Advances in Neural Information Processing Systems, volume 33, pages 16495-16507. Curran Associates, Inc.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369-2380.</p>
<p>Weihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng. 2020. Reclor: A reading comprehension dataset requiring logical reasoning. In ICLR 2020 : Eighth International Conference on Learning Representations.</p>
<p>Jipeng Zhang, Lei Wang, Roy Ka-Wei Lee, Yi Bin, Yan Wang, Jie Shao, and Ee-Peng Lim. 2020. Graph-totree learning for solving math word problems. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 39283937.</p>
<p>Ben Zhou, Daniel Khashabi, Qiang Ning, and Dan Roth. 2019. "going on a vacation" takes longer than "going for a walk": A study of temporal commonsense understanding. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3363-3369, Hong Kong, China. Association for Computational Linguistics.</p>
<h2>A Discourse Delimiter Library</h2>
<p>Our discourse delimiter library consists of two parts, the "Explicit" connectives annotated in Penn Discourse TreeBank 2.0 (DPTB 2.0) (Prasad et al., 2008), as well as a set of punctuation marks. The overall discourse delimiters used in our method are presented in Table 4.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Explicit Connectives</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">'once', 'although', 'though', 'but', 'because', 'nevertheless', 'before', 'for example', 'until', 'if', 'previously', 'when', 'and', 'so', 'then', 'while', 'as long as', 'however', 'also', 'after', 'separately', 'still', 'so that', 'or', 'moreover', 'in addition', 'instead', 'on the other hand', 'as', 'for instance', 'nonetheless', 'unless', 'meanwhile', 'yet', 'since', 'rather', 'in fact', 'indeed', 'later', 'ultimately', 'as a result', 'either or', 'therefore', 'in turn', 'thus', 'in particular', 'further', 'afterward', 'next', 'similarly', 'besides', 'if and when', 'nor', 'alternatively', 'whereas', 'overall', 'by comparison', 'till', 'in contrast', 'finally', 'otherwise', 'as if', 'thereby', 'now that', 'before and after', 'additionally', 'meantime', 'by contrast', 'if then', 'likewise', 'in the end', 'regardless', 'thereafter', 'earlier', 'in other words', 'as</td>
</tr>
<tr>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">soon as', 'except', 'in short', 'neither nor', 'furthermore', 'lest', 'as though', 'specifically', 'conversely', 'consequently', 'as well', 'much as', 'plus', 'and', 'hence', 'by then', 'accordingly', 'on the contrary', 'simultaneously', 'for', 'in sum', 'when and if', 'insofar as', 'else', 'as an alternative', 'on the one hand on the other hand'</td>
</tr>
<tr>
<td style="text-align: center;">Punctuation Marks</td>
</tr>
</tbody>
</table>
<p>Table 4: The discourse delimiter library in our implementation.</p>
<h2>B Implementation Details</h2>
<p>We fine-tune RoBERTa-Large (Liu et al., 2019) as the backbone pre-trained language model for DGAN, which contains 24 hidden layers with hidden size 1024. The overall model is end-to-end trained and updated by Adam (Kingma and Ba, 2015) optimizer with an overall learning rate of 5e6 and a weight decay of 0.01 . The overall dropout rate is 0.1 . The maximum sequence length is 256 . We tune the model on the dev set to obtain the best iteration steps of graph reasoning, which is 2 for ReClor data, and 3 for LogiQA data. The model is trained for 10 epochs with a batch size of 16 on Nvidia Tesla V100 GPU.</p>
<p>For the answer prediction module, the hidden size of GRU is the same as the token embeddings in the pre-trained language model, which is 1024. The two-layer perceptron first projects the concatenated vectors with a hidden size of $1024 \times 3$ to 1024 , then project 1024 to 1.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://bit.ly/2UOQfaS&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
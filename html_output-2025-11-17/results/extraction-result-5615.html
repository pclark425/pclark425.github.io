<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5615 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5615</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5615</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-114.html">extraction-schema-114</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <p><strong>Paper ID:</strong> paper-ca545ceefe7c3535ffea0fbcc1cc86bddced3767</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ca545ceefe7c3535ffea0fbcc1cc86bddced3767" target="_blank">Evaluating Mixed-initiative Conversational Search Systems via User Simulation</a></p>
                <p><strong>Paper Venue:</strong> Web Search and Data Mining</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes a conversational User Simulator, called USi, for automatic evaluation of conversational search systems, capable of automatically answering clarifying questions about the topic throughout the search session, and shows that responses generated by USi are both inline with the underlying information need and comparable to human-generated answers.</p>
                <p><strong>Paper Abstract:</strong> Clarifying the underlying user information need by asking clarifying questions is an important feature of modern conversational search system. However, evaluation of such systems through answering prompted clarifying questions requires significant human effort, which can be time-consuming and expensive. In this paper, we propose a conversational User Simulator, called USi, for automatic evaluation of such conversational search systems. Given a description of an information need, USi is capable of automatically answering clarifying questions about the topic throughout the search session. Through a set of experiments, including automated natural language generation metrics and crowdsourcing studies, we show that responses generated by USi are both inline with the underlying information need and comparable to human-generated answers. Moreover, we make the first steps towards multi-turn interactions, where conversational search systems asks multiple questions to the (simulated) user with a goal of clarifying the user need. To this end, we expand on currently available datasets for studying clarifying questions, i.e., Qulac and ClariQ, by performing a crowdsourcing-based multi-turn data acquisition. We show that our generative, GPT2-based model, is capable of providing accurate and natural answers to unseen clarifying questions in the single-turn setting and discuss capabilities of our model in the multi-turn setting. We provide the code, data, and the pre-trained model to be used for further research on the topic.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5615.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5615.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>USi (GPT-2)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>USi: GPT-2-based User Simulator for Conversational Search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A semantically-controlled user simulator (USi) built by fine-tuning GPT-2 to generate single- and multi-turn natural-language answers to clarifying questions, conditioned on an explicit information-need (facet) and conversation history, for offline evaluation of mixed-initiative conversational search systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (DoubleHead / history-aware fine-tuned variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Auto-regressive transformer language model (GPT-2) pre-trained on a large corpus of web text (~8M web pages). The paper uses a DoubleHead GPT-2 variant (language modelling + binary classification head) and a history-aware input formulation; fine-tuned on Qulac/ClariQ single-turn data and a crowdsourced multi-turn dataset. Inference uses temperature-controlled stochastic sampling with top-p (nucleus) and top-k filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Conversational Information Retrieval / Mixed-initiative Conversational Search (IR)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Generate natural-language answers to system-prompted clarifying questions given an explicit information need and initial query (single-turn), and track conversation history to answer multi-turn clarifying questions; used to simulate real users for offline evaluation of conversational search systems and to measure impact of answers on document retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Automated NLG metrics (BLEU-1/2/3, ROUGE_L), semantic similarity metrics (SkipThoughtCS, EmbeddingAvgCS), human pairwise judgments for naturalness and usefulness (crowdsourcing pairwise comparisons), document retrieval metrics (nDCG@1/5/20, P@1, MRR@100) measured with BM25 on ClueWeb09b; statistical tests (trinomial test for pairwise ties; two-sided t-test for retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Key reported numbers: Automated NLG (selected): on ClariQ dev set USi: BLEU-1=0.3029, BLEU-2=0.2404, BLEU-3=0.2054, ROUGE_L=0.2359, SkipThoughtCS=0.4025, EmbeddingAvgCS=0.7322; on Qulac sample USi: BLEU-1=0.2495, BLEU-2=0.1595, BLEU-3=0.1079, ROUGE_L=0.2495. Human evaluation (pairwise USi vs human, judged by 2 annotators on 230 pairs): Naturalness — USi wins 17%, Human wins 38%, Ties 45% (humans judged more natural with p<0.05); Usefulness — USi wins 22%, Human wins 27%, Ties 51% (no significant difference, p=0.43). Retrieval impact (BM25 on ClueWeb09b, comparing query-only vs query+Q+A): USi produced retrieval scores comparable to human/oracle answers: nDCG@1=0.1355 (+1% relative to oracle baseline reported), nDCG@5=0.1289, nDCG@20=0.1133, P@1=0.1862, MRR@100=0.2730; these improvements over seq2seq baselines were statistically significant for most metrics (two-sided t-test p<0.01).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>1) Pretraining and model capacity: large pre-trained GPT-2 (vs small/baseline seq2seq trained from scratch) improved performance. 2) Training data size and domain match: limited supervised data degraded seq2seq baselines; fine-tuning on Qulac/ClariQ and multi-turn crowdsourced data improved alignment. 3) Semantic control / objective: adding a classification head (DoubleHead) and conditioning input (information-need, query, question, segment embeddings) improved alignment. 4) Decoding strategy: sampling parameters (temperature 0.7, top-p=0.9) influence output quality. 5) Conversation history and edge cases: multi-turn and faulty/off-topic/repeated questions increase hallucination and topic drift, reducing accuracy. 6) Quality of clarifying question (repetition/off-topic) affects model responses and downstream retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Comparative experiments: USi (GPT-2) substantially outperformed LSTM-seq2seq and Transformer-seq2seq baselines on automated NLG metrics and human judgments — authors attribute this to GPT-2 pretraining and small fine-tuning data for baselines. DoubleHead variant improved generation in initial experiments (reported qualitatively). Retrieval experiments show that only USi-generated answers improved BM25 retrieval performance in line with human answers, while seq2seq baselines often degraded retrieval. Multi-turn dataset analysis and qualitative inspection revealed increased hallucination and topic drift in multi-turn models and when faced with edge-case questions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>1) Automated evaluation against oracle human answers using BLEU-1/2/3, ROUGE_L, SkipThoughtCS, EmbeddingAvgCS; 2) Crowdsourced pairwise comparisons (MTurk, US workers, ≥95% approval) of model vs human answers for naturalness and usefulness with two annotators per pair and trinomial test for significance; 3) Document retrieval evaluation: expand initial query with clarifying question and answer and run BM25 over ClueWeb09b, report nDCG@1/5/20, P@1, MRR@100 and two-sided t-tests; 4) Qualitative case studies and multi-turn error analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported limitations include: increased hallucination and topic drift in the multi-turn variant (model may generate answers not aligned with the information need), reduced naturalness relative to human answers (humans judged human answers significantly more natural), sensitivity to poor or repeated/off-topic clarifying questions (edge cases), and lack of explicit grounding mechanisms to prevent hallucination. Model size is unspecified and no strong ablation over model scale is provided. Some retrieval metrics (nDCG@1, P@1) did not show significant gains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Direct comparisons made: USi (GPT-2) vs LSTM-seq2seq and Transformer-seq2seq baselines — USi outperformed baselines on automated NLG metrics and human judgments, and only USi improved retrieval performance; USi-generated answers compared to human/oracle answers — comparable usefulness and retrieval impact, though humans judged naturalness higher. Authors also compared DoubleHead GPT-2 variant (language + classifier head) to language-only variants in early experiments (DoubleHead found superior qualitatively). Decoding parameter choices (temperature=0.7, top-p=0.9) are stated as chosen after initial experiments and prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>1) Use large pre-trained LMs (e.g., GPT-2) and fine-tune with semantically structured inputs that include an explicit information-need, query, question, and segment embeddings to control generation. 2) Combine language modelling loss with a discriminative/classification objective (DoubleHead) to reduce incorrect/distractor outputs. 3) Collect and fine-tune on multi-turn and edge-case conversational data to improve behavior in realistic sessions. 4) Use cautious decoding (recommended temperature 0.7, top-p=0.9 in this work) but validate with humans. 5) Include human evaluation (naturalness/usefulness) and retrieval-based downstream evaluation to measure practical utility. 6) Add grounding or other mechanisms to reduce multi-turn hallucination and topic drift (authors call for future work here).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Mixed-initiative Conversational Search Systems via User Simulation', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Hello, It's GPT-2-How Can I Help You? Towards the Use of Pretrained Language Models for Task-Oriented Dialogue Systems <em>(Rating: 2)</em></li>
                <li>Studying the Effectiveness of Conversational Search Refinement Through User Simulation <em>(Rating: 2)</em></li>
                <li>Evaluating Conversational Recommender Systems via User Simulation <em>(Rating: 2)</em></li>
                <li>Simulating User Satisfaction for the Evaluation of Task-oriented Dialogue Systems <em>(Rating: 1)</em></li>
                <li>Asking clarifying questions in open-domain information-seeking conversations <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5615",
    "paper_id": "paper-ca545ceefe7c3535ffea0fbcc1cc86bddced3767",
    "extraction_schema_id": "extraction-schema-114",
    "extracted_data": [
        {
            "name_short": "USi (GPT-2)",
            "name_full": "USi: GPT-2-based User Simulator for Conversational Search",
            "brief_description": "A semantically-controlled user simulator (USi) built by fine-tuning GPT-2 to generate single- and multi-turn natural-language answers to clarifying questions, conditioned on an explicit information-need (facet) and conversation history, for offline evaluation of mixed-initiative conversational search systems.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (DoubleHead / history-aware fine-tuned variants)",
            "model_description": "Auto-regressive transformer language model (GPT-2) pre-trained on a large corpus of web text (~8M web pages). The paper uses a DoubleHead GPT-2 variant (language modelling + binary classification head) and a history-aware input formulation; fine-tuned on Qulac/ClariQ single-turn data and a crowdsourced multi-turn dataset. Inference uses temperature-controlled stochastic sampling with top-p (nucleus) and top-k filtering.",
            "model_size": null,
            "scientific_subdomain": "Conversational Information Retrieval / Mixed-initiative Conversational Search (IR)",
            "simulation_task": "Generate natural-language answers to system-prompted clarifying questions given an explicit information need and initial query (single-turn), and track conversation history to answer multi-turn clarifying questions; used to simulate real users for offline evaluation of conversational search systems and to measure impact of answers on document retrieval.",
            "accuracy_metric": "Automated NLG metrics (BLEU-1/2/3, ROUGE_L), semantic similarity metrics (SkipThoughtCS, EmbeddingAvgCS), human pairwise judgments for naturalness and usefulness (crowdsourcing pairwise comparisons), document retrieval metrics (nDCG@1/5/20, P@1, MRR@100) measured with BM25 on ClueWeb09b; statistical tests (trinomial test for pairwise ties; two-sided t-test for retrieval).",
            "reported_accuracy": "Key reported numbers: Automated NLG (selected): on ClariQ dev set USi: BLEU-1=0.3029, BLEU-2=0.2404, BLEU-3=0.2054, ROUGE_L=0.2359, SkipThoughtCS=0.4025, EmbeddingAvgCS=0.7322; on Qulac sample USi: BLEU-1=0.2495, BLEU-2=0.1595, BLEU-3=0.1079, ROUGE_L=0.2495. Human evaluation (pairwise USi vs human, judged by 2 annotators on 230 pairs): Naturalness — USi wins 17%, Human wins 38%, Ties 45% (humans judged more natural with p&lt;0.05); Usefulness — USi wins 22%, Human wins 27%, Ties 51% (no significant difference, p=0.43). Retrieval impact (BM25 on ClueWeb09b, comparing query-only vs query+Q+A): USi produced retrieval scores comparable to human/oracle answers: nDCG@1=0.1355 (+1% relative to oracle baseline reported), nDCG@5=0.1289, nDCG@20=0.1133, P@1=0.1862, MRR@100=0.2730; these improvements over seq2seq baselines were statistically significant for most metrics (two-sided t-test p&lt;0.01).",
            "factors_affecting_accuracy": "1) Pretraining and model capacity: large pre-trained GPT-2 (vs small/baseline seq2seq trained from scratch) improved performance. 2) Training data size and domain match: limited supervised data degraded seq2seq baselines; fine-tuning on Qulac/ClariQ and multi-turn crowdsourced data improved alignment. 3) Semantic control / objective: adding a classification head (DoubleHead) and conditioning input (information-need, query, question, segment embeddings) improved alignment. 4) Decoding strategy: sampling parameters (temperature 0.7, top-p=0.9) influence output quality. 5) Conversation history and edge cases: multi-turn and faulty/off-topic/repeated questions increase hallucination and topic drift, reducing accuracy. 6) Quality of clarifying question (repetition/off-topic) affects model responses and downstream retrieval.",
            "evidence_for_factors": "Comparative experiments: USi (GPT-2) substantially outperformed LSTM-seq2seq and Transformer-seq2seq baselines on automated NLG metrics and human judgments — authors attribute this to GPT-2 pretraining and small fine-tuning data for baselines. DoubleHead variant improved generation in initial experiments (reported qualitatively). Retrieval experiments show that only USi-generated answers improved BM25 retrieval performance in line with human answers, while seq2seq baselines often degraded retrieval. Multi-turn dataset analysis and qualitative inspection revealed increased hallucination and topic drift in multi-turn models and when faced with edge-case questions.",
            "evaluation_method": "1) Automated evaluation against oracle human answers using BLEU-1/2/3, ROUGE_L, SkipThoughtCS, EmbeddingAvgCS; 2) Crowdsourced pairwise comparisons (MTurk, US workers, ≥95% approval) of model vs human answers for naturalness and usefulness with two annotators per pair and trinomial test for significance; 3) Document retrieval evaluation: expand initial query with clarifying question and answer and run BM25 over ClueWeb09b, report nDCG@1/5/20, P@1, MRR@100 and two-sided t-tests; 4) Qualitative case studies and multi-turn error analysis.",
            "limitations_or_failure_cases": "Reported limitations include: increased hallucination and topic drift in the multi-turn variant (model may generate answers not aligned with the information need), reduced naturalness relative to human answers (humans judged human answers significantly more natural), sensitivity to poor or repeated/off-topic clarifying questions (edge cases), and lack of explicit grounding mechanisms to prevent hallucination. Model size is unspecified and no strong ablation over model scale is provided. Some retrieval metrics (nDCG@1, P@1) did not show significant gains.",
            "comparisons": "Direct comparisons made: USi (GPT-2) vs LSTM-seq2seq and Transformer-seq2seq baselines — USi outperformed baselines on automated NLG metrics and human judgments, and only USi improved retrieval performance; USi-generated answers compared to human/oracle answers — comparable usefulness and retrieval impact, though humans judged naturalness higher. Authors also compared DoubleHead GPT-2 variant (language + classifier head) to language-only variants in early experiments (DoubleHead found superior qualitatively). Decoding parameter choices (temperature=0.7, top-p=0.9) are stated as chosen after initial experiments and prior work.",
            "recommendations_or_best_practices": "1) Use large pre-trained LMs (e.g., GPT-2) and fine-tune with semantically structured inputs that include an explicit information-need, query, question, and segment embeddings to control generation. 2) Combine language modelling loss with a discriminative/classification objective (DoubleHead) to reduce incorrect/distractor outputs. 3) Collect and fine-tune on multi-turn and edge-case conversational data to improve behavior in realistic sessions. 4) Use cautious decoding (recommended temperature 0.7, top-p=0.9 in this work) but validate with humans. 5) Include human evaluation (naturalness/usefulness) and retrieval-based downstream evaluation to measure practical utility. 6) Add grounding or other mechanisms to reduce multi-turn hallucination and topic drift (authors call for future work here).",
            "uuid": "e5615.0",
            "source_info": {
                "paper_title": "Evaluating Mixed-initiative Conversational Search Systems via User Simulation",
                "publication_date_yy_mm": "2022-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Hello, It's GPT-2-How Can I Help You? Towards the Use of Pretrained Language Models for Task-Oriented Dialogue Systems",
            "rating": 2
        },
        {
            "paper_title": "Studying the Effectiveness of Conversational Search Refinement Through User Simulation",
            "rating": 2
        },
        {
            "paper_title": "Evaluating Conversational Recommender Systems via User Simulation",
            "rating": 2
        },
        {
            "paper_title": "Simulating User Satisfaction for the Evaluation of Task-oriented Dialogue Systems",
            "rating": 1
        },
        {
            "paper_title": "Asking clarifying questions in open-domain information-seeking conversations",
            "rating": 1
        }
    ],
    "cost": 0.01098425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Evaluating Mixed-initiative Conversational Search Systems via User Simulation</h1>
<p>Ivan Sekulič<br>Università della Svizzera italiana<br>Lugano, Switzerland<br>ivan.sekulic@usi.ch</p>
<p>Mohammad Aliannejadi<br>University of Amsterdam<br>Amsterdam, The Netherlands<br>m.aliannejadi@uva.nl</p>
<p>Fabio Crestani<br>Università della Svizzera italiana<br>Lugano, Switzerland<br>fabio.crestani@usi.ch</p>
<h2>ABSTRACT</h2>
<p>Clarifying the underlying user information need by asking clarifying questions is an important feature of modern conversational search system. However, evaluation of such systems through answering prompted clarifying questions requires significant human effort, which can be time-consuming and expensive. In this paper, we propose a conversational User Simulator, called USi, for automatic evaluation of such conversational search systems. Given a description of an information need, USi is capable of automatically answering clarifying questions about the topic throughout the search session. Through a set of experiments, including automated natural language generation metrics and crowdsourcing studies, we show that responses generated by USi are both inline with the underlying information need and comparable to humangenerated answers. Moreover, we make the first steps towards multiturn interactions, where conversational search systems asks multiple questions to the (simulated) user with a goal of clarifying the user need. To this end, we expand on currently available datasets for studying clarifying questions, i.e., Qulac and ClariQ, by performing a crowdsourcing-based multi-turn data acquisition. We show that our generative, GPT2-based model, is capable of providing accurate and natural answers to unseen clarifying questions in the single-turn setting and discuss capabilities of our model in the multi-turn setting. We provide the code, data, and the pre-trained model to be used for further research on the topic. ${ }^{1}$</p>
<h2>KEYWORDS</h2>
<p>Conversational Search, Mixed-initiative Search, User Simulation</p>
<h2>ACM Reference Format:</h2>
<p>Ivan Sekulič, Mohammad Aliannejadi, and Fabio Crestani. 2022. Evaluating Mixed-initiative Conversational Search Systems via User Simulation. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining (WSDM '22), February 21-25, 2022, Tempe, AZ, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3488560.3498440</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>1 INTRODUCTION</h2>
<p>The primary goal of a conversational search system is to satisfy user's information need by retrieving relevant information from a given collection. In order to successfully do so, the system needs to have a clear understanding of the underlying user need. Since user's queries are often under-specified and vague, a mixed-initiative paradigm of conversational search allows the system to take initiative of the conversation and ask the user clarifying questions, or issue other requests. Clarifying the user information need has been shown beneficial to both the user and the conversational search system [3, 26, 63], providing a strong motivation for such mixedinitiative systems.</p>
<p>However, evaluation of the described mixed-initiative conversational search systems is not straightforward. The problem arises from the fact that expensive and time-consuming human-in-theloop and human evaluation are required to properly evaluate conversational systems. Such studies require real users to interact with the search system for several conversational turns and provide answers to potential clarifying questions prompted by the system. A relatively simple solution is to conduct offline corpus-based evaluation [3]. However, this limits the system to selecting clarifying questions from a pre-defined set of questions, which does not transfer well to the real-world scenario. Moreover, such offline evaluation remains limited to single-turn interaction, as the pre-defined questions are associated with corresponding answers and are not aware of any previous interactions. User simulation has been proposed to tackle the shortcomings of corpus-based and user-based evaluation methodologies. The aim of a simulated user is to capture the behaviour of a real user, i.e., being capable of having multi-turn interactions on unseen data, while still being scalable and inexpensive like other offline evaluation methods [44, 64].</p>
<p>In this paper, we propose a conversational User Simulator, called USi - a model capable of multi-turn interactions with a general mixed-initiative conversational search system. Given an initial information need, USi interacts with the conversational system by accurately answering clarifying questions prompted by the system. The answers are in line with the underlying information need and help elucidate the given intent. Moreover, USi generate answers in fluent and coherent natural language, making its responses comparable to real users. Previous work on the topic remained limited to retrieving answers from a pre-defined pool of human-generated answers to clarifying questions, e.g., CoSearcher [44], or providing feedback with template-based answers in recommender systems [64].</p>
<p>We base our proposed user simulator on a large-scale transformerbased language model, namely GPT-2 [37], ensuring the near-human</p>
<p>quality of generated text. Moreover, USi generates answers to clarifying questions in line with the initial information need, simulating the behaviour of a real user. We ensure that through a specific training procedure, resulting in a semantically-controlled language model. We evaluate the feasibility of our approach with an exhaustive set of experiments, including automated metrics as well as human judgements. First, we compare the quality of the answers generated by USi and several competitive sequence-to-sequence baselines by computing a number of automated natural language generation metrics. USi significantly outperforms the baselines, which supports our decision to base our model on pre-trained GPT-2. Second, we conduct a crowdsourcing study to assess how natural and accurate the generated answers are, compared to answers generated by humans. The crowdsourcing judgements show no significant difference in the naturalness and usefulness of generated and human responses. Third, we perform document retrieval evaluation following [1, 2], where retrieval is performed before and after answering a clarifying question. We observe improvements in the retrieval performance when the answer is provided to the retrieval model, matching the retrieval performance of human-generated answers. Next, we make the first steps towards multi-turn variant of the proposed model and present the multi-turn interaction dataset, acquired through crowdsourcing studies. Finally, we discuss the intended use of the framework and demonstrate it's feasibility through a qualitative case study.</p>
<p>Our contributions can be summarised as follows:</p>
<ul>
<li>We propose a user simulator, USi, for conversational search system evaluation, capable of answering clarifying questions prompted by the search system. We release the code and pre-trained USi for future research.</li>
<li>We perform extensive set of experiments to evaluate the feasibility of substituting real users with the user simulator, shedding new light on the upper bound of large-scale language models for the task.</li>
<li>We release a dataset of multi-turn interactions acquired through crowdsourcing, that we use to train our multi-turn version of the model. The dataset consists of 1000 conversations of up to three turns, where crowdsourcing workers played the roles of the system that asks clarifying questions and the user seeking information.</li>
</ul>
<h2>2 RELATED WORK</h2>
<p>Our work is part of a broad area of conversational information retrieval and user simulation. In this Section, we evaluate relevant work on the topics.
Conversational search. Recent advancements of conversational agents have stimulated research in various aspects of conversational information access [13, 54, 61]. In fact, the report from the Dagstuhl Seminar N. 19461 [4] identifies conversational search as one of the essential areas of information retrieval (IR) in the upcoming years. Moreover, Radlinski and Craswell [38] propose a theoretical framework for conversational search, highlighting the multi-turn user-system interactions as one of the desirable properties of modern conversational search. This property is tied with a mixed-initiative paradigm in IR [24], where the system is not only
passive, but prompt the user with engaging content, such as clarifying questions.</p>
<p>Clarification has attracted considerable attention of the research community, including studies on human-generated dialogues on question answering (QA) forums, utterance intent analysis, and asking clarifying questions [9]. Asking clarifying questions has been shown to be beneficial for the conversational search system and the user. E.g., Kiesel et al. [26] studied the impact of voice query clarification on user satisfaction and found that users like to be prompted for clarification. Moreover, Aliannejadi et al. [3] proposed an offline evaluation methodology for asking clarifying questions and showed the benefits of clarification in terms of improved performance in document retrieval once question is answered. Hashemi et al. [22] proposed a Guided Transformer model for document retrieval and next clarifying question selection in a conversational search setting. Furthermore, Zamani et al. [63] proposed reinforcement learning-based models for generating clarifying questions and the corresponding candidate answers from weak supervision data. Moreover, Sekulić et al. [47] proposed GPT2 based model for generating facet-driven clarifying questions. Although extensive work related to clarification in search exists, effective and efficient evaluation methodologies of mixed-initiative approaches are scarce.</p>
<p>Another research direction in conversational search area is multiturn passage retrieval, lead by the TREC Conversational Assistant Track (CAsT) [15]. The system needs to understand the conversational context and retrieve appropriate passages from the collection. As the further improvement, Ren et al. [42] introduced the task of conversations with search engines, where system generates a short, summarised response of the retrieved passages. Other studies in the area of conversational search include user intent classification [36], response ranking [15, 46, 50], document features for clarifying questions [49], user engagement prediction [31, 48], and query rewriting $[35,49,58]$.</p>
<p>In the field of natural language processing (NLP), researchers have studies question ranking [39] and generation [40, 60] in dialogue. These studies usually rely on large amount of data from query logs [41], industrial chatbots [60], and QA websites [39, 40, 57]. For example, Rao and Daumé [39] developed a neural model for question selection on an artificial dataset of clarifying questions and answers extracted from QA forums. In their later study, they proposed an adversarial training mechanism for generating clarifying questions given a product description from Amazon [40]. Unlike these studies, we study user-system interaction in an IR setting, where the user's information need is presented in the form of short queries (vs. a long detailed post on StackOverflow), with a result of ranked list of relevant documents. Furthermore, the IR system can ask clarifying question to elucidate user's information need, which then needs to be answered.
User simulation in information retrieval. Given the complexity of human-computer interactions and natural language, there has been an ongoing discussion in the NLP community about the credibility of automatic evaluation metrics that are based on text overlap [33]. Metrics such as BLEU and ROGUE that try to judge a system's output solely based on how much overlap it has with</p>
<p>a reference utterance cannot capture the performance of the system accurately [6]. Hence, human annotation should be done to evaluate a system's performance when a generative model is used, in tasks such as summarisation and machine translation. Moreover, evaluation of a system becomes even more complex if an ongoing interaction between the user and system exists. Not only must the system evaluate the generated utterance, it should also be able to incorporate a human response. For this reason, researchers adopt human-in-the-loop techniques to mimic human-computer interactions, and further perform human annotation to evaluate the whole system's performance (in response to human). Recent work of Lipani et al. [30] propose a metric for offline evaluation of conversational search systems based on user interaction model.</p>
<p>With an idea to alleviate the need for time-consuming and expensive human evaluation, researchers proposed replacing the user with a user simulation system [44, 53]. Simulation in IR has long been studied (1973) [14] with the idea of generating pseudo-docs and pseudo-queries to study literature search system performance. The work was then followed by Griffiths [21], proposing a general framework of simulation for IR systems. Tague et al. [56] later studied the problems for user simulation in bibliographic retrieval systems. User simulation for evaluation was first proposed in 1990 by Gordon [20] where the authors proposed a framework for generating simulated queries. This work has been long followed in the literature to study various hypothetical user and system actions (e.g., issuing 100 queries in a session) that cannot be done in a real system [5]. In particular, Azzopardi [5] proposed to study the cost and gain of user and system actions and studied the effect of different strategies using simulated queries and actions of users (e.g., clicking on relevant documents). Mostafa et al. [32] studied different dimensions of users' interests and their impact on user modelling and information filtering. Diaz and Arguello [17] adapted an offline vertical selection prediction model in the presence of user feedback for user simulation.</p>
<p>More recently, there has been research on simulating users to evaluate the effectiveness of systems [12, 44, 53, 62, 64]. Carterette et al. [12] proposed a conceptual framework for investigating various aspects of simulations, namely, system effectiveness, user models, and user utility. With the recent developments of conversational systems, more attention towards simulating users in a conversation has been drawn. Sun et al. [53] proposed a simulated user for evaluating conversational recommender systems based on predefined actions and structured response types. Salle et al. [44] proposed a parametric user simulator for information-seeking conversation where the simulator takes an information need and responds to the system accordingly. In fact, this work is the closest work to ours. However, we would like to draw attention to various limitations of this work. Even though this work takes an information need as input and aims at answering to the system's request according to that, it fails to generate responses. The approach is limited to predicting the relevance of the system's utterance to the user's information need and selecting an appropriate answer from a list of human-generated answers. In this work, we take one step further and generate human-like answers in natural language. Also, the work by Zhang and Balog [64] that simulates users for recommender system evaluation, uses structured data and response
types. In this work, we propose a simulator that generates natural language responses based on unstructured data.</p>
<h2>3 SIMULATED USER</h2>
<p>In this section, we explain the role of a user in the evaluation of conversational search and dialogue systems. Next, we define several characteristics a user simulator should have in order to be able to replace real users in certain evaluation tasks.</p>
<h3>3.1 User's role in evaluating conversational search systems</h3>
<p>Previous work in task-oriented dialogue systems and conversational search systems mostly evaluate the performance of the systems in an offline setting using a corpus-based approach [16]. However, offline evaluation does not accurately reflect the nature of conversational systems, as the evaluation is possible only at a single-turn level. Thus, in order to properly capture the nature of the conversational search task, it is necessary to involve users in the evaluation procedure [8, 28]. User involvement allows proper evaluation of multi-turn conversational systems, where user and system take turns in a conversation. Nonetheless, while such approach most precisely captures the performance of the systems in a real-world scenario, it is tiresome, expensive, and unscalable. In pursuit of alleviating the evaluation of dialogue systems, while still accurately capturing the overall performance, a simulated user approach has been proposed [53, 64]. The simulated user is intended to provide a substitute for real users, as it is easily scalable, cheap, fast, and consistent. Next, we formally describe the characteristics of a simulated user for conversational search system evaluation.</p>
<h3>3.2 Problem definition</h3>
<p>As already mentioned, evaluating conversational search systems is hard due to the necessity of human judgements at each turn of an interaction with the search system. In this work, we aim to alleviate the procedure of evaluating certain types of mixed-initiative conversational search systems. Specifically, we provide a simulated user with an imaginary information need capable of answering various types of clarifying questions prompted by any modern conversational search system.</p>
<p>Formally, our simulated user $U$ is initialised with a given information need in. Simulated user $U$ formulates its need in a form of the initial query $q$, which is then given to the general mixedinitiative conversational system $S$. The aim of the system $S$ is to elucidate the information need in through a series of clarifying questions $c q$. We do not go into details of the implementation of such a system, but different approaches have been proposed in recent literature [3, 22]. Next, the simulated user $U$ needs to provide an answer $a$ to the system's question. The answer $a$ needs to be in line with user's information need in.
Single-turn responses. Formally, user $U$ needs to generate an answer $a$ to the system's clarifying question $c q$, conditioned on the initial query $q$ and the original user's intent in:</p>
<p>$$
a=f(c q \mid i n, q)
$$</p>
<p>The user $U$ is expected to answer the question in line with its information need, not just based on a potentially vague and underspecified query, like traditional chatbots would be inclined to do.</p>
<p>Conversation history-aware user. Moreover, the system can take further initiative and ask additional clarifying questions. Thus, our simulated user $U$ needs to track the conversation flow as well. Formally, at the conversational turn $i, U$ generates an answer given by:</p>
<p>$$
a_{i}=f\left(c q_{i} \mid i n, q, H\right)
$$</p>
<p>where $H$ is conversational history, consisting the interaction between the user and the system up until the current turn: $H=$ $\left{\left(c q_{i}, a_{j}\right)\right}$, where $j \in[1 \ldots i-1]$. In the next Section, we explain how we modelled the described simulated user.</p>
<h2>4 SIMULATION METHODOLOGY</h2>
<p>In this section, we motivate and describe in detail our proposed User Simulator, USi. We make USi semantically-controlled through specific language modelling training. We base our simulated user on a large-scale transformer-based model, namely GPT-2 [37].</p>
<h3>4.1 Semantically-controlled text generation</h3>
<p>We define the task of generating answers to clarifying questions as a sequence generation task. Thus, we employ language modelling as our main tool for generating sequences. The goal of a language model (LM) is to learn the probability distribution $p_{\theta}(x)$ of a sequence of length $n: x=\left[x_{1}, x_{2}, \ldots, x_{n}\right]$, where $\theta$ are the parameters of the LM. Current state-of-the-art language models, such as GPT-2, learn the distribution in an auto-regressive manner, i.e., formulating the task as next-word prediction task:</p>
<p>$$
p_{\theta}(\boldsymbol{x})=\prod_{i=1}^{n} p_{\theta}\left(x_{i} \mid x_{&lt;i}\right)
$$</p>
<p>However, recent research showed that large-scale transformerbased language models, although generating text of near-human quality, are prone to "hallucination" [18] and in general lack semantic guidance [43]. Thus, with a specific fine-tuning technique and careful input arrangement, we fine-tune semantically-conditioned LM. As mentioned in the previous section, answer generation needs to be conditioned on the underlying information need. To this aim, we learn the probability distribution of generating an answer $a$ :</p>
<p>$$
p_{\theta}(\boldsymbol{a} \mid i n, q, c q)=\prod_{i=1}^{n} p_{\theta}\left(a_{i} \mid a_{&lt;i}, i n, q, c q\right)
$$</p>
<p>where $a_{i}$ is the current token of the answer, $a_{&lt;i}$ are all the previous ones, while in, $q$, and $c q$ correspond to the information need, the initial query, and the current clarifying question from Equation 1, respectively.</p>
<h3>4.2 GPT2-based simulated user</h3>
<p>GPT-2 is a large-scale transformer-based language model trained on a dataset of 8 million web pages, capable of synthesising text of near human quality [37]. Moreover, as it is trained on an extremely diverse dataset, it can generate text on various topics, which can be primed with an input sequence. GPT-2 has previously been used for various text generation tasks, including dialogue systems and chatbots [10]. Therefore, it is a suitable choice for our task of simulating users through generating answers to clarifying question in a conversational search system.</p>
<p>We base our proposed user simulator USi on the GPT-2 model with language modelling and classification losses, i.e., DoubleHead GPT-2. In this variant, the model not only learns to generate the appropriate sequence through the language modelling loss, but also how to distinguish a correct answer to the distractor one. This has been shown to improve the sequence generation [37] and has showed superior performance over only-language loss GPT-2 in the initial stage of experiments. The two losses are linearly combined.
Single-turn responses. We formulate the input to the GPT-2 model, based on Equation 4, as:</p>
<p>$$
\text { input_seq }=\text { in }[\text { SEP }] q[\text { SEP }] c q[\text { bos }] a[\text { eos }]
$$</p>
<p>where [bos], [eos], and [SEP] are special tokens indicating the beginning of sequence, the end of sequence, and a separation token, respectively. Information need in, initial query $q$, clarifying question $c q$, and a target answer $a$ are tokenized prior to constructing the full input sequence to the model. Additionally, we construct segment embeddings, which indicate different segments of the input sequence, namely in, $q, c q$, and $a$.</p>
<p>When training the DoubleHead variation of the model, we formulate the first part of the input as described above. Additionally, we sample the ClariQ dataset for distractor answers and process them in the same manner as the original answer, based on Equation 5. Therefore, the DoubleHead GPT-2 variant accepts as input two sequences, one with the original target answer in the end, and the other with the distractor answer. It then needs to not only learn to model the target answer, but also to distinguish between original and distractor answers and provide a binary label indicating which of the two answers is the desirable one. We sample the distractor answers from the aforementioned datasets. When possible, we ensure that if the target answer starts with "Yes", the distractor answers starts with "No", in order to enforce the connection between the answer, the clarifying question, and the information need. Likewise, if the answer starts with "No", we sample a distractor answer that starts with "Yes". Note that USi does not generate answers that begin strictly with a "yes" or a "no".
Conversation history-aware model. The conversation historyaware model calls for a different formulation of the input and the training. Specifically, the input to history-aware GPT-2 is constructed as:
input_seq $=i n[$ user $] q[$ system $] c q_{&lt;i}[$ user $] a_{&lt;i}[$ system $] c q_{i}[$ bos $] a_{i}[$ eos $]$
where [user] and [system] are additional special tokens indicating the conversational turns between the (simulated) user and the conversational system, respectively.
Inference. During inference, we omit the answer $a$ from the input sequence, as our goal is to generate this answer to a previously unseen question. In order to generate answers, we use a combination of state-of-the-art sampling techniques to generate a textual sequence from the trained model. Namely, we utilise temperaturecontrolled stochastic sampling with top- $k$ [19] and top- $p$ (nucleus) filtering [23]. After some initial experiments and consultation with previous work, we fix the parameters of the temperature to $0.7, k$ to 0 , and $p$ to 0.9 .</p>
<p>Table 1: Statistics for Qulac and ClariQ datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Qulac</th>
<th style="text-align: left;">ClariQ</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Number of topics</td>
<td style="text-align: left;">198</td>
<td style="text-align: left;">237</td>
</tr>
<tr>
<td style="text-align: left;">Number of facets</td>
<td style="text-align: left;">762</td>
<td style="text-align: left;">891</td>
</tr>
<tr>
<td style="text-align: left;">Number of questions</td>
<td style="text-align: left;">2,639</td>
<td style="text-align: left;">3,304</td>
</tr>
<tr>
<td style="text-align: left;">Number of question-answer pairs</td>
<td style="text-align: left;">10,277</td>
<td style="text-align: left;">11,489</td>
</tr>
</tbody>
</table>
<h2>5 DATA</h2>
<h3>5.1 Qulac and ClariQ</h3>
<p>For the purpose of training and evaluating our proposed simulated user $U S i$, we utilise two publicly available datasets, Qulac [3] and ClariQ [1]. The aim of both datasets is to foster research in the field of asking clarifying questions in open-domain conversational search. Qulac was created on top of the TREC Web Track 200912 collection. The Web Track collection contains ambiguous and faceted queries, which often require clarification when addressed in a conversational setting. Given a topic from the dataset, clarifying questions were collected via crowdsourcing. Then, given a topic and a specific facet of the topic, workers were employed to gather answers to these clarifying questions. This results in a tuple of (topic, facet, clarifying_question, answer). Most of the topics in the dataset are multi-faceted and ambiguous, meaning that the clarifying questions and answers need to be in line with the actual facet. ClariQ is an extension of Qulac created for the ConvAI3 challenge [1] and contains additional non-ambiguous topics. Relevant statistics of the datasets are presented in Table 1.</p>
<p>We utilise these datasets by feeding the corresponding elements to Equation 4. Specifically, facet from Qulac and ClariQ represents the underlying information need, as it describes in detail what the intent behind the issued query is. Moreover, question represents the current asked question, while answer is our language modelling target.</p>
<h3>5.2 Multi-turn conversational data</h3>
<p>A major drawback of Qulac and ClariQ is that they are both built for single-turn offline evaluation. In reality, a conversational search system is likely to engage in a multi-turn dialog in order to elucidate user need. To bridge the gap between single- and multi-turn interactions, we construct multi-turn data that resembles a more realistic interaction between a user and the system. Our user simulator $U S i$ is then further fine-tuned on this data.</p>
<p>To acquire the multi-turn data, we construct a crowdsourcingbased human-to-human interaction. At each conversational turn, crowdsourcing worker is tasked to behave as a search system by asking a clarifying question on the topic of the conversation. Then, another worker is tasked to provide the answer to that question having in mind the underlying information need and the conversation history, imitating the behaviour of the real user. We construct in 500 conversations up to depth of three, i.e., we have three sequential question-answers pairs for a topic and its facet.</p>
<p>In order to further study the effects certain clarifying questions have on the whole search experience, we construct several edge cases. In such cases, the clarifying question prompted by the search system is considered faulty, as it is either a repetition, off topic, unnecessary, or completely ignores previous user's answers. We obtain answers to these questions to provide a more realistic data</p>
<p>Table 2: Multi-turn dataset acquired through crowdsourcing for fine-tuning a more realistic user simulator. Sample conversations of depth 3 are omitted for space purposes.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question case</th>
<th style="text-align: center;">Description</th>
<th style="text-align: center;">Sample conversation</th>
<th style="text-align: center;">N</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Normal</td>
<td style="text-align: center;">A good system naturally <br> continues the conversation.</td>
<td style="text-align: center;">U: I'm looking for information on dieting <br> S: Are you looking for dieting tips? <br> U: Yes and exercise tips as well <br> S: Do you need anything specific in <br> relation to counting calories <br> you consume daily? <br> U: Yes, I would like to know more <br> about that topic.</td>
<td style="text-align: center;">500</td>
</tr>
<tr>
<td style="text-align: center;">Repeat</td>
<td style="text-align: center;">System repeats the <br> previous question.</td>
<td style="text-align: center;">U: Find information on raised gardens. <br> S: Do you need information on <br> materials needed? <br> U: No, I want to find plans <br> S: Do you need information on <br> materials needed? <br> U: I want what I previously asked for.</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: center;">Off-topic</td>
<td style="text-align: center;">System asks the user an <br> off-topic question.</td>
<td style="text-align: center;">U: I'm looking for an online world atlas. <br> S: Are you interested in satellite maps? <br> U: No, I want an online world atlas <br> S: Which mountain ski resort would you <br> like information around the pocsno area? <br> U: I am not interested in this topic.</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: center;">Similar</td>
<td style="text-align: center;">System asks a question <br> similar to the previous one, <br> ignoring the user's answer.</td>
<td style="text-align: center;">U: I'm looking for information about <br> mayo clinic Jacksonville FL <br> S: Would you like to request <br> an appointment? <br> U: yes <br> S: Are you looking for the address <br> of mayo clinic jacksonville fl? <br> U: I just want to request an appointment.</td>
<td style="text-align: center;">400</td>
</tr>
</tbody>
</table>
<p>for the training of our model, making our simulated user as humanlike as possible. These clarifying questions are intended to simulate a conversational search system of poor quality and provide insight into user's responses to such questions. We employ workers to provide answers to additional 500 clarifying questions of poor quality, up to the depth of two. The specific edge cases and their descriptions with examples are presented in Table 2. We publicly release the acquired multi-turn datasets.</p>
<h2>6 EVALUATION</h2>
<p>Our aim is to evaluate whether our proposed simulated user can replace real users in answering clarifying questions of conversational search systems, which would make the evaluation of such systems significantly less troublesome. Overall, we aim to answer four main research questions:
RQ1: To what extent are the generated answers in line with the underlying information need?
RQ2: How coherent and natural is the language of the generated answers?
RQ3: To what extent does the retrieval model of the conversational search system benefit from the generated answers?
RQ4: How does $U S i$ behave in multi-turn interactions?
To address these questions, we first compute several natural language generation metrics to compare the generated answers to the oracle human answers from ClariQ. As several NLG metrics received criticism from the NLP community, especially since they do not correlate well with the coherence of the text, we perform a crowdsourcing study to evaluate the naturalness of generated answers. In order to evaluate whether the generated answers are in line with the actual information need, we carry out additional crowdsourcing study, evaluating the usefulness of answers. Moreover, we analyse the impact of generated answers to retrieval model performance, by performing a document retrieval before</p>
<p>Table 3: Performance by automated NLG metrics on Qulac (25 topics) and ClariQ (dev set).</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">model</th>
<th style="text-align: center;">BLEU-1</th>
<th style="text-align: center;">BLEU-2</th>
<th style="text-align: center;">BLEU-3</th>
<th style="text-align: center;">ROUGE_L</th>
<th style="text-align: center;">SkipThoughtCS</th>
<th style="text-align: center;">EmbeddingAvgCS</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LSTM-seq2seq</td>
<td style="text-align: center;">0.1993</td>
<td style="text-align: center;">0.1446</td>
<td style="text-align: center;">0.1076</td>
<td style="text-align: center;">0.2428</td>
<td style="text-align: center;">0.3091</td>
<td style="text-align: center;">0.7468</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer-seq2seq</td>
<td style="text-align: center;">0.2071</td>
<td style="text-align: center;">0.1317</td>
<td style="text-align: center;">0.0886</td>
<td style="text-align: center;">0.1997</td>
<td style="text-align: center;">0.3118</td>
<td style="text-align: center;">0.6566</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$U S i$</td>
<td style="text-align: center;">0.2495</td>
<td style="text-align: center;">0.1595</td>
<td style="text-align: center;">0.1079</td>
<td style="text-align: center;">0.2495</td>
<td style="text-align: center;">0.4167</td>
<td style="text-align: center;">0.7896</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LSTM-seq2seq</td>
<td style="text-align: center;">0.1989</td>
<td style="text-align: center;">0.1401</td>
<td style="text-align: center;">0.0988</td>
<td style="text-align: center;">0.2210</td>
<td style="text-align: center;">0.3158</td>
<td style="text-align: center;">0.7012</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer-seq2seq</td>
<td style="text-align: center;">0.2041</td>
<td style="text-align: center;">0.1352</td>
<td style="text-align: center;">0.0936</td>
<td style="text-align: center;">0.2067</td>
<td style="text-align: center;">0.3666</td>
<td style="text-align: center;">0.7077</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$U S i$</td>
<td style="text-align: center;">0.3029</td>
<td style="text-align: center;">0.2404</td>
<td style="text-align: center;">0.2054</td>
<td style="text-align: center;">0.2359</td>
<td style="text-align: center;">0.4025</td>
<td style="text-align: center;">0.7322</td>
</tr>
</tbody>
</table>
<p>and after answering the prompted clarifying question, as described in Section 6.3. Finally, we perform qualitative analysis of generated answers.</p>
<p>We compare our GPT-2-based user simulator to two competitive sequence-to-sequence baselines. The first baseline is a multilayer bidirectional LSTM encoder-decoder network for sequence-to-sequence tasks [55]. ${ }^{2}$ The second baseline is a transformer-based encoder-decoder network, based on Vaswani et al. [59]. We perform hyperparameter search to select the learning rate, number of layers, and hidden dimension of the models. Both baselines are trained with the same input as our main model.</p>
<h3>6.1 Automated NLG metrics</h3>
<p>We first study the language generation ability of $U S i$ and of the aforementioned baselines. We compute several standard metrics for evaluating the generated language. We use two widely adopted metrics based on n-gram overlap between the generated and the reference text. These are BLEU [34] and ROUGE [29]. Next, we compute the EmbeddingAverage and SkipThought metrics aiming to capture the semantics of the generated text, as they are based on the word embeddings of each token in the generated and the target text. The metric is then defined as a cosine similarity between the means of the word embeddings in the two texts [27]. The models are trained on ClariQ training set and evaluated on unseen ClariQ development set. We evaluate on ClariQ's development set since the test set does not contain question-answer pairs. We take a small portion of the training set for our actual development set. The answers generated by $U S i$ and the baselines are compared against oracle answers from ClariQ, generated by humans.</p>
<h3>6.2 Response Naturalness and Usefulness</h3>
<p>In order to simulate a real user, the generated responses by our model need to be fluent and coherent. Thus, we study the naturalness of the generated answers. We define naturalness as an answer being natural, fluent, and likely generated by a human. Similarly, fluency [11] and humanness [45] have been used for evaluating generated text. Moreover, we assess the usefulness of the answers generated by our simulated user. We define usefulness as an answer being in line with the underlying information need and guiding the conversation towards the topic of the information need. This definition of usefulness can be related to similar metrics in previous work, such as adequacy [52] and informativeness [13].</p>
<p>We perform a crowdsourcing study to assess the naturalness and usefulness of generated answers to clarifying questions. We use Amazon MTurk for acquiring workers, based in US, with at</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>least $95 \%$ task approval rate. The study was done in a pair-wise setting, i.e., each worker was presented with a number of answer pairs, where one of the answers was generated by our model and the other was generated by a human, taken from the ClariQ collection. Their task was then to provide judgement on which answers is more natural or useful, depending on the study. The workers have been provided with the context, i.e., the initial query, facet description, and clarifying question.</p>
<p>We annotate 230 answer pairs for naturalness and 230 answer pairs for usefulness, each judged by two crowdsource workers. We define a win for our model if both annotators voted our generated answer as more natural/useful, and loss for our model if both voted the human generated answer as more natural/useful. In case the two workers voted differently on a single answer pair, we define that as a tie. With this study, we aim to shed light onto research questions RQ1 and RQ2, i.e., whether the generated answers are indeed natural and in line with the underlying information need, compared to human-generated answers. Additionally, we compare Transformer-seq2seq to $U S i$. The results of the study are discussed in Section 7.2.</p>
<h3>6.3 Impact of generated answers to document retrieval Performance</h3>
<p>As the basis for the offline evaluation of open-domain conversational search systems, Aliannejadi et al. [3] propose the document retrieval task with the answer to the prompted clarifying question. The initial query is expanded with the text of the clarifying question and the user's answer and then fed to a retrieval model, such as BM25 or Query Likelihood. The intuition is that if the clarifying question and the answer were both useful, the retrieval model will perform better with them in input, alongside the initial query. In fact, they show significant improvements in retrieval performance with the additional input compared to query-only setting, which is in general a strong motivation for asking clarifying questions in conversational search. The initial retrieval is performed on ClueWeb09b collection, while queries are taken from the ClariQ development set. Each query is associated with the information need (facet) description and several clarifying questions. We then generate answers to these questions and perform additional document retrieval with the initial query expanded with generated answers and corresponding questions.</p>
<p>We follow the described evaluation paradigm to assess whether our simulated user generates useful answers, compared to the humangenerated ones. With this study, we aim to answer research question RQ3, i.e., how beneficial are the generated answers to the retrieval model of a conversational search system. Our hypothesis is that the retrieval performance should increase when the initial</p>
<p>Table 4: Results on naturalness and usefulness of responses, $U S i$ vs human-generated..</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">$U S i$ Wins</th>
<th style="text-align: left;">Human Wins</th>
<th style="text-align: left;">Ties</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Naturalness</td>
<td style="text-align: left;">$17 \%$</td>
<td style="text-align: left;">$38 \%$</td>
<td style="text-align: left;">$45 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Usefulness</td>
<td style="text-align: left;">$22 \%$</td>
<td style="text-align: left;">$27 \%$</td>
<td style="text-align: left;">$51 \%$</td>
</tr>
</tbody>
</table>
<p>query is expanded with the generated answers. The results of the experiment are discussed in Section 7.3.</p>
<h2>7 RESULTS AND DISCUSSION</h2>
<h3>7.1 Automated NLG metrics</h3>
<p>Performance of the baseline model and our simulated user, as evaluated by automated NLG metrics described in Section 6.1, is presented in Table 3. USi significantly outperforms all baselines by all of the computed metrics both on Qulac and ClariQ. Even though LSTM-seq2seq showed strong performance in various sequence-tosequence tasks, such as translation [55] and dialogue generation [51], it performs quite poorly on our task. Similar outcome is observed for Transformer-seq2seq. We hypothesise that the poor performance in this task is due to limited training data, as the success of these seq2seq models on various different tasks was conditioned on large training sets. Our GPT2-based model does not suffer from the same problem, as it has been pre-trained on a large body of text, making the fine-tuning enough to capture the essence of the task, which is generating answers to clarifying questions.</p>
<h3>7.2 Naturalness and Usefulness</h3>
<p>Table 4 presents the results of the crowdsourcing study on usefulness and naturalness, comparing answers generated by $U S i$ and human, as described in Section 6.2. Both in terms of naturalness and usefulness, we observe a large number of ties, i.e., the two workers annotating the answer pair did not agree which one is more natural/useful. Since we are comparing answers generated by our GPT2-based simulated user with the answers written entirely by humans, this result goes in favour of our proposed model. Moreover, the difference between losses and wins for our model is relatively small ( $38 \%$ losses, $17 \%$ wins) for naturalness, and even smaller in terms of usefulness ( $32 \%$ losses, $23 \%$ wins). We conduct trinomial test for statistical significance [7], an alternative to the Sign and binomial tests that takes into account ties. ${ }^{3}$ In terms of naturalness, we reject the null hypothesis of equal performance with $p&lt;0.05$, i.e., human generated answers are more natural than ones generated by USi. Nonetheless, $45 \%$ of ties between USi- and human-generated answers suggests the high quality of the generated text. Regarding usefulness, we accept the null hypothesis of equal performance with $p=0.43$, i.e., there is no statistically significant difference between the performance of human annotators and $U S i$.</p>
<p>Table 5 presents the results of the comparison between the Transformer seq2seq and USi. We observe a win of the proposed USi over the baseline by a large margin. Our GPT-2-based model significantly outperforms the baseline ( $p&lt;0.05$ ) both in terms of naturalness ( $50 \%$ wins and $3 \%$ losses) and usefulness ( $66 \%$ wins and $3 \%$ losses). This finding is in line with the automated evaluation of generated answers.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 5: Results on naturalness and usefulness of responses, $U S i$ vs Transformer-seq2seq baseline.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">USi Wins</th>
<th style="text-align: left;">Baseline Wins</th>
<th style="text-align: left;">Ties</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Naturalness</td>
<td style="text-align: left;">$50 \%$</td>
<td style="text-align: left;">$3 \%$</td>
<td style="text-align: left;">$47 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Usefulness</td>
<td style="text-align: left;">$66 \%$</td>
<td style="text-align: left;">$3 \%$</td>
<td style="text-align: left;">$31 \%$</td>
</tr>
</tbody>
</table>
<p>Regarding the research questions RQ1 and RQ2, i.e., whether the responses generated by our model are in line with the underlying information need and at the same time coherent and fluent, we arrive to the conclusion of satisfactory performance of the simulated user. The generated answers to clarifying questions seem to be able to compete with the answers produced by humans both in terms of naturalness and usefulness. Moreover, strong performance of USi over Transformer-seq2seq additionally motivates the use of large-scale pre-trained language models, such as GPT-2, for the task. These results make a strong case for the utilisation of a user simulator for mixed-initiative conversational search system evaluation.</p>
<h3>7.3 Document retrieval performance</h3>
<p>The comparison of our simulated user and the baselines on the document retrieval performance, before and after answering a clarifying question, is presented in Table 6. The first row of the table shows the performance of the BM25 with only the initial query as input. The following rows report the performance of BM25 with input composed of the initial query, clarifying question, and answers generated by each of the models. Answers in the last row of the table are generated by humans and taken directly from ClariQ.</p>
<p>We observe that neither of the baseline models improves over the query-only baseline, i.e., we can not reject the null hypothesis of equal performance for neither of the metrics. This suggest that LSTM-seq2seq and Transformer-seq2seq do not yield useful and relevant answers to posed clarifying questions. In most of the cases, the answers generated by the baselines even have a negative effect on the document retrieval performance, suggesting that the answers confuse the retrieval model.</p>
<p>On the other hand, we observe clear and significant performance increase of our simulated user over both the query-only and seq2seq baselines. The performance is significant by all of the metrics, except nDCG@1 and precision@1. Similarly, oracle answers significantly outperform both baselines. This confirms the finding previous research, suggesting the document retrieval performance increases with answers to clarifying questions as input [3].</p>
<p>Interestingly, human-generated answers do not perform better than the answers generated by our model. This finding provides an answer to the research question RQ3, i.e, can the retrieval model benefit from the answers generated by USi. The equal performance of the generated and human answers on this task is in line with previously described analyses on the usefulness of the generated responses. Together, the studies strongly support the possibility of substituting a real user with the user simulator for answering clarifying questions in conversational search.</p>
<h3>7.4 Qualitative analysis</h3>
<p>Single-turn analysis. In this Section, we analyse several conversation samples of our user simulator with a hypothetical conversational search system. Table 7 shows four interaction examples. The</p>
<p>Table 6: Document retrieval performance based on the answers provided by our simulated user. Percentages in parentheses report relative increase or decrease in performance over Oracle. Symbols $\dagger$ and $\ddagger$ indicate statistically significant difference compared to the query-only baseline and the human-generated answers, respectively. The significance is reported under twosided t-test with $p&lt;0.01$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">nDCG@1</th>
<th style="text-align: left;">nDCG@5</th>
<th style="text-align: left;">nDCG@20</th>
<th style="text-align: left;">P@1</th>
<th style="text-align: left;">MRR@100</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Query-only</td>
<td style="text-align: left;">$0.1304(-3 \%)$</td>
<td style="text-align: left;">$0.1043(-21 \%)$</td>
<td style="text-align: left;">$0.0852(-26 \%)$</td>
<td style="text-align: left;">$0.1764(-4 \%)$</td>
<td style="text-align: left;">$0.2402(-12 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">LSTM-seq2seq</td>
<td style="text-align: left;">$0.1018 \ddagger(-24 \%)$</td>
<td style="text-align: left;">$0.0899 \ddagger(-31 \%)$</td>
<td style="text-align: left;">$0.0745 \ddagger(-35 \%)$</td>
<td style="text-align: left;">$0.1409 \ddagger(-23 \%)$</td>
<td style="text-align: left;">$0.2131 \ddagger(-22 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Transformer-seq2seq</td>
<td style="text-align: left;">$0.1124(-16 \%)$</td>
<td style="text-align: left;">$0.1040 \ddagger(-21 \%)$</td>
<td style="text-align: left;">$0.0847 \ddagger(-26 \%)$</td>
<td style="text-align: left;">$0.1559 \ddagger(-15 \%)$</td>
<td style="text-align: left;">$0.2309 \ddagger(-15 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">$U S i$</td>
<td style="text-align: left;">$0.1355(+1 \%)$</td>
<td style="text-align: left;">$0.1289 \dagger(-2 \%)$</td>
<td style="text-align: left;">$0.1133 \dagger(-2 \%)$</td>
<td style="text-align: left;">$0.1862(+1 \%)$</td>
<td style="text-align: left;">$0.2730 \dagger(+0 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">Human (Oracle)</td>
<td style="text-align: left;">0.1343</td>
<td style="text-align: left;">$0.1312 \dagger$</td>
<td style="text-align: left;">$0.1154 \dagger$</td>
<td style="text-align: left;">0.1839</td>
<td style="text-align: left;">$0.2725 \dagger$</td>
</tr>
</tbody>
</table>
<p>Table 7: Qualitative analysis of answers generated by user simulator $U S i$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Information need</th>
<th style="text-align: left;">Clarifying question</th>
<th style="text-align: left;">Generated answer</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">How do you treat severe <br> angular cheilitis</td>
<td style="text-align: left;">are you looking for the <br> definition of angular cheilitis?</td>
<td style="text-align: left;">no i want to know <br> about treatment</td>
</tr>
<tr>
<td style="text-align: left;">What home remedies are <br> there for angular cheilitis</td>
<td style="text-align: left;">are you looking for the <br> definition of angular cheilitis?</td>
<td style="text-align: left;">no i am looking for <br> home remedies</td>
</tr>
<tr>
<td style="text-align: left;">Find a list of hobby stores <br> in Maryland</td>
<td style="text-align: left;">is this for a school <br> craft or play?</td>
<td style="text-align: left;">no i want to find a list <br> of hobby stores in maryland</td>
</tr>
<tr>
<td style="text-align: left;">What hobby stores <br> carry trains</td>
<td style="text-align: left;">what location do you want <br> to find a hobby store in?</td>
<td style="text-align: left;">i want to know where i <br> can purchase trains</td>
</tr>
<tr>
<td style="text-align: left;">What hobby stores <br> carry trains</td>
<td style="text-align: left;">is this a hobby <br> for an adult?</td>
<td style="text-align: left;">yes</td>
</tr>
</tbody>
</table>
<p>user simulator $U S i$ is initialised with the information need description text. Given an initial query (omitted in the table for space), the conversational search system asks a clarifying question to elucidate $U S i$ 's intent. Then, $U S i$ generates the answer to the prompted question. The information need and the questions for these examples are taken from ClariQ development set. We note that most of the TREC-style datasets contain the information need (facet/topic) description alongside the initial query. Thus, our simulated user can help in evaluation of conversational search systems on any of such datasets, as it only requires the information need description as initialisation. Then, the system we aim to evaluate can produce clarifying questions and receive answers from $U S i$.</p>
<p>First two examples in the Table 7 initialise $U S i$ with different information needs. However, given the same initial query "How to cure angular cheilitis" and the same prompted clarifying question, $U S i$ answers differently, in line with the actual information need for each of the cases. In the last three rows the table, we have different information needs for one broad topic of hobby stores. Given the initial query "I'm looking for information on hobby stores", $U S i$ again answers questions in line with the underlying information need. We notice that the text produced by our GPT-2-based user simulator is coherent and fluent, and, in given examples, indeed in line with the underlying information need. Moreover, $U S i$ is not bound by answering the question in a "yes" or "no" fashion, but can rather produce various types of answers and even express its uncertainty (e.g., "I don't know").
Multi-turn analysis. We perform initial case study on the multiturn variant of $U S i$. While the initial analysis of multi-turn conversations suggests that usefulness and naturalness of single-turn interactions transfer into a multi-turn setting, additional evaluation is needed to strongly support that claim. Thus, future work includes a pair-wise comparison of multi-turn conversations, inspired by ACUTE-Eval [28].</p>
<p>Moreover, we aim to observe user simulator behaviour in unexpected, edge case scenarios. For example, initial analysis of the created multi-turn dataset showed that humans tend to repeat their
previous answer when the clarifying question is off-topic or repeated. Similarly, our multi-turn $U S i$ has been observed to generate answers such as "I already told you what I'm looking for" when prompted with a repeated question. However, such edge cases tend to confuse the multi-turn model, which leads to higher presence of hallucination than in the single-turn variation. This means that the user simulator drifts off the topic of the conversation and starts generating answers that are not not inline with the actual information need. This effect is well-documented in recent literature on text generation [18] and should be approached carefully. Although edge cases are as well present in the acquired dataset, the GPT2based model needs additional mechanisms in order to simulate the behaviour of users in such cases. We leave deeper analysis of the topic for future research.</p>
<h2>8 CONCLUSIONS</h2>
<p>In this paper, we have proposed a user simulator $U S i$ for alleviating evaluation of mixed-initiative conversational search systems. More specifically, we demonstrated the feasibility of substituting expensive and time-consuming user studies with scalable and inexpensive user simulator. Through a number of experiments, including automated metrics and crowdsourcing studies, we showed $U S i$ 's capabilities in generating fluent and accurate answers to clarifying questions prompted by the search system. In fact, a crowdsourcing study of answer usefulness and naturalness showed that answers generated by $U S i$ tied with human-generated answers in $51 \%$ and $45 \%$ of cases, respectively. Moreover, we demonstrated the positive impact of generated answers on the performance of the retrieval model of the conversational search system, as the performance significantly increased when the answers to clarifying questions were taken into account.</p>
<p>Furthermore, we acquired an additional dataset for the training of the multi-turn model. Specifically, we utilise crowdsourcing workers to gather multi-turn question-answer interaction about certain topics, where one worker takes the role of a search system and asks question, while the other worker responds to them. Finally, we performed qualitative analysis of answers generated by $U S i$. We publicly release the code and the datasets for future research.
Acknowledgements. This work was supported in part by the NWO Innovational Research Incentives Scheme Vidi (016.Vidi.189.039).</p>
<h2>REFERENCES</h2>
<p>[1] Mohammad Aliannejadi, Julia Kiseleva, Aleksandr Chuklin, Jeff Dalton, and Mikhail Burtsev. 2020. ConvAD: Generating Clarifying Questions for OpenDomain Dialogue Systems (ClariQ). (2020).</p>
<p>[2] Mohammad Aliannejadi, Julia Kiseleva, Aleksandr Chuklin, Jeff Dalton, and Mikhail S. Burtsev. 2021. Building and Evaluating Open-Domain Dialogue Corpora with Clarifying Questions. In EMNLP. 4473-4484.
[3] Mohammad Aliannejadi, Hamed Zamani, Fabio Crestani, and W Bruce Croft. 2019. Asking clarifying questions in open-domain information-seeking conversations. In SIGIR. 475-480.
[4] Avishek Anand, Lawrence Cavedon, Hideo Joho, Mark Sanderson, and Benno Stein. 2020. Conversational Search (Dagstuhl Seminar 19461). In Dagstuhl Reports, Vol. 9. Schloss Dagstuhl-Leibniz-Zentrum für Informatik.
[5] Leif Azzopardi. 2011. The economics in interactive information retrieval. In SIGIR. ACM, 15-24.
[6] Anja Belz and Ehud Reiter. 2006. Comparing automatic and human evaluation of NLG systems. In ACL.
[7] Guorui Bian, Michael McAleer, and Wing-Keung Wong. 2011. A trinomial test for paired data when there are many ties. Mathematics and Computers in Simulation 81, 6 (2011), 1153-1160.
[8] Alan W Black, Susanne Burger, Alistair Conkie, Helen Hastie, Simon Keizer, Oliver Lemon, Nicolas Merigaud, Gabriel Parent, Gabriel Schubiner, Blaise Thomson, et al. 2011. Spoken dialog challenge 2010: Comparison of live and control test results. In SIGDIAL. 2-7.
[9] Pavel Braslavski, Denis Savenkov, Eugene Agichtein, and Alina Dubatovka. 2017. What Do You Mean Exactly?: Analyzing Clarification Questions in CQA. In CHIR.
[10] Paweł Budzianowski and Ivan Vulic. 2019. Hello, It's GPT-2-How Can I Help You? Towards the Use of Pretrained Language Models for Task-Oriented Dialogue Systems. EMNLP-IJCNLP 2019 (2019), 15.
[11] Chris Callison-Burch, Miles Osborne, and Philipp Koehn. 2006. Re-evaluating the role of BLEU in machine translation research. In ACL.
[12] Ben Carterette, Evangelos Kanoulas, and Emine Yilmaz. 2011. Simulating simple user behavior for system effectiveness evaluation. In CIKM. 611-620.
[13] Aleksandr Chuklin, Aliaksei Severyn, Johanne R Trippas, Enrique Alfonsesca, Hanna Silen, and Damiano Spina. 2019. Using audio transformations to improve comprehension in voice question answering. In CLEP. Springer, 164-170.
[14] Michael D Cooper. 1973. A simulation model of an information retrieval system. Information Storage and Retrieval 9, 1 (1973), 13-32.
[15] Jeffrey Dalton, Chenyun Xiong, and Jamie Callan. 2020. TREC CAsT 2019: The conversational assistance track overview. arXiv preprint arXiv:2003.13624 (2020).
[16] Jan Deriu, Alvaro Rodrigo, Arantxa Otegi, Guillermo Echegoyen, Sophie Rosset, Eneko Agitre, and Mark Cieliebak. 2021. Survey on evaluation methods for dialogue systems. Artificial Intelligence Review 54, 1 (2021), 755-810.
[17] Fernando Diaz and Jaime Arguello. 2009. Adaptation of offline vertical selection predictions in the presence of user feedback. In SIGIR. 323-330.
[18] Nouha Dziri, Andrea Madotto, Osmar Zaiane, and Avishek Joey Bose. 2021. Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding. arXiv preprint arXiv:2104.08455 (2021).
[19] Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. arXiv preprint arXiv:1805.04833 (2018).
[20] Michael D Gordon. 1990. Evaluating the effectiveness of information retrieval systems using simulated queries. Journal of the American Society for Information Science 41, 5 (1990), 313-325.
[21] José-Marie Griffiths. 1976. The computer simulation of information retrieval systems. Ph.D. Dissertation. University of London (University College).
[22] Helja Hashemi, Hamed Zamani, and W Bruce Croft. 2020. Guided Transformer: Leveraging Multiple External Sources for Representation Learning in Conversational Search. In SIGIR. 1131-1140.
[23] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751 (2019).
[24] Eric Horvitz. 1999. Principles of mixed-initiative user interfaces. In CHI. 159166.
[25] Gaya K Jayasinghe, William Webber, Mark Sanderson, Lasitha S Dharmasena, and J Shane Culpepper. 2015. Statistical comparisons of non-deterministic IR systems using two dimensional variance. Information Processing \&amp; Management 51, 5 (2015), 677-694.
[26] Johannes Kiesel, Arefeh Bahrami, Benno Stein, Avishek Anand, and Matthias Hagen. 2018. Toward voice query clarification. In SIGIR. 1257-1260.
[27] Wojciech Kryecinski, Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Neural Text Summarization: A Critical Evaluation. In EMNLP-IJCNLP. 540-551.
[28] Margaret Li, Jason Weston, and Stephen Roller. 2019. Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons. arXiv preprint arXiv:1909.03087 (2019).
[29] Chin-Tew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out. 74-81.
[30] Aldo Lipani, Ben Carterette, and Emine Yilmaz. 2021. How Am I Doing?: Evaluating Conversational Search Systems Offline. ACM TOD (2021).
[31] Tom Lotze, Stefan Klut, Mohammad Aliannejadi, and Evangelos Kanoulas. 2021. Ranking Clarifying Questions Based on Predicted User Engagement. CoRR abs/2103.06192 (2021).
[32] Javed Mostafa, Snehasis Mukhopadhyay, and Mathew Palakal. 2003. Simulation studies of different dimensions of users' interests and their impact on user modeling and information filtering. Information Retrieval 6, 2 (2003), 199-223.
[33] Jekaterina Novikova, Ondřej Dušek, Amanda Cercas Curry, and Verena Rieser. 2017. Why We Need New Evaluation Metrics for NLG. In EMNLP. 2241-2252.
[34] Kishore Papiнеш, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In ACL. 311-318.
[35] Baolin Peng, Chenguang Zhu, Chianyuan Li, Xiujun Li, Jinshao Li, Michael Zeng, and Jianfeng Gao. 2020. Few-shot natural language generation for task-oriented dialog. arXiv preprint arXiv:2002.12328 (2020).
[36] Chen Qu, Liu Yang, W Bruce Croft, Yongfeng Zhang, Johanne R Trippas, and Minghui Qiu. 2019. User intent prediction in information-seeking conversations. In CHIR. 25-33.
[37] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language Models are Unsupervised Multitask Learners. (2019).
[38] Filip Radlinski and Nick Craswell. 2017. A theoretical framework for conversational search. In CHIR. 117-126.
[39] Sudha Rao and Hal Daumé. 2018. Learning to Ask Good Questions: Ranking Clarification Questions using Neural Expected Value of Perfect Information. In ACL (1). 2736-2745.
[40] Sudha Rao and Hal Daumé III. 2019. Answer-based Adversarial Training for Generating Clarification Questions. arXiv:1904.02281 (2019).
[41] Gary Ren, Xiaochuan Ni, Manish Malik, and Qifa Ke. 2018. Conversational Query Understanding Using Sequence to Sequence Modeling. In WWW. 17151724.
[42] Pengjie Ren, Zhumin Chen, Zhaochun Ren, Evangelos Kanoulas, Christof Monz, and Maarten de Rijke. 2020. Conversations with Search Engines. ACM Transactions on Information Systems 1, 1 (2020).
[43] Corbin Rosset, Chenyan Xiong, Xia Song, Daniel Campos, Nick Craswell, Saurabh Tiwary, and Paul Bennett. 2020. Leading conversational search by suggesting useful questions. In TheWebConference. 1160-1170.
[44] Alexandre Salle, Shervin Malmasi, Oleg Rokhlenko, and Eugene Agichtein. 2021. Studying the Effectiveness of Conversational Search Refinement Through User Simulation. In ECIR. 587-602.
[45] Abigail See, Stephen Roller, Douwe Kiela, and Jason Weston. 2019. What makes a good conversation? How controllable attributes affect human judgments. In NAACL. 1702-1723.
[46] Ivan Sekulic, Mohammad Aliannejadi, and Fabio Crestani. 2020. Extending the Use of Previous Relevant Utterances for Response Ranking in Conversational Search. In Proceedings of the Twenty-Ninth Text REtrieval Conference, TREC.
[47] Ivan Sekulic, Mohammad Aliannejadi, and Fabio Crestani. 2021. Towards FacetDriven Generation of Clarifying Questions for Conversational Search. In ICTIR.
[48] Ivan Sekulic, Mohammad Aliannejadi, and Fabio Crestani. 2021. User Engagement Prediction for Clarification in Search. In ECIR (1). 619-633.
[49] Ivan Sekulic, Mohammad Aliannejadi, and Fabio Crestani. 2022. Exploiting Document-based Features for Clarification in Conversational Search. In ECIR.
[50] Ivan Sekulic, Amir Soleimani, Mohammad Aliannejadi, and Fabio Crestani. 2020. Longformer for MS MARCO Document Re-ranking Task. arXiv preprint arXiv:2009.09392 (2020).
[51] Yuandong Shao, Stephan Gouws, Denny Britz, Anna Goldie, Brian Strope, and Ray Kurzweil. 2017. Generating High-Quality and Informative Conversation Responses with Sequence-to-Sequence Models. In EMNLP. 2210-2219.
[52] Amanda Steut, Matthew Marge, and Mohit Singhai. 2005. Evaluating evaluation methods for generation in the presence of variation. In CICLing. 341-351.
[53] Weiwei Sun, Shuo Zhang, Krisztian Balog, Zhaochun Ren, Pengjie Ren, Zhumin Chen, and Maarten de Rijke. 2021. Simulating User Satisfaction for the Evaluation of Task-oriented Dialogue Systems. arXiv preprint arXiv:2105.03748 (2021).
[54] Yueming Sun and Yi Zhang. 2018. Conversational Recommender System. In SIGIR. 235-244.
[55] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to Sequence Learning with Neural Networks. NeurIPS 27 (2014), 3104-3112.
[56] Jean Tague, Michael Nelson, and Harry Wu. 1980. Problems in the simulation of bibliographic retrieval systems. In SIGIR. 236-255.
[57] Zhiliang Tian, Rui Yan, Lili Mou, Yiping Song, Yansong Feng, and Dongyan Zhao. 2017. How to Make Context More Useful? An Empirical Study on Context-Aware Neural Conversational Models. In ACL (2). 231-236.
[58] Svillana Vakulenko, Nikos Voskarakos, Zhucheng Yu, and Shayne Longgee. 2021. A Comparison of Question Rewriting Methods for Conversational Passage Retrieval. In ECIR.
[59] Ashish Vazerani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. arXiv preprint arXiv:1706.05762 (2017).
[60] Yansen Wang, Chenyi Liu, Minlie Huang, and Liqiang Nie. 2018. Learning to Ask Questions in Open-domain Conversational Systems with Typed Decoders. In ACL (1). 2193-2203.
[61] Rui Yan, Yiping Song, and Hua Wu. 2016. Learning to Respond with Deep Neural Networks for Retrieval-Based Human-Computer Conversation System. In SIGIR. $55-64$.</p>
<p>[62] Grace Hui Yang and Ian Soboroff. 2016. TREC 2016 Dynamic Domain Track Overview. In TREC.
[63] Hamed Zamani, Susan Dumais, Nick Craswell, Paul Bennett, and Gord Lueck. 2020. Generating clarifying questions for information retrieval. In TheWebConference. $418-428$.
[64] Shuo Zhang and Krisztian Balog. 2020. Evaluating Conversational Recommender Systems via User Simulation. In KDD. 1512-1520.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ Another point-of-view would be to test for equivalent effectiveness Jayasinghe et al. [25], however, we refrain from it since it does not take ties into account.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
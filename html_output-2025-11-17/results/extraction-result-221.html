<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-221 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-221</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-221</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-11.html">extraction-schema-11</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <p><strong>Paper ID:</strong> paper-6d7b8a478801bd9d21df82d5f33ae6eced90da5e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6d7b8a478801bd9d21df82d5f33ae6eced90da5e" target="_blank">Solving math word problems with process- and outcome-based feedback</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work runs the first comprehensive comparison between process- and outcome-based approaches trained on a natural language task, GSM8K, and finds that pure outcome- based supervision produces similar final-answer error rates with less label supervision.</p>
                <p><strong>Paper Abstract:</strong> Recent work has shown that asking language models to generate reasoning steps improves performance on many reasoning tasks. When moving beyond prompting, this raises the question of how we should supervise such models: outcome-based approaches which supervise the final result, or process-based approaches which supervise the reasoning process itself? Differences between these approaches might naturally be expected not just in final-answer errors but also in reasoning errors, which can be difficult to detect and are problematic in many real-world domains such as education. We run the first comprehensive comparison between process- and outcome-based approaches trained on a natural language task, GSM8K. We find that pure outcome-based supervision produces similar final-answer error rates with less label supervision. However, for correct reasoning steps we find it necessary to use process-based supervision or supervision from learned reward models that emulate process-based feedback. In total, we improve the previous best results from 16.8% $\to$ 12.7% final-answer error and 14.0% $\to$ 3.4% reasoning error among final-answer-correct solutions.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e221.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e221.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SFT (GSM8K traces)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Supervised fine-tuning on human-written GSM8K reasoning traces</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Supervised finetuning of the base LM on full, human-written step-by-step reasoning traces from the GSM8K dataset; used as a process-based supervision signal that trains the model to generate complete reasoning traces (not just final answers).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Our Base-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>SFT</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>reasoning / math word problem QA</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Human-written chain-of-thought reasoning traces (full-step solutions) from GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>7118 training examples (after splitting out 256 validation; 1319 test examples)</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>Natural-language step-by-step solutions, human-verified correctness of steps in reference solutions, dense token-level supervision per example (hundreds of tokens per question)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>final-answer error rate (exact match on integer answers) and trace error rate (human-annotated incorrect steps among final-answer-correct solutions)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Majority voting decoding: final-answer error 22.3%, trace error 11.4%; RM-weighted decoding (ORM): final-answer error 14.8%, trace error 4.4%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>Few-shot (5-shot) majority voting final-answer error 41.5%; Few-shot+Final-Answer RL majority voting final-answer error 23.5%, trace error 19.8%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>Compared to few-shot majority voting, SFT majority voting improves final-answer error by ~19.2 percentage points (41.5% -> 22.3%); RM reranking on SFT lowers final-answer error further by 7.5 pp (22.3% -> 14.8%) and trace error by ~7 pp (11.4% -> 4.4%).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Process-based SFT on GSM8K yields similar final-answer error to outcome-focused approaches once RMs are used, but substantially lower trace error (more human-correct intermediate steps); RM reranking further reduces trace error to below ~5%.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Solving math word problems with process- and outcome-based feedback', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e221.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e221.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Final-Answer RL (Outcome-based RL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement learning that optimizes for final-answer correctness (outcome-based RL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RL policy-improvement procedure that selects/rewards full-solution samples based solely on whether the final answer matches the reference; corresponds to outcome-based supervision in the paper (also referred to as STaR / Self-taught Reasoner in prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Our Base-70B (few-shot or SFT initializations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>RL</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>reasoning / math word problem QA</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Filtered model-generated full-solution trajectories labeled by final-answer correctness (binary outcome labels); requires only final-answer labels per question</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>Uses K=96 samples per problem for selection during policy improvement; label supervision per question is 1-4 tokens (final answer) rather than full traces</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>Very label-efficient (only final answer per question); weak/aggregate supervision over multi-step trace; can include many incorrect intermediate steps as long as final answer is correct</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>final-answer error rate and trace error rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Few-shot+Final-Answer RL majority voting: final-answer error 23.5%, trace error 19.8%; SFT+Final-Answer RL majority voting: final-answer error 20.2%, trace error 12.1%; with ORM reranking, Few-shot+Final-Answer RL final 16.6%, trace 12.4%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>Few-shot majority voting final-answer error 41.5%; SFT majority voting final 22.3%, trace 11.4%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>From few-shot baseline to Few-shot+Final-Answer RL: final-answer error reduced from 41.5% -> 23.5% (18.0 pp); however trace error often increases relative to process-based SFT (e.g., Few-shot+Final-Answer RL trace 19.8% vs SFT 11.4%).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Optimizing directly for final-answer correctness is label-efficient and reduces final-answer error vs few-shot, but tends to yield higher trace error (worse intermediate-step correctness) unless combined with a reward model that emulates process-based feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Solving math word problems with process- and outcome-based feedback', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e221.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e221.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ORM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Outcome-supervised Reward Model (ORM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reward model (verifier) trained with outcome labels that predicts a binary 'correct'/'incorrect' label for each step where the label indicates whether the full-sample final answer matches the reference; used for reranking and as an RL reward.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Training verifiers to solve math word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Reward Model (initialized from SFT or base LM, evaluated with Our Base-70B policy)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B (architecture consistent with base LM; paper reuses SFT parameters when available)</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>multiple</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>reasoning / math word problem QA (verifier/reranker)</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Model-generated solution samples labeled by final-answer correctness (outcome labels); training uses K=96 sampled completions per problem</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>Trained on samples from policy; K=96 samples per problem used to form training set; exact total examples depends on policy and iterations (not summarized as a single number)</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>Outcome-based binary labels per step (derived from full-solution final-answer correctness), regularized with dropout; often initialized from SFT parameters to speed learning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>agreement with PRM labels, final-answer error and trace error when used for reranking or RL</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Using ORM for reranking with SFT: final-answer error falls from 22.3% -> 14.8% and trace error from 11.4% -> 4.4%; ORM-RL (training with ORM as reward) + SFT leads to final-answer error 12.7% and trace error 3.4% (best trace+final combination).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>SFT without RM reranking: final 22.3%, trace 11.4%; Final-Answer RL without RM: higher trace errors (e.g., Few-shot+Final-Answer RL trace 19.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>ORM reranking yields large reductions in final-answer error (~7.5 pp on SFT) and substantial trace-error reductions (~7 pp on SFT); using ORM as an RL reward further improves both metrics (final ~12.7%, trace ~3.4%).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Although trained only on final-answer outcome labels, the ORM tends to learn signals that align closely with stepwise (process) correctness and can emulate process-based feedback, enabling large gains in trace correctness when used for reranking or as an RL objective.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Solving math word problems with process- and outcome-based feedback', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e221.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e221.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PRM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Process-supervised Reward Model (PRM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reward model trained on human step-level annotations that indicate whether the steps-so-far are correct; used for reranking and as a reward during RL to encourage correct intermediate steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Reward Model (PRM initialized from ORM parameters, applied with Our Base-70B policy)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B (initialized from ORM parameters in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>multiple</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>reasoning / math word problem QA (verifier/reranker)</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Human-annotated stepwise correctness labels for model-generated solutions (first-major-mistake labeling), giving binary 'steps-so-far correct' labels per step</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>1560 model samples across 530 training problems (after cleaning), corresponding to 9856 step-level binary labels; validation: 162 samples, 913 steps; evaluation used 200 problems per model with duplicate labeling</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>Human judgments of the first major mistake per sample; labels mark all steps before first major mistake as 'correct' and subsequent steps as 'incorrect'; focused annotation budget on problems where SFT majority prediction was incorrect</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>final-answer error and trace error when used for reranking or as RL reward; also used to measure RM vs RM agreement</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>SFT with PRM reranking: final-answer error 14.1%, trace error 3.5%; SFT+PRM-RL with PRM reranking: final 12.9%, trace 3.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>SFT without PRM: final 22.3%, trace 11.4%; SFT with ORM reranking: final 14.8%, trace 4.4%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>PRM reranking reduces trace error from SFT baseline 11.4% -> 3.5% (â‰ˆ7.9 pp reduction) while also reducing final-answer error (22.3% -> 14.1%).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Direct process-supervised reward models trained on human step-level labels produce the lowest trace errors (~3.5%) and comparable final-answer error to ORM-based approaches; PRMs are effective but require human step annotations (higher annotation cost).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Solving math word problems with process- and outcome-based feedback', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e221.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e221.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RM reranking / verifier-voting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reward-model-based reranking (RM-weighted decoding / verifier-voting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Decode by sampling many (K=96) full-solution candidates and select the best sample/final answer by aggregating or weighting by RM scores (ORM or PRM); an effective decoding-time use of reward models to improve both final-answer and trace accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Our Base-70B with ORM or PRM reranker</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>multiple</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>reasoning / math word problem QA (decoding/selection)</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>K=96 model-generated full-solution samples per problem scored by a reward model trained on either outcome (ORM) or process (PRM) labels</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>96 candidate samples per test problem used at decode-time; RM training data comes from ORM or PRM descriptions above</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>Aggregates RM probabilities across samples sharing the same final answer to select an answer (RM-weighted voting); relies on exact string matching of final answers for voting tie-breaking</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>final-answer error and trace error</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>For SFT + ORM reranking: final-answer error 14.8%, trace error 4.4%; SFT + PRM reranking: final 14.1%, trace 3.5%; selective prediction with RM thresholds reduces final-answer error further (e.g., abstaining on 30% yields final-answer error 2.7% for SFT+PRM).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>Majority-vote decoding without RM for SFT: final 22.3%, trace 11.4%; greedy/ single-sample decoding much worse.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>RM reranking produces substantial lifts: e.g., SFT final-error drops ~7.5 pp (22.3% -> 14.8%) and trace error drops ~7.0 pp (11.4% -> 4.4%) with ORM; PRM yields similar or slightly better trace reduction.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Using RMs at decoding (RM-weighted reranking) substantially improves final-answer and trace metrics versus majority voting; RMs trained on outcome labels often emulate process correctness sufficiently to provide much of the benefit of explicit process supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Solving math word problems with process- and outcome-based feedback', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e221.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e221.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of training data types and their impact on model performance during supervised fine-tuning (SFT), direct preference optimization (DPO), or reinforcement learning (RL) stages, especially for question-answering tasks over scientific literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot + ORM-RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot initialization + RL optimizing an outcome-trained reward model (ORM-RL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that starts from a few-shot prompted base LM (no finetuning on traces) and performs RL where the reward is provided by an ORM (trained on outcome labels), combining label-efficiency of few-shot with RM-driven policy improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Our Base-70B (few-shot initial policy)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>training_stage</strong></td>
                            <td>RL</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>reasoning / math word problem QA</td>
                        </tr>
                        <tr>
                            <td><strong>is_scientific_domain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Few-shot prompt demonstrations for initial policy; ORM (outcome-labeled RM) trained on model samples used as reward; no human step traces used</td>
                        </tr>
                        <tr>
                            <td><strong>data_size</strong></td>
                            <td>Few-shot uses 5 prompt examples; ORM trained using K=96 samples per problem from the policy (total RM training examples depend on iterations)</td>
                        </tr>
                        <tr>
                            <td><strong>data_properties</strong></td>
                            <td>Extremely label-light upstream supervision (only a few-shot prompt), leverages RM to provide richer learning signal; avoids human step labels</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>final-answer error and trace error</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_data</strong></td>
                            <td>Few-shot+ORM-RL with ORM reranking: final-answer error 13.8%, trace error 5.5% (ORM-RL improves trace error from 12.4% with ORM-reranked Final-Answer RL down to 5.5%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_baseline</strong></td>
                            <td>Few-shot+Final-Answer RL with ORM reranking: final 16.6%, trace 12.4%; few-shot majority voting baseline final 41.5%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_lift</strong></td>
                            <td>ORM-RL yields large improvements over few-shot baselines (final-answer error reduced from 41.5% -> 13.8% with ORM-RL+reranking) and reduces trace error substantially compared to Final-Answer RL by using RM as a proxy for process correctness (e.g., trace 12.4% -> 5.5%).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_data_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Starting from few-shot prompts, optimizing against an ORM (rather than direct final-answer correctness) substantially improves both final-answer and trace error, implying outcome-trained RMs can emulate process-based supervision enough to guide RL toward more correct traces.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Solving math word problems with process- and outcome-based feedback', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Training verifiers to solve math word problems. <em>(Rating: 2)</em></li>
                <li>Star: Bootstrapping reasoning with reasoning. <em>(Rating: 2)</em></li>
                <li>WebGPT: Browser-assisted question-answering with human feedback. <em>(Rating: 1)</em></li>
                <li>Solving quantitative reasoning problems with language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-221",
    "paper_id": "paper-6d7b8a478801bd9d21df82d5f33ae6eced90da5e",
    "extraction_schema_id": "extraction-schema-11",
    "extracted_data": [
        {
            "name_short": "SFT (GSM8K traces)",
            "name_full": "Supervised fine-tuning on human-written GSM8K reasoning traces",
            "brief_description": "Supervised finetuning of the base LM on full, human-written step-by-step reasoning traces from the GSM8K dataset; used as a process-based supervision signal that trains the model to generate complete reasoning traces (not just final answers).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Our Base-70B",
            "model_size": "70B",
            "training_stage": "SFT",
            "task_type": "reasoning / math word problem QA",
            "is_scientific_domain": false,
            "data_type": "Human-written chain-of-thought reasoning traces (full-step solutions) from GSM8K",
            "data_size": "7118 training examples (after splitting out 256 validation; 1319 test examples)",
            "data_properties": "Natural-language step-by-step solutions, human-verified correctness of steps in reference solutions, dense token-level supervision per example (hundreds of tokens per question)",
            "performance_metric": "final-answer error rate (exact match on integer answers) and trace error rate (human-annotated incorrect steps among final-answer-correct solutions)",
            "performance_with_data": "Majority voting decoding: final-answer error 22.3%, trace error 11.4%; RM-weighted decoding (ORM): final-answer error 14.8%, trace error 4.4%",
            "performance_baseline": "Few-shot (5-shot) majority voting final-answer error 41.5%; Few-shot+Final-Answer RL majority voting final-answer error 23.5%, trace error 19.8%",
            "performance_lift": "Compared to few-shot majority voting, SFT majority voting improves final-answer error by ~19.2 percentage points (41.5% -&gt; 22.3%); RM reranking on SFT lowers final-answer error further by 7.5 pp (22.3% -&gt; 14.8%) and trace error by ~7 pp (11.4% -&gt; 4.4%).",
            "compares_data_types": true,
            "key_finding": "Process-based SFT on GSM8K yields similar final-answer error to outcome-focused approaches once RMs are used, but substantially lower trace error (more human-correct intermediate steps); RM reranking further reduces trace error to below ~5%.",
            "uuid": "e221.0",
            "source_info": {
                "paper_title": "Solving math word problems with process- and outcome-based feedback",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Final-Answer RL (Outcome-based RL)",
            "name_full": "Reinforcement learning that optimizes for final-answer correctness (outcome-based RL)",
            "brief_description": "An RL policy-improvement procedure that selects/rewards full-solution samples based solely on whether the final answer matches the reference; corresponds to outcome-based supervision in the paper (also referred to as STaR / Self-taught Reasoner in prior work).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Our Base-70B (few-shot or SFT initializations)",
            "model_size": "70B",
            "training_stage": "RL",
            "task_type": "reasoning / math word problem QA",
            "is_scientific_domain": false,
            "data_type": "Filtered model-generated full-solution trajectories labeled by final-answer correctness (binary outcome labels); requires only final-answer labels per question",
            "data_size": "Uses K=96 samples per problem for selection during policy improvement; label supervision per question is 1-4 tokens (final answer) rather than full traces",
            "data_properties": "Very label-efficient (only final answer per question); weak/aggregate supervision over multi-step trace; can include many incorrect intermediate steps as long as final answer is correct",
            "performance_metric": "final-answer error rate and trace error rate",
            "performance_with_data": "Few-shot+Final-Answer RL majority voting: final-answer error 23.5%, trace error 19.8%; SFT+Final-Answer RL majority voting: final-answer error 20.2%, trace error 12.1%; with ORM reranking, Few-shot+Final-Answer RL final 16.6%, trace 12.4%",
            "performance_baseline": "Few-shot majority voting final-answer error 41.5%; SFT majority voting final 22.3%, trace 11.4%",
            "performance_lift": "From few-shot baseline to Few-shot+Final-Answer RL: final-answer error reduced from 41.5% -&gt; 23.5% (18.0 pp); however trace error often increases relative to process-based SFT (e.g., Few-shot+Final-Answer RL trace 19.8% vs SFT 11.4%).",
            "compares_data_types": true,
            "key_finding": "Optimizing directly for final-answer correctness is label-efficient and reduces final-answer error vs few-shot, but tends to yield higher trace error (worse intermediate-step correctness) unless combined with a reward model that emulates process-based feedback.",
            "uuid": "e221.1",
            "source_info": {
                "paper_title": "Solving math word problems with process- and outcome-based feedback",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "ORM",
            "name_full": "Outcome-supervised Reward Model (ORM)",
            "brief_description": "A reward model (verifier) trained with outcome labels that predicts a binary 'correct'/'incorrect' label for each step where the label indicates whether the full-sample final answer matches the reference; used for reranking and as an RL reward.",
            "citation_title": "Training verifiers to solve math word problems.",
            "mention_or_use": "use",
            "model_name": "Reward Model (initialized from SFT or base LM, evaluated with Our Base-70B policy)",
            "model_size": "70B (architecture consistent with base LM; paper reuses SFT parameters when available)",
            "training_stage": "multiple",
            "task_type": "reasoning / math word problem QA (verifier/reranker)",
            "is_scientific_domain": false,
            "data_type": "Model-generated solution samples labeled by final-answer correctness (outcome labels); training uses K=96 sampled completions per problem",
            "data_size": "Trained on samples from policy; K=96 samples per problem used to form training set; exact total examples depends on policy and iterations (not summarized as a single number)",
            "data_properties": "Outcome-based binary labels per step (derived from full-solution final-answer correctness), regularized with dropout; often initialized from SFT parameters to speed learning",
            "performance_metric": "agreement with PRM labels, final-answer error and trace error when used for reranking or RL",
            "performance_with_data": "Using ORM for reranking with SFT: final-answer error falls from 22.3% -&gt; 14.8% and trace error from 11.4% -&gt; 4.4%; ORM-RL (training with ORM as reward) + SFT leads to final-answer error 12.7% and trace error 3.4% (best trace+final combination).",
            "performance_baseline": "SFT without RM reranking: final 22.3%, trace 11.4%; Final-Answer RL without RM: higher trace errors (e.g., Few-shot+Final-Answer RL trace 19.8%).",
            "performance_lift": "ORM reranking yields large reductions in final-answer error (~7.5 pp on SFT) and substantial trace-error reductions (~7 pp on SFT); using ORM as an RL reward further improves both metrics (final ~12.7%, trace ~3.4%).",
            "compares_data_types": true,
            "key_finding": "Although trained only on final-answer outcome labels, the ORM tends to learn signals that align closely with stepwise (process) correctness and can emulate process-based feedback, enabling large gains in trace correctness when used for reranking or as an RL objective.",
            "uuid": "e221.2",
            "source_info": {
                "paper_title": "Solving math word problems with process- and outcome-based feedback",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "PRM",
            "name_full": "Process-supervised Reward Model (PRM)",
            "brief_description": "A reward model trained on human step-level annotations that indicate whether the steps-so-far are correct; used for reranking and as a reward during RL to encourage correct intermediate steps.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Reward Model (PRM initialized from ORM parameters, applied with Our Base-70B policy)",
            "model_size": "70B (initialized from ORM parameters in experiments)",
            "training_stage": "multiple",
            "task_type": "reasoning / math word problem QA (verifier/reranker)",
            "is_scientific_domain": false,
            "data_type": "Human-annotated stepwise correctness labels for model-generated solutions (first-major-mistake labeling), giving binary 'steps-so-far correct' labels per step",
            "data_size": "1560 model samples across 530 training problems (after cleaning), corresponding to 9856 step-level binary labels; validation: 162 samples, 913 steps; evaluation used 200 problems per model with duplicate labeling",
            "data_properties": "Human judgments of the first major mistake per sample; labels mark all steps before first major mistake as 'correct' and subsequent steps as 'incorrect'; focused annotation budget on problems where SFT majority prediction was incorrect",
            "performance_metric": "final-answer error and trace error when used for reranking or as RL reward; also used to measure RM vs RM agreement",
            "performance_with_data": "SFT with PRM reranking: final-answer error 14.1%, trace error 3.5%; SFT+PRM-RL with PRM reranking: final 12.9%, trace 3.8%.",
            "performance_baseline": "SFT without PRM: final 22.3%, trace 11.4%; SFT with ORM reranking: final 14.8%, trace 4.4%.",
            "performance_lift": "PRM reranking reduces trace error from SFT baseline 11.4% -&gt; 3.5% (â‰ˆ7.9 pp reduction) while also reducing final-answer error (22.3% -&gt; 14.1%).",
            "compares_data_types": true,
            "key_finding": "Direct process-supervised reward models trained on human step-level labels produce the lowest trace errors (~3.5%) and comparable final-answer error to ORM-based approaches; PRMs are effective but require human step annotations (higher annotation cost).",
            "uuid": "e221.3",
            "source_info": {
                "paper_title": "Solving math word problems with process- and outcome-based feedback",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "RM reranking / verifier-voting",
            "name_full": "Reward-model-based reranking (RM-weighted decoding / verifier-voting)",
            "brief_description": "Decode by sampling many (K=96) full-solution candidates and select the best sample/final answer by aggregating or weighting by RM scores (ORM or PRM); an effective decoding-time use of reward models to improve both final-answer and trace accuracy.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Our Base-70B with ORM or PRM reranker",
            "model_size": "70B",
            "training_stage": "multiple",
            "task_type": "reasoning / math word problem QA (decoding/selection)",
            "is_scientific_domain": false,
            "data_type": "K=96 model-generated full-solution samples per problem scored by a reward model trained on either outcome (ORM) or process (PRM) labels",
            "data_size": "96 candidate samples per test problem used at decode-time; RM training data comes from ORM or PRM descriptions above",
            "data_properties": "Aggregates RM probabilities across samples sharing the same final answer to select an answer (RM-weighted voting); relies on exact string matching of final answers for voting tie-breaking",
            "performance_metric": "final-answer error and trace error",
            "performance_with_data": "For SFT + ORM reranking: final-answer error 14.8%, trace error 4.4%; SFT + PRM reranking: final 14.1%, trace 3.5%; selective prediction with RM thresholds reduces final-answer error further (e.g., abstaining on 30% yields final-answer error 2.7% for SFT+PRM).",
            "performance_baseline": "Majority-vote decoding without RM for SFT: final 22.3%, trace 11.4%; greedy/ single-sample decoding much worse.",
            "performance_lift": "RM reranking produces substantial lifts: e.g., SFT final-error drops ~7.5 pp (22.3% -&gt; 14.8%) and trace error drops ~7.0 pp (11.4% -&gt; 4.4%) with ORM; PRM yields similar or slightly better trace reduction.",
            "compares_data_types": true,
            "key_finding": "Using RMs at decoding (RM-weighted reranking) substantially improves final-answer and trace metrics versus majority voting; RMs trained on outcome labels often emulate process correctness sufficiently to provide much of the benefit of explicit process supervision.",
            "uuid": "e221.4",
            "source_info": {
                "paper_title": "Solving math word problems with process- and outcome-based feedback",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Few-shot + ORM-RL",
            "name_full": "Few-shot initialization + RL optimizing an outcome-trained reward model (ORM-RL)",
            "brief_description": "A pipeline that starts from a few-shot prompted base LM (no finetuning on traces) and performs RL where the reward is provided by an ORM (trained on outcome labels), combining label-efficiency of few-shot with RM-driven policy improvement.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Our Base-70B (few-shot initial policy)",
            "model_size": "70B",
            "training_stage": "RL",
            "task_type": "reasoning / math word problem QA",
            "is_scientific_domain": false,
            "data_type": "Few-shot prompt demonstrations for initial policy; ORM (outcome-labeled RM) trained on model samples used as reward; no human step traces used",
            "data_size": "Few-shot uses 5 prompt examples; ORM trained using K=96 samples per problem from the policy (total RM training examples depend on iterations)",
            "data_properties": "Extremely label-light upstream supervision (only a few-shot prompt), leverages RM to provide richer learning signal; avoids human step labels",
            "performance_metric": "final-answer error and trace error",
            "performance_with_data": "Few-shot+ORM-RL with ORM reranking: final-answer error 13.8%, trace error 5.5% (ORM-RL improves trace error from 12.4% with ORM-reranked Final-Answer RL down to 5.5%).",
            "performance_baseline": "Few-shot+Final-Answer RL with ORM reranking: final 16.6%, trace 12.4%; few-shot majority voting baseline final 41.5%.",
            "performance_lift": "ORM-RL yields large improvements over few-shot baselines (final-answer error reduced from 41.5% -&gt; 13.8% with ORM-RL+reranking) and reduces trace error substantially compared to Final-Answer RL by using RM as a proxy for process correctness (e.g., trace 12.4% -&gt; 5.5%).",
            "compares_data_types": true,
            "key_finding": "Starting from few-shot prompts, optimizing against an ORM (rather than direct final-answer correctness) substantially improves both final-answer and trace error, implying outcome-trained RMs can emulate process-based supervision enough to guide RL toward more correct traces.",
            "uuid": "e221.5",
            "source_info": {
                "paper_title": "Solving math word problems with process- and outcome-based feedback",
                "publication_date_yy_mm": "2022-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Training verifiers to solve math word problems.",
            "rating": 2
        },
        {
            "paper_title": "Star: Bootstrapping reasoning with reasoning.",
            "rating": 2
        },
        {
            "paper_title": "WebGPT: Browser-assisted question-answering with human feedback.",
            "rating": 1
        },
        {
            "paper_title": "Solving quantitative reasoning problems with language models",
            "rating": 2
        }
    ],
    "cost": 0.016117,
    "model_str": null
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Solving math word problems with processand outcome-based feedback</h1>
<p>Jonathan Uesato ${ }^{1^{<em>}}$, Nate Kushman ${ }^{1^{</em>}}$, Ramana Kumar ${ }^{1^{<em>}}$, Francis Song ${ }^{1}$, Noah Siegel ${ }^{1}$, Lisa Wang ${ }^{1}$, Antonia Creswell ${ }^{1}$, Geoffrey Irving ${ }^{1}$ and Irina Higgins ${ }^{1}$<br>${ }^{1}$ DeepMind, ${ }^{\text {</em> }}$ Equal contributions</p>
<h4>Abstract</h4>
<p>Recent work has shown that asking language models to generate reasoning steps improves performance on many reasoning tasks. When moving beyond prompting, this raises the question of how we should supervise such models: outcome-based approaches which supervise the final result, or process-based approaches which supervise the reasoning process itself? Differences between these approaches might naturally be expected not just in final-answer errors but also in reasoning errors, which can be difficult to detect and are problematic in many real-world domains such as education. We run the first comprehensive comparison between process- and outcome-based approaches trained on a natural language task, GSM8K. We find that pure outcome-based supervision produces similar final-answer error rates with less label supervision. However, for correct reasoning steps we find it necessary to use processbased supervision or supervision from learned reward models that emulate process-based feedback. In total, we improve the previous best results from $16.8 \% \rightarrow 12.7 \%$ final-answer error and $14.0 \% \rightarrow 3.4 \%$ reasoning error among final-answer-correct solutions.</p>
<h2>1. Introduction</h2>
<p>Recent work has shown that asking language models to use step-by-step reasoning improves performance on reasoning tasks (Shwartz et al., 2020; Nakano et al., 2021; Cobbe et al., 2021; Wei et al., 2022; Kojima et al., 2022; Lewkowycz et al., 2022). While these works have primarily focused on prompting language models, prior work suggests that finetuning should outperform prompting alone (Stiennon et al., 2020; Perez et al., 2021; Ouyang et al., 2022). This raises the question of how best to supervise such models. Two natural approaches are outcome-based approaches, which supervise the final result, and process-based approaches, which supervise each step of the reasoning process, including the last step outputting the final result.</p>
<p>Process-based approaches emphasize human understanding - in order to demonstrate or select good reasoning steps, human annotators need to understand the task. Human-comprehensibility is of direct interest in many domains. For example, in educational settings, an answer without an (understandable) explanation may often confuse more than it explains. In the longer term, human-comprehensibility may also help with detecting when ML systems may be using deceptive or unethical actions to achieve superficially appealing outcomes, such as by subtly manipulating people or systems to increase various metrics (Amodei et al., 2016). Recent work suggests that outcome-based approaches often lack in this area. For example, recent work on natural-language-based reasoning (Zelikman et al., 2022; Creswell et al., 2022) suggests that models optimized exclusively for finalanswer correctness can produce the correct final answer, even when their generated reasoning traces are incorrect. Similarly, work on AI safety (StuhlmÃ¼ller and Byun, 2022; Krakovna et al., 2020) suggests that such optimization may result in models which execute difficult-to-understand strategies.</p>
<p>This suggests that the choice of supervision approach for language models (LMs) with verbalized reasoning traces likely has important consequences. In this work, we conduct the first comprehensive comparison between process- and outcome-based approaches trained on a natural language task. For this, we use the recently proposed GSM8K dataset (Cobbe et al., 2021) of math word problems. In all</p>
<p>cases, we generate a sequence of reasoning steps leading to the final answer, but vary whether or not supervision is provided only on the final answers (outcome-based) or on individual reasoning steps (process-based). While the limited scope of the dataset prevents studying certain safety problems, the dataset enables a clean comparison between outcome- and process-based approaches. For processbased approaches we consider supervision provided by both offline human-generated reasoning traces from the GSM8K dataset itself, as well as online human correctness annotations, which we collect for reasoning steps within model-generated samples.</p>
<p>We compare these approaches in the context of a number of different modeling and training components, including: few-shot prompting, supervised fine-tuning, reinforcement learning (RL) via expert iteration and reward modeling for both reranking and RL. All of our models are based on a large pre-trained LM (Hoffmann et al., 2022).</p>
<p>Throughout, we consider two primary metrics: trace error rate, which measures how often the model makes any mistake in its reasoning trace according to human annotators, and final-answer error rate, which only considers the model's final answer and ignores the reasoning trace. By "reasoning trace" we refer to all textual steps of reasoning, including the last step which in GSM8K is the final numeric answer.</p>
<p>Key results We find that our best approach, which combines supervised learning with reward-model-based reinforcement learning, significantly improves the state-of-the-art for both trace error rate, from $14.0 \% \rightarrow 3.4 \%$, and final-answer error rate, from $16.8 \% \rightarrow 12.7 \%$. Final-answer error rate is further lowered to $2.7 \%$ when the model is allowed to abstain on $30 \%$ of questions. Our key findings regarding process- and outcome-based feedback are as follows:</p>
<ul>
<li>Outcome-based and process-based approaches lead to similar final-answer error rates. Both without reward models ( $23.5 \%$ vs. $22.3 \%$ ) and with reward models ( $16.6 \%$ vs. $14.8 \%$ ), LMs supervised with final-answer correctness attain nearly the same final-answer error rate as those trained to imitate human-provided solutions.</li>
<li>Both process- and outcome-supervised reward models learn to emulate process-based feedback. Somewhat surprisingly, we find that even reward models trained with outcome-based labels (indicating whether the final answer is correct), result in predictions that agree more closely with the process-based labels (indicating whether each reasoning step is correct) than they do with the outcome-based labels themselves. While this effect may be dataset-specific, as discussed in Section 3, it helps explain the effectiveness of reward models for improving trace error, and we hope that it is investigated further in future work.</li>
<li>Low trace error requires either process-based feedback, or a reward model that emulates it. All models using reinforcement learning directly against final-answer correctness resulted in high trace error, with a best trace error of $12.4 \%$, compared to only $3.8 \%$ for our best process-based method. Building on our previous finding, reinforcement learning against a reward model rather than final-answer correctness closes much of this gap, reducing trace error to $5.5 \%$.</li>
</ul>
<p>In the rest of this paper, we describe the approaches we compare in Section 2 and our results in Section 3. Section 4 discusses implications for process- and outcome-based feedback more generally, Section 5 discusses related work, and Section 6 concludes.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1 | Method Overview. This schematic provides an overview of the various modeling and training components considered and how they fit together. Some details (covered in the text) are omitted for readability.</p>
<h1>2. Problem and methods</h1>
<p>This section describes the dataset, evaluation metrics and the different modelling components evaluated in this paper. See Fig. 1 for an overview of how they all fit together.</p>
<h3>2.1. Dataset and evaluation metrics</h3>
<p>We conduct all experiments on the GSM8K dataset (Cobbe et al., 2021), composed of grade school math word problems. We chose GSM8K because it is a competitive benchmark, and contains natural language reasoning traces. We focus on a single dataset, since the need to recruit human annotators with the domain expertise to accurately evaluate reasoning traces imposes a large up-front cost. Table 2 and Appendix A show several example problems. We split out our own validation set of 256 examples from the original training set, which leaves us with 7118 training and 1319 test examples.</p>
<p>We report two main metrics for all methods evaluated on the GSM8K test set. Final-answer error rate is the fraction of problems for which the method does not produce the correct final answer. Because all final answers on GSM8K are integers, this can be measured with exact string matching. Trace error rate is the fraction of problems with correct final answers for which the method produces at least one incorrect reasoning step. We estimate this via human annotations of the correctness of each reasoning step, using the rating interface discussed in Section 2.7.</p>
<p>We report final-answer and trace errors as two separate metrics because, from a safety perspective, we are particularly interested in errors which remain undetected after applying easy-to-compute proxy metrics (in this case, final-answer errors). For example in an educational setting it is important to show a student the correct steps to get the answer, and we can easily filter out incorrect traces that lead to the wrong answer, but it is much more difficult to filter out incorrect traces that lead to the correct answer. We additionally report Selective final-answer error rate to assess performance when abstaining is allowed and Out-of-distribution (OOD) error rate on pre-algebra problems from the MATH dataset to assess generalization, as described in Sections 3.4 and 3.5, respectively.</p>
<h3>2.2. Training: Overview</h3>
<p>Our goal is to train a system for the sequence-to-sequence task (Sutskever et al., 2014) of taking the text of a problem as input and generating the text of an answer as output. For math word problems, the answer is a full reasoning trace: a newline-separated sequence of steps, where the last step is expected to provide the final answer. For GSM8K, the final answer is always an integer.</p>
<p>Our approach broadly follows prior work on RL for LMs (Ziegler et al., 2019; Nakano et al., 2021; Menick et al., 2022). We use an LM as a policy, which maps the problem statement and steps-so-far to a next step. In the RL formalism, this treats each step as an action, and the observation is provided by all the tokens so far. The policy can be obtained through any of few-shot prompting, supervised finetuning (Section 2.3), or RL (Section 2.6). We also train LMs as reward models (Section 2.4), which score proposed completions or partial completions from the policy, and can be used both for reranking samples from the policy, or as the source of rewards during reinforcement learning. In the following subsections, we describe how we train and assemble these components.</p>
<h1>2.3. Supervised finetuning</h1>
<p>In supervised finetuning (SFT), we finetune an LM to maximize the log-likelihood of a sequence of target tokens, given a sequence of input tokens. In our paper, we use SFT as a process-based approach by taking the reasoning traces provided in the GSM8K dataset as the target tokens (as opposed to the outcome-based approach of using only the final answer as the target), with the problem statement as the input tokens.</p>
<p>Training details We finetune using AdamW (Loshchilov and Hutter, 2017) with a learning rate of $2 \times 10^{-6}$ and a batch size of 256 . We stop finetuning once the language modeling loss begins to increase on the validation set. For our SFT model, this happens after 70 steps, amounting to slightly more than 2 training set epochs.</p>
<h3>2.4. Reward models</h3>
<p>We evaluate two main approaches to training reward models (RMs) (Christiano et al., 2017; Ziegler et al., 2019; Menick et al., 2022), also known as verifiers (Cobbe et al., 2021). In both approaches, we implement the RM as a LM, trained to predict a binary label as either a 'correct' or 'incorrect' token after each step. In the outcome-supervised RM (ORM), the binary label for each step indicates whether the resulting final answer of that full sample matched the reference final answer, as proposed by Cobbe et al. (2021). A policy which maximizes the ORM score at each step thus maximizes the RM-estimated probability at each step of eventually reaching the correct final answer. For the process-supervised RM (PRM), the binary label after each step indicates whether the steps so far are correct. Because we lack reliable programmatic means for determining the correctness of intermediate steps, we use human annotations for these labels, as described in Section 2.7. A policy which maximizes the PRM score thus selects each step to maximize the RM-estimated probability of the steps so far being correct. If the steps so far are correct, this typically means such a policy minimizes the probability of introducing a mistake on the current step. As reported in Section 3.2, we find this outperforms the approach from Li et al. (2022), which is similar to our PRM but replaces human evaluations with a heuristic based on string matching the results of the intermediate calculations.</p>
<p>Training details Unless otherwise noted, for all approaches which include an ORM, we train the ORM using samples from the policy for that approach, taking $K=96$ samples with temperature 1.0. We follow Cobbe et al. (2021) and regularize with dropout, with a dropout parameter of 0.1 , and otherwise reuse the hyperparameters used for SFT from Section 2.3. To speed up learning in the SFT-based approaches, we initialize the ORM training using the SFT model parameters, while for the few-shot based approaches we initialize from the base pretrained LM. For the PRM, we annotate 3 samples per problem from the SFT policy, restricting to problems where the SFT majority prediction (see Section 2.5) was incorrect, in order to make the most of our human annotation budget. Due to the small size of our human-annotated dataset ( 1560 full solutions), we initialize the PRM parameters</p>
<p>to the ORM parameters and lower the learning rate to $1 \times 10^{-7}$. The RM loss curves have some fluctuation, and so we select the RM with the best validation loss before 2000 steps.</p>
<h1>2.5. Decoding</h1>
<p>For all test-time decoding, we first generate $K=96$ samples of full solutions, and then select the best sample, either by ensembling across samples or by using an RM. In early experiments, we also tried RM reranking after each generated step (rather than the full solution), but found that this led to slightly worse performance, increasing final-answer error by 1-2\%. We sample with temperature $T=1.0$, and use the syntax from Cobbe et al. (2021) to allow the model to decide when to use a calculator.</p>
<p>We use two approaches to select the best sample. When no RM is available, we use majority voting. For this, we first select the most common final answer from the $K$ samples, then select a random sample from among those yielding this selected final answer. This is called self-consistency by Wang et al. (2022), and is similar to more general techniques like Minimum Bayes Risk decoding (Kumar and Byrne, 2004). Otherwise, we use RM-weighted decoding, also called verifier-voting by Li et al. (2022). Here, we weight each sample according to the RM-estimated correctness probability, select the final answer with the largest total weight, and then select the sample with the highest RM score from those yielding the selected final answer. More formally, we select the final answer $f^{<em>}=\arg \max <em y__i="y_{i">{f} \sum</em>$ are the model samples, then select the best sample according to $y^{}: \text { final_ans }\left(y_{i}\right)=f} \mathrm{rm} \operatorname{prob}\left(y_{i}\right)$, where $y_{1}, \ldots, y_{K</em>}=\arg \max <em i="i">{y}:$ final_ans $\left(y</em>\right)$. This works slightly better compared to simply selecting the sample with the highest RM score (about $1 \%$ final-answer error with the SFT model, slightly more with RL). However, we note that both majority voting and RM-weighted decoding are slightly less general due to their reliance on exact string-matching between final answers.}\right)=f$. rm_prob $\left(y_{i</p>
<h3>2.6. RL via Expert Iteration</h3>
<p>All our RL experiments use expert iteration (Silver et al., 2017; Anthony et al., 2017). As a metaalgorithm, expert iteration alternates between two high-level operations. In policy improvement, we combine a base policy with a search procedure to produce samples from a so-called expert policy. Then in distillation, we perform supervised learning on these expert samples to improve the base policy towards the expert policy. We use 5 epochs and select the best model of the 5, based on final-answer test error with RM-weighted decoding, or majority voting if no RM is available.</p>
<p>SFT-based vs. few-shot-based implementations The initial base policy can be either the SFT policy, or a 5 -shot prompted version of our base LM. We particularly note that, aside from the 5 random training examples used for the prompt, none of the few-shot-based approaches ever use the intermediate reasoning steps provided in the GSM8K dataset, our human annotations, or any models derived from this data. When initializing from the SFT model, we follow Polu and Sutskever (2020) and reuse expert samples from each iteration, so that our training set grows each epoch. We do not do this with few-shot approaches because in that setting, the samples from the early epochs have many trace errors which we do not want the RL model to imitate. Correspondingly, there are several minor implementation differences between the two cases, which we note throughout our detailed descriptions of the policy improvement and distillation procedures.</p>
<p>Policy Improvement We consider three versions of the policy improvement procedure (Figure 2). In the Final-answer RL approach, also called Self-taught Reasoner and proposed by Zelikman et al. (2022), we generate $K$ full traces per problem and filter by final-answer correctness. For the few-shot</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2 | Policy improvement. This schematic summarizes the different policy improvement operators for the Final-Answer RL, ORM-RL, and PRM-RL cases, which are described in the main text.
version, we select all traces yielding the correct final answer, while for the SFT-based version, we only use one randomly chosen sample per problem. In the ORM-RL approach, we generate $K$ full traces per problem, and select the sample with the highest score according to the ORM model. In the PRM-RL approach, we instead treat each step as an individual episode. At each step, we generate $K$ candidate steps, select the candidate with the highest PRM score, and continue from the selected step until the model outputs a step with the final answer indicator text, or a maximum of 15 steps. We set $K=96$ across all experiments. For few-shot-based approaches, we retrain the RM after every expert iteration. For SFT-based approaches, we skip this step and use a fixed RM, since somewhat surprisingly, this did not make a significant difference in preliminary experiments.</p>
<p>Distillation For distillation, we use the same hyperparameters as SFT. As with SFT, we apply early stopping by validation loss, where our validation set is constructed from expert policy samples on the validation set. For SFT-based approaches, we initialize with the SFT parameters at each distillation step, while for few-shot-based approaches, we initialize with the base model parameters.</p>
<h1>2.7. Data annotation</h1>
<p>As discussed in Section 2.4, the PRM is trained on stepwise labels indicating whether the steps so far are correct. To collect this data, we present human annotators with the problem statement, the reference solution from GSM8K, and the generated model solution, and ask them to indicate the first model step with a major mistake, if any exist. Our instructions define a major mistake as "a step where the information expressed is incorrect, or it would no longer be possible to reach the correct solution without undoing that step". From these annotations, we can label every step with a binary label indicating whether the steps so far are correct: all steps before the first major mistake are labeled 'correct', while the remainder are labeled 'incorrect'.</p>
<p>We applied a small amount of dataset cleaning by removing samples from annotators with low inter-annotator agreement (measured on the $20 \%$ of solutions where we used duplicate labelling), as well as those from GSM8K problems flagged by annotators as ambiguous. This removed about $20 \%$ of our data, leaving annotations for 1560 model samples across 530 training set problems, corresponding to 9856 step-level binary labels. For the validation set, we used the same procedure, but added duplicate labelling and a manual pass by the paper authors to resolve inter-annotator disagreements. Our validation set contained 162 model samples, with 913 total steps. For evaluation, we used 200 problems with correct final answers per model. This was done for each of the 10 models in Table 1, again with duplicate labelling. We describe full details of our data collection procedure in Appendix B.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Approach</th>
<th style="text-align: center;">Base model</th>
<th style="text-align: center;">Error rate (\%)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Trace</td>
<td style="text-align: center;">Final-answer</td>
</tr>
<tr>
<td style="text-align: center;">Few-shot (Wang et al., 2022; Wei et al., 2022)</td>
<td style="text-align: center;">PaLM-540B</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">25.6</td>
</tr>
<tr>
<td style="text-align: center;">Few-shot (Lewkowycz et al., 2022)</td>
<td style="text-align: center;">Minerva-540B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">21.5</td>
</tr>
<tr>
<td style="text-align: center;">Few-shot+Final-Answer RL (Zelikman, 2022)</td>
<td style="text-align: center;">GPT-J-6B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">89.3</td>
</tr>
<tr>
<td style="text-align: center;">Few-shot, ORM reranking (Li et al., 2022)</td>
<td style="text-align: center;">Codex-175B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">16.8</td>
</tr>
<tr>
<td style="text-align: center;">Zero-shot (Kojima et al., 2022)</td>
<td style="text-align: center;">InstructGPT-175B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">59.3</td>
</tr>
<tr>
<td style="text-align: center;">SFT, ORM reranking (Cobbe et al., 2021)</td>
<td style="text-align: center;">GPT-175B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">45.0</td>
</tr>
<tr>
<td style="text-align: center;">Few-shot, Majority Voting</td>
<td style="text-align: center;">Our Base-70B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">41.5</td>
</tr>
<tr>
<td style="text-align: center;">Few-shot+Final-Answer RL, Majority Voting</td>
<td style="text-align: center;">Our Base-70B</td>
<td style="text-align: center;">19.8 (7.9-31.7)</td>
<td style="text-align: center;">23.5</td>
</tr>
<tr>
<td style="text-align: center;">SFT+Final-Answer RL, Majority Voting</td>
<td style="text-align: center;">Our Base-70B</td>
<td style="text-align: center;">12.1 (4.6-19.6)</td>
<td style="text-align: center;">20.2</td>
</tr>
<tr>
<td style="text-align: center;">SFT, Majority Voting</td>
<td style="text-align: center;">Our Base-70B</td>
<td style="text-align: center;">11.4 (4.8-18.0)</td>
<td style="text-align: center;">22.3</td>
</tr>
<tr>
<td style="text-align: center;">Few-shot, ORM reranking</td>
<td style="text-align: center;">Our Base-70B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">27.8</td>
</tr>
<tr>
<td style="text-align: center;">Few-shot+Final-Answer RL, ORM reranking</td>
<td style="text-align: center;">Our Base-70B</td>
<td style="text-align: center;">12.4 (2.1-22.8)</td>
<td style="text-align: center;">16.6</td>
</tr>
<tr>
<td style="text-align: center;">SFT+Final-Answer RL, ORM reranking</td>
<td style="text-align: center;">Our Base-70B</td>
<td style="text-align: center;">3.7 (0.5-6.9)</td>
<td style="text-align: center;">14.2</td>
</tr>
<tr>
<td style="text-align: center;">SFT, ORM reranking</td>
<td style="text-align: center;">Our Base-70B</td>
<td style="text-align: center;">4.4 (0.6-8.3)</td>
<td style="text-align: center;">14.8</td>
</tr>
<tr>
<td style="text-align: center;">SFT, PRM reranking</td>
<td style="text-align: center;">Our Base-70B</td>
<td style="text-align: center;">3.5 (0.5-6.5)</td>
<td style="text-align: center;">14.1</td>
</tr>
<tr>
<td style="text-align: center;">Few-shot+ORM-RL, ORM reranking</td>
<td style="text-align: center;">Our Base-70B</td>
<td style="text-align: center;">5.5 (2.6-8.4)</td>
<td style="text-align: center;">13.8</td>
</tr>
<tr>
<td style="text-align: center;">SFT+ORM-RL, ORM reranking</td>
<td style="text-align: center;">Our Base-70B</td>
<td style="text-align: center;">3.4 (0.0-6.8)</td>
<td style="text-align: center;">12.7</td>
</tr>
<tr>
<td style="text-align: center;">SFT+PRM-RL, PRM reranking</td>
<td style="text-align: center;">Our Base-70B</td>
<td style="text-align: center;">3.8 (0.5-7.1)</td>
<td style="text-align: center;">12.9</td>
</tr>
</tbody>
</table>
<p>Table 1 | Results overview. We show trace and final-answer error rates. We suggest reading this table alongside or after the description of key results in Section 3. Trace error rates are averaged across raters. In parentheses, we provide a min-max range, depending on whether errors require both raters to agree (min) or just a single rater (max). While there is significant noise in the trace error rates, we can still observe general trends. Within each group, we list approaches in order from most outcome-based (top) to most process-based (bottom), other than the few-shot model, which has no finetuning procedure to supervise.</p>
<h1>3. Results</h1>
<p>Our results are summarized in Table 1 and Fig. 3. The ORM-RL and PRM-RL models achieve a final-answer error rate below $13 \%$, improving on the $16.8 \%$ final-answer error for the current state-of-the-art model (Li et al., 2022). This is further reduced to $2.7 \%$ when the model is allowed to abstain on only $30 \%$ of questions. The corresponding trace errors are $3.4 \%$ and $3.8 \%$, which significantly improve on the $14 \%$ reported by the best prior work (Wang et al., 2022; Wei et al., 2022). Beyond these quantitative results, we highlight three key takeaways:</p>
<p>Supervising final-answer correctness alone suffices for low final-answer error rate. The SFT and Few-shot+Final-Answer RL models attain similar final-answer error rates both without an RM ( $22.3 \%$ vs. $23.5 \%$ ) and with an ORM ( $14.8 \%$ vs. $16.6 \%$ ). This is notable, as Few-shot+Final-Answer RL only requires demonstrators to provide a final answer, rather than a full reasoning trace. Put another way, Few-shot+Final-Answer RL uses 1-4 tokens of label supervision per question, while SFT uses hundreds. This suggests that in cases where final-answer correctness is sufficient, outcome-based approaches can provide a label-efficient approach with competitive performance.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3 | Trace and final answer errors. Approaches investigated in this paper in comparison to the baselines. Vertical lines are used where trace error is not available. Approaches that lie off the chart have final answer error rates shown in brackets.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4 | Agreement matrix between RMs and RM labels We focus on the ORM predictions (yellow box), though provide the full agreement matrix for completeness (which is symmetric by construction). Notably, the ORM model has higher agreement with the PRM labels compared to the ORM labels. Results are for all steps on the PRM validation set.</p>
<p>ORM-supervised reward models approximate PRM labels. Despite the fact that ORMs are only trained to predict whether the final answer is correct, we can see in Fig. 4 that ORM predictions tend to agree more with the PRM labels than with the ORM labels themselves ( $85 \%$ vs. $77 \%$ averaged over all steps). ${ }^{1}$ We suspect this is because it is simpler for the ORM to learn to recognize when steps are correct, than it is to check the answer by internally computing the final answer itself. This is further supported by that fact that, even though trace error is measured only on samples with the correct final answer, RM reranking significantly improves trace error relative to SFT alone ( $4.4 \%$ vs. $11.4 \%$ ). This suggests that RMs are checking the reasoning steps, and not just the final answers. However, we caution against over-generalizing: the fact that the ORM model approximates the PRM labels may be domain-specific. This may depend on both the relative difficulty for the model to compute the correct answer with and without reasoning traces, and the lack of spurious solutions (Goldman et al., 2017) in math problems, where incorrect reasoning steps are unlikely to lead to the correct final answer.</p>
<p>Low trace error requires either process-based feedback or a reward model that emulates it. Fig. 3 shows that despite similar final-answer error rates, there is a significantly higher trace error rate for the outcome-based Few-shot+Final-Answer RL vs. the process-based SFT model (19.8\% vs. $11.4 \%$ ). This discrepancy persists with RM reranking: Few-shot+Final-Answer RL with ORM reranking underperforms SFT with ORM/PRM reranking ( $12.4 \%$ vs. $4.4 \% / 3.5 \%$ ). However, we find that when we train the few-shot RL model using an ORM (Few-shot+ORM-RL) rather than training directly against final-answer correctness, the trace error drops significantly from $12.4 \%$ to $5.5 \%$, closing much of this gap. We believe this results from the previous finding, i.e. that the ORM is basically learning to emulate the PRM allowing the model to learn from emulated process-based</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Problem: Tyrion changes his face mask two times every time he goes out. If he goes out three times a day, how many face masks does he use every 2 days?
Model answer (Trace incorrect, final answer correct): Tyrion goes out 3 times every day and changes his face mask 2 times every time he goes out so that's a total of $3 * 2=Â« 3 * 2=6 Â» 6$ face masks every day.
Tyrion goes out 6 face masks every day and there are 2 days in a day so that's $6^{<em>} 2=Â« 6^{</em>} 2=12 Â» 12$ face masks every 2 days.
Final answer: 12
Table 2 | Trace error example. In this lemon-picked example from the Few-shot+Final-Answer RL model, the final answer is correct, but the reasoning steps are not. Because Final-Answer RL only optimizes for final-answer error, the intermediate steps need not be understandable to a human evaluator, or correct under standard English semantics.
feedback and resulting in relatively low trace error rates.
In the rest of this section we provide a more detailed analysis of our results. We group the main results based on how they use the reward model, covering approaches that use no reward model in Section 3.1, approaches that use a reward model only for reranking in Section 3.2, and approaches that use a reward model both during reinforcement learning and for reranking in Section 3.3. We cover selective accuracy and OOD generalization separately in Sections 3.4 and 3.5.</p>
<h1>3.1. No reward model</h1>
<p>Comparing the most process-based to most outcome-based approach, SFT and Few-shot+Final-Answer RL have similar final-answer error rates, but SFT has significantly better trace error. Further, when starting from the SFT model, applying Final-Answer RL does decrease final answer error ( $22.3 \%$ to $20.2 \%$ ), but increases trace error ( $11.4 \%$ to $12.1 \%$, though we note the difference is not statistically significant). These both support the view that outcome-based approaches can find ways to produce correct answers for incorrect reasons. Table 2 provides a qualitative example.</p>
<p>Finetuning improves performance relative to prompting alone. Our focus on different approaches to supervising LMs assumes that, provided sufficient data, finetuning outperforms prompting alone. We first validate this assumption for GSM8K. We find that the 5-shot prompted policy alone (Few-shot) achieves $41.5 \%$ final-answer error with $K=96$ majority voting (and $77.7 \%$ error when using a single sample). While this is impressive considering that the few-shot policy requires no additional finetuning data, it leaves significant performance on the table compared to both the Few-shot+Final-Answer RL and SFT models, thus validating our initial assumption.</p>
<h3>3.2. Reward model for reranking only</h3>
<p>Overall, reward models provide a significant boost to both trace and final answer accuracy. We find that RM reranking significantly improves trace error, reducing it from $11.4 \%$ to below 5\% for SFT. RM reranking also benefits the Few-shot+Final-Answer RL model, though trace error remains significantly higher than in the SFT case. As found by Cobbe et al. (2021), our results also show that RMs decrease final-answer error, from $22.3 \%$ to below $15 \%$. We also experimented with the "step-level voting verifier" from Li et al. (2022). This works similarly to the PRM approach, but computes labels using a heuristic based on intermediate numeric results. This resulted in a slightly worse $15.9 \%$ final-answer error.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Error rate</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">Greedy</td>
<td style="text-align: center;">Majority</td>
<td style="text-align: center;">RM-weighted</td>
</tr>
<tr>
<td style="text-align: left;">Few-shot</td>
<td style="text-align: center;">54.3</td>
<td style="text-align: center;">41.4</td>
<td style="text-align: center;">27.8</td>
</tr>
<tr>
<td style="text-align: left;">Few-shot + Final-answer RL</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">23.8</td>
<td style="text-align: center;">16.6</td>
</tr>
<tr>
<td style="text-align: left;">Few-shot + ORM-RL</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">18.8</td>
<td style="text-align: center;">13.8</td>
</tr>
<tr>
<td style="text-align: left;">SFT</td>
<td style="text-align: center;">41.1</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">14.1</td>
</tr>
<tr>
<td style="text-align: left;">SFT + Final-answer RL</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">20.2</td>
<td style="text-align: center;">14.2</td>
</tr>
<tr>
<td style="text-align: left;">SFT + ORM-RL</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">17.8</td>
<td style="text-align: center;">12.7</td>
</tr>
<tr>
<td style="text-align: left;">SFT + PRM-RL</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">17.6</td>
<td style="text-align: center;">12.9</td>
</tr>
</tbody>
</table>
<p>Table 3 | Final-answer error with different decoding strategies. This repeats some results from Table 1 but includes all decoding strategies. RL always provides a significant boost in the few-shot setting (top), and with greedy sampling (first column), which is important for latency-sensitive applications. ORM-RL and PRM-RL are notably better than Final-Answer RL in all settings.</p>
<h1>3.3. Reinforcement learning with a reward model</h1>
<p>RL significantly improves few-shot performance, but provides more modest gains on top of SFT. We can see from Table 3 that when starting from a few-shot model, RL cuts the final answer error rate by half, regardless of the decoding method. In contrast, when starting with an SFT model, RL has very little effect on top of using an RM for decoding, though does provide a significant improvement when using greedy decoding ( $41.1 \%$ to $31.2 \%$ ).</p>
<p>Optimizing against an RM outperforms optimizing final-answer correctness directly. In both the few-shot and SFT settings, ORM-RL and PRM-RL outperform Final-Answer RL across all three decoding strategies. On face, this may be surprising given that ORM-RL optimizes an approximation (the ORM full-solution scores) of final-answer correctness. However, our earlier RM analysis (Fig. 4) suggests that the ORM approximates process-based feedback, and checks reasoning steps rather than the final answer directly. Thus, one potential explanation is that Final-Answer RL only checks that solutions reach the correct final answer, whereas PRM-RL and ORM-RL check for solutions which reach the right answer for the right reason.</p>
<h3>3.4. Selective prediction</h3>
<p>In many practical applications, it is possible to abstain. For instance, if an ML system was used to explain a problem to a student, or assist in a calculation, it would be preferable for the model to abstain rather than produce an incorrect output. This motivates the selective prediction setting (El-Yaniv et al., 2010; Geifman and El-Yaniv, 2017), where the model is allowed to abstain on $r \%$ of inputs, and selective error rate is measured on the inputs where the model does not abstain. To determine which inputs to abstain on, we set a threshold on the RM score of the selected sample, with the threshold determined by $r$.</p>
<p>Selective prediction greatly reduces final-answer error, particularly for models with low trace error. Figure 5 shows that by abstaining on $r=30 \%$ of inputs, we reduce final-answer error rate from $14.1 \% \rightarrow 2.7 \%$, which can be further reduced to $1.5 \%$ at $r=50 \%$. Further, at $r=30 \%$, selective prediction with SFT and the ORM or PRM yields a $5 \times$ final-answer error reduction, compared to $3 \times$ in the Few-shot+Final-Answer RL case. This may be related to the improved trace error for SFT:</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5 | Selective error. By abstaining, we can greatly decrease the error rate on answered questions. (Left) For the SFT policy with PRM reranking, final-answer error rate falls from $14.1 \%$ to $2.7 \%$ when abstaining on $30 \%$ of inputs, a roughly $5 \times$ reduction factor. (Right) Models trained with SFT are significantly more amenable to selective prediction compared to those trained via Final-Answer RL.
when trace error is lower, the RM can more reliably use intermediate step correctness to determine its confidence. However, further investigation would be necessary to properly understand this effect.</p>
<h1>3.5. OOD generalization</h1>
<p>To evaluate out-of-distribution generalization, we evaluate our models zero-shot on the pre-algebra split of the MATH (Hendrycks et al., 2021) dataset. We limited to problems without Asymptote diagrams, and removed some LaTeX formatting with simple regular expressions, e.g. converting \$ frac ${3}{4} \$$ to $3 / 4$. Overall, we see noticeable OOD generalization, with final-answer error rate of $64.6 \%$ for our SFT+ORM-RL model with majority voting ( $74.2 \%$ error with no filtering, assuming the model fails all questions with Asymptote diagrams). This is significantly worse than the $29 \%$ error on pre-Algebra questions from Lewkowycz et al. (2022), which uses a much larger base LM and trains on more math data, but significantly better than the previous best result of $92.3 \%$ error for GPT-3 from Hendrycks et al. (2021). All models have final-answer error in the $60 \%-70 \%$ range, and we do not observe noticeable trends based on the type of supervision. We provide full details and results in Appendix D.</p>
<h2>4. Discussion</h2>
<h3>4.1. When to use process- vs. outcome-based feedback?</h3>
<p>Process- and outcome-based feedback have different strengths, and the appropriate choice will often be context-dependent. As a general rule, outcome-based feedback tends to be appropriate when a reliable and complete evaluation metric is available, while process-based feedback is most appropriate otherwise. Here we discuss the considerations that lead us to this view. We start by discussing relative strengths and weaknesses with respect to final-answer and trace error, before moving to motivations for process-based feedback that are yet to be empirically validated.</p>
<h1>4.1.1. Final-answer vs. trace error</h1>
<p>Our results suggest that when low final-answer error is sufficient, outcome-based approaches provide a label-efficient method for obtaining this, whereas when low trace error is desired, it is helpful to use process-based feedback, or an approximation of it. The relative importance of these metrics is context-dependent. In a context where desired outcomes are easy to evaluate and the evaluation process is robust, final-answer error is appropriate. For example, if final answers can be quickly validated, either programmatically or by quick user inspection, then low final-answer error is a fairly complete performance measure. In contrast, Menick et al. (2022) provide question-answering as an example where low trace error is necessary, since even if the final answer is correct, it is very difficult for users to rely on this answer without also seeing the sources which led to that answer. In other domains such as education, the reasoning steps themselves may also be of direct interest.</p>
<h3>4.1.2. Process-based approaches both require and facilitate human understanding</h3>
<p>Relative to outcome-based approaches, process-based approaches tend to require greater human understanding (Krakovna et al., 2020). For example, outcome-based feedback based on powerconsumption, chip area, and other metrics can be used to optimize computer chip layouts (Mirhoseini et al., 2021), while a process-based approach would require detailed expert knowledge on designing chip layouts. In order to be competitive in general, process-based approaches require us to improve human understanding, either aided by ML systems such as in Amplification or Debate (Christiano et al., 2018; Irving et al., 2018) or through broader means, such as by working with experts (Rauh et al., 2022), training people (Stiennon et al., 2020), or providing them assistive tools.</p>
<p>Second, process-based approaches may facilitate human understanding because they select for reasoning steps that humans understand. By contrast, outcome-based optimization may find hard-tounderstand strategies, and result in less understandable systems, if these strategies are the easiest way to achieve highly-rated outcomes. For example in GSM8K, when starting from SFT, adding Final-Answer RL decreases final-answer error, but increases (though not significantly) trace error.</p>
<h3>4.1.3. Process-based approaches avoid tampering incentives</h3>
<p>A common concern within the AI safety literature is from RL agents which tamper (Everitt et al., 2017), i.e., corrupt their feedback mechanisms in order to receive positive feedback. As a hypothetical example, consider an assistive agent which repeatedly interacts with users. If optimized for total user satisfaction, such an agent may influence users towards preferences which are easier to satisfy (e.g. easier to predict, or generally more amenable to ML-generated proposals), in order to increase user-reported satisfaction (Kenton et al., 2021). A similarly long-term concern is with agents which gain power and take complete control of their feedback procedures in order to ensure positive feedback (Cotra, 2022).</p>
<p>In contrast, consider training from process-based feedback, using user evaluations of individual actions, rather than overall satisfaction ratings. ${ }^{2}$ While this does not directly prevent actions which influence future user preferences, these future changes would not affect rewards for the corresponding actions, and so would not be optimized for by process-based feedback. We refer to Kumar et al. (2020) and Uesato et al. (2020) for a formal presentation of this argument. Their decoupling algorithms present a particularly pure version of process-based feedback, which prevent the feedback from depending directly on outcomes.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>An alternate approach to avoiding tampering is to continually improve the outcome-based metrics, by monitoring for sequences of actions leading to tampering, and penalizing these sequences. However, this approach only scales to tampering we can detect: in other cases, effects can be difficult to observe or measure (e.g., how would we determine if a system influences user preferences?) or understand (e.g., innocuous-looking decisions may still have large effects, particularly in aggregate). Broadly, a risk with the incremental outcome-based approach is addressing the most easily-noticed problems, without addressing root causes or subtler cases.</p>
<h1>4.2. Limitations to generalizability of our results</h1>
<p>We generally expect process-based and outcome-based feedback to align more closely for math compared to other domains. For math problems, incorrect traces are typically harmful for reaching correct final answers. This matches our earlier finding that outcome-supervised RMs approximate process-based feedback. In contrast, in other domains, undesirable behaviors may be helpful for highly-rated outcomes, e.g., manipulation may increase reported user satisfaction. As a result, we believe optimizing for outcomes (final-answer correctness) for math problems has a stronger effect on inducing a correct process than it would in other domains.</p>
<h3>4.3. Concepts related to process- and outcome-based feedback</h3>
<p>In this work, we focus on different forms of supervision available for training LMs, and discuss these forms of supervision in terms of a distinction between process- and outcome-based approaches. This framing has been used in blog posts (StuhlmÃ¼ller and Byun, 2022) and informal discussion, though to our knowledge this is the first empirical paper to discuss it. Here, we discuss similarities and differences to other related distinctions used throughout the literature.</p>
<p>Supervised learning vs. reinforcement learning Broadly, supervised approaches tend to be more process-based, and RL approaches tend to be more outcome-based. Indeed, the most process-based approach we consider is purely supervised (SFT), while the most outcome-based approach is pure RL (Few-shot+Final-Answer RL). However, RL approaches can be more or less process-based, such as when comparing PRM-RL to ORM-RL or Final-Answer RL. Conversely, supervised imitation of reasoning traces filtered by outcomes (e.g., traces which led to highly-rated user interactions) blurs together with RL approaches based on supervised learning of high-return trajectories (Silver et al., 2017; Anthony et al., 2017; Abdolmaleki et al., 2018). The process- and outcome-based categorization thus acknowledges that the resulting model depends on the broader data-providing procedure, which often will not be fully described by code.</p>
<p>Strong vs. weak supervision While the meanings of strong vs. weak supervision can vary depending on context, approaches which supervise intermediate steps are often referred to as strongly supervised (Yang et al., 2018; Perez et al., 2020). Similar to the above, while process-based approaches tend towards strong supervision, and outcome-based approaches towards weak supervision, strongly supervised approaches can be either process- or outcome-based. For instance, a person evaluating intermediate reasoning steps could either be directly evaluating those reasoning steps, or deferring to evaluations of their resulting outcomes.</p>
<p>Verbalized reasoning Verbalized reasoning traces do not necessarily imply process-based approaches. Indeed, whereas all approaches in this work use verbalized reasoning traces, they use both process- and outcome-based feedback. Additionally, the verbalized reasoning traces do not necessarily</p>
<p>represent the model's internal reasoning process, with the bulk of the reasoning happening inside of the large neural network activations, except in strictly modular approaches (Creswell et al., 2022). This caution holds particularly for domains with the potential for deception (Kenton et al., 2021).</p>
<p>Nonetheless, verbalized reasoning still helps enable process-based feedback. In contrast to approaches based on iterative computations performed in activation space (Guez et al., 2019; Dehghani et al., 2018; Graves, 2016; Schrittwieser et al., 2020), humans can directly supervise iterative reasoning steps in natural language.</p>
<h1>5. Related work</h1>
<p>Solving math problems with LMs Math word problems have been a popular domain for studying reasoning in LMs (Kushman et al., 2014; Ling et al., 2017; Amini et al., 2019; Miao et al., 2020; Hendrycks et al., 2021; Cobbe et al., 2021). Several recent papers have demonstrated that few-shot prompting alone can lead to impressive performance on GSM8K (Chowdhery et al., 2022; Lewkowycz et al., 2022; Wei et al., 2022; Wang et al., 2022). All of these papers include the reasoning traces in the few-shot prompts which encourages the model to generate verbalized reasoning steps (also referred to as self-talk (Shwartz et al., 2020) and chain-of-thought prompting (Wei et al., 2022)). Prompting benefits significantly from training on a large dataset of mathematical content (Lewkowycz et al., 2022), as well as from finetuning for instruction following (Ouyang et al., 2022). Kojima et al. (2022) and Li et al. (2022) demonstrate improvements in final-answer error rate from $81.1 \%$ for GPT-3 (Brown et al., 2020) to $41.8 \%$ for InstructGPT (Ouyang et al., 2022), and further to $23.3 \%$ for Codex (Chen et al., 2021).</p>
<p>We focus on finetuning because we are interested in the effects of different feedback procedures, and because it significantly outperforms prompting alone for our base LM. The original GSM8K paper (Cobbe et al., 2021) demonstrated significant benefits of reward models or verifiers, and we use their ORM approach. Li et al. (2022) also study RMs and propose a heuristic-based step-aware RM, which slightly degrades performance on GSM8K, but boosts performance on a wide range of other benchmarks. We find that human evaluations of each step provide an improvement. We also use STaR (Zelikman et al., 2022) (referred to as Few-shot+Final-Answer RL throughout this paper), and show its GSM8K final-answer error can be reduced from their reported $89 \%$ to $23.5 \%$ through the use of a better base model (Hoffmann et al., 2022) and further reduced to $13.8 \%$ by using an RM-based RL instead of their final answer RL procedure. In contrast to the above prior work, we not only show improved performance, but also provide a comprehensive comparison across different types of feedback, with a focus on trace error rate in addition to final-answer error rate.</p>
<p>Multistep reasoning in LMs Moving beyond math problems, a large body of work studies multistep reasoning for LMs. While a full review is beyond the scope of this work, we discuss a few representative categories of approaches. Prior work has suggested improvements to the base model (Lewkowycz et al., 2022; Ouyang et al., 2022), as well as prompt-based approaches (Perez et al., 2020; Shwartz et al., 2020; Wei et al., 2022; Kojima et al., 2022; Dohan et al., 2022). Our focus on supervision techniques for finetuning is complementary to such improvements.</p>
<p>Other work has focused exclusively either on outcome-based or on process-based approaches. For example, on the process-based side, Wu et al. (2021) summarize full-length books by supervising individual summaries, which are recursively composed. This provides an example where the outcomebased approach (directly using human approval of full-book summaries) would be prohibitively expensive, due to the cost and sparsity of such feedback, but process-based supervision of each step individually is possible. Creswell et al. (2022) and Nye et al. (2021) use process-based SFT in synthetic</p>
<p>settings where reasoning traces can be synthesized programatically, while we instead focus on a natural language setting where reasoning traces and feedback must be generated by humans. On the outcome-based side, Zelikman et al. (2022) apply an outcome-based approach to multi-step reasoning questions. In contrast to all of this work, however, we directly compare outcome-based and process-based techniques, and include a detailed analysis on trace error rates.</p>
<p>To our knowledge, the prior work which most directly compares process- and outcome-based feedback for multistep LM reasoning is WebGPT (Nakano et al., 2021). In WebGPT, intermediate steps are web browser interactions, which they supervise either via SFT, or via outcome-based RL on resulting answers. Similar to us, they observe significant improvements over SFT from outcome-based RM reranking, but adding full-solution RL on top has minimal effect. However, we more comprehensively explore both process- and outcome-based supervision, additionally evaluating process-supervised RMs, the PRM-RL approach, and purely outcome-supervised RL policies (without SFT). This allows us to draw broader conclusions regarding the effects of supervision on final-answer vs. trace error compared to prior work.</p>
<p>Comparisons between process- and outcome-based approaches to algorithmic tasks. Most prior work on head-to-head comparisons of process- and outcome-based approaches has been on algorithmic tasks such as sorting lists of numbers, which avoid working with human data. Early outcome-based approaches such as Neural Turing Machines (Graves et al., 2014) are trained end-toend to predict the final answer. In contrast, Neural Programmer-Interpreters (Reed and De Freitas, 2015; Li et al., 2016; Cai et al., 2017) are trained to imitate each step in an execution trace, which can then be chained together. This requires stronger supervision but improves generalization. Iterated Amplification (Christiano et al., 2018) uses a bootstrapping procedure to train models to approximate a potentially exponentially sized tree of reasoning steps, assuming an oracle that cannot directly answer hard problems, but can decompose them into easier ones. Our work extends these results to natural language, where neither programmatic execution traces nor decomposition procedures are available, and these must be learned from human feedback.</p>
<p>Related datasets In this work we chose to work with the GSM8K dataset, because it provides natural language reasoning traces that allow for a detailed comparison between process- and outcome-based approaches without requiring us to collect the traces ourselves. Alternative datasets which also contain full reasoning traces include EntailmentBank (Dalvi et al., 2021), StrategyQA (Geva et al., 2021), Proof Writer (Tafjord et al., 2020), and CLUTTR (Gontier et al., 2020). However, compared to GSM8K, these datasets either contain templated problems (Proof Writer and CLUTTR) or are significantly smaller in size (EntailmentBank and StrategyQA). Furthermore, working with multiple datasets would be expensive because of the need to train human annotators for each task, and to collect a significant number of human feedback annotations for each dataset.</p>
<h1>6. Conclusion</h1>
<p>In this work, we run the first comprehensive comparison between process- and outcome-based supervision on a natural language task. We find that both types of supervision lead to similar finalanswer error rates, with our best models improving the state-of-the-art final-answer error on GSM8K from $16.8 \%$ to $13.8 \%$ when using outcome-based supervision and to $12.9 \%$ when using processbased supervision. In contrast, we find that obtaining low trace error requires either process-based supervision, or a reward model that emulates it. A purely process-based approach of SFT with PRM reranking reduces the state-of-the-art trace error rate from $14.0 \%$ to $3.4 \%$, while its outcome-based analogue achieves $12.7 \%$ trace error. However, somewhat surprisingly, we find that reward models</p>
<p>trained with outcome-based labels result in predictions that agree more closely with the process-based labels than they do with the outcome-based labels themselves. By using this reward model during RL training, we close most of this gap, reducing trace error from $12.7 \%$ to $5.5 \%$. While some of these conclusions may be specific to our setting of math word problems, we hope that future work explores the extent to which they generalize to other domains.</p>
<h1>References</h1>
<p>A. Abdolmaleki, J. T. Springenberg, Y. Tassa, R. Munos, N. Heess, and M. Riedmiller. Maximum a posteriori policy optimisation. arXiv preprint arXiv:1806.06920, 2018.
A. Amini, S. Gabriel, P. Lin, R. Koncel-Kedziorski, Y. Choi, and H. Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. arXiv preprint arXiv:1905.13319, 2019.
D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and D. ManÃ©. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.
T. Anthony, Z. Tian, and D. Barber. Thinking fast and slow with deep learning and tree search. Advances in Neural Information Processing Systems, 30, 2017.
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
J. Cai, R. Shin, and D. Song. Making neural programming architectures generalize via recursion. arXiv preprint arXiv:1704.06611, 2017.
M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374.
A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.
P. Christiano, B. Shlegeris, and D. Amodei. Supervising strong learners by amplifying weak experts. arXiv preprint arXiv:1810.08575, 2018.
P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.
K. Cobbe, V. Kosaraju, M. Bavarian, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.
A. Cotra. Without specific countermeasures, the easiest path to transformative ai likely leads to ai takeover, 2022. URL https://www.alignmentforum.org/posts/pRkFkzwKZ2zfa3R6H/ without-specific-countermeasures-the-easiest-path-to.</p>
<p>A. Creswell, M. Shanahan, and I. Higgins. Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712, 2022.
B. Dalvi, P. A. Jansen, O. Tafjord, Z. Xie, H. Smith, L. Pipatanangkura, and P. Clark. Explaining answers with entailment trees. ArXiv, abs/2104.08661, 2021.
M. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and Å. Kaiser. Universal transformers. arXiv preprint arXiv:1807.03819, 2018.
D. Dohan, W. Xu, A. Lewkowycz, J. Austin, D. Bieber, R. G. Lopes, Y. Wu, H. Michalewski, R. A. Saurous, J. Sohl-dickstein, et al. Language model cascades. arXiv preprint arXiv:2207.10342, 2022.
R. El-Yaniv et al. On the foundations of noise-free selective classification. Journal of Machine Learning Research, 11(5), 2010.
T. Everitt, V. Krakovna, L. Orseau, M. Hutter, and S. Legg. Reinforcement learning with a corrupted reward channel. arXiv preprint arXiv:1705.08417, 2017.
Y. Geifman and R. El-Yaniv. Selective classification for deep neural networks. Advances in neural information processing systems, 30, 2017.
M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, and J. Berant. Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies. Transactions of the Association for Computational Linguistics (TACL), 2021.
O. Goldman, V. Latcinnik, U. Naveh, A. Globerson, and J. Berant. Weakly-supervised semantic parsing with abstract examples. arXiv preprint arXiv:1711.05240, 2017.
N. Gontier, K. Sinha, S. Reddy, and C. Pal. Measuring systematic generalization in neural proof generation with transformers. Advances in Neural Information Processing Systems, 33:22231-22242, 2020.
A. Graves. Adaptive computation time for recurrent neural networks. arXiv preprint arXiv:1603.08983, 2016.
A. Graves, G. Wayne, and I. Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014.
A. Guez, M. Mirza, K. Gregor, R. Kabra, S. RacaniÃ¨re, T. Weber, D. Raposo, A. Santoro, L. Orseau, T. Eccles, et al. An investigation of model-free planning. In International Conference on Machine Learning, pages 2464-2473. PMLR, 2019.
D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.
J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. v. d. Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, J. W. Rae, O. Vinyals, and L. Sifre. Training computeoptimal large language models. arXiv preprint arXiv:2203.15556, 2022.
G. Irving, P. Christiano, and D. Amodei. Ai safety via debate. arXiv preprint arXiv:1805.00899, 2018.
Z. Kenton, T. Everitt, L. Weidinger, I. Gabriel, V. Mikulik, and G. Irving. Alignment of language agents. arXiv preprint arXiv:2103.14659, 2021.</p>
<p>T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.
V. Krakovna, J. Uesato, V. Mikulik, M. Rahtz, T. Everitt, R. Kumar, Z. Kenton, J. Leike, and S. Legg. Specification gaming: the flip side of ai ingenuity, 2020. URL https://deepmindsafetyresearch.medium.com/ specification-gaming-the-flip-side-of-ai-ingenuity-c85bdb0deeb4.
R. Kumar, J. Uesato, R. Ngo, T. Everitt, V. Krakovna, and S. Legg. REALab: An embedded perspective on tampering. arXiv preprint arXiv:2011.08820, 2020.
S. Kumar and W. Byrne. Minimum bayes-risk decoding for statistical machine translation. Technical report, JOHNS HOPKINS UNIV BALTIMORE MD CENTER FOR LANGUAGE AND SPEECH PROCESSING (CLSP), 2004.
N. Kushman, Y. Artzi, L. Zettlemoyer, and R. Barzilay. Learning to automatically solve algebra word problems. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 271-281, 2014.
A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo, Y. Wu, B. Neyshabur, G. Gur-Ari, and V. Misra. Solving quantitative reasoning problems with language models, 2022. URL https://arxiv.org/abs/2206.14858.
C. Li, D. Tarlow, A. L. Gaunt, M. Brockschmidt, and N. Kushman. Neural program lattices. 2016.
Y. Li, Z. Lin, S. Zhang, Q. Fu, B. Chen, J.-G. Lou, and W. Chen. On the advance of making language models better reasoners. arXiv preprint arXiv:2206.02336, 2022.
W. Ling, D. Yogatama, C. Dyer, and P. Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146, 2017.
I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.
J. Menick, M. Trebacz, V. Mikulik, J. Aslanides, F. Song, M. Chadwick, M. Glaese, S. Young, L. CampbellGillingham, G. Irving, et al. Teaching language models to support answers with verified quotes. arXiv preprint arXiv:2203.11147, 2022.
S.-y. Miao, C.-C. Liang, and K.-Y. Su. A diverse corpus for evaluating and developing english math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 975-984, 2020.
A. Mirhoseini, A. Goldie, M. Yazgan, J. W. Jiang, E. Songhori, S. Wang, Y.-J. Lee, E. Johnson, O. Pathak, A. Nazi, et al. A graph placement methodology for fast chip design. Nature, 594(7862):207-212, 2021.
R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.
M. Nye, A. J. Andreassen, G. Gur-Ari, H. Michalewski, J. Austin, D. Bieber, D. Dohan, A. Lewkowycz, M. Bosma, D. Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.</p>
<p>L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with human feedback, 2022. URL https://arxiv.org/abs/2203.02155.
E. Perez, P. Lewis, W.-t. Yih, K. Cho, and D. Kiela. Unsupervised question decomposition for question answering. arXiv preprint arXiv:2002.09758, 2020.
E. Perez, D. Kiela, and K. Cho. True few-shot learning with language models. Advances in Neural Information Processing Systems, 34:11054-11070, 2021.
S. Polu and I. Sutskever. Generative language modeling for automated theorem proving, 2020. URL https://arxiv.org/abs/2009.03393.
M. Rauh, J. Mellor, J. Uesato, P.-S. Huang, J. Welbl, L. Weidinger, S. Dathathri, A. Glaese, G. Irving, I. Gabriel, et al. Characteristics of harmful text: Towards rigorous benchmarking of language models. arXiv preprint arXiv:2206.08325, 2022.
S. Reed and N. De Freitas. Neural programmer-interpreters. arXiv preprint arXiv:1511.06279, 2015.
J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lockhart, D. Hassabis, T. Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604-609, 2020.
V. Shwartz, P. West, R. L. Bras, C. Bhagavatula, and Y. Choi. Unsupervised commonsense question answering with self-talk. arXiv preprint arXiv:2004.05483, 2020.
D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, et al. Mastering the game of go without human knowledge. nature, 550(7676):354-359, 2017.
N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008-3021, 2020.
A. StuhlmÃ¼ller and J. Byun. Supervise process, not outcomes. 2022. URL https://ought.org/ updates/2022-04-06-process.
I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27, 2014.
O. Tafjord, B. D. Mishra, and P. Clark. Proofwriter: Generating implications, proofs, and abductive statements over natural language. arXiv preprint arXiv:2012.13048, 2020.
J. Uesato, R. Kumar, V. Krakovna, T. Everitt, R. Ngo, and S. Legg. Avoiding tampering incentives in deep RL via decoupled approval. arXiv preprint arXiv:2011.08827, 2020.
X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, and D. Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.
J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting elicits reasoning in large language models, 2022.
J. Wu, L. Ouyang, D. M. Ziegler, N. Stiennon, R. Lowe, J. Leike, and P. Christiano. Recursively summarizing books with human feedback. arXiv preprint arXiv:2109.10862, 2021.</p>
<p>Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. W. Cohen, R. Salakhutdinov, and C. D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018.
E. Zelikman, Y. Wu, and N. D. Goodman. Star: Bootstrapping reasoning with reasoning. arXiv preprint arXiv:2203.14465, 2022.
D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.</p>
<h1>A. Example GSM8K problems and solutions</h1>
<p>We include several examples to provide a qualitative sense of the task and learned model behavior. Table 4 contains 10 randomly sampled problems, and the output of the SFT+ORM-RL model with ORM reranking. Table 5 contains 5 trace errors, where the final answer is correct, but at least one of the steps has an error.</p>
<p>Table 4
Problem: The glee club ordered 20 pizzas and ate $70 \%$ of them. The football team ordered twice as many pizzas and ate $80 \%$ of them. How many pizzas are left?
Model answer (final answer correct): The football team ordered $20 * 2=\approx 20 * 2=40 \approx 40$ pizzas.
So, the total number of pizzas ordered is $40+20=\approx 40+20=60 \approx 60$ pizzas.
The glee club ate $20 * 0.7=\approx 20 * 0.7=14.0 \approx 14$ pizzas.
The football team ate $40 * 0.8=\approx 40 * 0.8=32.0 \approx 32$ pizzas.
Therefore, the total number of pizzas eaten is $32+14=\approx 32+14=46 \approx 46$.
Hence, the number of pizzas left is $60-46=\approx 60-46=14 \approx 14$.
Final answer: 14</p>
<p>Problem: Johnny took his allowance of $\$ 20$ and added an extra $\$ 10$ to it. He then invested this sum of money, which tripled in a year. How much money did he have after a year?
Model answer (final answer correct): Johnny had $\$ 20+\$ 10=\$ \approx 20+10=30.00 \approx 30$.
He then had $\$ 30 \times 3=\$ \approx 30 * 3=90.00 \approx 90$.
Therefore, he had $\$ 90$ after a year.
Final answer: 90</p>
<p>Problem: A jellyfish tank has numerous jellyfish in it. A fifth of the jellyfish are large, and a third of the large jellyfish change color from green to blue under UV light. The other jellyfish are small and always stay blue. When a UV light turned on, 6 jellyfish changed color. How many jellyfish are in the tank?
Model answer (final answer correct): If a third of the large jellyfish change color, and there are a fifth of all the jellyfish that are large, there are $1 / 3 * 1 / 5=1 / 15$ of all the jellyfish that change color.
If 6 jellyfish changed color under the light, and $1 / 15$ of all the jellyfish change color, then there are $15 /(1 / 15)=90$ jellyfish in the tank.
Final answer: 90</p>
<p>Problem: Farmer Brown's farm is 200 acres, and Farmer Smith's farm is 100 acres more than twice that. How many acres do the two farms have, together?
Model answer (final answer correct): Twice the size of Farmer Brown's 200-acre farm is $2 * 200=\approx 2 * 200=400 \approx 400$ acres.
Farmer Smith's farm is 100 more acres than twice the size of Farmer Brown's farm, or $100+400=\approx 100+400=500 \approx 500$ acres.
Therefore, together, the two farms have $500+200=\approx 500+200=700 \approx 700$ acres.
Final answer: 700</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Note that the outcome-based vs. process-based spectrum applies in general to supervising any sequence of actions based on the resulting outcomes, or based on each action individually. Because the only actions in GSM8K are reasoning steps, we typically refer directly to reasoning steps in this paper, but the different approaches to supervision apply more generally.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
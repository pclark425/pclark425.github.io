<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2474 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2474</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2474</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-260400794</p>
                <p><strong>Paper Title:</strong> Active Machine Learning for Chemical Engineers: a Bright 1 Future Lies Ahead!</p>
                <p><strong>Paper Abstract:</strong> : 17 By combining machine learning with design of experiments, so-called active machine learning, 18 more efficient and cheaper research can be conducted. Machine learning algorithms are more 19 flexible, and are better at investigating the processes spanning all length scales of chemical 20 engineering. While the active machine learning algorithms are maturing, its applications are 21 lacking behind. Three types of challenges faced by active machine learning are identified and 22 ways to overcome them are discussed: the convincing of the experimental researcher, the 23 flexibility of data creation, and the robustness of the active machine learning algorithms. A 24 bright future lies ahead for active machine learning in chemical engineering thanks to increasing 25 automation and more efficient algorithms to drive novel discoveries. 26</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2474.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2474.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Active ML</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Active Machine Learning (active learning + Bayesian optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative supervised learning workflow that lets the model select new experiments to perform, using acquisition criteria (uncertainty, expected improvement) to minimize experiments while maximizing information or objective value.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Active machine learning (general framework)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A two-branch iterative DoE framework combining active learning (for modeling/coverage of a design space) and Bayesian optimization (for objective optimization). Workflow: initialization (define problem and design space) → iteratively query selection by acquisition functions based on surrogate model uncertainty or expected improvement → perform experiments or expensive simulations → update model and repeat. Surrogate models (e.g., Gaussian processes, Bayesian neural nets) provide uncertainty estimates used to guide selection. Supports extensions such as multi-objective BO, multi-fidelity pretraining, constrained and batch selection, and transfer learning.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Chemical engineering broadly (reaction and reactor design, catalyst and molecule discovery, CFD-based reactor optimization, materials discovery); also computationally expensive simulations and molecular design.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Selects experiments iteratively using acquisition functions derived from surrogate model predictions and uncertainties (uncertainty sampling for active learning; acquisition functions like expected improvement or other utility functions for BO), thereby allocating experimental budget to points expected to maximize information gain or objective improvement per experiment. Can be adapted to batch selection, constrained feasibility, and multi-objective trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Typically measured as number of performed experiments or expensive simulations (e.g., CFD runs, ab initio calculations), and/or wall-clock time and experimental throughput; the paper emphasizes experiment/simulation count and time-to-next-datapoint as primary cost proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Uncertainty reduction (model predictive variance), expected improvement (EI) or other acquisition-function-derived expected utility; in active learning, uncertainty-based criteria; in BO, expected improvement / surrogate-model acquisition scores.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Implemented via acquisition functions that trade off model-predicted mean (exploitation) and predictive uncertainty (exploration) — e.g., uncertainty sampling, EI, upper confidence bound variants; initial iterations tend to explore broadly (full design-space exploration) and later exploit promising regions.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Diversity is encouraged implicitly by uncertainty-driven exploration and explicitly via multi-objective BO or by enforcing constraints and batch-selection heuristics; the paper discusses using multi-objective approaches and measuring synthesizability-versus-creativity trade-offs and recommends measuring multiple outputs and pretraining for broader coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed experiment/simulation count, time limits (time-to-next-sample, stabilization times), monetary cost, and computational resource constraints for simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Incorporate constraints into acquisition decision (e.g., preference for time-efficient experiments, constrained BO); use batch selection for high-throughput settings; pretrain with low-cost/low-fidelity data (multi-fidelity) to reduce costly queries; add practical constraints (e.g., prefer increasing temperature over decreasing if apparatus easier to heat) to bias selections toward lower-cost actions.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Implicitly defined by objective improvement (BO) or by discovery of high-performing candidates (e.g., high yield, novel compositions); breakthrough potential assessed via objective maxima found and novelty/synthesizability considerations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Examples cited in the paper: Shields et al. reached >99% yield after 40 experiments vs standard 60% (Mitsunobu reaction); Nugraha et al. found optimal catalyst composition after 47 experiments out of 5151 possible; Schweidtmann et al. identified Pareto front after 68 experiments for a 4D reaction optimization; Ureel et al. reported benefits with as few as 18 experiments. Metrics reported: number of experiments to achieve optimum/pareto-front, achieved yield or objective values, and model prediction accuracy over design space.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against traditional DoE or manual/standard reaction conditions and often random or exhaustive search baselines; traditional single-factor-at-a-time strategies are contrasted.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Reported substantial improvements in experiment efficiency and outcome quality: e.g., Shields improved yield from typical 60% to >99% in 40 experiments; Nugraha found optima with 47 experiments versus 5151 total possibilities, demonstrating far fewer experiments needed than exhaustive search.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Reported reductions in experiments needed (examples: optimal found in 47/5151 trials; Pareto front in 68 experiments for 4D optimization; benefits seen with as few as 18 experiments), qualitative statements of large time and cost savings for expensive simulations/experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper discusses trade-offs qualitatively: early-wide exploration can produce counter-intuitive queries that reduce user trust; synthesizability vs creativity (larger design spaces increase creativity but reduce guaranteed synthesizability); multi-fidelity and pretraining reduce cost but introduce heterogeneity/noise trade-offs; batch vs sequential selection trades off throughput and optimality; need to include multiple outputs to enhance reusability.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Recommendations: incorporate prior/literature data (pretraining, multi-fidelity) to improve initial allocations, enforce realistic experimental constraints in acquisition (feasibility, apparatus limitations, batching rules), prefer representations guaranteeing synthesizability (or use latent-space generative models plus constraint classifiers), use heteroscedastic or multi-fidelity models to handle differing noise levels, and collaborate closely with experimentalists to encode cost/time constraints; these improve allocation efficiency and robustness.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2474.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2474.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-fidelity AL / Pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-fidelity Active Machine Learning and Pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods that combine abundant low-fidelity data with sparse high-fidelity experiments to pretrain or inform surrogate models, reducing the number of costly experiments while improving initial allocations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Multi-fidelity active learning / active transfer learning</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses datasets of differing fidelities (e.g., low-quality/high-throughput experimental or simulated data and high-quality expensive experiments) to pretrain or co-train surrogate models that estimate both mean and fidelity relationships; active queries then prioritize high-value high-fidelity experiments where they are most informative. Relatedly, active transfer learning leverages related but not identical datasets to initialize models and guide early selections.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Chemical engineering experiments and simulations, materials design, molecular property prediction, reaction condition prediction — any context with heterogeneous data fidelities.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate limited high-fidelity experimental budget to regions where the multi-fidelity model predicts largest expected value of information relative to cost, while using low-fidelity data to reduce uncertainty elsewhere; initial queries influenced by pretraining to avoid wasted early exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Cost measured by fidelity-specific expense (cheap low-fidelity data count vs costly high-fidelity experiment/simulation runtime or monetary cost).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected reduction in high-fidelity model uncertainty conditioned on low-fidelity information; acquisition functions extended to multi-fidelity EI or value-of-information calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploitation favors regions predicted to be high-performing in high-fidelity surrogate; exploration encouraged where multi-fidelity disagreement or uncertainty is large. Pretraining biases early iterations toward exploitation of known-good regions while retaining exploration where low-fidelity signals are weak.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Induced partly via uncertainty from low/high-fidelity disagreement and via transfer-learning across related datasets; no specific diversity algorithm described beyond leveraging varied data sources to broaden coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Monetary/experimental budget for high-fidelity measurements and computational budget for high-cost simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Prioritize low-cost low-fidelity data for model shaping; explicitly account for fidelity-specific costs in the acquisition criterion to select high-fidelity experiments only when expected information per cost is high.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Higher probability of finding high-performing candidates per high-fidelity experiment owing to better-informed priors; measured indirectly via faster convergence or fewer high-cost experiments required.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper cites multi-fidelity approaches as promising; specific numeric gains are not provided in this perspective, but references to works on pretraining improving BO and multi-fidelity model case studies are given.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared conceptually to single-fidelity active learning or naive initialization from scratch.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Reported qualitative improvement in initial experiment selection and overall performance; concrete quantitative results deferred to cited references.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Said to improve initial selection confidence and reduce wasted expensive experiments, but no single numeric aggregate given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Highlights need to handle differing noise quality between literature (low-fidelity) and new data (high-fidelity); heteroscedastic models alone may not be sufficient, and multi-fidelity frameworks that explicitly model fidelities are recommended.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Use multi-fidelity pretraining and active transfer learning to seed models and reduce early exploration costs; explicitly model and include fidelity-specific noise and cost in acquisition functions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2474.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2474.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Constrained BO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Constrained Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Bayesian optimization variants that incorporate feasibility or experimental constraints (known or learned) into the acquisition process, ensuring proposed queries are executable and safe.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Constrained Bayesian Optimization for Automatic Chemical Design</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Constrained Bayesian optimization</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Incorporates constraint models (either known analytic constraints or learned feasibility classifiers) into the BO acquisition rule so that proposed designs maximize expected improvement (or other utility) subject to feasibility probability thresholds. Can be combined with latent-space representations to ensure synthesizability and with learned classifiers to reject infeasible simulations or experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecular and catalyst design, automated chemical design, computationally expensive simulations (CFD) where some parameter combinations are infeasible or unsafe.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Optimizes acquisition under feasibility constraints: only allocate experiments to candidates that satisfy known constraints or have high predicted feasibility; often uses constrained EI or penalized acquisition functions that reduce allocation to infeasible/unsafe choices.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of feasible high-cost experiments/simulations; cost of evaluating feasibility classifiers may be small relative to full experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Constrained expected improvement / acquisition values conditioned on feasibility; uncertainty in both objective and constraint models informs utility.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Acquisition balances querying promising high-utility candidates that are also feasible vs exploring uncertain-but-potentially-feasible regions (via probabilistic feasibility estimates).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Feasibility-aware exploration leads to diversity constrained to the feasible manifold; latent-space constraints can be combined to explore structurally diverse but synthesizable candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Experimental safety/feasibility constraints plus typical budget constraints (experiment count/time).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Incorporate constraints directly into acquisition; use feasibility classification to block costly infeasible experiments and reduce wasted budget.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Objective improvement within feasible region; degree of novelty constrained by synthesizability criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>This paper cites constrained BO works (Griffiths & Hernández-Lobato) but does not report new numerical performance; referenced works demonstrate practical gains in chemical design.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared conceptually to unconstrained BO (which may propose infeasible queries) and to manual constrained search.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Constrained BO avoids infeasible/wasteful experiments and thus improves effective utilization of budget; quantitative comparisons present in cited works rather than in this perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Efficiency arises from avoiding infeasible experiments and focusing budget on feasible, high-utility candidates; no single number given here.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper emphasizes trade-off between creativity (large unconstrained search) and synthesizability/feasibility; constrained BO recommended to ensure realistic experimental allocations and safety.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Incorporate known experimental constraints and learned feasibility models to direct acquisition toward viable candidates; combine with latent-space generative models for synthesizable proposal generation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2474.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2474.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>pBO-2GP-3B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>pBO-2GP-3B (batch parallel known/unknown constrained Bayesian optimization with feasibility classification)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A batch-parallel constrained Bayesian optimization algorithm that classifies feasibility and selects batches of queries for computationally expensive simulations like CFD.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>pBO-2GP-3B:  A batch parallel known/unknown constrained Bayesian optimization with feasibility classification and its applications in computational fluid dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>pBO-2GP-3B (batch parallel constrained BO)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A BO variant designed for batch parallel selection under mixed known/unknown constraints, using two Gaussian process surrogates (for objective and constraints) and feasibility classification to avoid allocating batch members that are likely infeasible; tailored for expensive CFD simulations where batches of designs are evaluated in parallel.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Computational fluid dynamics and other expensive simulation-driven design problems where parallel evaluations are used.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Selects batches of candidate simulations that maximize a constrained acquisition function while screening with a feasibility classifier to reduce wasted simulation runs; balances the finite parallel computational resources among batch members to maximize expected utility.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of expensive CFD simulations (wall-clock runtime per simulation) and the degree of parallel compute utilization (batch size).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Constrained expected utility across the batch considering surrogate uncertainty and predicted feasibility; batch-aware acquisition approximations of information gain.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Batch acquisition trades off exploring uncertain regions and exploiting promising ones across batch members; feasibility classifier steers batch composition away from infeasible points.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Batch selection inherently allows diversity inside a batch; acquisition design can encourage coverage across promising/uncertain regions while maintaining feasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Parallel computational resource limits (number of concurrent CFD jobs), and total allowed simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Optimize batch composition given parallel compute budget to maximize constrained expected utility; avoid infeasible simulations by feasibility classification.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Improved multi-objective metrics in reactor design (e.g., maximize gas-holdup, minimize power consumption) per number of CFD runs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Cited as an application for CFD; specific numeric gains are reported in the original paper (Tran et al.) but are not enumerated in detail in this perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against naive or sequential BO and unconstrained batch selection; original work benchmarks on CFD problems.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Described as improving efficiency for CFD-driven optimization by reducing infeasible runs and leveraging batch parallelism — precise quantitative comparisons are in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Reduces wasted simulations through feasibility screening and accelerates search by parallel batch evaluations; no aggregate number provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Highlights trade-offs between parallel throughput and the increased risk of proposing infeasible or low-value batch members; feasibility classification mitigates this.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>When CFD or other expensive simulations are used, batch-constrained BO with feasibility screening maximizes use of parallel compute while avoiding infeasible experiments; encoding known/learned constraints is critical.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2474.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2474.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Heteroscedastic BO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Heteroscedastic Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>BO methods that explicitly model input-dependent (heteroscedastic) measurement noise to achieve robustness when experimental noise varies across the design space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Achieving robustness to aleatoric uncertainty with heteroscedastic Bayesian optimisation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Heteroscedastic Bayesian optimization</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Surrogate models (e.g., Gaussian processes with heteroscedastic likelihoods) estimate both predictive mean and input-dependent noise levels; acquisition functions are adapted to account for heteroscedastic noise so that decisions reflect uncertainty from both epistemic and aleatoric sources.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Chemical experiments and simulations where measurement noise or experiment reliability varies across conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Preferentially allocate expensive experiments to regions where epistemic uncertainty or expected improvement is high relative to irreducible aleatoric noise; avoid spending budget where high aleatoric noise reduces information return.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of experiments/simulations and costs wasted due to noisy/low-information experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Acquisition functions modified to discount expected improvement by aleatoric uncertainty; focus on epistemic information gain.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Balance via acquisition functions that consider both reducible (epistemic) and irreducible (aleatoric) uncertainties; exploration favored where epistemic uncertainty is large and aleatoric noise is low enough to permit informative measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity mechanism beyond uncertainty-aware selection; heteroscedastic modeling can encourage sampling diverse low-noise informative regions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed experimental budget; desire to avoid budget spent on high-aleatoric-noise regions.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Account for variable noise in acquisition to improve budget allocation by selecting experiments with higher signal-to-noise expected utility.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Higher reliability of discoveries by favoring low-aleatoric-noise informative experiments; breakthrough defined by objective improvement with reliable measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper references heteroscedastic BO as a robustness improvement; specific experimental numbers are in referenced works rather than this perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared conceptually to homoscedastic BO which ignores input-dependent noise.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Expected improved robustness and fewer wasted experiments in noisy regions; quantitative details available in the cited heteroscedastic BO literature.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Improved effective information per expensive experiment by avoiding high-aleatoric-noise regions; no single numeric value provided in the perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Emphasizes distinguishing reducible vs irreducible uncertainty to better allocate budget; heteroscedastic methods are recommended where noise varies meaningfully.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Model aleatoric noise explicitly and adapt acquisition functions accordingly to allocate expensive experiments where reducible uncertainty is largest relative to noise level.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2474.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2474.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Latent-space + constrained BO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Latent-space Generative Models combined with Constrained Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of learned continuous latent representations (VAE/GAN) of synthesizable molecules/catalysts together with constrained BO to propose novel but synthesizable candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Constrained Bayesian optimization for automatic chemical design using variational autoencoders</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Latent-space generative design + constrained BO</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Train generative models (variational auto-encoders, GANs) on sets of synthesizable molecules/catalysts to produce a continuous latent representation (latent space). Perform BO in latent space with constraints (e.g., synthesizability classifiers, safety) to propose novel but synthesizable candidates; decode latent points into molecular/catalyst recipes for synthesis and testing.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecular and catalyst design, automated chemical design, inverse molecular design.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate experimental budget to latent-space candidates with high expected objective improvement and high predicted synthesizability/feasibility; constraints and feasibility classifiers filter proposals to avoid infeasible costly synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of synthesized compounds (wet-lab experiments) and associated monetary/time cost; decoding and feasibility checks are inexpensive relative to synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected improvement or uncertainty reduction computed in latent space, conditioned on feasibility/synthesizability constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>BO in latent space trades off exploration of novel latent regions and exploitation of regions predicted to yield high property values, with constraints ensuring practical synthesizability.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Latent-space sampling enables structurally diverse candidate generation; constrained BO and objective-augmented generative objectives (e.g., ORGANIC) can encourage diversity or novelty while maintaining validity.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Monetary/experimental cost of synthesis and testing, and safety/synthesizability constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Use latent-space constraints and feasibility classifiers to avoid wasting budget on unsynthesizable designs; acquisition can penalize low-feasibility or high-cost candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Novelty and high objective performance among synthesized/validated candidates; novelty measured by distance from training set in latent/feature space.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Referenced works demonstrate discovery of novel molecules/designs using constrained BO + VAE; the perspective does not present new numerical results but cites successes in literature.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to unconstrained generative proposals and naive BO in discrete chemical space; constrained latent BO reduces infeasible suggestions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Cited literature shows constrained latent-space BO yields more synthesizable proposals and better use of experimental budget; details in original papers.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Reduces wasted synthesis attempts on invalid molecules and increases hit-rate of synthesizable high-performing candidates; no single number in the perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper highlights the trade-off between creativity (large latent-space exploration) and guaranteed synthesizability; recommends learned representations plus constraints to harmonize both.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Use learned latent representations to propose valid candidates and enforce synthesizability via constraints or decoding validity checks; combine with BO acquisition calibrated for feasibility and cost.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2474.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2474.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Automated platforms</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated and Reconfigurable Experimental Platforms (robotic chemists, reconfigurable systems)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Hardware platforms (robotic chemists, reconfigurable automated reactors) that execute experimental campaigns under algorithmic control, enabling closed-loop active learning and fast parallel data acquisition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A mobile robotic chemist</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Automated experimental platforms / robotic chemists</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Physical laboratory automation systems — reconfigurable platforms, mobile robotic chemists, and high-throughput reactors — that can be coupled to active ML algorithms to execute suggested experiments, collect data, and feed results back to the model in closed-loop. Requirements include reconfigurability, broad operating ranges, and safety constraint encoding. Batch operation and parallelization are common features.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecular synthesis, catalyst synthesis and testing, flow chemistry, automated reaction optimization, high-throughput materials discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Active ML algorithms allocate experiments to the automated platform, often using batch selection to match platform parallelism, while the platform's operational constraints (e.g., fixed batch variables, heating/cooling times) inform feasible allocations.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Experiment throughput (number of experiments per unit time), platform reconfiguration time, and monetary cost of automated operation; also includes simulation compute when virtual experiments are used.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Acquisition scores from the coupled active ML model (uncertainty, EI) mapped to platform-executable batches; information gain often measured per wall-clock time or per batch executed.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Acquisition functions guide which experiments the platform runs; batch/scheduling heuristics account for platform-specific time/cost constraints (e.g., prefer sequences that reduce heating/cooling overhead).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Batch design and DoE principles can enforce diversity across parallel experiments; platform constraints may reduce or shape diversity (e.g., fixed temperatures per batch).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Time throughput, monetary cost, safety limits, and apparatus-specific operational constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Encourage acquisition to prefer experiments that minimize platform overhead (e.g., temperature ramp direction), enforce batching constraints (fix variables per batch), and include safety constraints to avoid hazardous conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Number of high-performing synthesized candidates or improved process metrics achieved per unit time; also can be measured by speed-to-optimum.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Perspective cites works where automated platforms coupled with active ML produced accelerated optimization (multiple literature references), but does not give new aggregate numeric metrics here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to manual experimentation and non-automated sequential active ML; automated + active ML yields faster wall-clock discovery and higher throughput.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Described qualitatively as yielding huge time savings and enabling autonomous scientific discovery; numerical specifics are in the referenced platform papers.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Enables faster experiment cycles, parallelism, and efficient use of acquisition strategies; concrete percentages/times depend on platform and task (not quantified here).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper stresses trade-offs between automation cost/complexity and experimental flexibility/reusability; also highlights safety and the need to encode constraints to prevent dangerous automated experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Tight coupling of active ML with platform operational constraints (batch rules, ramping times, reconfigurability) and safety knowledge improves effective allocation and accelerates discovery.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Constrained Bayesian optimization for automatic chemical design using variational autoencoders <em>(Rating: 2)</em></li>
                <li>pBO-2GP-3B:  A batch parallel known/unknown constrained Bayesian optimization with feasibility classification and its applications in computational fluid dynamics <em>(Rating: 2)</em></li>
                <li>Achieving robustness to aleatoric uncertainty with heteroscedastic Bayesian optimisation <em>(Rating: 2)</em></li>
                <li>On-the-fly closed-loop materials discovery via Bayesian active learning <em>(Rating: 2)</em></li>
                <li>Accelerated discovery of CO2 electrocatalysts using active machine learning <em>(Rating: 2)</em></li>
                <li>Pre-training helps Bayesian optimization too <em>(Rating: 2)</em></li>
                <li>Reconfigurable system for automated optimization of diverse chemical reactions <em>(Rating: 2)</em></li>
                <li>A mobile robotic chemist <em>(Rating: 2)</em></li>
                <li>Active Learning for Materials Design and Discovery <em>(Rating: 1)</em></li>
                <li>Active learning across intermetallics to guide discovery of electrocatalysts for CO2 reduction and H2 evolution <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2474",
    "paper_id": "paper-260400794",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "Active ML",
            "name_full": "Active Machine Learning (active learning + Bayesian optimization)",
            "brief_description": "An iterative supervised learning workflow that lets the model select new experiments to perform, using acquisition criteria (uncertainty, expected improvement) to minimize experiments while maximizing information or objective value.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "Active machine learning (general framework)",
            "system_description": "A two-branch iterative DoE framework combining active learning (for modeling/coverage of a design space) and Bayesian optimization (for objective optimization). Workflow: initialization (define problem and design space) → iteratively query selection by acquisition functions based on surrogate model uncertainty or expected improvement → perform experiments or expensive simulations → update model and repeat. Surrogate models (e.g., Gaussian processes, Bayesian neural nets) provide uncertainty estimates used to guide selection. Supports extensions such as multi-objective BO, multi-fidelity pretraining, constrained and batch selection, and transfer learning.",
            "application_domain": "Chemical engineering broadly (reaction and reactor design, catalyst and molecule discovery, CFD-based reactor optimization, materials discovery); also computationally expensive simulations and molecular design.",
            "resource_allocation_strategy": "Selects experiments iteratively using acquisition functions derived from surrogate model predictions and uncertainties (uncertainty sampling for active learning; acquisition functions like expected improvement or other utility functions for BO), thereby allocating experimental budget to points expected to maximize information gain or objective improvement per experiment. Can be adapted to batch selection, constrained feasibility, and multi-objective trade-offs.",
            "computational_cost_metric": "Typically measured as number of performed experiments or expensive simulations (e.g., CFD runs, ab initio calculations), and/or wall-clock time and experimental throughput; the paper emphasizes experiment/simulation count and time-to-next-datapoint as primary cost proxies.",
            "information_gain_metric": "Uncertainty reduction (model predictive variance), expected improvement (EI) or other acquisition-function-derived expected utility; in active learning, uncertainty-based criteria; in BO, expected improvement / surrogate-model acquisition scores.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Implemented via acquisition functions that trade off model-predicted mean (exploitation) and predictive uncertainty (exploration) — e.g., uncertainty sampling, EI, upper confidence bound variants; initial iterations tend to explore broadly (full design-space exploration) and later exploit promising regions.",
            "diversity_mechanism": "Diversity is encouraged implicitly by uncertainty-driven exploration and explicitly via multi-objective BO or by enforcing constraints and batch-selection heuristics; the paper discusses using multi-objective approaches and measuring synthesizability-versus-creativity trade-offs and recommends measuring multiple outputs and pretraining for broader coverage.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed experiment/simulation count, time limits (time-to-next-sample, stabilization times), monetary cost, and computational resource constraints for simulations.",
            "budget_constraint_handling": "Incorporate constraints into acquisition decision (e.g., preference for time-efficient experiments, constrained BO); use batch selection for high-throughput settings; pretrain with low-cost/low-fidelity data (multi-fidelity) to reduce costly queries; add practical constraints (e.g., prefer increasing temperature over decreasing if apparatus easier to heat) to bias selections toward lower-cost actions.",
            "breakthrough_discovery_metric": "Implicitly defined by objective improvement (BO) or by discovery of high-performing candidates (e.g., high yield, novel compositions); breakthrough potential assessed via objective maxima found and novelty/synthesizability considerations.",
            "performance_metrics": "Examples cited in the paper: Shields et al. reached &gt;99% yield after 40 experiments vs standard 60% (Mitsunobu reaction); Nugraha et al. found optimal catalyst composition after 47 experiments out of 5151 possible; Schweidtmann et al. identified Pareto front after 68 experiments for a 4D reaction optimization; Ureel et al. reported benefits with as few as 18 experiments. Metrics reported: number of experiments to achieve optimum/pareto-front, achieved yield or objective values, and model prediction accuracy over design space.",
            "comparison_baseline": "Compared against traditional DoE or manual/standard reaction conditions and often random or exhaustive search baselines; traditional single-factor-at-a-time strategies are contrasted.",
            "performance_vs_baseline": "Reported substantial improvements in experiment efficiency and outcome quality: e.g., Shields improved yield from typical 60% to &gt;99% in 40 experiments; Nugraha found optima with 47 experiments versus 5151 total possibilities, demonstrating far fewer experiments needed than exhaustive search.",
            "efficiency_gain": "Reported reductions in experiments needed (examples: optimal found in 47/5151 trials; Pareto front in 68 experiments for 4D optimization; benefits seen with as few as 18 experiments), qualitative statements of large time and cost savings for expensive simulations/experiments.",
            "tradeoff_analysis": "Paper discusses trade-offs qualitatively: early-wide exploration can produce counter-intuitive queries that reduce user trust; synthesizability vs creativity (larger design spaces increase creativity but reduce guaranteed synthesizability); multi-fidelity and pretraining reduce cost but introduce heterogeneity/noise trade-offs; batch vs sequential selection trades off throughput and optimality; need to include multiple outputs to enhance reusability.",
            "optimal_allocation_findings": "Recommendations: incorporate prior/literature data (pretraining, multi-fidelity) to improve initial allocations, enforce realistic experimental constraints in acquisition (feasibility, apparatus limitations, batching rules), prefer representations guaranteeing synthesizability (or use latent-space generative models plus constraint classifiers), use heteroscedastic or multi-fidelity models to handle differing noise levels, and collaborate closely with experimentalists to encode cost/time constraints; these improve allocation efficiency and robustness.",
            "uuid": "e2474.0"
        },
        {
            "name_short": "Multi-fidelity AL / Pretraining",
            "name_full": "Multi-fidelity Active Machine Learning and Pretraining",
            "brief_description": "Methods that combine abundant low-fidelity data with sparse high-fidelity experiments to pretrain or inform surrogate models, reducing the number of costly experiments while improving initial allocations.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Multi-fidelity active learning / active transfer learning",
            "system_description": "Uses datasets of differing fidelities (e.g., low-quality/high-throughput experimental or simulated data and high-quality expensive experiments) to pretrain or co-train surrogate models that estimate both mean and fidelity relationships; active queries then prioritize high-value high-fidelity experiments where they are most informative. Relatedly, active transfer learning leverages related but not identical datasets to initialize models and guide early selections.",
            "application_domain": "Chemical engineering experiments and simulations, materials design, molecular property prediction, reaction condition prediction — any context with heterogeneous data fidelities.",
            "resource_allocation_strategy": "Allocate limited high-fidelity experimental budget to regions where the multi-fidelity model predicts largest expected value of information relative to cost, while using low-fidelity data to reduce uncertainty elsewhere; initial queries influenced by pretraining to avoid wasted early exploration.",
            "computational_cost_metric": "Cost measured by fidelity-specific expense (cheap low-fidelity data count vs costly high-fidelity experiment/simulation runtime or monetary cost).",
            "information_gain_metric": "Expected reduction in high-fidelity model uncertainty conditioned on low-fidelity information; acquisition functions extended to multi-fidelity EI or value-of-information calculations.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Exploitation favors regions predicted to be high-performing in high-fidelity surrogate; exploration encouraged where multi-fidelity disagreement or uncertainty is large. Pretraining biases early iterations toward exploitation of known-good regions while retaining exploration where low-fidelity signals are weak.",
            "diversity_mechanism": "Induced partly via uncertainty from low/high-fidelity disagreement and via transfer-learning across related datasets; no specific diversity algorithm described beyond leveraging varied data sources to broaden coverage.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Monetary/experimental budget for high-fidelity measurements and computational budget for high-cost simulations.",
            "budget_constraint_handling": "Prioritize low-cost low-fidelity data for model shaping; explicitly account for fidelity-specific costs in the acquisition criterion to select high-fidelity experiments only when expected information per cost is high.",
            "breakthrough_discovery_metric": "Higher probability of finding high-performing candidates per high-fidelity experiment owing to better-informed priors; measured indirectly via faster convergence or fewer high-cost experiments required.",
            "performance_metrics": "Paper cites multi-fidelity approaches as promising; specific numeric gains are not provided in this perspective, but references to works on pretraining improving BO and multi-fidelity model case studies are given.",
            "comparison_baseline": "Compared conceptually to single-fidelity active learning or naive initialization from scratch.",
            "performance_vs_baseline": "Reported qualitative improvement in initial experiment selection and overall performance; concrete quantitative results deferred to cited references.",
            "efficiency_gain": "Said to improve initial selection confidence and reduce wasted expensive experiments, but no single numeric aggregate given in this paper.",
            "tradeoff_analysis": "Highlights need to handle differing noise quality between literature (low-fidelity) and new data (high-fidelity); heteroscedastic models alone may not be sufficient, and multi-fidelity frameworks that explicitly model fidelities are recommended.",
            "optimal_allocation_findings": "Use multi-fidelity pretraining and active transfer learning to seed models and reduce early exploration costs; explicitly model and include fidelity-specific noise and cost in acquisition functions.",
            "uuid": "e2474.1"
        },
        {
            "name_short": "Constrained BO",
            "name_full": "Constrained Bayesian Optimization",
            "brief_description": "Bayesian optimization variants that incorporate feasibility or experimental constraints (known or learned) into the acquisition process, ensuring proposed queries are executable and safe.",
            "citation_title": "Constrained Bayesian Optimization for Automatic Chemical Design",
            "mention_or_use": "mention",
            "system_name": "Constrained Bayesian optimization",
            "system_description": "Incorporates constraint models (either known analytic constraints or learned feasibility classifiers) into the BO acquisition rule so that proposed designs maximize expected improvement (or other utility) subject to feasibility probability thresholds. Can be combined with latent-space representations to ensure synthesizability and with learned classifiers to reject infeasible simulations or experiments.",
            "application_domain": "Molecular and catalyst design, automated chemical design, computationally expensive simulations (CFD) where some parameter combinations are infeasible or unsafe.",
            "resource_allocation_strategy": "Optimizes acquisition under feasibility constraints: only allocate experiments to candidates that satisfy known constraints or have high predicted feasibility; often uses constrained EI or penalized acquisition functions that reduce allocation to infeasible/unsafe choices.",
            "computational_cost_metric": "Number of feasible high-cost experiments/simulations; cost of evaluating feasibility classifiers may be small relative to full experiments.",
            "information_gain_metric": "Constrained expected improvement / acquisition values conditioned on feasibility; uncertainty in both objective and constraint models informs utility.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Acquisition balances querying promising high-utility candidates that are also feasible vs exploring uncertain-but-potentially-feasible regions (via probabilistic feasibility estimates).",
            "diversity_mechanism": "Feasibility-aware exploration leads to diversity constrained to the feasible manifold; latent-space constraints can be combined to explore structurally diverse but synthesizable candidates.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Experimental safety/feasibility constraints plus typical budget constraints (experiment count/time).",
            "budget_constraint_handling": "Incorporate constraints directly into acquisition; use feasibility classification to block costly infeasible experiments and reduce wasted budget.",
            "breakthrough_discovery_metric": "Objective improvement within feasible region; degree of novelty constrained by synthesizability criteria.",
            "performance_metrics": "This paper cites constrained BO works (Griffiths & Hernández-Lobato) but does not report new numerical performance; referenced works demonstrate practical gains in chemical design.",
            "comparison_baseline": "Compared conceptually to unconstrained BO (which may propose infeasible queries) and to manual constrained search.",
            "performance_vs_baseline": "Constrained BO avoids infeasible/wasteful experiments and thus improves effective utilization of budget; quantitative comparisons present in cited works rather than in this perspective.",
            "efficiency_gain": "Efficiency arises from avoiding infeasible experiments and focusing budget on feasible, high-utility candidates; no single number given here.",
            "tradeoff_analysis": "Paper emphasizes trade-off between creativity (large unconstrained search) and synthesizability/feasibility; constrained BO recommended to ensure realistic experimental allocations and safety.",
            "optimal_allocation_findings": "Incorporate known experimental constraints and learned feasibility models to direct acquisition toward viable candidates; combine with latent-space generative models for synthesizable proposal generation.",
            "uuid": "e2474.2"
        },
        {
            "name_short": "pBO-2GP-3B",
            "name_full": "pBO-2GP-3B (batch parallel known/unknown constrained Bayesian optimization with feasibility classification)",
            "brief_description": "A batch-parallel constrained Bayesian optimization algorithm that classifies feasibility and selects batches of queries for computationally expensive simulations like CFD.",
            "citation_title": "pBO-2GP-3B:  A batch parallel known/unknown constrained Bayesian optimization with feasibility classification and its applications in computational fluid dynamics",
            "mention_or_use": "mention",
            "system_name": "pBO-2GP-3B (batch parallel constrained BO)",
            "system_description": "A BO variant designed for batch parallel selection under mixed known/unknown constraints, using two Gaussian process surrogates (for objective and constraints) and feasibility classification to avoid allocating batch members that are likely infeasible; tailored for expensive CFD simulations where batches of designs are evaluated in parallel.",
            "application_domain": "Computational fluid dynamics and other expensive simulation-driven design problems where parallel evaluations are used.",
            "resource_allocation_strategy": "Selects batches of candidate simulations that maximize a constrained acquisition function while screening with a feasibility classifier to reduce wasted simulation runs; balances the finite parallel computational resources among batch members to maximize expected utility.",
            "computational_cost_metric": "Number of expensive CFD simulations (wall-clock runtime per simulation) and the degree of parallel compute utilization (batch size).",
            "information_gain_metric": "Constrained expected utility across the batch considering surrogate uncertainty and predicted feasibility; batch-aware acquisition approximations of information gain.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Batch acquisition trades off exploring uncertain regions and exploiting promising ones across batch members; feasibility classifier steers batch composition away from infeasible points.",
            "diversity_mechanism": "Batch selection inherently allows diversity inside a batch; acquisition design can encourage coverage across promising/uncertain regions while maintaining feasibility.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Parallel computational resource limits (number of concurrent CFD jobs), and total allowed simulations.",
            "budget_constraint_handling": "Optimize batch composition given parallel compute budget to maximize constrained expected utility; avoid infeasible simulations by feasibility classification.",
            "breakthrough_discovery_metric": "Improved multi-objective metrics in reactor design (e.g., maximize gas-holdup, minimize power consumption) per number of CFD runs.",
            "performance_metrics": "Cited as an application for CFD; specific numeric gains are reported in the original paper (Tran et al.) but are not enumerated in detail in this perspective.",
            "comparison_baseline": "Compared against naive or sequential BO and unconstrained batch selection; original work benchmarks on CFD problems.",
            "performance_vs_baseline": "Described as improving efficiency for CFD-driven optimization by reducing infeasible runs and leveraging batch parallelism — precise quantitative comparisons are in the cited work.",
            "efficiency_gain": "Reduces wasted simulations through feasibility screening and accelerates search by parallel batch evaluations; no aggregate number provided here.",
            "tradeoff_analysis": "Highlights trade-offs between parallel throughput and the increased risk of proposing infeasible or low-value batch members; feasibility classification mitigates this.",
            "optimal_allocation_findings": "When CFD or other expensive simulations are used, batch-constrained BO with feasibility screening maximizes use of parallel compute while avoiding infeasible experiments; encoding known/learned constraints is critical.",
            "uuid": "e2474.3"
        },
        {
            "name_short": "Heteroscedastic BO",
            "name_full": "Heteroscedastic Bayesian Optimization",
            "brief_description": "BO methods that explicitly model input-dependent (heteroscedastic) measurement noise to achieve robustness when experimental noise varies across the design space.",
            "citation_title": "Achieving robustness to aleatoric uncertainty with heteroscedastic Bayesian optimisation",
            "mention_or_use": "mention",
            "system_name": "Heteroscedastic Bayesian optimization",
            "system_description": "Surrogate models (e.g., Gaussian processes with heteroscedastic likelihoods) estimate both predictive mean and input-dependent noise levels; acquisition functions are adapted to account for heteroscedastic noise so that decisions reflect uncertainty from both epistemic and aleatoric sources.",
            "application_domain": "Chemical experiments and simulations where measurement noise or experiment reliability varies across conditions.",
            "resource_allocation_strategy": "Preferentially allocate expensive experiments to regions where epistemic uncertainty or expected improvement is high relative to irreducible aleatoric noise; avoid spending budget where high aleatoric noise reduces information return.",
            "computational_cost_metric": "Number of experiments/simulations and costs wasted due to noisy/low-information experiments.",
            "information_gain_metric": "Acquisition functions modified to discount expected improvement by aleatoric uncertainty; focus on epistemic information gain.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Balance via acquisition functions that consider both reducible (epistemic) and irreducible (aleatoric) uncertainties; exploration favored where epistemic uncertainty is large and aleatoric noise is low enough to permit informative measurements.",
            "diversity_mechanism": "No explicit diversity mechanism beyond uncertainty-aware selection; heteroscedastic modeling can encourage sampling diverse low-noise informative regions.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed experimental budget; desire to avoid budget spent on high-aleatoric-noise regions.",
            "budget_constraint_handling": "Account for variable noise in acquisition to improve budget allocation by selecting experiments with higher signal-to-noise expected utility.",
            "breakthrough_discovery_metric": "Higher reliability of discoveries by favoring low-aleatoric-noise informative experiments; breakthrough defined by objective improvement with reliable measurements.",
            "performance_metrics": "Paper references heteroscedastic BO as a robustness improvement; specific experimental numbers are in referenced works rather than this perspective.",
            "comparison_baseline": "Compared conceptually to homoscedastic BO which ignores input-dependent noise.",
            "performance_vs_baseline": "Expected improved robustness and fewer wasted experiments in noisy regions; quantitative details available in the cited heteroscedastic BO literature.",
            "efficiency_gain": "Improved effective information per expensive experiment by avoiding high-aleatoric-noise regions; no single numeric value provided in the perspective.",
            "tradeoff_analysis": "Emphasizes distinguishing reducible vs irreducible uncertainty to better allocate budget; heteroscedastic methods are recommended where noise varies meaningfully.",
            "optimal_allocation_findings": "Model aleatoric noise explicitly and adapt acquisition functions accordingly to allocate expensive experiments where reducible uncertainty is largest relative to noise level.",
            "uuid": "e2474.4"
        },
        {
            "name_short": "Latent-space + constrained BO",
            "name_full": "Latent-space Generative Models combined with Constrained Bayesian Optimization",
            "brief_description": "Use of learned continuous latent representations (VAE/GAN) of synthesizable molecules/catalysts together with constrained BO to propose novel but synthesizable candidates.",
            "citation_title": "Constrained Bayesian optimization for automatic chemical design using variational autoencoders",
            "mention_or_use": "mention",
            "system_name": "Latent-space generative design + constrained BO",
            "system_description": "Train generative models (variational auto-encoders, GANs) on sets of synthesizable molecules/catalysts to produce a continuous latent representation (latent space). Perform BO in latent space with constraints (e.g., synthesizability classifiers, safety) to propose novel but synthesizable candidates; decode latent points into molecular/catalyst recipes for synthesis and testing.",
            "application_domain": "Molecular and catalyst design, automated chemical design, inverse molecular design.",
            "resource_allocation_strategy": "Allocate experimental budget to latent-space candidates with high expected objective improvement and high predicted synthesizability/feasibility; constraints and feasibility classifiers filter proposals to avoid infeasible costly synthesis.",
            "computational_cost_metric": "Number of synthesized compounds (wet-lab experiments) and associated monetary/time cost; decoding and feasibility checks are inexpensive relative to synthesis.",
            "information_gain_metric": "Expected improvement or uncertainty reduction computed in latent space, conditioned on feasibility/synthesizability constraints.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "BO in latent space trades off exploration of novel latent regions and exploitation of regions predicted to yield high property values, with constraints ensuring practical synthesizability.",
            "diversity_mechanism": "Latent-space sampling enables structurally diverse candidate generation; constrained BO and objective-augmented generative objectives (e.g., ORGANIC) can encourage diversity or novelty while maintaining validity.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Monetary/experimental cost of synthesis and testing, and safety/synthesizability constraints.",
            "budget_constraint_handling": "Use latent-space constraints and feasibility classifiers to avoid wasting budget on unsynthesizable designs; acquisition can penalize low-feasibility or high-cost candidates.",
            "breakthrough_discovery_metric": "Novelty and high objective performance among synthesized/validated candidates; novelty measured by distance from training set in latent/feature space.",
            "performance_metrics": "Referenced works demonstrate discovery of novel molecules/designs using constrained BO + VAE; the perspective does not present new numerical results but cites successes in literature.",
            "comparison_baseline": "Compared to unconstrained generative proposals and naive BO in discrete chemical space; constrained latent BO reduces infeasible suggestions.",
            "performance_vs_baseline": "Cited literature shows constrained latent-space BO yields more synthesizable proposals and better use of experimental budget; details in original papers.",
            "efficiency_gain": "Reduces wasted synthesis attempts on invalid molecules and increases hit-rate of synthesizable high-performing candidates; no single number in the perspective.",
            "tradeoff_analysis": "Paper highlights the trade-off between creativity (large latent-space exploration) and guaranteed synthesizability; recommends learned representations plus constraints to harmonize both.",
            "optimal_allocation_findings": "Use learned latent representations to propose valid candidates and enforce synthesizability via constraints or decoding validity checks; combine with BO acquisition calibrated for feasibility and cost.",
            "uuid": "e2474.5"
        },
        {
            "name_short": "Automated platforms",
            "name_full": "Automated and Reconfigurable Experimental Platforms (robotic chemists, reconfigurable systems)",
            "brief_description": "Hardware platforms (robotic chemists, reconfigurable automated reactors) that execute experimental campaigns under algorithmic control, enabling closed-loop active learning and fast parallel data acquisition.",
            "citation_title": "A mobile robotic chemist",
            "mention_or_use": "mention",
            "system_name": "Automated experimental platforms / robotic chemists",
            "system_description": "Physical laboratory automation systems — reconfigurable platforms, mobile robotic chemists, and high-throughput reactors — that can be coupled to active ML algorithms to execute suggested experiments, collect data, and feed results back to the model in closed-loop. Requirements include reconfigurability, broad operating ranges, and safety constraint encoding. Batch operation and parallelization are common features.",
            "application_domain": "Molecular synthesis, catalyst synthesis and testing, flow chemistry, automated reaction optimization, high-throughput materials discovery.",
            "resource_allocation_strategy": "Active ML algorithms allocate experiments to the automated platform, often using batch selection to match platform parallelism, while the platform's operational constraints (e.g., fixed batch variables, heating/cooling times) inform feasible allocations.",
            "computational_cost_metric": "Experiment throughput (number of experiments per unit time), platform reconfiguration time, and monetary cost of automated operation; also includes simulation compute when virtual experiments are used.",
            "information_gain_metric": "Acquisition scores from the coupled active ML model (uncertainty, EI) mapped to platform-executable batches; information gain often measured per wall-clock time or per batch executed.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Acquisition functions guide which experiments the platform runs; batch/scheduling heuristics account for platform-specific time/cost constraints (e.g., prefer sequences that reduce heating/cooling overhead).",
            "diversity_mechanism": "Batch design and DoE principles can enforce diversity across parallel experiments; platform constraints may reduce or shape diversity (e.g., fixed temperatures per batch).",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Time throughput, monetary cost, safety limits, and apparatus-specific operational constraints.",
            "budget_constraint_handling": "Encourage acquisition to prefer experiments that minimize platform overhead (e.g., temperature ramp direction), enforce batching constraints (fix variables per batch), and include safety constraints to avoid hazardous conditions.",
            "breakthrough_discovery_metric": "Number of high-performing synthesized candidates or improved process metrics achieved per unit time; also can be measured by speed-to-optimum.",
            "performance_metrics": "Perspective cites works where automated platforms coupled with active ML produced accelerated optimization (multiple literature references), but does not give new aggregate numeric metrics here.",
            "comparison_baseline": "Compared to manual experimentation and non-automated sequential active ML; automated + active ML yields faster wall-clock discovery and higher throughput.",
            "performance_vs_baseline": "Described qualitatively as yielding huge time savings and enabling autonomous scientific discovery; numerical specifics are in the referenced platform papers.",
            "efficiency_gain": "Enables faster experiment cycles, parallelism, and efficient use of acquisition strategies; concrete percentages/times depend on platform and task (not quantified here).",
            "tradeoff_analysis": "Paper stresses trade-offs between automation cost/complexity and experimental flexibility/reusability; also highlights safety and the need to encode constraints to prevent dangerous automated experiments.",
            "optimal_allocation_findings": "Tight coupling of active ML with platform operational constraints (batch rules, ramping times, reconfigurability) and safety knowledge improves effective allocation and accelerates discovery.",
            "uuid": "e2474.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Constrained Bayesian optimization for automatic chemical design using variational autoencoders",
            "rating": 2,
            "sanitized_title": "constrained_bayesian_optimization_for_automatic_chemical_design_using_variational_autoencoders"
        },
        {
            "paper_title": "pBO-2GP-3B:  A batch parallel known/unknown constrained Bayesian optimization with feasibility classification and its applications in computational fluid dynamics",
            "rating": 2,
            "sanitized_title": "pbo2gp3b_a_batch_parallel_knownunknown_constrained_bayesian_optimization_with_feasibility_classification_and_its_applications_in_computational_fluid_dynamics"
        },
        {
            "paper_title": "Achieving robustness to aleatoric uncertainty with heteroscedastic Bayesian optimisation",
            "rating": 2,
            "sanitized_title": "achieving_robustness_to_aleatoric_uncertainty_with_heteroscedastic_bayesian_optimisation"
        },
        {
            "paper_title": "On-the-fly closed-loop materials discovery via Bayesian active learning",
            "rating": 2,
            "sanitized_title": "onthefly_closedloop_materials_discovery_via_bayesian_active_learning"
        },
        {
            "paper_title": "Accelerated discovery of CO2 electrocatalysts using active machine learning",
            "rating": 2,
            "sanitized_title": "accelerated_discovery_of_co2_electrocatalysts_using_active_machine_learning"
        },
        {
            "paper_title": "Pre-training helps Bayesian optimization too",
            "rating": 2,
            "sanitized_title": "pretraining_helps_bayesian_optimization_too"
        },
        {
            "paper_title": "Reconfigurable system for automated optimization of diverse chemical reactions",
            "rating": 2,
            "sanitized_title": "reconfigurable_system_for_automated_optimization_of_diverse_chemical_reactions"
        },
        {
            "paper_title": "A mobile robotic chemist",
            "rating": 2,
            "sanitized_title": "a_mobile_robotic_chemist"
        },
        {
            "paper_title": "Active Learning for Materials Design and Discovery",
            "rating": 1,
            "sanitized_title": "active_learning_for_materials_design_and_discovery"
        },
        {
            "paper_title": "Active learning across intermetallics to guide discovery of electrocatalysts for CO2 reduction and H2 evolution",
            "rating": 1,
            "sanitized_title": "active_learning_across_intermetallics_to_guide_discovery_of_electrocatalysts_for_co2_reduction_and_h2_evolution"
        }
    ],
    "cost": 0.018737,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Active Machine Learning for Chemical Engineers: a Bright Future Lies Ahead!</p>
<p>Yannick Ureel 
22</p>
<p>Maarten R Dobbelaere 
Yi Ouyang 
Kevin De Ras 
Maarten K Sabbe 
Guy B Marin 
Kevin M Van Geem </p>
<p>Department of Materials, Textiles and Chemical 7 Engineering
Laboratory for Chemical Technology
Ghent University
Technologiepark 1259052GentBelgium</p>
<p>Active Machine Learning for Chemical Engineers: a Bright Future Lies Ahead!
1 of 22 1 8 9 10 * Corresponding author: Kevin.VanGeem@UGent.be, Technologiepark 125, 9052 Gent, 11 Belgium; 12 13 14Active machine learningActive learningBayesian optimizationChemical 15 engineeringDesign-of-experiments 16
17By combining machine learning with design of experiments, so-called active machine learning, 18 more efficient and cheaper research can be conducted. Machine learning algorithms are more 19 flexible, and are better at investigating the processes spanning all length scales of chemical 20 engineering. While the active machine learning algorithms are maturing, its applications are 21 lacking behind. Three types of challenges faced by active machine learning are identified and 22 ways to overcome them are discussed: the convincing of the experimental researcher, the 23 flexibility of data creation, and the robustness of the active machine learning algorithms. A 24 bright future lies ahead for active machine learning in chemical engineering thanks to increasing 25 automation and more efficient algorithms to drive novel discoveries. 26
 Figure 1 
illustrates the general workflow of active machine learning algorithms with first the 54 initialization followed by an iterative loop consisting of three phases. The critical first step of 55 initialization consists of clearly defining the research problem as either the modeling of an 56 output (active learning) or the optimization of an objective (Bayesian optimization). An 57 example of active learning is the investigation of the effect of reaction conditions, such as 58 temperature and pressure, on the conversion, whereas with Bayesian optimization the goal is to 59 find the optimal reaction conditions to maximize this conversion. In both cases a design space 60 is set up which defines the ranges of the studied variables by considering the objectives and the 61</p>
<p>In reaction or process design, the goal with Bayesian optimization is to determine the optimal 109 operating conditions to maximize the product yields, minimize the emissions per product, 110  This survey shows that chemical engineering is a broad and diverse research field with a whole 129 spectrum of possible active machine learning applications. Nevertheless its use is not yet 130 widespread and there are some hurdles to overcome before it is a trusted asset in the chemical 131 engineer's toolkit. 132</p>
<p>In this "Perspective" we focus on active machine learning as a DoE technique for an 133 experimentalist and how to popularize it. We identify three types of thresholds: the convincing 134 of the experimental researcher, the flexibility of data creation, and the robustness of the active 135 machine learning algorithms (Figure 2). In the following sections we will discuss each of these 136 challenges and how they can be overcome. 137 Initially, all active machine learning algorithms explore the entire design space which can result 152 in counter-intuitive or trivial queries. Consequently, the experimentalist loses confidence in the 153 machine learning tool. The initial selection of experiments does not rely on any preliminary or 154 physical knowledge within the machine learning models. Therefore, this issue is related to both 155 the human bias and the perception of these algorithms by their users, and to the absence of 156 preliminary knowledge within these models. The incorporation of preliminary knowledge into 157 active machine learning models will be discussed in section 4.1. today. This is closely related to another issue that limits the applicability, namely its ease-of-165 use. The current active machine learning packages require programming skills to be used and 166 have no graphical user interface (GUI). The absence of a GUI hampers the usage of these 167 methodologies as programming expertise is required before they can be applied. There is at 168 present a substantial time investment of the researchers needed to use Active Machine Learning. 169</p>
<p>This "activation barrier" is for many too high, in particular because of the required ability to 170 code. Therefore, active machine learning strategies should be constrained to the unit on which they 181 are used, to allow for an optimal experimental efficiency to make them applicable to "Real 182 World applications" [54]. In the example above it is often easier to heat up an experimental unit 183 than to cool it, therefore an extra constraint should be added to the algorithm which prefers to 184 select experiments which increase in temperature rather than decrease in temperature. 185</p>
<p>Next to constraints resulting from how the experimental equipment operates, these are also 186 important for simulations [40,42]. Consider the case when optimizing a reactor in silico with 187 CFD. When defining the reactor geometry for CFD it is not trivial that every type of geometry 188 is feasible to simulate nor that it can be properly meshed or that the results are mesh 189 independent. When these constraints are non-trivial, a separate machine learning model can be Finding an adequate representation is always important in machine learning problems. 306</p>
<p>Definitely with active machine learning, this representation is essential to harmonize both 307 synthesizability and creativity. 308</p>
<p>Conclusions and Perspectives 309</p>
<p>Active machine learning is excellently suited for chemical engineering researchers to speed up 310 experimental campaigns ranging from molecule and catalyst design, up to reaction and reactor 311 design. However, among experimental researchers active machine learning is less known and 312 many active machine learning applications are not user-friendly today. A better collaboration 313 between machine learning experts and chemical engineers can overcome these barriers. This 314 interaction also helps to tune active machine learning algorithms depending on the applied 315 (automated) experimental units and procedures, which improves the performance of these 316 algorithms. To fully profit from the creativity of active machine learning, improvements on the 317 machine learning methods related to data transfer and synthesizability are still required. By 318 harmonizing synthesizability and creativity, active machine learning is bound to make 319 significant advances in the fields of molecule and catalyst synthesis. The recent promising 320 breakthroughs will allow active machine learning to become an essential tool for the chemical 321 engineer and further facilitate autonomous and efficient scientific discoveries which will 322 contribute to a more sustainable chemical industry in the future. 323 </p>
<p>Acknowledgements</p>
<p>Figure 1 .
1basis of engineering research. In chemical engineering, these activities are aimed 29 at e.g. the development and optimization of catalysts, reaction conditions and reactor 30 configurations. In the chemical industry, 51 billion USD was spent in 2017 on research and 31 development [1]. This illustrates the importance of high quality data, however, obtaining 32 accurate data is tedious and error prone. Design of experiments (DoE) can help by extracting 33 maximal information with a minimum of effort [2, 3], making sure that the time and resources 34 are spent efficiently. By integrating machine learning with DoE, a more flexible and efficient 35 DoE is achieved. This so-called "active machine learning" allows, in particular for high-36 dimensional and highly-nonlinear phenomena, a more effective selection of experimental 37 conditions [4]. 38 In this "Perspective", we discuss the potential of combining DoE and machine learning, i.e. 39 active machine learning. Olsson defines active machine learning as a supervised machine 40 learning technique in which the learner, being the machine learning model, is in control of the 41 data from which it learns [5]. With active machine learning, machine learning algorithms are 42 used to iteratively determine new experimental data, so-called training data, based on 43 uncertainty criteria. Note that "experimental" can also refer to computationally expensive high-44 level simulations e.g. high level ab initio calculations of molecular properties or large eddy 45 simulations of reactive flow with Computational Fluid Dynamics (CFD) codes [6]. Active 46 machine learning consists of two branches with two different purposes: active learning and 47 Bayesian optimization. Active learning aims to explore and model a process with a minimum 48 of "experiments" to ensure accurate predictions over the entire design space [7]. Bayesian 49 optimization is essentially a machine learning-based optimization strategy, where iteratively 50 new experimental data is selected to find the experiment which optimizes the objective [Overview of the general active machine learning workflow, depicting the initialization and the iterative query selection (based on [9]).53</p>
<p>122 Computational fluid dynamics (CFD) has become an important tool for reactor, optimization 123 and trouble shooting. Bayesian optimization allows to find an optimal reactor configuration 124 with a minimum of computationally intensive CFD simulations. Park et al. demonstrated the 125 power of multi-objective Bayesian optimization by maximizing the gas-holdup and minimizing 126 the power consumption of a stirred tank reactor [41]. Clearly integrating active machine 127 learning in CFD allows for a faster and more efficient reactor design. 128</p>
<p>Figure 2 .
2Three different types of thresholds for the breakthrough of active machine there exists a knowledge gap between the experimentalist community and the 141 machine learning experts [51]. This is at the origin of active machine learning not yet being 142 systematically applied by the former. First, there is a misconception that for active machine 143 learning "big data" is mandatory and an enormous experimental campaign is required to make 144 it feasible. Nugraha et al. found an optimal catalyst composition performing only 47 from a 145 total of 5151 possible experiments [50]. Similarly, Schweidtmann et al. identified their pareto-146 front after 68 experiments for a four dimensional reaction optimization [33]. Moreover, Ureel 147 et al. showed that active learning strategies are already beneficial for experimental campaigns 148 consisting of as little as 18 experiments [9]. These examples illustrate that both active learning 149 and Bayesian optimization are already feasible for smaller datasets. 150A second issue is less related to the experimental researcher but more to the intrinsic algorithms. 151</p>
<p>learning strategies, multiple factors are varied at a time whereas regular DoE 160 strategies often vary a single factor at a time. This makes the post-processing of the experiments 161 less trivial as the effect of the factors is not isolated. As a result, a statistical analysis is required 162 to draw conclusions from the experimental campaign [52]. These tools are incorporated in 163 regular DoE software but not in the active machine learning packages that are available as of 164</p>
<p>of active machine learning algorithms is often done on simulated data where 174 there are no practical limitations on the data creation side [27, 31, 53]. However, in real-life 175 experimental units or procedures do not allow this flexibility. For example even a completely 176 automated experimental unit often needs to heat up or cool down or time to stabilize, which 177 slows down the generation of a new datapoint when different temperatures are selected by the 178 algorithm. Additionally, experiments are often performed in parallel (e.g. in high-throughput 179 units) as opposed to the algorithms which assume a sequential selection of experiments. 180</p>
<p>190 trained to learn the constraints and enforce the viability of the simulations [40]. 191 Another example of constrained experimental units are high-throughput experimental 192 campaigns which are for example used to screen different catalytic materials. Within these 193 units, several experimental variables such as temperature and pressure are often fixed for every 194 type of experiment per batch. This requires another constraint on the batch selection of these 195 experiments as variables need to be fixed for all selected queries. To tune the active machine 196learning algorithms according to their application, a close collaboration between the machine 197 learning expert and experimentalist is thus required. In this way, the benefits of applying active 198 machine learning are also available to lessflexible experimental units.  199    This symbiosis between experimentalist and machine learning scientist will benefit both parties. 200First of all, it will extend the fields of application for active machine learning as researchers 201 become more aware of the benefits of active machine learning. This close collaboration will 202 help in identifying useful features within these active machine learning algorithms such as 203 blocking, or automatic post-processing. More practical constraints might be added to the 204 experimental selection, such as the time or cost required for a proposed experiment. ideal case, active machine learning is coupled with a flexible automated experimental unit 209 or are even equipped by a robot[33, 35, 55]. In this way control and optimization of the 210 performance of the experiments can become optimal, and thus saving valuable time and effort. 211Automated experimental units are being increasingly applied for molecular synthesis and 212 chemical engineering but these units are not yet commonplace [56-58]. One requirement of 213 automated robotic units is that they should be reconfigurable [59]. They moreover should have 214 a broad application range and not be limited to the investigation of a single reaction type or 215 narrow temperature range. The use of automated units is of course not self-evident as these 216 often are expensive and currently not well-suited for every problem. For example, despite past 217 efforts [60] the automated synthesis and testing of catalysts is a challenging task, definitely 218 when studying a broad design space [61]. By coupling these systems with active machine 219 learning techniques, a huge time saving is expected for experimental campaigns, which will 220 speed up reaction and catalyst optimization, and the acquisition of scientific knowledge. A last 221 threshold of these automated units is of course the question of safety of these units. By 222 expanding the catalyst or reaction design space, safety concerns rise as this increases the 223 probability of undesired reactions to occur. Therefore, a good chemical knowledge is still 224 required when employing these units to identify and incorporate the safety constraints. The 225 definition of safety constraints again requires a close collaboration between experimental 226 experts and machine learning scientists. experiments, it is advantageous when these experiments are widely 230 applicable and serve multiple purposes. The information gathered in experiments should be 231 made available according to the FAIR-principles and can then be of value for other researchers 232 [62]. However, with active machine learning one objective is chosen which determines the 233 experimental selection. This hampers the applicability of the experiments as only one 234 experimental output is well-studied. For example when investigating reactions, the conversion 235 is typically selected as output of interest but this limits the information on other properties such 236 as yields or selectivities. In the worst case, the yields are not measured and no information is 237 gathered, on the other hand when these yields would be measured no guarantee is given that all 238 trends are considered in this example. As the goal of the active machine learning was to model 239 conversions, it ignores the behavior of interesting reaction yields which can result in trends to 240 remain hidden. With Bayesian optimization this does not pose an issue as the goal here is to 241 optimize an objective, which makes the data per definition less generally applicable. Multi-242 objective Bayesian optimization techniques exist while for active learning only single objective 243 strategies are possible, meaning that all interesting outputs should be incorporated within the 244 single active learning objective [33, 38, 41]. Therefore, to ensure the reusability of the gathered 245 data, it is important that during experiments not only the modeled output is measured but also 246 the potential other relevant outputs. 247 After creating data that is of wide interest, it is also important to be able to incorporate that 248 knowledge in active machine learning tools. When pretraining an active machine learning 249 model on literature data, an improved initial experimental selection is achieved which resolves the issue of the earlier mentioned suboptimal initial selection [63]. The incorporation of 251 literature data is trivial when the experimental uncertainty is similar to the newly gathered data. 252 However, when the literature data is of better or inferior quality than the gathered data, it is 253 important that the machine learning model can make a distinction between both. 254 Heteroscedastic machine learning models exist [53], but these do not necessarily allow the 255 incorporation of two separate noise factors, as the variation in noise is dependent on the variable 256 in heteroscedastic models. Conversely, multi-fidelity active machine learning strategies allow 257 to employ widely abundant low-quality data for an accurate pretraining of the active machine 258 learning model [64, 65]. These methods have been developed based on simulated 259 "experimental" data only, but are very promising for improving the performance of active 260 machine learning tools when applied to real experimental data. 261 Data that is closely related, but not similar in nature, can also serve as initialization of active 262 machine learning models [66]. For example, when modeling reactions with one type of catalyst 263 and literature data on another catalyst is available, this might still contain valuable information 264 for an active learning model [67]. With active transfer learning, the goal is to leverage this 265 knowledge from nearly similar data to obtain a machine learning model with an improved 266 perception of the examined problem. In this way, rudimentary physical knowledge is introduced 267 in the machine learning model which again improves the initial experimental selection. This 268 methodology has been proven to work on reaction yield classification of cross-coupling 269 reactions, by pretraining a machine learning model on reactions with different nucleophiles 270 [67]. 271The reuse of literature data within active machine learning applications will further enhance the 272 performance of these tools. The first active transfer learning approaches are being developed 273 within chemical engineering, but a further development on algorithms is crucial for learning determines the optimal query for the either optimization or modeling 277 purposes. However, for certain problems it is not evident that these queries are executable.For 278 instance in catalyst or molecule design, novel compounds are proposed to synthesize and test 279 on the property of interest. Here, the representation of the catalyst or molecule is crucial for the 280 synthesizability of the queries. Synthesizability is defined as the feasibility of the proposed 281 queries, referring to whether the proposed catalysts or molecules can be synthesized. Often, a 282 simple representation of a catalyst is a vector containing the catalyst composition [50, 68]. This 283 guarantees the synthesizability of the catalyst but limits the design space explored by the active 284 machine learning algorithm as only the composition is varied but no structural or geometrical 285properties are considered. Ideally, one considers the complete catalyst space for every problem 286 by for example considering the complete 3D-geometry as a representation for the catalyst site 287 or molecule. However, not every imaginable catalyst or molecule 3D-geometry can be 288 synthesizable, which makes that there is trade-off between the magnitude of the design space, 289 so-called creativity, and synthesizability. 290As illustrated by the problem of synthesizability this essentially boils down to a problem of 291 representation on which constraints are added. One intuitive approach is to use the synthesis 292 process of the catalyst or molecule as the machine learning representation. A vector containing 293 the catalyst composition, calcination temperature and time, presence of ion exchange or 294 impregnation, can be used to represent a catalyst. In this way, the synthesizability of the queries 295 is guaranteed, as every proposed recipe is executable. However, this representation does not 296 necessarily ensure an easy mapping to the property of interest, which might require an increased 297 amount of data to model this relation. 298Next to this intuitive approach, learned machine learning representations allow to create a 299 continuous representation which ensures the validity of the proposed queries [69, 70]. By 300 training recently developed methodologies such as variational auto-encoders or generative 301 adversarial neural networks on a set of synthesizable molecules or catalysts, a learned machine 302 learning representation, a so-called latent space, can be developed which guarantees the 303 synthesizability of the proposed queries [69, 71, 72]. Upon this representation additional 304 constraints on the catalyst or molecule can be enforced according to the application [26]. 305</p>
<dl>
<dt>achieve the highest energy efficiency, etc. Optimization of reaction conditions has been 111 demonstrated multiple times, from multi-objective reaction optimization with both discrete and 112 continuous variables which makes it probably the most well-developed field of active machine 113 learning in chemical engineering [33-35]. Shields et al. applied Bayesian optimization to 114 optimize the reaction conditions for a Mitsunobu reaction, and found an optimal yield (&gt;99%) 115 for several non-intuitive reaction conditions after 40 experiments, beating the standard reaction 116 yield of 60% [37]. With active learning the goal is to acquire reaction knowledge which can be 117 used for reactor and catalyst design, process control or retrosynthesis. Eyke et al. demonstrated 118 the potential of active learning for DoE in reaction design by predicting reaction yields for 119 combinations of catalysts and solvents with a minimum of available data [20]. Recently, a DoE-120 tool for the study of chemical reactions has been developed and validated on the catalytic 121 pyrolysis of plastic waste by Ureel et al [9].</dt>
<dd>
<p>324</p>
</dd>
</dl>
<p>:Yannick Ureel, Maarten Dobbelaere, and Kevin De Ras acknowledge financial support from 325 the Fund for Scientific Research Flanders (FWO Flanders) respectively through doctoral 326 fellowship grants 1185822N, 1S45522N, and 3F018119. The authors acknowledge funding 327 from the European Research Council under the European Union's Horizon 2020 research and 328 innovation programme / ERC grant agreement n° 818607. 329 Lazic ZR. Design of experiments in chemical engineering: a practical guide. First ed. 333 Weinheim: Wiley-VCH Verlag; 2006. 334 [3] Franceschini G, Macchietto S. Model-based design of experiments for parameter 335 precision: State of the art. Chemical Engineering Science 2008;63(19):4846-72. 336 [4] Melnikov AA, Poulsen Nautrup H, Krenn M, Dunjko V, Tiersch M, Zeilinger A, et al. 337 Active learning machine learns to create new quantum experiments. Proceedings of the Marin GB, Galvita VV, Yablonsky GS. Kinetics of chemical processes: From molecular 342 to industrial scale. Journal of Catalysis 2021;404:745-59. 343 [7] Settles B. Active learning. Synthesis Lectures on Artificial Intelligence and Machine 344 Learning 2012;18:1-111. 345 [8] Frazier PI. A Tutorial on Bayesian Optimization. 2018(Section 5):1-22. 346 [9] Ureel Y, Dobbelaere MR, Akin O, Varghese RJ, Pernalete CG, Thybaut JW, et al. 347 Active learning-based exploration of the catalytic pyrolysis of plastic waste. Fuel 348 2022;328:125340. 349 [10] Thrun S. Exploration in active learning. Handbook of Brain Science and Neural 350 Networks 1995:381-4. 351 [11] Rasmussen CE, Williams CKI. Gaussian processes for machine learning. the MIT Press, 352 Massachusetts Institute of Technology; 2006. 353 [12] Podryabinkin EV, Shapeev AV. Active learning of linearly parametrized interatomic 354 potentials. Computational Materials Science 2017;140:171-80. 355 [13] Vandermause J, Torrisi SB, Batzner S, Xie Y, Sun L, Kolpak AM, et al. On-the-fly 356 active learning of interpretable Bayesian force fields for atomistic rare events. npj 357 Computational Materials 2020;6(1):1-11. 358 [14] Riis C, Antunes FN, Hüttel FB, Azevedo CL, Pereira FC. Bayesian Active Learning 359 with Fully Bayesian Gaussian Processes. arXiv preprint arXiv:220510186 2022. 3606. References </p>
<p>330 </p>
<p>[1] 
The global chemical industry: Catalyzing growth and addressing our world's 
331 
sustainability challenges. Oxford Economics; 2019:29. 
332 
[2] 
338 
National Academy of Sciences 2018;115(6):1221-6. 
339 
[5] 
Olsson F. A literature survey of active machine learning in the context of natural 
340 
language processing. 2009. 
341 
[6] </p>
<p>C Blundell, J Cornebise, K Kavukcuoglu, D Wierstra, Weight Uncertainty in Neural 361 Networks. 32nd International Conference on Machine Learning. 2Blundell C, Cornebise J, Kavukcuoglu K, Wierstra D. Weight Uncertainty in Neural 361 Networks. 32nd International Conference on Machine Learning, ICML 2015 362 2015;2:1613-22.</p>
<p>Deep Bayesian active learning with image data. Y Gal, R Islam, Z Ghahramani, 34th 364 International Conference on Machine Learning. 3Gal Y, Islam R, Ghahramani Z. Deep Bayesian active learning with image data. 34th 364 International Conference on Machine Learning, ICML 2017 2017;3:1923-32.</p>
<p>Noise contrastive priors for 366 functional uncertainty. D Hafner, D Tran, T Lillicrap, A Irpan, J Davidson, Hafner D, Tran D, Lillicrap T, Irpan A, Davidson J. Noise contrastive priors for 366 functional uncertainty. PMLR:905-14.</p>
<p>Bayesian semi-supervised learning for uncertainty-calibrated 368 prediction of molecular properties and active learning. Y Zhang, A A Lee, Chemical Science. 36935Zhang Y, Lee AA. Bayesian semi-supervised learning for uncertainty-calibrated 368 prediction of molecular properties and active learning. Chemical Science 369 2019;10(35):8154-63.</p>
<p>Multiscale Modeling Combined with Active Learning for 371. M Núñez, D G Vlachos, Núñez M, Vlachos DG. Multiscale Modeling Combined with Active Learning for 371</p>
<p>Microstructure Optimization of Bifunctional Catalysts. Industrial &amp; Engineering 372. Chemistry Research. 5815Microstructure Optimization of Bifunctional Catalysts. Industrial &amp; Engineering 372 Chemistry Research 2019;58(15):6146-54.</p>
<p>Iterative experimental design based on active machine 374 learning reduces the experimental burden associated with reaction screening. N S Eyke, W H Green, K F Jensen, Reaction 375 Chemistry &amp; Engineering. 510Eyke NS, Green WH, Jensen KF. Iterative experimental design based on active machine 374 learning reduces the experimental burden associated with reaction screening. Reaction 375 Chemistry &amp; Engineering 2020;5(10):1963-72.</p>
<p>Machine-377 learned interatomic potentials by active learning: amorphous and liquid hafnium 378 dioxide. G Sivaraman, A N Krishnamoorthy, M Baur, C Holm, M Stan, G Csányi, Computational Materials. 61104Sivaraman G, Krishnamoorthy AN, Baur M, Holm C, Stan M, Csányi G, et al. Machine- 377 learned interatomic potentials by active learning: amorphous and liquid hafnium 378 dioxide. npj Computational Materials 2020;6(1):104.</p>
<p>Active learning for computational 380 chemogenomics. D Reker, P Schneider, G Schneider, J B Brown, Future Medicinal Chemistry. 94Reker D, Schneider P, Schneider G, Brown JB. Active learning for computational 380 chemogenomics. Future Medicinal Chemistry 2017;9(4):381-402.</p>
<p>Machine Learning in Nanoscience: Big 382 Data at Small Scales. K A Brown, S Brittman, D Jariwala, U Celano, Nano Lett. 2027Brown KA, Brittman S, Jariwala D, Celano U. Machine Learning in Nanoscience: Big 382 Data at Small Scales. Nano Lett 2020;20(2):7-.</p>
<p>An Atomistic 384 Machine Learning Package for Surface Science and Catalysis. M H Hansen, J Antonio, G Torres, P C Jennings, Z Wang, J R Boes, Hansen MH, Antonio J, Torres G, Jennings PC, Wang Z, Boes JR, et al. An Atomistic 384 Machine Learning Package for Surface Science and Catalysis. 2019.</p>
<p>Constrained Bayesian Optimization for 386 Automatic Chemical Design. Griffiths R-R, J M Hernández-Lobato, Griffiths R-R, Hernández-Lobato JM. Constrained Bayesian Optimization for 386 Automatic Chemical Design. 2017.</p>
<p>Constrained Bayesian optimization for automatic 388 chemical design using variational autoencoders. Griffiths R-R, J M Hernández-Lobato, Chemical science. 112Griffiths R-R, Hernández-Lobato JM. Constrained Bayesian optimization for automatic 388 chemical design using variational autoencoders. Chemical science 2020;11(2):577-86.</p>
<p>Active learning across intermetallics to guide discovery of 390 electrocatalysts for CO2 reduction and H2 evolution. K Tran, Z W Ulissi, Nature Catalysis. 19Tran K, Ulissi ZW. Active learning across intermetallics to guide discovery of 390 electrocatalysts for CO2 reduction and H2 evolution. Nature Catalysis 2018;1(9):696- 391 703.</p>
<p>On-the-fly 393 closed-loop materials discovery via Bayesian active learning. A G Kusne, H Yu, C Wu, H Zhang, J Hattrick-Simpers, B Decost, Nature Communications. 39420205966Kusne AG, Yu H, Wu C, Zhang H, Hattrick-Simpers J, DeCost B, et al. On-the-fly 393 closed-loop materials discovery via Bayesian active learning. Nature Communications 394 2020;11(1):5966.</p>
<p>ARTICLE Active 396 learning for accelerated design of layered materials. L Bassman, P Rajak, R K Kalia, A Nakano, F Sha, J Sun, Bassman L, Rajak P, Kalia RK, Nakano A, Sha F, Sun J, et al. ARTICLE Active 396 learning for accelerated design of layered materials.</p>
<p>Machine learning in catalysis. J R Kitchin, Nature Catalysis. 14Kitchin JR. Machine learning in catalysis. Nature Catalysis 2018;1(4):230-2.</p>
<p>Bias Free Multiobjective 399. K M Jablonka, Melpatti Jothiappan, G Wang, S Smit, B Yoo, B , Jablonka KM, Melpatti Jothiappan G, Wang S, Smit B, Yoo B. Bias Free Multiobjective 399</p>
<p>Active Learning for Materials Design and Discovery. Nature communications. 4001Active Learning for Materials Design and Discovery. Nature communications 400 2021;12(1):1-10.</p>
<p>Solvent Selection for Mitsunobu Reaction Driven 402 by an Active Learning Surrogate Model. C Zhang, Y Amar, L Cao, A A Lapkin, Organic Process Research &amp; Development. 4032020Zhang C, Amar Y, Cao L, Lapkin AA. Solvent Selection for Mitsunobu Reaction Driven 402 by an Active Learning Surrogate Model. Organic Process Research &amp; Development 403 2020;24(12):2864-73.</p>
<p>. A M Schweidtmann, A D Clayton, N Holmes, E Bradford, R A Bourne, A A Lapkin, Schweidtmann AM, Clayton AD, Holmes N, Bradford E, Bourne RA, Lapkin AA.</p>
<p>Machine learning meets continuous flow chemistry: Automated optimization towards 406. Machine learning meets continuous flow chemistry: Automated optimization towards 406</p>
<p>the Pareto front of multiple objectives. Chemical Engineering Journal. 352the Pareto front of multiple objectives. Chemical Engineering Journal 2018;352:277- 407 82.</p>
<p>Machine learning and 409 molecular descriptors enable rational solvent selection in asymmetric catalysis. Y Amar, A M Schweidtmann, P Deutsch, L Cao, A A Lapkin, Chemical Science. 1027Amar Y, Schweidtmann AM, Deutsch P, Cao L, Lapkin AA. Machine learning and 409 molecular descriptors enable rational solvent selection in asymmetric catalysis. 410 Chemical Science 2019;10(27):6697-706.</p>
<p>412 Automated self-optimisation of multi-step reaction and separation processes using 413 machine learning. A D Clayton, A M Schweidtmann, G Clemens, J A Manson, C J Taylor, C G Niño, Chemical Engineering Journal. 384123340Clayton AD, Schweidtmann AM, Clemens G, Manson JA, Taylor CJ, Niño CG, et al. 412 Automated self-optimisation of multi-step reaction and separation processes using 413 machine learning. Chemical Engineering Journal 2020;384:123340-.</p>
<p>. A D Clayton, J A Manson, C J Taylor, T W Chamberlain, B A Taylor, G Clemens, Clayton AD, Manson JA, Taylor CJ, Chamberlain TW, Taylor BA, Clemens G, et al.</p>
<p>Algorithms for the self-optimisation of chemical reactions. Reaction Chemistry and 416 Engineering. 4. Royal Society of Chemistry. 2019Algorithms for the self-optimisation of chemical reactions. Reaction Chemistry and 416 Engineering. 4. Royal Society of Chemistry; 2019:1545-54.</p>
<p>Bayesian 418 reaction optimization as a tool for chemical synthesis. B J Shields, J Stevens, J Li, M Parasram, F Damani, Jim Alvarado, Nature. 5907844Shields BJ, Stevens J, Li J, Parasram M, Damani F, Alvarado JIM, et al. Bayesian 418 reaction optimization as a tool for chemical synthesis. Nature 2021;590(7844):89-96.</p>
<p>Summit: Benchmarking Machine Learning Methods 420 for Reaction Optimisation. K C Felton, J G Rittig, A A Lapkin, Felton KC, Rittig JG, Lapkin AA. Summit: Benchmarking Machine Learning Methods 420 for Reaction Optimisation. ChemRxiv; 2020.</p>
<p>Multi-task Bayesian Optimization of Chemical 422 Reactions. K Felton, D Wigh, A A Lapkin, Felton K, Wigh D, Lapkin AA. Multi-task Bayesian Optimization of Chemical 422 Reactions. 2020.</p>
<p>pBO-2GP-3B: 424 A batch parallel known/unknown constrained Bayesian optimization with feasibility 425 classification and its applications in computational fluid dynamics. A Tran, J Sun, J M Furlan, K V Pagalthivarthi, R J Visintainer, Y Wang, Computer Methods 426 in Applied Mechanics and Engineering. 347Tran A, Sun J, Furlan JM, Pagalthivarthi KV, Visintainer RJ, Wang Y. pBO-2GP-3B: 424 A batch parallel known/unknown constrained Bayesian optimization with feasibility 425 classification and its applications in computational fluid dynamics. Computer Methods 426 in Applied Mechanics and Engineering 2019;347:827-52.</p>
<p>Multi-objective Bayesian optimization of chemical 428 reactor design using computational fluid dynamics. S Park, J Na, M Kim, J M Lee, Computers &amp; Chemical 429 Engineering. 119Park S, Na J, Kim M, Lee JM. Multi-objective Bayesian optimization of chemical 428 reactor design using computational fluid dynamics. Computers &amp; Chemical 429 Engineering 2018;119:25-37.</p>
<p>Applying 431 Bayesian optimization with Gaussian process regression to computational fluid 432 dynamics problems. Y Morita, S Rezaeiravesh, N Tabatabaei, R Vinuesa, K Fukagata, P Schlatter, Journal of Computational Physics. 449110788Morita Y, Rezaeiravesh S, Tabatabaei N, Vinuesa R, Fukagata K, Schlatter P. Applying 431 Bayesian optimization with Gaussian process regression to computational fluid 432 dynamics problems. Journal of Computational Physics 2022;449:110788.</p>
<p>La catalyse en chimie organique. P Sabatier, Sabatier P. La catalyse en chimie organique. 1920.</p>
<p>Harmonious optimum conditions for heterogeneous catalytic reactions 435 derived analytically with Polanyi relation and Bronsted relation. S Ichikawa, Journal of Catalysis. 436Ichikawa S. Harmonious optimum conditions for heterogeneous catalytic reactions 435 derived analytically with Polanyi relation and Bronsted relation. Journal of Catalysis 436 2021;404:706-15.</p>
<p>Hydrocracking phenanthrene and 438 1-methyl naphthalene: Development of linear free energy relationships. Catalytic 439 hydroprocessing of petroleum and distillates. R Landau, S Korre, M Neurock, M Klein, R Quann, CRC Press2020Landau R, Korre S, Neurock M, Klein M, Quann R. Hydrocracking phenanthrene and 438 1-methyl naphthalene: Development of linear free energy relationships. Catalytic 439 hydroprocessing of petroleum and distillates. CRC Press; 2020, p. 421-32.</p>
<p>Limits to scaling relations between 441 adsorption energies?. S Vijay, G Kastlunger, K Chan, J K Nørskov, 231102. 442The Journal of Chemical Physics. 15623Vijay S, Kastlunger G, Chan K, Nørskov JK. Limits to scaling relations between 441 adsorption energies? The Journal of Chemical Physics 2022;156(23):231102. 442</p>
<p>How doped MoS2 breaks transition-metal scaling 443 relations for CO2 electrochemical reduction. X Hong, K Chan, C Tsai, J K Nørskov, Acs Catalysis. 67Hong X, Chan K, Tsai C, Nørskov JK. How doped MoS2 breaks transition-metal scaling 443 relations for CO2 electrochemical reduction. Acs Catalysis 2016;6(7):4428-37.</p>
<p>Strategies to break linear scaling relationships. J Pérez-Ramírez, N López, Catalysis. 44511NaturePérez-Ramírez J, López N. Strategies to break linear scaling relationships. Nature 445 Catalysis 2019;2(11):971-6.</p>
<p>Accelerated discovery of 447 CO2 electrocatalysts using active machine learning. M Zhong, K Tran, Y Min, C Wang, Z Wang, C-T Dinh, Nature. 5817807Zhong M, Tran K, Min Y, Wang C, Wang Z, Dinh C-T, et al. Accelerated discovery of 447 CO2 electrocatalysts using active machine learning. Nature 2020;581(7807):178-83.</p>
<p>. A S Nugraha, G Lambard, J Na, Msa Hossain, T Asahi, W Chaikittisilp, Nugraha AS, Lambard G, Na J, Hossain MSA, Asahi T, Chaikittisilp W, et al.</p>
<p>Mesoporous trimetallic PtPdAu alloy films toward enhanced electrocatalytic activity in 450 methanol oxidation: unexpected chemical compositions discovered by Bayesian 451 optimization. Journal of Materials Chemistry A. 827Mesoporous trimetallic PtPdAu alloy films toward enhanced electrocatalytic activity in 450 methanol oxidation: unexpected chemical compositions discovered by Bayesian 451 optimization. Journal of Materials Chemistry A 2020;8(27):13532-40.</p>
<p>Machine 453 learning in chemical engineering: strengths, weaknesses, opportunities, and threats. M R Dobbelaere, P P Plehiers, R Van De Vijver, C V Stevens, K M Van Geem, Engineering. 79Dobbelaere MR, Plehiers PP, Van de Vijver R, Stevens CV, Van Geem KM. Machine 453 learning in chemical engineering: strengths, weaknesses, opportunities, and threats. 454 Engineering 2021;7(9):1201-11.</p>
<p>QUANTIS: data quality assessment tool by clustering analysis. S H Symoens, S U Aravindakshan, F H Vermeire, K De Ras, M R Djokic, G B Marin, International Journal 457 of Chemical Kinetics. 5111Symoens SH, Aravindakshan SU, Vermeire FH, De Ras K, Djokic MR, Marin GB, et 456 al. QUANTIS: data quality assessment tool by clustering analysis. International Journal 457 of Chemical Kinetics 2019;51(11):872-85.</p>
<p>Achieving robustness to 459 aleatoric uncertainty with heteroscedastic Bayesian optimisation. Griffiths R-R, A A Aldrick, M Garcia-Ortegon, V Lalchand, Machine Learning: 460 Science and Technology. 3115004Griffiths R-R, Aldrick AA, Garcia-Ortegon M, Lalchand V. Achieving robustness to 459 aleatoric uncertainty with heteroscedastic Bayesian optimisation. Machine Learning: 460 Science and Technology 2021;3(1):015004.</p>
<p>Bayesian optimization with known 462 experimental and design constraints for chemistry applications. R J Hickman, M Aldeghi, F Häse, A Aspuru-Guzik, arXiv:220317241 2022arXiv preprint 463Hickman RJ, Aldeghi M, Häse F, Aspuru-Guzik A. Bayesian optimization with known 462 experimental and design constraints for chemistry applications. arXiv preprint 463 arXiv:220317241 2022.</p>
<p>A mobile 465 robotic chemist. B Burger, P M Maffettone, V V Gusev, C M Aitchison, Y Bai, X Wang, Nature. 5837815Burger B, Maffettone PM, Gusev VV, Aitchison CM, Bai Y, Wang X, et al. A mobile 465 robotic chemist. Nature 2020;583(7815):237-41.</p>
<p>Integrated 467 Strategy for Lead Optimization Based on Fragment Growing: The Diversity-Oriented-468 Target-Focused-Synthesis Approach. L Hoffer, Y V Voitovich, B Raux, K Carrasco, C Muller, A Y Fedorov, Journal of Medicinal Chemistry. 46913Hoffer L, Voitovich YV, Raux B, Carrasco K, Muller C, Fedorov AY, et al. Integrated 467 Strategy for Lead Optimization Based on Fragment Growing: The Diversity-Oriented- 468 Target-Focused-Synthesis Approach. Journal of Medicinal Chemistry 469 2018;61(13):5719-32.</p>
<p>. A-C Bédard, A Adamo, K C Aroh, M G Russell, A A Bedermann, J Torosian, Bédard A-C, Adamo A, Aroh KC, Russell MG, Bedermann AA, Torosian J, et al.</p>
<p>Reconfigurable system for automated optimization of diverse chemical reactions. Science. 3616408Reconfigurable system for automated optimization of diverse chemical reactions. 472 Science 2018;361(6408):1220-5.</p>
<p>Automated platforms for reaction self-474 optimization in flow. C Mateos, M J Nieves-Remacha, J A Rincón, Reaction Chemistry &amp; Engineering. 49Mateos C, Nieves-Remacha MJ, Rincón JA. Automated platforms for reaction self- 474 optimization in flow. Reaction Chemistry &amp; Engineering 2019;4(9):1536-44.</p>
<p>Toward machine learning-enhanced high-476 throughput experimentation. N S Eyke, B A Koscher, K F Jensen, Trends in Chemistry. 32Eyke NS, Koscher BA, Jensen KF. Toward machine learning-enhanced high- 476 throughput experimentation. Trends in Chemistry 2021;3(2):120-32.</p>
<p>478 Experimental equipment for high-throughput synthesis and testing of catalytic 479 materials. I Hahndorf, O Buyevskaya, M Langpape, G Grubert, S Kolf, E Guillon, Chemical Engineering Journal. 891-3Hahndorf I, Buyevskaya O, Langpape M, Grubert G, Kolf S, Guillon E, et al. 478 Experimental equipment for high-throughput synthesis and testing of catalytic 479 materials. Chemical Engineering Journal 2002;89(1-3):119-25.</p>
<p>Automated synthesis and 481 data accumulation for fast production of high-performance Ni nanocatalysts. K H Oh, H-K Lee, S W Kang, J-I Yang, Nam G Lim, T , Journal of 482 Industrial and Engineering Chemistry. 106Oh KH, Lee H-K, Kang SW, Yang J-I, Nam G, Lim T, et al. Automated synthesis and 481 data accumulation for fast production of high-performance Ni nanocatalysts. Journal of 482 Industrial and Engineering Chemistry 2022;106:449-59.</p>
<p>484 Comment: The FAIR Guiding Principles for scientific data management and 485 stewardship. M D Wilkinson, M Dumontier, I J Aalbersberg, G Appleton, M Axton, A Baak, Scientific Data. 31Wilkinson MD, Dumontier M, Aalbersberg IJ, Appleton G, Axton M, Baak A, et al. 484 Comment: The FAIR Guiding Principles for scientific data management and 485 stewardship. Scientific Data 2016;3(1):1-9.</p>
<p>Pre-training helps 487 Bayesian optimization too. Z Wang, G E Dahl, K Swersky, C Lee, Z Mariet, Z Nado, arXiv:220703084 2022arXiv preprintWang Z, Dahl GE, Swersky K, Lee C, Mariet Z, Nado Z, et al. Pre-training helps 487 Bayesian optimization too. arXiv preprint arXiv:220703084 2022.</p>
<p>Multi-fidelity prediction of molecular 489 optical peaks with deep learning. K P Greenman, W H Green, R Gómez-Bombarelli, Chemical science. 134Greenman KP, Green WH, Gómez-Bombarelli R. Multi-fidelity prediction of molecular 489 optical peaks with deep learning. Chemical science 2022;13(4):1152-62.</p>
<p>Multi-fidelity machine learning models for 491 accurate bandgap predictions of solids. G Pilania, J E Gubernatis, T Lookman, Computational Materials Science. 129Pilania G, Gubernatis JE, Lookman T. Multi-fidelity machine learning models for 491 accurate bandgap predictions of solids. Computational Materials Science 2017;129:156- 492 63.</p>
<p>Opportunities and challenges of artificial intelligence 494 for green manufacturing in the process industry. S Mao, B Wang, Y Tang, F Qian, Engineering. 56Mao S, Wang B, Tang Y, Qian F. Opportunities and challenges of artificial intelligence 494 for green manufacturing in the process industry. Engineering 2019;5(6):995-1002.</p>
<p>Predicting 496 reaction conditions from limited data through active transfer learning. E Shim, J A Kammeraad, Z Xu, A Tewari, T Cernak, P M Zimmerman, Chemical Science. 49722Shim E, Kammeraad JA, Xu Z, Tewari A, Cernak T, Zimmerman PM. Predicting 496 reaction conditions from limited data through active transfer learning. Chemical Science 497 2022;13(22):6655-68.</p>
<p>. M Kim, M Y Ha, W-B Jung, J Yoon, E Shin, I-D Kim, 499Kim M, Ha MY, Jung W-B, Yoon J, Shin E, Kim I-d, et al. Searching for an Optimal 499</p>
<p>Multi-Metallic Alloy Catalyst by Active Learning Combined with Experiments. Multi-Metallic Alloy Catalyst by Active Learning Combined with Experiments.</p>
<p>. Advanced Materials. 342108900Advanced Materials 2022;34(19):2108900.</p>
<p>. R Gómez-Bombarelli, J N Wei, D Duvenaud, J M Hernández-Lobato, Sánchez- 502Gómez-Bombarelli R, Wei JN, Duvenaud D, Hernández-Lobato JM, Sánchez- 502</p>
<p>Automatic Chemical Design Using a Data-Driven 503. B Lengeling, D Sheberla, Lengeling B, Sheberla D, et al. Automatic Chemical Design Using a Data-Driven 503</p>
<p>Continuous Representation of Molecules. ACS Central Science. 42Continuous Representation of Molecules. ACS Central Science 2018;4(2):268-76.</p>
<p>Data Analytics and Machine Learning for Smart Process 505 Manufacturing: Recent Advances and Perspectives in the Big Data Era. C Shang, F You, Engineering. 5066Shang C, You F. Data Analytics and Machine Learning for Smart Process 505 Manufacturing: Recent Advances and Perspectives in the Big Data Era. Engineering 506 2019;5(6):1010-6.</p>
<p>Optimizing 508 distributions over molecular space. An objective-reinforced generative adversarial 509 network for inverse-design chemistry (ORGANIC. B Sanchez-Lengeling, C Outeiral, G L Guimaraes, A Aspuru-Guzik, Sanchez-Lengeling B, Outeiral C, Guimaraes GL, Aspuru-Guzik A. Optimizing 508 distributions over molecular space. An objective-reinforced generative adversarial 509 network for inverse-design chemistry (ORGANIC). 2017.</p>
<p>. Z Jensen, S Kwon, D Schwalbe-Koda, C Paris, R Gómez-Bombarelli, Román-Leshkov. 511Jensen Z, Kwon S, Schwalbe-Koda D, Paris C, Gómez-Bombarelli R, Román-Leshkov 511</p>
<p>Discovering Relationships between OSDAs and Zeolites through Data Mining 512 and Generative Neural Networks. Y , ACS Central Science. 75Y, et al. Discovering Relationships between OSDAs and Zeolites through Data Mining 512 and Generative Neural Networks. ACS Central Science 2021;7(5):858-67.</p>            </div>
        </div>

    </div>
</body>
</html>
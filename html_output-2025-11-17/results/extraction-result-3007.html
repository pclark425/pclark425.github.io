<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3007 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3007</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3007</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-270257920</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.02592v1.pdf" target="_blank">LOLAMEME: Logic, Language, Memory, Mechanistic Framework</a></p>
                <p><strong>Paper Abstract:</strong> The performance of Large Language Models has achieved superhuman breadth with unprecedented depth. At the same time, the language models are mostly black box models and the underlying mechanisms for performance have been evaluated using synthetic or mechanistic schemes. We extend current mechanistic schemes to incorporate Logic, memory, and nuances of Language such as latent structure. The proposed framework is called LOLAMEME and we provide two instantiations of LOLAMEME: LoLa and MeMe languages. We then consider two generative language model architectures: transformer-based GPT-2 and convolution-based Hyena. We propose the hybrid architecture T HEX and use LOLAMEME framework is used to compare three architectures. T HEX outperforms GPT-2 and Hyena on select tasks.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3007.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3007.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2 (124M)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2 (decoder Transformer, 124M parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoder-only Transformer (GPT-2 124M) used as a baseline in this paper; randomly initialized (no pretraining) and trained on synthetic LOLAMEME datasets to evaluate arithmetic-like expression execution, memorization, and in-context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (124M)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only Transformer (GPT-2) with layer hidden dimensionality set to 768 for the GPT-2 layer in the hybrid; 124M parameter configuration; models initialized randomly (no pretraining) and trained from scratch on LOLAMEME synthetic datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Execution of chained arithmetic expressions and statements (assignment + print), using operators: addition, subtraction, multiplication, division, modulo; chained multi-statement programs with local and global variables; latent-type-modified arithmetic; varying statement counts (5 and 15) and variable name tokenization effects.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Primarily memorization/in-context retrieval of repeatedly-seen global variable values when available; limited operator-learning initially (improves with operator-pretraining). No direct evidence of learned algorithmic arithmetic procedures; performance appears to rely on pattern/memory of mappings rather than robust algorithmic generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Large drop in exact-match when global variables are removed from the test prompt indicates reliance on memorized global-variable values (Table 1); operator-pretraining (large dataset of operator examples) dramatically increases exact-match for all models indicating that lack of operator learning, not inherent arithmetic reasoning, limits performance; relative stability noted in 'without global' settings compared to Hyena in some experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Very low exact-match scores on many settings (especially without global variables) argue against algorithmic arithmetic generalization; the paper does not present circuit-level or representational probes showing algorithmic internal representations, so there is no direct evidence of internal arithmetic algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Dataset interventions (include vs exclude global variables; operator-pretraining with 40M samples), architecture interventions are applied to Hyena/T-HEX but not to GPT-2 in isolation; variable-length/tokenization manipulations and varying number of global variables.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Operator-pretraining substantially improves GPT-2 performance (paper states 'dramatically'); however, GPT-2 remains weaker overall on memorization tasks compared to Hyena and some T-HEX variants; removing global variables causes large performance drops, showing GPT-2 benefits from explicit in-context values.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Exact-match (mean ± std): With Global: 0.0066 (0.0010); Without Global: 0.0036 (0.0002) on the LOLA dataset experiments reported in Table 1. On Listops-1000 (long-input benchmark) GPT-2 exact-match reported as 0.361 (single-epoch). (Units: proportion exact match.)</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Almost zero performance when test prompts omit global variables (strong reliance on memorized globals); poor operator/generalization learning without operator-pretraining; weak memorization when dataset size and number of distinct global variables scale up; overall low exact-match on many LOLAMEME settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LOLAMEME: Logic, Language, Memory, Mechanistic Framework', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3007.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3007.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hyena (153M)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hyena (convolution-based HyenaOperator model, ~153M parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A convolution-based large-context architecture (HyenaHierarchy / HyenaOperator) used as a baseline; randomly initialized and trained from scratch on LOLAMEME synthetic arithmetic/program-execution tasks, showing stronger memorization in many settings compared to GPT-2.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Hyena (153M)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Convolutional-style sequence model using the HyenaOperator (Hyena hierarchy) as described by Poli et al.; in experiments model has ~153M parameters and a Hyena layer dimensionality of 864; trained from scratch (no pretraining) on the LOLAMEME datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same as GPT-2: chained execution of arithmetic expressions/statements with assignment and print; operators include +, -, *, /, modulo; tasks include memorization of global variables, in-context execution, latent-type transformations, varied variable name lengths, and longer programs (5→15 statements).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Stronger capacity for memorization of global variables (i.e., memorizing mapping from prompt/module patterns to output values) than GPT-2 in small/medium dataset regimes; relies on dataset memorization when many repeated global-variable appearances occur; limited general algorithmic arithmetic reasoning absent evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Exact-match much higher than GPT-2 in 'with global' settings (Table 1: Hyena 0.1432 ± 0.0037 vs GPT-2 0.0066 ± 0.0010), and Hyena retains better memorization at 100 global variables; drop to near-zero performance at 1000 globals indicates capacity limits and reliance on repetition/coverage in training; Hyena outperforms GPT-2 on many LOLAMEME memorization tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>When global-variable count increases or available training coverage is insufficient (e.g., dataset size fixed at 100k), Hyena fails to memorize (performance drops dramatically), showing memorization rather than algorithmic extrapolation. No internal probing to reveal algorithmic circuits.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Same dataset interventions (varying number of global variables, variable length, inclusion/exclusion of globals); architectural intervention consists of replacing one Hyena layer with a GPT-2 attention layer to create T-HEX variants (see separate entry).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Replacing Hyena layers by attention (T-HEX) often improves in-context learning and sometimes memorization compared to pure Hyena; Hyena alone memorizes better than GPT-2 at smaller global variable counts but both fail at larger unique global counts without more data.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Exact-match (mean ± std) on LOLA (Table 1): With Global: 0.1432 (0.0037); Without Global: 0.0033 (0.0007). On Listops-1000 exact-match reported as 0.185 (single-epoch). In other mixture experiments and variable-length sweeps Hyena generally outperforms GPT-2 on 'with global' settings.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Fails to memorize when unique global-variable vocabulary becomes large relative to dataset size (e.g., at 1000 globals performance drops near zero); poor generalization when test lacks memorized globals; struggles more on longer inputs when not combined with attention in lower layers (some T-HEX variants outperform Hyena on long inputs).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LOLAMEME: Logic, Language, Memory, Mechanistic Framework', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3007.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3007.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T-HEX (hybrid)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T-HEX (Hybrid model: Hyena with inserted GPT-2 attention layer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid architecture that replaces a single Hyena layer with a GPT-2 attention layer (T-HEX-n denotes replacement at the nth layer), designed to combine Hyena's memorization strengths with Transformer's in-context learning capabilities; evaluated on the same LOLAMEME arithmetic/program-execution tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T-HEX (variants T-HEX-9..T-HEX-17; best-reported T-HEX-12/13/15)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Hybrid model formed by taking a Hyena (153M) stack and replacing one Hyena layer with a GPT-2 attention layer; GPT-2 layer hidden dim set to 768, Hyena layers at 864, with linear connectors between them; networks randomly initialized and trained from scratch on LOLAMEME.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same LOLAMEME arithmetic/program execution tasks: chained arithmetic with assignments, local/global variables, latent-type modifications, varying variable lengths and counts, longer programs (5→15 statements), and operator-learning/in-context tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Combination of mechanisms observed: Hyena-like memorization capacity for frequently-seen globals plus improved in-context operator/local-variable learning attributable to inserted attention layer; architecture-level intervention suggests attention layers can enhance in-context learning while Hyena provides memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>T-HEX variants achieve higher exact-match than both Hyena and GPT-2 in many settings (Table 1: e.g., T-HEX-12 With Global 0.3631 ± 0.0351 is much higher than pure Hyena 0.1432); experiments varying which layer is replaced show that replacing higher layers (e.g., layer 12 or 13) tends to yield the best improvements for LOLA memorization/in-context tasks; T-HEX variants outperform Hyena on operator-pretraining and mixed-language experiments; variable-length experiments indicate tokenization effects interact with T-HEX memory benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Some T-HEX variants (certain layer replacements) perform poorly or show unstable training (examples: T-HEX-16 and T-HEX-17 worse in some settings; some T-HEX runs achieved 0 loss but 0 exact match indicating optimization/training pathologies); no fine-grained interpretability analyses to show how attention layers implement algorithmic arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Architectural intervention: replacing individual Hyena layers with GPT-2 attention layers (T-HEX-n). Dataset interventions also applied (operator-pretraining, varying global variables, variable-length tokenization).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Replacing specific Hyena layers with attention improves in-context learning and sometimes memorization — e.g., T-HEX-12 reaches With-Global exact-match 0.3631 ± 0.0351 (Table 1) while many T-HEX variants beat Hyena; operator-pretraining combined with T-HEX yields best overall performance on operator learning tasks; location of attention insertion matters (layers ~11-13 often best for LOLA, lower layers better for long-input Listops).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Selected exact-match (mean ± std) on LOLA (Table 1): T-HEX-9: With Global 0.1663 (0.2247), Without Global 0.0058 (0.0007); T-HEX-10: With Global 0.2886 (0.0738), Without Global 0.0046 (0.0014); T-HEX-11: With Global 0.2954 (0.0770), Without Global 0.0045 (0.0015); T-HEX-12: With Global 0.3631 (0.0351), Without Global 0.0047 (0.0008); T-HEX-13: With Global 0.2932 (0.0785), Without Global 0.0046 (0.0003); T-HEX-14: With Global 0.1652 (0.1475), Without Global 0.0048 (0.0019); T-HEX-15: With Global 0.2163 (0.1826), Without Global 0.0044 (0.0013); T-HEX-16: With Global 0.1372 (0.1164), Without Global 0.0048 (0.0008); T-HEX-17: With Global 0.0298 (0.0214), Without Global 0.0036 (0.0008). (Units: proportion exact match.) On Listops-1000 some T-HEX variants: T-HEX-4 0.362, T-HEX-1 0.360, T-HEX-13 0.321, T-HEX-15 0.276 (single-epoch values shown in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Some layer placements produce unstable or degenerate training (e.g., T-HEX-16/17 and cases with 0 loss but 0 exact-match); across all architectures, models perform near-zero when asked to generalize without memorized global variables unless operator-pretraining is applied; capacity limits when unique global variable vocabulary large relative to training data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LOLAMEME: Logic, Language, Memory, Mechanistic Framework', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3007.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3007.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LOLAMEME (dataset/framework)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LOLAMEME (LOgical, LAnguage, MEmory, MEchanistic) synthetic language/framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A synthetic mechanistic-evaluation framework introduced in this paper that simulates natural-language nuances (grammar, memory via global variables, logical chains, latent variable types, noise, variable-length tokens) using programming-language-like arithmetic statements to probe models' memorization and in-context arithmetic execution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LOLAMEME framework / dataset (LOLA and MEME instantiations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A configurable synthetic language framework with two instantiations (LoLa and MeMe) that generates modules of several statements containing assignments and arithmetic operators (+, -, *, /, %), global vs local variables, latent-type modifications, noise statements, variable-name length variation, and controlled dataset statistics (e.g., GLOBAL_VAR_COUNT, STATEMENT_COUNT). Dataset sizes used: training 100k samples by default, operator-pretraining 40M samples in a special experiment (total tokens ~4B in one setting). Labels are program outputs produced by an interpreter.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Program-execution arithmetic: evaluation of multi-statement programs involving chained arithmetic operations, variable assignment/lookup (global vs local), nested expressions (depth 1..3), modulo and integer arithmetic, latent-type value transformations, and mixed-language syntax variants.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Designed to tease apart memorization (global variable recall across training) from in-context execution (when variables are provided in prompt), as well as to test operator learning vs memorization; latent types create value-transform mechanisms to test deeper semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Paper shows systematic experiments demonstrating that inclusion/exclusion of global variables, number of unique global variables, and operator-pretraining produce predictable changes in model performance, supporting that tasks isolate memorization vs operator learning. Variable-length tokenization experiments show tokenization interacts with memory (longer variable names may split and aid memory).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>As a dataset/framework it does not itself provide mechanistic explanations of internal model computations; the paper notes that simpler synthetic tasks used in other work have limited operator counts, and LOLAMEME attempts to be richer but does not include circuit-level ground truth for models.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Dataset-control interventions: toggling 'include global' vs 'not include global' in test; varying GLOBAL_VAR_COUNT (100..1000), STATEMENT_COUNT (5..15), variable name lengths (3..9 mean), operator-pretraining (40M samples), mixing of LoLa and MeMe syntaxes, latent-type probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Changing dataset controls yields clear behavioral differences: including globals markedly raises exact-match (models exploit memorization), increasing unique global count reduces memorization ability (performance falls), operator-pretraining converts near-zero performance into much higher exact-match across models, and mixing languages stresses generalization where T-HEX variants show advantages.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used as the evaluation benchmark; metrics reported in paper are exact-match proportions (see model entries). Example dataset configs: default training size 100,000, test 10,000; operator-pretraining used 40,960,000 samples (approx 4B tokens total) in one experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Even with LOLAMEME's controlled complexity, models often resort to memorization rather than learning algorithmic execution; large unique-variable vocabularies overwhelm memorization capacity given fixed dataset sizes; models trained on mixed-language data may fail when constructs mixed within a module.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LOLAMEME: Logic, Language, Memory, Mechanistic Framework', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Progress measures for grokking via mechanistic interpretability <em>(Rating: 2)</em></li>
                <li>Unveiling Transformers with LEGO: a synthetic reasoning task <em>(Rating: 2)</em></li>
                <li>Hyena hierarchy: Towards larger convolutional language models <em>(Rating: 2)</em></li>
                <li>A mathematical framework for transformer circuits <em>(Rating: 1)</em></li>
                <li>Mechanistic interpretability, variables, and the importance of interpretable bases <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3007",
    "paper_id": "paper-270257920",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "GPT-2 (124M)",
            "name_full": "GPT-2 (decoder Transformer, 124M parameters)",
            "brief_description": "A decoder-only Transformer (GPT-2 124M) used as a baseline in this paper; randomly initialized (no pretraining) and trained on synthetic LOLAMEME datasets to evaluate arithmetic-like expression execution, memorization, and in-context learning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2 (124M)",
            "model_description": "Decoder-only Transformer (GPT-2) with layer hidden dimensionality set to 768 for the GPT-2 layer in the hybrid; 124M parameter configuration; models initialized randomly (no pretraining) and trained from scratch on LOLAMEME synthetic datasets.",
            "arithmetic_task_type": "Execution of chained arithmetic expressions and statements (assignment + print), using operators: addition, subtraction, multiplication, division, modulo; chained multi-statement programs with local and global variables; latent-type-modified arithmetic; varying statement counts (5 and 15) and variable name tokenization effects.",
            "reported_mechanism": "Primarily memorization/in-context retrieval of repeatedly-seen global variable values when available; limited operator-learning initially (improves with operator-pretraining). No direct evidence of learned algorithmic arithmetic procedures; performance appears to rely on pattern/memory of mappings rather than robust algorithmic generalization.",
            "evidence_for_mechanism": "Large drop in exact-match when global variables are removed from the test prompt indicates reliance on memorized global-variable values (Table 1); operator-pretraining (large dataset of operator examples) dramatically increases exact-match for all models indicating that lack of operator learning, not inherent arithmetic reasoning, limits performance; relative stability noted in 'without global' settings compared to Hyena in some experiments.",
            "evidence_against_mechanism": "Very low exact-match scores on many settings (especially without global variables) argue against algorithmic arithmetic generalization; the paper does not present circuit-level or representational probes showing algorithmic internal representations, so there is no direct evidence of internal arithmetic algorithms.",
            "intervention_type": "Dataset interventions (include vs exclude global variables; operator-pretraining with 40M samples), architecture interventions are applied to Hyena/T-HEX but not to GPT-2 in isolation; variable-length/tokenization manipulations and varying number of global variables.",
            "effect_of_intervention": "Operator-pretraining substantially improves GPT-2 performance (paper states 'dramatically'); however, GPT-2 remains weaker overall on memorization tasks compared to Hyena and some T-HEX variants; removing global variables causes large performance drops, showing GPT-2 benefits from explicit in-context values.",
            "performance_metrics": "Exact-match (mean ± std): With Global: 0.0066 (0.0010); Without Global: 0.0036 (0.0002) on the LOLA dataset experiments reported in Table 1. On Listops-1000 (long-input benchmark) GPT-2 exact-match reported as 0.361 (single-epoch). (Units: proportion exact match.)",
            "notable_failure_modes": "Almost zero performance when test prompts omit global variables (strong reliance on memorized globals); poor operator/generalization learning without operator-pretraining; weak memorization when dataset size and number of distinct global variables scale up; overall low exact-match on many LOLAMEME settings.",
            "comparison_to_humans_or_symbolic": null,
            "uuid": "e3007.0",
            "source_info": {
                "paper_title": "LOLAMEME: Logic, Language, Memory, Mechanistic Framework",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Hyena (153M)",
            "name_full": "Hyena (convolution-based HyenaOperator model, ~153M parameters)",
            "brief_description": "A convolution-based large-context architecture (HyenaHierarchy / HyenaOperator) used as a baseline; randomly initialized and trained from scratch on LOLAMEME synthetic arithmetic/program-execution tasks, showing stronger memorization in many settings compared to GPT-2.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Hyena (153M)",
            "model_description": "Convolutional-style sequence model using the HyenaOperator (Hyena hierarchy) as described by Poli et al.; in experiments model has ~153M parameters and a Hyena layer dimensionality of 864; trained from scratch (no pretraining) on the LOLAMEME datasets.",
            "arithmetic_task_type": "Same as GPT-2: chained execution of arithmetic expressions/statements with assignment and print; operators include +, -, *, /, modulo; tasks include memorization of global variables, in-context execution, latent-type transformations, varied variable name lengths, and longer programs (5→15 statements).",
            "reported_mechanism": "Stronger capacity for memorization of global variables (i.e., memorizing mapping from prompt/module patterns to output values) than GPT-2 in small/medium dataset regimes; relies on dataset memorization when many repeated global-variable appearances occur; limited general algorithmic arithmetic reasoning absent evidence.",
            "evidence_for_mechanism": "Exact-match much higher than GPT-2 in 'with global' settings (Table 1: Hyena 0.1432 ± 0.0037 vs GPT-2 0.0066 ± 0.0010), and Hyena retains better memorization at 100 global variables; drop to near-zero performance at 1000 globals indicates capacity limits and reliance on repetition/coverage in training; Hyena outperforms GPT-2 on many LOLAMEME memorization tasks.",
            "evidence_against_mechanism": "When global-variable count increases or available training coverage is insufficient (e.g., dataset size fixed at 100k), Hyena fails to memorize (performance drops dramatically), showing memorization rather than algorithmic extrapolation. No internal probing to reveal algorithmic circuits.",
            "intervention_type": "Same dataset interventions (varying number of global variables, variable length, inclusion/exclusion of globals); architectural intervention consists of replacing one Hyena layer with a GPT-2 attention layer to create T-HEX variants (see separate entry).",
            "effect_of_intervention": "Replacing Hyena layers by attention (T-HEX) often improves in-context learning and sometimes memorization compared to pure Hyena; Hyena alone memorizes better than GPT-2 at smaller global variable counts but both fail at larger unique global counts without more data.",
            "performance_metrics": "Exact-match (mean ± std) on LOLA (Table 1): With Global: 0.1432 (0.0037); Without Global: 0.0033 (0.0007). On Listops-1000 exact-match reported as 0.185 (single-epoch). In other mixture experiments and variable-length sweeps Hyena generally outperforms GPT-2 on 'with global' settings.",
            "notable_failure_modes": "Fails to memorize when unique global-variable vocabulary becomes large relative to dataset size (e.g., at 1000 globals performance drops near zero); poor generalization when test lacks memorized globals; struggles more on longer inputs when not combined with attention in lower layers (some T-HEX variants outperform Hyena on long inputs).",
            "comparison_to_humans_or_symbolic": null,
            "uuid": "e3007.1",
            "source_info": {
                "paper_title": "LOLAMEME: Logic, Language, Memory, Mechanistic Framework",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "T-HEX (hybrid)",
            "name_full": "T-HEX (Hybrid model: Hyena with inserted GPT-2 attention layer)",
            "brief_description": "A hybrid architecture that replaces a single Hyena layer with a GPT-2 attention layer (T-HEX-n denotes replacement at the nth layer), designed to combine Hyena's memorization strengths with Transformer's in-context learning capabilities; evaluated on the same LOLAMEME arithmetic/program-execution tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T-HEX (variants T-HEX-9..T-HEX-17; best-reported T-HEX-12/13/15)",
            "model_description": "Hybrid model formed by taking a Hyena (153M) stack and replacing one Hyena layer with a GPT-2 attention layer; GPT-2 layer hidden dim set to 768, Hyena layers at 864, with linear connectors between them; networks randomly initialized and trained from scratch on LOLAMEME.",
            "arithmetic_task_type": "Same LOLAMEME arithmetic/program execution tasks: chained arithmetic with assignments, local/global variables, latent-type modifications, varying variable lengths and counts, longer programs (5→15 statements), and operator-learning/in-context tasks.",
            "reported_mechanism": "Combination of mechanisms observed: Hyena-like memorization capacity for frequently-seen globals plus improved in-context operator/local-variable learning attributable to inserted attention layer; architecture-level intervention suggests attention layers can enhance in-context learning while Hyena provides memorization.",
            "evidence_for_mechanism": "T-HEX variants achieve higher exact-match than both Hyena and GPT-2 in many settings (Table 1: e.g., T-HEX-12 With Global 0.3631 ± 0.0351 is much higher than pure Hyena 0.1432); experiments varying which layer is replaced show that replacing higher layers (e.g., layer 12 or 13) tends to yield the best improvements for LOLA memorization/in-context tasks; T-HEX variants outperform Hyena on operator-pretraining and mixed-language experiments; variable-length experiments indicate tokenization effects interact with T-HEX memory benefits.",
            "evidence_against_mechanism": "Some T-HEX variants (certain layer replacements) perform poorly or show unstable training (examples: T-HEX-16 and T-HEX-17 worse in some settings; some T-HEX runs achieved 0 loss but 0 exact match indicating optimization/training pathologies); no fine-grained interpretability analyses to show how attention layers implement algorithmic arithmetic.",
            "intervention_type": "Architectural intervention: replacing individual Hyena layers with GPT-2 attention layers (T-HEX-n). Dataset interventions also applied (operator-pretraining, varying global variables, variable-length tokenization).",
            "effect_of_intervention": "Replacing specific Hyena layers with attention improves in-context learning and sometimes memorization — e.g., T-HEX-12 reaches With-Global exact-match 0.3631 ± 0.0351 (Table 1) while many T-HEX variants beat Hyena; operator-pretraining combined with T-HEX yields best overall performance on operator learning tasks; location of attention insertion matters (layers ~11-13 often best for LOLA, lower layers better for long-input Listops).",
            "performance_metrics": "Selected exact-match (mean ± std) on LOLA (Table 1): T-HEX-9: With Global 0.1663 (0.2247), Without Global 0.0058 (0.0007); T-HEX-10: With Global 0.2886 (0.0738), Without Global 0.0046 (0.0014); T-HEX-11: With Global 0.2954 (0.0770), Without Global 0.0045 (0.0015); T-HEX-12: With Global 0.3631 (0.0351), Without Global 0.0047 (0.0008); T-HEX-13: With Global 0.2932 (0.0785), Without Global 0.0046 (0.0003); T-HEX-14: With Global 0.1652 (0.1475), Without Global 0.0048 (0.0019); T-HEX-15: With Global 0.2163 (0.1826), Without Global 0.0044 (0.0013); T-HEX-16: With Global 0.1372 (0.1164), Without Global 0.0048 (0.0008); T-HEX-17: With Global 0.0298 (0.0214), Without Global 0.0036 (0.0008). (Units: proportion exact match.) On Listops-1000 some T-HEX variants: T-HEX-4 0.362, T-HEX-1 0.360, T-HEX-13 0.321, T-HEX-15 0.276 (single-epoch values shown in paper).",
            "notable_failure_modes": "Some layer placements produce unstable or degenerate training (e.g., T-HEX-16/17 and cases with 0 loss but 0 exact-match); across all architectures, models perform near-zero when asked to generalize without memorized global variables unless operator-pretraining is applied; capacity limits when unique global variable vocabulary large relative to training data.",
            "comparison_to_humans_or_symbolic": null,
            "uuid": "e3007.2",
            "source_info": {
                "paper_title": "LOLAMEME: Logic, Language, Memory, Mechanistic Framework",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "LOLAMEME (dataset/framework)",
            "name_full": "LOLAMEME (LOgical, LAnguage, MEmory, MEchanistic) synthetic language/framework",
            "brief_description": "A synthetic mechanistic-evaluation framework introduced in this paper that simulates natural-language nuances (grammar, memory via global variables, logical chains, latent variable types, noise, variable-length tokens) using programming-language-like arithmetic statements to probe models' memorization and in-context arithmetic execution.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LOLAMEME framework / dataset (LOLA and MEME instantiations)",
            "model_description": "A configurable synthetic language framework with two instantiations (LoLa and MeMe) that generates modules of several statements containing assignments and arithmetic operators (+, -, *, /, %), global vs local variables, latent-type modifications, noise statements, variable-name length variation, and controlled dataset statistics (e.g., GLOBAL_VAR_COUNT, STATEMENT_COUNT). Dataset sizes used: training 100k samples by default, operator-pretraining 40M samples in a special experiment (total tokens ~4B in one setting). Labels are program outputs produced by an interpreter.",
            "arithmetic_task_type": "Program-execution arithmetic: evaluation of multi-statement programs involving chained arithmetic operations, variable assignment/lookup (global vs local), nested expressions (depth 1..3), modulo and integer arithmetic, latent-type value transformations, and mixed-language syntax variants.",
            "reported_mechanism": "Designed to tease apart memorization (global variable recall across training) from in-context execution (when variables are provided in prompt), as well as to test operator learning vs memorization; latent types create value-transform mechanisms to test deeper semantics.",
            "evidence_for_mechanism": "Paper shows systematic experiments demonstrating that inclusion/exclusion of global variables, number of unique global variables, and operator-pretraining produce predictable changes in model performance, supporting that tasks isolate memorization vs operator learning. Variable-length tokenization experiments show tokenization interacts with memory (longer variable names may split and aid memory).",
            "evidence_against_mechanism": "As a dataset/framework it does not itself provide mechanistic explanations of internal model computations; the paper notes that simpler synthetic tasks used in other work have limited operator counts, and LOLAMEME attempts to be richer but does not include circuit-level ground truth for models.",
            "intervention_type": "Dataset-control interventions: toggling 'include global' vs 'not include global' in test; varying GLOBAL_VAR_COUNT (100..1000), STATEMENT_COUNT (5..15), variable name lengths (3..9 mean), operator-pretraining (40M samples), mixing of LoLa and MeMe syntaxes, latent-type probabilities.",
            "effect_of_intervention": "Changing dataset controls yields clear behavioral differences: including globals markedly raises exact-match (models exploit memorization), increasing unique global count reduces memorization ability (performance falls), operator-pretraining converts near-zero performance into much higher exact-match across models, and mixing languages stresses generalization where T-HEX variants show advantages.",
            "performance_metrics": "Used as the evaluation benchmark; metrics reported in paper are exact-match proportions (see model entries). Example dataset configs: default training size 100,000, test 10,000; operator-pretraining used 40,960,000 samples (approx 4B tokens total) in one experiment.",
            "notable_failure_modes": "Even with LOLAMEME's controlled complexity, models often resort to memorization rather than learning algorithmic execution; large unique-variable vocabularies overwhelm memorization capacity given fixed dataset sizes; models trained on mixed-language data may fail when constructs mixed within a module.",
            "comparison_to_humans_or_symbolic": null,
            "uuid": "e3007.3",
            "source_info": {
                "paper_title": "LOLAMEME: Logic, Language, Memory, Mechanistic Framework",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Progress measures for grokking via mechanistic interpretability",
            "rating": 2,
            "sanitized_title": "progress_measures_for_grokking_via_mechanistic_interpretability"
        },
        {
            "paper_title": "Unveiling Transformers with LEGO: a synthetic reasoning task",
            "rating": 2,
            "sanitized_title": "unveiling_transformers_with_lego_a_synthetic_reasoning_task"
        },
        {
            "paper_title": "Hyena hierarchy: Towards larger convolutional language models",
            "rating": 2,
            "sanitized_title": "hyena_hierarchy_towards_larger_convolutional_language_models"
        },
        {
            "paper_title": "A mathematical framework for transformer circuits",
            "rating": 1,
            "sanitized_title": "a_mathematical_framework_for_transformer_circuits"
        },
        {
            "paper_title": "Mechanistic interpretability, variables, and the importance of interpretable bases",
            "rating": 1,
            "sanitized_title": "mechanistic_interpretability_variables_and_the_importance_of_interpretable_bases"
        }
    ],
    "cost": 0.014518,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>31 May 2024</p>
<p>Jay Desai 
Xiaobo Guo xiaobo.guo.gr@dartmouth.edu 
Srinivasan H Sengamedu sengamed@amazon.com 31 May 2024EF796CCF654C07ACB6C286D205D34031arXiv:2406.02592v1[cs.LG]
The performance of Large Language Models has achieved superhuman breadth with unprecedented depth.At the same time, the language models are mostly black box models and the underlying mechanisms for performance have been evaluated using synthetic or mechanistic schemes.We extend current mechanistic schemes to incorporate Logic, memory, and nuances of Language such as latent structure.The proposed framework is called LOLAMEME and we provide two instantiations of LOLAMEME: LoLa and MeMe languages.We then consider two generative language model architectures: transformer-based GPT-2 and convolution-based Hyena.We propose the hybrid architecture T HEX and use LO-LAMEME framework is used to compare three architectures.T HEX outperforms GPT-2 and Hyena on select tasks.</p>
<p>INTRODUCTION</p>
<p>Language and language understanding have several nuances such as grammar, types, memory, reasoning, logic, short-term memory, and long-term memory.For example, consider the three sentences "Albert is a famous physicist", "Albert Fermi is a famous physicist", "Albert Fermi is a famous physicist in the new book Everywhere Nowhere".Each of the sentences constitutes the short-term memory (or prompt, in Language Model parlance).Understanding of these sentences requires interaction with the long-term memory or fact store.The first sentence is likely to match with "Albert Einstein" while the second one is likely to match "Albert Einstein" and "Enrico Fermi".The last sentence is most likely to be interpreted in its own context and it may not have matches.One can consider long-term memory1 to be shaped by pre-training and fine-tuning while short-term memory is shaped by prompts and in-context learning.While pre-training.fine-tuning, in-context learning, and prompting have made it to the mainstream, the mechanistic understanding of these are hard given the mammoth sizes of datasets used for training the models and ever-lingering doubts of train-test overlap.</p>
<p>In addition, different Language Model architectures capture different aspects of language, and frameworks to systematically compare them do not exist.The emerging framework of mechanistic evaluation is very promising.Our goal is to contribute to mechanistic evaluation research with a new framework which is capable of mimicking the many nuances of natural language.Our framework, called LOLAMEME, supports the following aspects of natural language: word sizes, permanent vs temporary facts, latent types, and noise.</p>
<p>We then use the LOLAMEME framework to explore the capabilities of different language model architectures.Transformer-based architectures are most widely used for language models and there are emerging convolution-based architectures such as Hyena.We evaluate the performance of these models using LOLAMEME and propose a hybrid architecture, called T HEX, which performs better on certain aspects of natural language.</p>
<p>Our contributions are are as follows.</p>
<ol>
<li>
<p>We propose a framework called LOLAMEME which is a mechanistic framework closer to natural language.</p>
</li>
<li>
<p>We create multiple datasets based on LOLAMEME consisting of several billion tokens.</p>
</li>
<li>
<p>We evaluate two popular architectures, GPT-2 and Hyena, and show that the architectures have complementary strengths.</p>
</li>
<li>
<p>We propose a new hybrid architecture, T HEX that is based on GPT-2 and Hyena.T HEX outperforms GPT-2 and Hyena on most LOLAMEME tasks and on a related benchmark dataset.</p>
</li>
</ol>
<p>RELATED WORK</p>
<p>This research touches upon two lines of recent work: mechanistic interpretation and language model architectures.</p>
<p>MECHANISTIC INTERPRETATION</p>
<p>Mechanistic Interpretability is the reverse-engineering of neural network models into human understandable concepts Olah (2022).There is lot of recent research interest in Mechanistic Interpretability of language models.For example, Elhage et al. (2021) takes the approach of analyzing graphs of underlying computation.Other approaches use synthetic datasets for benchmarking.Hyena Hierarchy Poli et al. (2023), for example, is evaluated using the following tasks on synthetic dataset: associative recall, majority, counting, ICL of functions, and arithmetic.LEGO Zhang et al. (2022) is a synthetic reasoning task that consists of a series of assignments and arithmetic operations involving variables.Nanda et al. (2023) investigate "grokking" through the task of modular addition.See also Pearce et al. (2023).</p>
<p>Our fundamental observation is that these tasks involve in-context learning.The number of tasks or operators involved (such as associative recall, majority, or arithmetic) are small.In contrast, real-world language understanding requires more operators, memorization of lot more facts, and reasoning over memorized facts.LOLAMEME fills this gap.LOLAMEME includes LOgical reasoning over MEmorized facts for MEchanistic interpretation.In addition, LOLAMEME supports types, latent variables as well as noise which brings it closer to natural language.The role of transformers and convolutions in language understanding has to be explored.In this paper, we use LOLAMEME framework and show architectures which interleave Transformer and Hyena blocks perform certain tasks well.</p>
<p>LANGUAGE MODEL ARCHITECTURES</p>
<p>LOLAMEME-LANGUAGE FEATURES</p>
<p>LOLAMEME tries to mimic several aspects of natural language though it is couched in programming language syntax.Actually, there are few syntax variations and the variations are also mixed up to ensure such phenomenon in natural languages.We use programming language because it is easy to define the semantics of language constructs.We explore the following aspects of natural language through LOLAMEME constructs.</p>
<p>ALPHABET AND VOCABULARY</p>
<p>Natural languages have have a certain vocabulary size and words in the vocabulary have certain number of characters.We quantify these aspects through the number of characters of variable names and the number of variable names.</p>
<p>GRAMMAR</p>
<p>We use arithmetic expression like syntax for the "sentences" of the language.We use five operators: addition, subtraction, multiplication, division and modulo.</p>
<p>MEMORY</p>
<p>Memory is based on tokens not present in the prompt.In case of associative recall, the strings are part of training and the model has to retrieve the most appropriate result from memory.We use global variables, variable which have the same value in all expressions, as a measure of memory.</p>
<p>We vary the number of global variables in the corpus.</p>
<p>LOGIC/REASONING</p>
<p>Logic or reasoning is based on a chain of reasoning such as the following:
a = 2; b = 1; c = (a/b) + X; e = c * Y ; print(e);
The models are expected to interpret and execute the above sequence of expressions containing both local variables (in lower case, in this example) and global variables (in upper case, in this example).In other words, logical reasoning is performed on a sequence of arithmetic expressions.See Appendix B for examples of actual data.</p>
<p>TWO INSTANTIATIONS: LOLA AND MEME</p>
<p>We now describe two of LOLAMEME in programming language based syntax and semantics.The language supports variable names of specified sizes.The operators supported are addition, subtraction, multiplication, and modulo.The two languages, LoLa and MeMe, have differences in syntax as shown below.</p>
<p>Operation LoLa MeMe Addition
+ | Subtraction − ! Multiplication * @ Division % / Grouping () {} Case Camel Snake
A module or paragraph consists of a sequence of roughly "STATEMENT COUNT" number of statements.The output of the module is the result of execution of the module.This is the semantics of the module.</p>
<p>Global Variables for Memory: Each corpus has 'GLOBAL VAR COUNT' number of global variables.The global variables are assigned the same value in the training corpus (at least "GLOBAL VARIABLES NUM APPEARANCE" times).These variables are used but not assigned in the test corpus with global variables.</p>
<p>Local Variables: These are used in module and its values have scope limited to that module only.</p>
<p>Noise: In a module or paragraph, there can be statements which may not used to calculate the final result.These should be ignored.</p>
<p>Mixing: In the test corpus, we mix LoLa and MeMe constructs.We test which model generalizes better.</p>
<p>Variable Names: Variable names can be between "MIN VAR LENGTH" and "MAX VAR LENGTH" characters long.We generate local variable names and global variable names based on dataset configuration.</p>
<p>Latent Types: Variables have latent types which changes their value.Assume variables starting with a-A have Type A and those beginning with A-B have Type B and those beginning with B-z has type C.Here A,B are a characters in x-z such that a ≤ A &lt; B ≤ z.The probability of the type is p.</p>
<p>The following is the rule used with latent types:</p>
<p>In the expression var1 op var2, if the type of var1 is Type A and that of var2 is Type B, then the effective value is 2 * var1 op var2/2.</p>
<p>Interpreter: In addition to data set creation, we need to create a simple interpreter which executes the code and outputs the result.We write the interpreter for only one language.We translate the other language as well as the mixed language to this language by string substitution.(It may be easier to convert snake case to camel case.)</p>
<p>All this is controlled through the dataset configuration show in in Appendix A. Considering that the dimensionality of the hidden states might influence the performance of the model, we set it to be 768 for GPT-2 layer and 864 for Hyena layer, and add linear layer for connecting the GPT-2 layer and Hyena Layer.</p>
<p>As baselines, we also test GPT-2 Radford et al. (2019)(124M) which is a decoder transformer model.Hyena (153M) Poli et al. (2023), which uses their proposed HyenaOperator which is a drop-in replacement for attention.</p>
<p>For all models, as we focus on testing its performance resulting from the architecture instead of the pre-training, all models are initialize randomly instead of using the pretraining weights.</p>
<p>Metrics:</p>
<p>In our experiments, we utilize exact match as the metric for evaluating models' performance.</p>
<p>T HEX VARIATIONS</p>
<p>In our experiments, the difficult of the tasks include two aspects: 1) the ability for logical reasoning; 2) the ability to remember the values of the variables used for calculating.Therefore, we create two test sets: 1) the global variables used for calculating is not provided which requires the model to remember the global variables during the training; 2) all the variables used for calculating is provided as the prompts, which focuses on the models' in-context learning ability.</p>
<p>In Table 1 , we show the performance of GPT-2, Hyena and T HEX.Since the T HEX replace one layer of the Hyena model to the GPT-2 layer, we use T HEX-n to represent the T HEX which replace the nth layers counting from the inputs.Our decision to choose n&gt;=9 were based on early experiment described in Section D.</p>
<p>For the settings of including global variable shown in Table 1, both Hyena, and T HEX achieves higher performance than the GPT-2, which shows that Hyena Operator based model is better at synthetic reasoning task.Meanwhile, we observe that the most of the T HEX settings achieve better performance than the Hyena model (except for T HEX-16 and T HEX-17).This phenomenon shows that the introduction of the GPT-2 layer can increase the performance of Hyena Operator.</p>
<p>For the settings not including global variables, from Table 1, we observe that the performance of all models drop dramatically.This proves that compared with in-context learning, all these models are more likely to remember the parameters shown repeatedly in the training.Among all models, GPT-2 is the most stable one since its performance with the setting of including global variables is quite poor.When comparing Hyena and T HEX, we observe that all versions of T HEX achieve better performance than the Hyena, which shows that the introduction of GPT-2 layer can increase the in-context learning ability of the Hyena model.</p>
<p>Model</p>
<p>With Global Without Global Hyena 0.1432 (0.0037) 0.0033 (0.0007) GPT-2 0.0066 (0.0010) 0.0036 (0.0002) T HEX-9 0.1663 (0.2247) 0.0058 (0.0007) T HEX-10 0.2886 (0.0738) 0.0046 (0.0014) T HEX-11 0.2954 (0.0770) 0.0045 (0.0015) T HEX-12 0.3631 (0.0351) 0.0047 (0.0008) T HEX-13 0.2932 (0.0785) 0.0046 (0.0003) T HEX-14 0.1652 (0.1475) 0.0048 (0.0019) T HEX-15 0.2163 (0.1826) 0.0044 (0.0013) T HEX-16 0.1372 (0.1164) 0.0048 (0.0008) T HEX-17 0.0298 (0.0214) 0.0036 (0.0008) Table 1: The performance of GPT-2, Hyena and T HEX tested with LOLAMEME framework with the settings of including global variables and not including global variables.The results are reported with the LOLA datasets.In the experiments, we replace the 9-17 layers of Hyena.We report the mean and standard deviation (shown in the bracket) of exact match with various random seed.</p>
<p>With these two settings, we show that the combination of Hyena and GPT-2 can increase the incontext learning ability as well as keeping or even increasing models' ability for memorization.The results of MEME are shown in Appendix C.1.1 and the observations are similar to the results of LOLA reported here.</p>
<p>ARE MODELS GOOD AT MEMORIZATION?</p>
<p>The number of global variable is an important factor to influence the difficulty of LOLAMEME.Therefore, we conduct experiments with different number of global variables to analyze how global variable number influence the performance of the model.The dataset config for this experiment is shown in Appendix A.3.In addition to global variables, there are latent variables in the dataset which will test if the models are able to remember what the latent type meaning.</p>
<p>We show the performance of different models as a function of number of global variables in Figure 1 and Figure 2. It can be seen that Hyena model memorizes much better than GPT-2 model when the number of global variables are 100.When they are increased to 1000, the performance drops and neither model memorizes anything.This could be due to the fact that the train dataset size is constant i.e. 100,000.This shows that hyena model learns much better than GPT-2 when dataset size is smaller.T HEX-13 in Figure 1 however performs better than rest on an average from 100-1000 global variables.This shows replacing hyena block with attention block in 13th layer has the highest impact.</p>
<p>In Figure 1 when number of global variables are 800 -1000, attention in 15th layer becomes highest performing.In addition to this, when number of global variables is 700, there's a dip in scores for all models.We'd investigate this behaviour in future work.</p>
<p>These experiments are conducted for 3 times and we plot mean scores.Figure 2: Memorization performance comparison between Hyena and GPT-2.This dataset does not includes global variables.</p>
<p>DOES LENGTH OF VARIABLES INFLUENCE MODEL PERFORMANCE?</p>
<p>In this experiment, we conduct experiments to show how the variable length influences the performance of different models.More specifically, we create multiple datasets with the mean length of variable as 3, 4, 5, 6, 7, 8, and 9.For each mean length, we set the interval of the variable length to be In Figure 3, we show the exact match for GPT-2, Hyena, and T HEX with the settings including global variables (left) and not including global variables (right) for the "LOLA".(The results of "MEME" are shown in Appendix C.1) Within the settings of including global variables, Hyena consistently outperforms GPT-2 across all mean variable lengths in terms of "exact match".We observe that for most variable lengths, both the T HEX-13 and T HEX-15 achieve better performance than the Hyena model.This shows that the improvement of T HEX model for the settings with global variable is stable across all variable lengths.We also notice that, with the increase of variable length, the performance of T HEX increases.This might because longer variable name are more likely to be split into multiple tokens which can help T HEX to remember the values of variables more correctly.Also we notice that T HEX-13 is a better choice than T HEX-15 which is in consist with the results reported before.These observations are also supported by the trends shown in the loss values in Figure 4.</p>
<p>For the settings of not including global variables shown in Figure 3 (right), Mirroring observations from settings with global variables, T HEX-13 achieves the best performance for most of the time.</p>
<p>Being different from settings with global variable, Hyena is worse than the GPT-2 at most of the variable length.Given the minute values of the exact match, this anomaly may be attributed to random variations.By comparing the performance shown in Table 1, we observe that all models (T HEX, Hyena and GPT-2) perform poorly on the test dataset without global variables.We assume this is because the models, especially T HEX, focus on remembering the global variables and ignore the learning of operators and local variables.Therefore, we create a dataset without global variables and use it to train all models and test the performance of non-global variables.In this setting, we train and evaluate model's ability for learning the operation and the in-context learning of local variables.</p>
<p>More specifically, we create a operator-pre-training dataset with 40,000,000 samples, and each sample includes 5 statements.The mean token number for each sample is about 100 tokens.For each model, we train 40,960,000 samples and the total training token number is 4 billion.We also create a evaluation and test sets in which there are 10000 samples for testing.In This experiment setting, for T HEX, we only test the T HEX-13 and T HEX-15 which achieves the best performance in previous experiments with and without global variables.</p>
<p>The result are show in Table 2.We observe that compared with the reported performance in Table 1 the performance of all models increase dramatically after operator-pre-training.This means that all models can learn operation and have the ability of in-context learning to remember the value of local variables.By comparing the performance of all models, we notice that the GPT-2 is significantly poorer than the other models.Meanwhile, the performance of T HEX-13 is the best among all models.The exact match of models before and after operator-pre-training.The reported result is the mean of 3 rounds and the standard deviation is reported in the bracket.</p>
<p>Model</p>
<p>MODEL BEHAVIOUR ON LONGER INPUT LENGTH</p>
<p>In this experiment, we increase the number of statement from 5 to 15 in basic config shown in Appendix A.1.The number of global variables is 1000 where the models performed worse in memorization experiment in Section 6.2.Therefore this dataset becomes even harder as there are 1000 global variables and the number of statements is increased by 10.With this increased complexity, we keep all the training setting same.The results are shown in Table 3.</p>
<p>Based on the results, we can see that all the models struggle to learn this long and more complex dataset.Hyena performs the best followed by T HEX-9.Evident from the pretraining in Section 6.4, more pre-training larger dataset would be required for better results.But based on current training, for longer input size, attention in layers ≤ 9 shows better performance.T HEX-11 to T HEX-15 showed 0 loss after few epochs and showed 0 exact match.We would investigate this issue in our future work.For training, we create a mixing dataset in in which half of the data is created based on "LOLA" and the other half is based on "MEME".For testing, we have two different settings: 1) samplemixing: each sample is either "LOLA" or "MEME"; 2) module-mixing: the statements can be the combination of "LOLA" and "MEME" (e.g.zAb = (2@3}).</p>
<p>Model</p>
<p>In Table 4, we show the results of two mixture methods.As we can observe that GPT-2 still achieves the worst performance among all models similar to the single-language experiments.For the comparison between hyena and T HEX, T HEX-13 achieves the best performance.We can observe difference between Hyena and T HEX-13 is significantly bigger in sample-mixing but shrinks in module-mixing.</p>
<p>HOW IS PERFORMANCE ON EXISTING DATASETS?</p>
<p>Listops We train all the models on Listops-1000 dataset for 1 epoch.The results are shown in Table 5.Based on the results, it can be seen that for longer input length as in Listops dataset, lower layers of attention in T HEX has better performance.And based on the results on LOLAMEME in Section 6.1, when the input length was comparatively smaller, attention in higher layers has more impact.This is in accordance with our observation in Section 6.5 in which for longer and more complex input, T HEXwith attention in lower layers showed better performance.This shows that LOLAMEME framework can effectively be used to mimic various aspects of language.</p>
<p>exact-match GPT-2 0.361 Hyena 0.185 T HEX-4 0.362 T HEX-1 0.360 T HEX-13 0.321 T HEX-15 0.276  A sample of Output.In total there are 1047 unique output values in test set.</p>
<p>[ '18', '18', '90', '77', '-24'] C Table 6: The performance of GPT-2, Hyena and T HEX tested with LOLAMEME framework with the settings of including global variables ("Include Global") and not including global variables ("Not include Globale").The results are reported with the MEME datasets.In the experiments, we replace the 9-17 layers of Hyena.We report the mean and standard deviation (shown in the bracket) of exact match with various random seed.</p>
<p>D EARLY EXPERIMENT TO SELECT ATTENTION LAYERS</p>
<p>In the memorization experiment with 500 global variables, we replaced these hyena layers with attention to scan and then dive deep in that direction.The results are shown in Table 7. From this experiment, we found that higher layers showed high performance.</p>
<p>Model</p>
<p>1.The dataset creation process is shown in Appendix D.1 and sample dataset is shown in Appendix B.5 MODELSInspired by previous workLee-Thorp et al. (2022), we propose the T HEX, which is the combination of transformer attention and Hyena Operator.More specifically, we replace certain layer of the Hyena (153M) model with the GPT-2 layer.While other combinations are possible, we start with this simple hybrid model.Since the T HEX replaces one layer of the Hyena model to the GPT-2 layer, we use T HEX-n to represent the T HEX which replaces the n-th layers counting from the input layer.</p>
<p>[round(0.75* M ), round(1.25 * M )].We conduct experiments with and without global variables.The detailed settings are shown in Appendix A.2.</p>
<p>Figure 3 :
3
Figure3: Exact match and loss for GPT-2 and Hyena with different variable Length.In figure, we provide the range of the variable length, and the mean length in the format of mean[min,max].This is on dataset with (left) and without (right) global variables.</p>
<p>Figure 4 :
4
Figure4: Loss for T HEX , GPT-2 and Hyena with different variable Length.In figure, we provide the range of the variable length, and the mean length in the format of mean[min,max].This is on dataset with (left) and without (right) global variables.</p>
<p>(qqbsV + vDU);xAkt = (wSTCa * ywP);xAkt = 76;pcbZAu = (ybU + rqnDmId);print(pcbZAu)', 'bHJFL = (pFzd * 91);fuB = (ehL -yNOkq);xAkt = (dCE % jvL);xnbolt = xaQeVwz;print(fuB)']</p>
<p>DOES LENGTH OF VARIABLES INFLUENCE MODEL PERFORMANCE?-APPENDIX C.1 RESULTS FOR MEME C.1.1 T HEX VARIATIONSIn Table6, we show the results of replacing different layer from the Hyena model with the MEME settings.Similar to the results we have for the LOLA shown in Section 6.1, Hyena and most of the T HEX(except for T HEX-13 and T HEX-17) achieve better performance than the GPT-2 on both settings of including and not including global variables.Meanwhile most of the T HEX achieves better performance than Hyena.</p>
<p>Table 2 :
2</p>
<p>Table 4 :
4
Performance of all models with different language mixture settings.The reported results are the mean exact match of multiple rounds.The standard deviation is reported in the bracket.
Include Global Not Include GlobalHyena0.0440.005GPT-20.0050.003T HEX-10.0100.002T HEX-30.0170.004T HEX-50.0120.003T HEX-70.0080.002T HEX-90.0270.004T HEX-110.0000.00T HEX-130.0000.00T HEX-140.0000.00T HEX-150.0000.00Table 3: Model performance on longer input length.6.6 DOES MODEL LEARN FROM MULTIPLE LANGUAGES?
For our framework, there are two languages: "LOLA" and "MEME".Although they have the same syntax, the symbol for the operators and the variable named rules are different.Therefore, one question is that if the model can only learn these two languages in different samples, what's the performance of it when combining these two languages in one sample.2</p>
<p>Table 5 :
5
Performance on Listops-1000.</p>
<p>Table 7 :
7
Early Experiment to scan attention layers.
With Global Without GlobalHyena0.00920.0058GPT-20.00950.0056T HEX-00.01240.0045T HEX-10.21340.0052T HEX-90.22140.0041T HEX-140.33690.0005
The uses of terms short-term and long-term in the introduction are figurative and not literal or technical. The main contributions do not depend on literal or metaphorical interpretations.
EXPERIMENTSWe perform the following experiments.T HEX variations: We vary the layer in the T HEX model which has the GPT-2 layer. Section 6.1 Memorization: We create test sets with and without global variables. The training set always has global variables. We vary the number of global variables in the training set from 100 to 1000 in steps of 100. See Section 6.2. Variable Length: We vary the average number of characters in the variable name from 3 to 8. The actual size is ±1 or ±2 over the average. See Section 6.3. Long Input: We increase the input size by increasing number of statements from 5 to 15. We experiment to see model's behaviour on longer more complex dataset. See Section 6.5. In-context learning: We experiment to see if models can be taught to do in context learning. We do this by training the model on large dataset. See Section 6.4. Learn from Multiple languages: We experiment to see if models learns aspects of multiple languages. We create a dataset with two languages and test the model in two different ways. See Section 6.6. Performance on public dataset: We test models on Listops[Nangia &amp; Bowman (2018)] dataset.See Section 6.7.
The situation is to mimic the situation that when learning multiple languages, there is only one language in each sentence, but when using, multiple languages might show in one sentence.
We have proposed a new framework, called LOLAMEME for mimicking different aspects of natural language.LOLAMEME can be used to test models on various aspects of language in a controlled fashion.We have instantiated LOLAMEME with two different manifestations, called LoLa and MeMe.We have shown how two different architectures, GPT-2 and Hyena, perform with respect to different aspects of language.We have also proposed a hybrid architecture, T HEX, that combines GPT-2 and Hyena blocks.T HEX outperforms GPT-2 and Hyena on most LOLAMEME tasks and on a related benchmark dataset.We believe ideas presented in this paper will inspire research on more systematic evaluations of large language models and on more effective model architectures.−&gt;v a r / 2 'LATENT TYPE C MODIFICATION ' : ' / 2 ' , # i f v a r s t a r s w i t h any c h a r i n b e t w e e n a s c i i 23 −26 , var −&gt;v a r / 2 "GLOBAL VAR COUNT" : 5 0 0 , # number o f u n i q u e g l o b a l v a r s "LOCAL VAR COUNT" : 1 0 , # number o f u n i q u e l o c a l v a r s "STATEMENT COUNT" : 5 , # number o f s t a t e m e n t s i n one d a t a i t e m "EXPRESSION MIN DEPTH" : 1 , # min r e c u r s i o n s i n a s t a t e m e n t "EXPRESSION MAX DEPTH" : 2 , # max r e c u r s i o n s i n a s t a t e m e n t "MIN VAR LENGTH" : 3 , # min s t r i n g l e n g t h o f v a r i a b l e s "MAX VAR LENGTH" : 1 0 , #max s t r i n g l e n g t h o f v a r i a b l e s "MIN INT VALUE" : 1 , # min v a l u e s a s s i g n e d o f v a r i a b l e s "MAX INT VALUE" : 1 0 0 , #max v a l u e s a s s i g n e d o f v a r i a b l e s "DATASET SIZE" : i n t ( 1 e5 ) , # t r a i n d a t a s e t s i z e " TEST DATASET SIZE " : i n t ( 1 e4 ) , # t e s t d a t a s e t s i z e "GLOBAL VARIABLES NUM APPEARANCE" : 1 0 0 0 , # number o f t i m e s e a c h g l o b a l v a r a p p e a r s i n d a t a s e t "USE FAKER" : F a l s e , # u s e f a k e r t o g e n e r a t e v a r i a b l e s } }A.2 CONFIGFORVARIABLELENGTHBaseConfig is used with following changes:T h e r e a r e no l a t e n t v a r i a b l e s u s e d f o r t h i s e x p e r i m e n t .A sample of Output.In total there are 2506 unique output values in training set.['84', '47', '-48', '86', '36']A sample of Global Variables A sample of Output.In total there are 1996 unique output values in test set.D.1 DATASET CREATION PROCESSThe objective of our dataset creation process is to simulate a rich diversity of scenarios, encompassing different programming constructs, variable naming conventions, operators, and expression depths, all governed by a highly configurable system.D.1.1 CONFIGURATION SYSTEMOur system is initiated with a configuration dictionary (config) (base config is shown in Appendix A.1), which contains various parameters to steer the data generation process.D.1.2 GLOBAL VARIABLE GENERATIONA specified number of global variables are created.Each variable name is generated based on the naming convention specified in the config.After ensuring uniqueness, each global variable is assigned a random integer value within a defined range.D.1.3 LOCAL VARIABLE GENERATIONDepending on the configuration, a fixed number of local variables or a dynamic set can be generated.Similar to global variables, each local variable's name adheres to the naming convention and ensures no overlap with already generated names.D.1.4 STATEMENTS AND DEPTHOne of the core components of the dataset is the generation of programming constructs, or as we commonly refer to them, "statements".A statement in our dataset can range from simple assignments to more complex expressions involving multiple operators, variables, and nested subexpressions.Depth is a crucial factor in determining the complexity of these statements.Depth refers to the number of nested sub-expressions within a single statement.For instance, a statement with a depth of 1 might look like a + b, while a depth of 2 could yield a + (b * c), and a depth of 3 could further nest as a + (b * (d -e)).The config allows users to specify the minimum and maximum depths (EXPRES-SION MIN DEPTH and EXPRESSION MAX DEPTH), ensuring a varied level of complexity across the dataset.D.1.5 INPUT ITEM CREATIONOne input item in dataset has "STATEMENT COUNT" number of statements.Each statement is created one by one.The first variable is however is global variable or local variable using a value from constant or global variable or is function using global variables, constants and operators chosen randomly with some probability.Once the first variable for statement is created, rest of the statement is made by using this i.e. depth is recursively added.All the variables used in statements are tracked to use in future statements.It is made sure that no unassigned variable is used to assign to a variable.Only variables which are assigned a value are allowed to be used for assignment or in a function.Future statements are created using variables randomly chosen between tracked local variables, global variables, constants or new local variables are created.Total of "STATEMENT COUNT" number of statements are created like this and at the end one of the randomly chosen tracked variables is printed.We convert the input item to python language and use python interpreter to generate output and that becomes the label.This makes input and label pair."DATASET SIZE" number of unique input output pairs are then created to make a dataset.
Longformer: The long-document transformer. Iz Beltagy, Matthew E Peters, Arman Cohan, 2020</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 2018</p>
<p>Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova Dassarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam Mccandlish, Chris Olah, A mathematical framework for transformer circuits. Transformer Circuits. 2021</p>
<p>FNet: Mixing tokens with Fourier transforms. James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon, 10.18653/v1/2022.naacl-main.319Proceedings of the 2022 Conference of the North American Chapter. the 2022 Conference of the North American ChapterSeattle, United StatesAssociation for Computational LinguisticsJuly 2022</p>
<p>BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, ACL. 2020</p>
<p>Progress measures for grokking via mechanistic interpretability. Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, Jacob Steinhardt, ICLR2023</p>
<p>Listops: A diagnostic dataset for latent tree learning. Nikita Nangia, Samuel R Bowman, arXiv:1804.060282018arXiv preprint</p>
<p>Mechanistic interpretability, variables, and the importance of interpretable bases. Transformer Circuits. Chris Olah, 2022</p>
<p>Do machine learning models memorize or generalize? People + AI Research. Adam Pearce, Asma Ghandeharioun, Nada Hussein, Nithum Thain, Martin Wattenberg, Lucas Dixon, 2023</p>
<p>Hyena hierarchy: Towards larger convolutional language models. Michael Poli, Stefano Massaroli, Eric Q Nguyen, Daniel Y Fu, Tri Dao, Stephen A Baccus, Yoshua Bengio, Stefano Ermon, Christopher Ré, ArXiv, abs/2302.108662023257050308</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, NeurIPS2017</p>
<p>Unveiling Transformers with LEGO: a synthetic reasoning task. Yi Zhang, Arturs Backurs, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Tal Wagner, arXiv:2206.04301ArXiv. 2022</p>            </div>
        </div>

    </div>
</body>
</html>
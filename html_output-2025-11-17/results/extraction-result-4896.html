<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4896 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4896</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4896</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-103.html">extraction-schema-103</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <p><strong>Paper ID:</strong> paper-1266477120913d274346b044b4cc72ea893b1382</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1266477120913d274346b044b4cc72ea893b1382" target="_blank">Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work introduces MemWalker, a method that first processes the long context into a tree of summary nodes, and upon receiving a query, the model navigates this tree in search of relevant information, and responds once it gathers sufficient information.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have advanced in large strides due to the effectiveness of the self-attention mechanism that processes and compares all tokens at once. However, this mechanism comes with a fundamental issue -- the predetermined context window is bound to be limited. Despite attempts to extend the context window through methods like extrapolating the positional embedding, using recurrence, or selectively retrieving essential parts of the long sequence, long-text understanding continues to be a challenge. We propose an alternative approach which instead treats the LLM as an interactive agent, allowing it to decide how to read the text via iterative prompting. We introduce MemWalker, a method that first processes the long context into a tree of summary nodes. Upon receiving a query, the model navigates this tree in search of relevant information, and responds once it gathers sufficient information. On long-text question answering tasks our method outperforms baseline approaches that use long context windows, recurrence, and retrieval. We show that, beyond effective reading, MemWalker enhances explainability by highlighting the reasoning steps as it interactively reads the text; pinpointing the relevant text segments related to the query.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4896.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4896.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MEMWALKER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MEMWALKER (Memory Walker interactive reading agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interactive reading agent that treats an LLM as a navigator over a hierarchical memory tree of textual summaries, iteratively prompting the LLM to traverse, inspect, and optionally accumulate working memory to answer queries on long texts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MEMWALKER</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Two-stage system (memory tree construction + navigation) that uses an underlying instruction-tuned LLM (Stable Beluga 2 in experiments) to (1) recursively summarize fixed-size segments into a tree of summary nodes, and (2) at query time triage child summaries and descend to leaf segments via zero-shot prompts that ask the LLM to produce natural-language reasoning and an action (enter child / revert / commit and answer). Supports revert actions and an explicit working memory that accumulates visited node contents (truncated to fit the LLM context).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Hierarchical textual memory (summary tree) + working memory (scratchpad)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Offline-built tree of summary nodes: leaf nodes are summaries of fixed text segments and internal nodes are recursive compressions of child summaries; at query time the LLM navigates the tree via prompts that present child summaries (triage) or leaf content (leaf prompt). Working memory is the textual accumulation of previously visited node contents appended into the prompt (truncated to fit the context window).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-context question answering</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a long input document x and a query q, generate the correct answer r. Evaluated as multiple-choice or QA by accuracy on long documents (some examples far beyond the model context window).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>QuALITY; SummScreenFD; GovReport (from the SCROLLS suite)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>QuALITY: 67.4% (Orig) / 73.6% (Long subset); SummScreenFD: 67.3% (Orig) / 64.5% (Long); GovReport: 59.4% (Orig) / 60.4% (Long) — accuracy (%) as reported in Table 2 (Stable Beluga 2 underlying LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Reported ablation: removing working memory causes a 5–13 percentage-point drop in accuracy across tasks (exact per-dataset values given in Figure 3); other ablations (e.g., removing reasoning prompts) also reduce performance (see Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>MEMWALKER outperforms recurrence and retrieval baselines and several open long-context models on long-sequence subsets; it also often outperforms full-context truncation baselines on Long subsets. Working memory substantially improves accuracy (5–13% drop when removed). Asking the LLM to produce reasoning before action improves performance for strong LLMs ( Stable Beluga 2), but hurts weaker models. MEMWALKER reads only ~63–69% of the original text on average (59–64% on successful paths), showing efficiency gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Tree construction can become costly for extremely long sequences (scalability); requires a sufficiently strong, instruction-tuned LLM (authors find >70B with instruction tuning necessary) because weaker models make navigation errors that cascade; summarization granularity introduces a fidelity vs. depth trade-off (large summaries can lose detail); current implementation uses zero-shot prompting only (no fine-tuning) and so may be improved further with fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Treating an LLM as an interactive agent that navigates a structured memory (summary tree) and maintains a working memory enables reasoning over inputs longer than the context window and better localizes relevant segments; success depends critically on the LLM's reasoning capability and on retaining visited context (working memory); summarization granularity and node fanout are important trade-offs between fidelity and navigational difficulty; revert actions and reasoning outputs aid recovery from navigation errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4896.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4896.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Recurrence baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrence-based summarization baseline (recurrent summarization carry-forward)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that recurrently carries compressed summaries of previous segments forward so the model processes long inputs beyond the context window by passing summarized state between steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Recurrence baseline (summarization recurrence)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Implemented by the authors as a recurrence baseline: split long text into segments (each 2,500 tokens) and recurrently carry forward a summary (max 500 tokens) from previous segments to the current step, using the same underlying LLM (Stable Beluga 2).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Recurrent compressed summaries (episodic/recurrence memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>At each segment step a compressed summary of prior segments is forwarded to the next segment as context; information is compressed at each recurrence step.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-context question answering</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same QA tasks (QuALITY, SummScreenFD, GovReport) where documents often exceed the LLM context length and the recurrence baseline attempts to preserve earlier content via summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>QuALITY; SummScreenFD; GovReport</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>QuALITY: 51.3% (Orig) / 56.0% (Long); SummScreenFD: 47.7% / 45.4%; GovReport: 35.6% / 33.8% — accuracy (%) as reported in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>MEMWALKER substantially outperforms the recurrence baseline across all tasks and length settings, indicating recurrence's tendency to lose important earlier information through repeated compression.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Each recurrence step compresses information and the training objective does not dictate how best to compress for downstream tasks; leads to weak recall of older segments and information loss over many steps.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Recurrence is limited by compounding compression error over multiple steps; interactive navigation with selective reading (MEMWALKER) is more effective for long coherent texts in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4896.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4896.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval baseline (Contriever)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-based baseline using Contriever dense retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented baseline that embeds text segments and retrieves top segments by similarity to the query, concatenating them into the LLM context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Retrieval baseline (Contriever)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Embeds fixed segments of the long document using Contriever and retrieves the highest-scoring segments for the query; concatenates retrieved segments into the LLM prompt until the context window is filled.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Retrieval-augmented external memory (dense vector store over segments)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Vector-embed segments offline and retrieve by query similarity; retrieved textual segments are supplied to the LLM as context.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-context question answering</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same QA tasks (QuALITY, SummScreenFD, GovReport) where retrieval selects relevant parts instead of presenting whole long document.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>QuALITY; SummScreenFD; GovReport</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>QuALITY: 63.1% (Orig) / 64.8% (Long); SummScreenFD: 63.7% / 62.2%; GovReport: 54.0% / 52.1% — accuracy (%) as reported in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>MEMWALKER outperforms retrieval on these coherent long-text reading tasks; the paper notes retrieval can be less effective within a single coherent long document because retrieval systems are often tuned to distinguish across documents rather than disambiguate segments inside the same document.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Retrieval effectiveness depends on embedding quality and retrieval granularity; may retrieve segments that are less relevant or fail to capture small but critical details inside coherent long texts.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>For long coherent documents, interactive selective reading that reasons about summaries (MEMWALKER) can better localize needed content than off-the-shelf retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4896.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4896.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Full Context (truncation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Full-context baseline with truncation (keep-left / keep-right)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baselines that feed as much of the original document as fits in the LLM context window (4,096 tokens) by truncating either the left (earliest tokens) or right (most recent tokens) side of the document.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Full Context (keep-left / keep-right)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses the model's native context window (4,096 tokens) and truncates the long document to the left or right to fit; no explicit external memory or navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>None (limited to the model's context window)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Relies solely on the contiguous truncated portion of the original text inserted into the model prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-context question answering</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same QA tasks; performance depends on where relevant content lies relative to truncation.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>QuALITY; SummScreenFD; GovReport</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Full Context (keep left): QuALITY 56.7% / 64.8%; SummScreenFD 62.7% / 62.7%; GovReport 59.4% / 56.3%. Full Context (keep right): QuALITY 70.1% / 72.5%; SummScreenFD 64.7% / 63.1%; GovReport 50.5% / 50.0% — accuracy (%) as reported in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>On shorter or positionally biased examples, full-context truncation can perform well (depending whether relevant content lies in the kept side). However, on long examples MEMWALKER outperforms these truncation baselines consistently.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Fixed context window leads to truncation-induced information loss; choice of left vs right truncation is dataset-dependent and can bias results.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>When context exceeds model window, selective reading/navigation (MEMWALKER) can outperform naive truncation, especially for longer inputs where relevant content is not concentrated at one end.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4896.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4896.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Stable Beluga 2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stable Beluga 2 (instruction-tuned LLaMA-2 70B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned 70B LLaMA-2 variant used as the underlying LLM for MEMWALKER and baselines; 4,096-token context length.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Stable Beluga 2</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Instruction-tuned LLaMA-2 (70B) model used zero-shot for both tree construction and navigation; shown to have sufficient reasoning ability for MEMWALKER to be effective.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Underlying LLM for long-context QA experiments</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provides the generative and reasoning capability used to summarize segments, triage child summaries, and generate navigation actions and answers in MEMWALKER.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>QuALITY; SummScreenFD; GovReport</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>When used as the underlying LLM for MEMWALKER, yields the MEMWALKER accuracies reported (e.g., QuALITY 67.4% / 73.6% etc.). Table 3 also shows Stable Beluga 2 MEMWALKER accs: QuALITY 67.4% (Valid Action 92.5%), SummScreenFD 67.3% (95.1%), GovReport 59.4% (97.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>The effectiveness of MEMWALKER depends strongly on the LLM's reasoning capability; Stable Beluga 2 benefits from the reasoning prompt (performance with reasoning > without for this LLM), while weaker LLMs did not benefit or performed worse when asked to output reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Limited to 4,096 context tokens (necessitating the memory-tree approach); requires strong instruction-following and reasoning ability for MEMWALKER to work well.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>A sufficiently capable LLM is required for interactive memory-navigation approaches; asking for intermediate reasoning improves navigation for stronger models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4896.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4896.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memorizing Transformers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memorizing Transformers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model family that augments attention with kNN-like access to external memory so the model can attend to memorized past states or retrieved external memory entries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Memorizing transformers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Memorizing Transformers (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Referenced as a kNN-augmented model that attends to external memory entries rather than processing the entire long sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Retrieval / key-value external memory (kNN attention)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Attends to memorized external entries via a kNN mechanism to bring in information beyond the immediate context (as described in Wu et al., 2022).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-sequence modeling / memory-augmented tasks (discussed in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mentioned in related work as an approach to let transformers reference external memorized content; not evaluated in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Mentioned as related work (kNN external memory approaches) but not experimentally compared in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>The paper positions MEMWALKER as an alternative interactive-memory approach distinct from kNN / memorizing-transformer style memory access.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4896.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4896.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PEARL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PEARL: Prompting large language models to plan and execute actions over long documents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting approach that asks the model to generate pseudo-API calls to focus on relevant parts of long documents within the LLM context window.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pearl: Prompting large language models to plan and execute actions over long documents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PEARL (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Referenced method that prompts an LLM to plan and call pseudo-APIs to focus on parts of a long document, but operates within the model's context window rather than as an external memory-access approach.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Prompted planning with in-context focus (not external memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Generates plans and API-like calls to select or focus on parts of the long document; operates inside the context window rather than an external memory structure.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-document planning and execution</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Discussed in related work as a planning/execution approach for long documents; not directly evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Paper contrasts PEARL's in-context API-style approach with MEMWALKER's memory-access tree that goes beyond the context window.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>PEARL is noted as operating within the context window, whereas MEMWALKER provides a route beyond the context limit via a structured memory.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4896.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4896.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Notes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Notes (learning to reason and memorize with self-notes)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that interleaves generation of self-notes with input data to improve reasoning and memory for downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning to reason and memorize with self-notes</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Self-Notes (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Referenced as an approach that interleaves model-generated notes (scratchpad) with input data to aid multi-step reasoning and memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Scratchpad / self-generated notes (working memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Generates intermediate textual notes that are kept and used to guide future reasoning steps; mentioned as related prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Iterative reasoning and memorization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Related work referenced in context of iterative prompting and internal note-taking; not experimented within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Paper cites Self-Notes as an example of interleaving notes and input to improve reasoning; related to MEMWALKER's working memory concept but different in that MEMWALKER uses a prebuilt summary tree plus working memory.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Self-generated intermediate memory (notes) can improve reasoning — MEMWALKER similarly benefits from carrying working memory during traversal.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4896.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4896.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RecurrentGPT / Re3 / WebGPT / WebShop</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RecurrentGPT; Re3; WebGPT; WebShop (related interactive agent works)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior works that enable iterative prompting, web interaction, or recurrent reprompting for longer generation or information acquisition; cited as related agent-like approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RecurrentGPT / Re3 / WebGPT / WebShop (referenced group)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Group of related agentic approaches: RecurrentGPT (interactive generation of arbitrarily long text), Re3 (recursive reprompting for long generation), WebGPT/WebShop (browser- or web-interacting agents for search and task completion). Cited to place MEMWALKER in context of agent-based approaches to information access.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Iterative prompting / action-based interaction (some use external retrieval or browsing as memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>These methods use series of interactions (scrolling web pages, issuing searches, recursive reprompting) to gather or generate content beyond single-pass context, but differ from MEMWALKER's offline memory-tree + navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Interactive search, long-generation, and web-based question answering (related tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mentioned as prior agentic systems enabling multi-step action-taking (e.g., web browsing) or iterative reprompting; not directly evaluated in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>These works exemplify agentic, action-based approaches; MEMWALKER differs by focusing on structured memory navigation (tree) for coherent long-text reading, rather than browsing or in-context recursion alone.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Agent-style iterative access to information is a promising paradigm for long-input understanding; MEMWALKER contributes a structured-memory traversal variant tailored to long coherent documents.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Memorizing transformers <em>(Rating: 2)</em></li>
                <li>Pearl: Prompting large language models to plan and execute actions over long documents <em>(Rating: 2)</em></li>
                <li>Learning to reason and memorize with self-notes <em>(Rating: 2)</em></li>
                <li>Recurrentgpt: Interactive generation of (arbitrarily) long text <em>(Rating: 2)</em></li>
                <li>Webgpt: Browser-assisted question-answering with human feedback <em>(Rating: 1)</em></li>
                <li>Webshop: Towards scalable real-world web interaction with grounded language agents <em>(Rating: 1)</em></li>
                <li>Re3: Generating longer stories with recursive reprompting and revision <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4896",
    "paper_id": "paper-1266477120913d274346b044b4cc72ea893b1382",
    "extraction_schema_id": "extraction-schema-103",
    "extracted_data": [
        {
            "name_short": "MEMWALKER",
            "name_full": "MEMWALKER (Memory Walker interactive reading agent)",
            "brief_description": "An interactive reading agent that treats an LLM as a navigator over a hierarchical memory tree of textual summaries, iteratively prompting the LLM to traverse, inspect, and optionally accumulate working memory to answer queries on long texts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MEMWALKER",
            "agent_description": "Two-stage system (memory tree construction + navigation) that uses an underlying instruction-tuned LLM (Stable Beluga 2 in experiments) to (1) recursively summarize fixed-size segments into a tree of summary nodes, and (2) at query time triage child summaries and descend to leaf segments via zero-shot prompts that ask the LLM to produce natural-language reasoning and an action (enter child / revert / commit and answer). Supports revert actions and an explicit working memory that accumulates visited node contents (truncated to fit the LLM context).",
            "memory_type": "Hierarchical textual memory (summary tree) + working memory (scratchpad)",
            "memory_description": "Offline-built tree of summary nodes: leaf nodes are summaries of fixed text segments and internal nodes are recursive compressions of child summaries; at query time the LLM navigates the tree via prompts that present child summaries (triage) or leaf content (leaf prompt). Working memory is the textual accumulation of previously visited node contents appended into the prompt (truncated to fit the context window).",
            "task_name": "Long-context question answering",
            "task_description": "Given a long input document x and a query q, generate the correct answer r. Evaluated as multiple-choice or QA by accuracy on long documents (some examples far beyond the model context window).",
            "benchmark_name": "QuALITY; SummScreenFD; GovReport (from the SCROLLS suite)",
            "performance_with_memory": "QuALITY: 67.4% (Orig) / 73.6% (Long subset); SummScreenFD: 67.3% (Orig) / 64.5% (Long); GovReport: 59.4% (Orig) / 60.4% (Long) — accuracy (%) as reported in Table 2 (Stable Beluga 2 underlying LLM).",
            "performance_without_memory": "Reported ablation: removing working memory causes a 5–13 percentage-point drop in accuracy across tasks (exact per-dataset values given in Figure 3); other ablations (e.g., removing reasoning prompts) also reduce performance (see Table 3).",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "MEMWALKER outperforms recurrence and retrieval baselines and several open long-context models on long-sequence subsets; it also often outperforms full-context truncation baselines on Long subsets. Working memory substantially improves accuracy (5–13% drop when removed). Asking the LLM to produce reasoning before action improves performance for strong LLMs ( Stable Beluga 2), but hurts weaker models. MEMWALKER reads only ~63–69% of the original text on average (59–64% on successful paths), showing efficiency gains.",
            "limitations_or_challenges": "Tree construction can become costly for extremely long sequences (scalability); requires a sufficiently strong, instruction-tuned LLM (authors find &gt;70B with instruction tuning necessary) because weaker models make navigation errors that cascade; summarization granularity introduces a fidelity vs. depth trade-off (large summaries can lose detail); current implementation uses zero-shot prompting only (no fine-tuning) and so may be improved further with fine-tuning.",
            "key_insights": "Treating an LLM as an interactive agent that navigates a structured memory (summary tree) and maintains a working memory enables reasoning over inputs longer than the context window and better localizes relevant segments; success depends critically on the LLM's reasoning capability and on retaining visited context (working memory); summarization granularity and node fanout are important trade-offs between fidelity and navigational difficulty; revert actions and reasoning outputs aid recovery from navigation errors.",
            "uuid": "e4896.0",
            "source_info": {
                "paper_title": "Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Recurrence baseline",
            "name_full": "Recurrence-based summarization baseline (recurrent summarization carry-forward)",
            "brief_description": "A baseline that recurrently carries compressed summaries of previous segments forward so the model processes long inputs beyond the context window by passing summarized state between steps.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Recurrence baseline (summarization recurrence)",
            "agent_description": "Implemented by the authors as a recurrence baseline: split long text into segments (each 2,500 tokens) and recurrently carry forward a summary (max 500 tokens) from previous segments to the current step, using the same underlying LLM (Stable Beluga 2).",
            "memory_type": "Recurrent compressed summaries (episodic/recurrence memory)",
            "memory_description": "At each segment step a compressed summary of prior segments is forwarded to the next segment as context; information is compressed at each recurrence step.",
            "task_name": "Long-context question answering",
            "task_description": "Same QA tasks (QuALITY, SummScreenFD, GovReport) where documents often exceed the LLM context length and the recurrence baseline attempts to preserve earlier content via summaries.",
            "benchmark_name": "QuALITY; SummScreenFD; GovReport",
            "performance_with_memory": "QuALITY: 51.3% (Orig) / 56.0% (Long); SummScreenFD: 47.7% / 45.4%; GovReport: 35.6% / 33.8% — accuracy (%) as reported in Table 2.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "MEMWALKER substantially outperforms the recurrence baseline across all tasks and length settings, indicating recurrence's tendency to lose important earlier information through repeated compression.",
            "limitations_or_challenges": "Each recurrence step compresses information and the training objective does not dictate how best to compress for downstream tasks; leads to weak recall of older segments and information loss over many steps.",
            "key_insights": "Recurrence is limited by compounding compression error over multiple steps; interactive navigation with selective reading (MEMWALKER) is more effective for long coherent texts in these experiments.",
            "uuid": "e4896.1",
            "source_info": {
                "paper_title": "Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Retrieval baseline (Contriever)",
            "name_full": "Retrieval-based baseline using Contriever dense retrieval",
            "brief_description": "A retrieval-augmented baseline that embeds text segments and retrieves top segments by similarity to the query, concatenating them into the LLM context.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Retrieval baseline (Contriever)",
            "agent_description": "Embeds fixed segments of the long document using Contriever and retrieves the highest-scoring segments for the query; concatenates retrieved segments into the LLM prompt until the context window is filled.",
            "memory_type": "Retrieval-augmented external memory (dense vector store over segments)",
            "memory_description": "Vector-embed segments offline and retrieve by query similarity; retrieved textual segments are supplied to the LLM as context.",
            "task_name": "Long-context question answering",
            "task_description": "Same QA tasks (QuALITY, SummScreenFD, GovReport) where retrieval selects relevant parts instead of presenting whole long document.",
            "benchmark_name": "QuALITY; SummScreenFD; GovReport",
            "performance_with_memory": "QuALITY: 63.1% (Orig) / 64.8% (Long); SummScreenFD: 63.7% / 62.2%; GovReport: 54.0% / 52.1% — accuracy (%) as reported in Table 2.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "MEMWALKER outperforms retrieval on these coherent long-text reading tasks; the paper notes retrieval can be less effective within a single coherent long document because retrieval systems are often tuned to distinguish across documents rather than disambiguate segments inside the same document.",
            "limitations_or_challenges": "Retrieval effectiveness depends on embedding quality and retrieval granularity; may retrieve segments that are less relevant or fail to capture small but critical details inside coherent long texts.",
            "key_insights": "For long coherent documents, interactive selective reading that reasons about summaries (MEMWALKER) can better localize needed content than off-the-shelf retrieval.",
            "uuid": "e4896.2",
            "source_info": {
                "paper_title": "Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Full Context (truncation)",
            "name_full": "Full-context baseline with truncation (keep-left / keep-right)",
            "brief_description": "Baselines that feed as much of the original document as fits in the LLM context window (4,096 tokens) by truncating either the left (earliest tokens) or right (most recent tokens) side of the document.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Full Context (keep-left / keep-right)",
            "agent_description": "Uses the model's native context window (4,096 tokens) and truncates the long document to the left or right to fit; no explicit external memory or navigation.",
            "memory_type": "None (limited to the model's context window)",
            "memory_description": "Relies solely on the contiguous truncated portion of the original text inserted into the model prompt.",
            "task_name": "Long-context question answering",
            "task_description": "Same QA tasks; performance depends on where relevant content lies relative to truncation.",
            "benchmark_name": "QuALITY; SummScreenFD; GovReport",
            "performance_with_memory": "Full Context (keep left): QuALITY 56.7% / 64.8%; SummScreenFD 62.7% / 62.7%; GovReport 59.4% / 56.3%. Full Context (keep right): QuALITY 70.1% / 72.5%; SummScreenFD 64.7% / 63.1%; GovReport 50.5% / 50.0% — accuracy (%) as reported in Table 2.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "On shorter or positionally biased examples, full-context truncation can perform well (depending whether relevant content lies in the kept side). However, on long examples MEMWALKER outperforms these truncation baselines consistently.",
            "limitations_or_challenges": "Fixed context window leads to truncation-induced information loss; choice of left vs right truncation is dataset-dependent and can bias results.",
            "key_insights": "When context exceeds model window, selective reading/navigation (MEMWALKER) can outperform naive truncation, especially for longer inputs where relevant content is not concentrated at one end.",
            "uuid": "e4896.3",
            "source_info": {
                "paper_title": "Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Stable Beluga 2",
            "name_full": "Stable Beluga 2 (instruction-tuned LLaMA-2 70B)",
            "brief_description": "An instruction-tuned 70B LLaMA-2 variant used as the underlying LLM for MEMWALKER and baselines; 4,096-token context length.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Stable Beluga 2",
            "agent_description": "Instruction-tuned LLaMA-2 (70B) model used zero-shot for both tree construction and navigation; shown to have sufficient reasoning ability for MEMWALKER to be effective.",
            "memory_type": null,
            "memory_description": null,
            "task_name": "Underlying LLM for long-context QA experiments",
            "task_description": "Provides the generative and reasoning capability used to summarize segments, triage child summaries, and generate navigation actions and answers in MEMWALKER.",
            "benchmark_name": "QuALITY; SummScreenFD; GovReport",
            "performance_with_memory": "When used as the underlying LLM for MEMWALKER, yields the MEMWALKER accuracies reported (e.g., QuALITY 67.4% / 73.6% etc.). Table 3 also shows Stable Beluga 2 MEMWALKER accs: QuALITY 67.4% (Valid Action 92.5%), SummScreenFD 67.3% (95.1%), GovReport 59.4% (97.0%).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "The effectiveness of MEMWALKER depends strongly on the LLM's reasoning capability; Stable Beluga 2 benefits from the reasoning prompt (performance with reasoning &gt; without for this LLM), while weaker LLMs did not benefit or performed worse when asked to output reasoning.",
            "limitations_or_challenges": "Limited to 4,096 context tokens (necessitating the memory-tree approach); requires strong instruction-following and reasoning ability for MEMWALKER to work well.",
            "key_insights": "A sufficiently capable LLM is required for interactive memory-navigation approaches; asking for intermediate reasoning improves navigation for stronger models.",
            "uuid": "e4896.4",
            "source_info": {
                "paper_title": "Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Memorizing Transformers",
            "name_full": "Memorizing Transformers",
            "brief_description": "A model family that augments attention with kNN-like access to external memory so the model can attend to memorized past states or retrieved external memory entries.",
            "citation_title": "Memorizing transformers",
            "mention_or_use": "mention",
            "agent_name": "Memorizing Transformers (referenced)",
            "agent_description": "Referenced as a kNN-augmented model that attends to external memory entries rather than processing the entire long sequence.",
            "memory_type": "Retrieval / key-value external memory (kNN attention)",
            "memory_description": "Attends to memorized external entries via a kNN mechanism to bring in information beyond the immediate context (as described in Wu et al., 2022).",
            "task_name": "Long-sequence modeling / memory-augmented tasks (discussed in related work)",
            "task_description": "Mentioned in related work as an approach to let transformers reference external memorized content; not evaluated in this paper's experiments.",
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "memory_comparison_summary": "Mentioned as related work (kNN external memory approaches) but not experimentally compared in this paper.",
            "limitations_or_challenges": null,
            "key_insights": "The paper positions MEMWALKER as an alternative interactive-memory approach distinct from kNN / memorizing-transformer style memory access.",
            "uuid": "e4896.5",
            "source_info": {
                "paper_title": "Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "PEARL",
            "name_full": "PEARL: Prompting large language models to plan and execute actions over long documents",
            "brief_description": "A prompting approach that asks the model to generate pseudo-API calls to focus on relevant parts of long documents within the LLM context window.",
            "citation_title": "Pearl: Prompting large language models to plan and execute actions over long documents",
            "mention_or_use": "mention",
            "agent_name": "PEARL (referenced)",
            "agent_description": "Referenced method that prompts an LLM to plan and call pseudo-APIs to focus on parts of a long document, but operates within the model's context window rather than as an external memory-access approach.",
            "memory_type": "Prompted planning with in-context focus (not external memory)",
            "memory_description": "Generates plans and API-like calls to select or focus on parts of the long document; operates inside the context window rather than an external memory structure.",
            "task_name": "Long-document planning and execution",
            "task_description": "Discussed in related work as a planning/execution approach for long documents; not directly evaluated here.",
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "memory_comparison_summary": "Paper contrasts PEARL's in-context API-style approach with MEMWALKER's memory-access tree that goes beyond the context window.",
            "limitations_or_challenges": null,
            "key_insights": "PEARL is noted as operating within the context window, whereas MEMWALKER provides a route beyond the context limit via a structured memory.",
            "uuid": "e4896.6",
            "source_info": {
                "paper_title": "Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Self-Notes",
            "name_full": "Self-Notes (learning to reason and memorize with self-notes)",
            "brief_description": "A method that interleaves generation of self-notes with input data to improve reasoning and memory for downstream tasks.",
            "citation_title": "Learning to reason and memorize with self-notes",
            "mention_or_use": "mention",
            "agent_name": "Self-Notes (referenced)",
            "agent_description": "Referenced as an approach that interleaves model-generated notes (scratchpad) with input data to aid multi-step reasoning and memorization.",
            "memory_type": "Scratchpad / self-generated notes (working memory)",
            "memory_description": "Generates intermediate textual notes that are kept and used to guide future reasoning steps; mentioned as related prior work.",
            "task_name": "Iterative reasoning and memorization",
            "task_description": "Related work referenced in context of iterative prompting and internal note-taking; not experimented within this paper.",
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "memory_comparison_summary": "Paper cites Self-Notes as an example of interleaving notes and input to improve reasoning; related to MEMWALKER's working memory concept but different in that MEMWALKER uses a prebuilt summary tree plus working memory.",
            "limitations_or_challenges": null,
            "key_insights": "Self-generated intermediate memory (notes) can improve reasoning — MEMWALKER similarly benefits from carrying working memory during traversal.",
            "uuid": "e4896.7",
            "source_info": {
                "paper_title": "Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "RecurrentGPT / Re3 / WebGPT / WebShop",
            "name_full": "RecurrentGPT; Re3; WebGPT; WebShop (related interactive agent works)",
            "brief_description": "Prior works that enable iterative prompting, web interaction, or recurrent reprompting for longer generation or information acquisition; cited as related agent-like approaches.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "RecurrentGPT / Re3 / WebGPT / WebShop (referenced group)",
            "agent_description": "Group of related agentic approaches: RecurrentGPT (interactive generation of arbitrarily long text), Re3 (recursive reprompting for long generation), WebGPT/WebShop (browser- or web-interacting agents for search and task completion). Cited to place MEMWALKER in context of agent-based approaches to information access.",
            "memory_type": "Iterative prompting / action-based interaction (some use external retrieval or browsing as memory)",
            "memory_description": "These methods use series of interactions (scrolling web pages, issuing searches, recursive reprompting) to gather or generate content beyond single-pass context, but differ from MEMWALKER's offline memory-tree + navigation.",
            "task_name": "Interactive search, long-generation, and web-based question answering (related tasks)",
            "task_description": "Mentioned as prior agentic systems enabling multi-step action-taking (e.g., web browsing) or iterative reprompting; not directly evaluated in this paper's experiments.",
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "memory_comparison_summary": "These works exemplify agentic, action-based approaches; MEMWALKER differs by focusing on structured memory navigation (tree) for coherent long-text reading, rather than browsing or in-context recursion alone.",
            "limitations_or_challenges": null,
            "key_insights": "Agent-style iterative access to information is a promising paradigm for long-input understanding; MEMWALKER contributes a structured-memory traversal variant tailored to long coherent documents.",
            "uuid": "e4896.8",
            "source_info": {
                "paper_title": "Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Memorizing transformers",
            "rating": 2
        },
        {
            "paper_title": "Pearl: Prompting large language models to plan and execute actions over long documents",
            "rating": 2
        },
        {
            "paper_title": "Learning to reason and memorize with self-notes",
            "rating": 2
        },
        {
            "paper_title": "Recurrentgpt: Interactive generation of (arbitrarily) long text",
            "rating": 2
        },
        {
            "paper_title": "Webgpt: Browser-assisted question-answering with human feedback",
            "rating": 1
        },
        {
            "paper_title": "Webshop: Towards scalable real-world web interaction with grounded language agents",
            "rating": 1
        },
        {
            "paper_title": "Re3: Generating longer stories with recursive reprompting and revision",
            "rating": 1
        }
    ],
    "cost": 0.018736,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Walking Down the Memory Maze: Beyond CONTEXT LIMIT THROUGH INTERACTIVE READING</h1>
<p>Howard Chen*<br>Princeton University</p>
<p>Ramakanth Pasunuru Meta AI</p>
<p>Jason Weston Meta AI</p>
<p>Asli Celikyilmaz Meta AI</p>
<h2>AbSTRACT</h2>
<h4>Abstract</h4>
<p>Large language models (LLMs) have advanced in large strides due to the effectiveness of the self-attention mechanism that processes and compares all tokens at once. However, this mechanism comes with a fundamental issue - the predetermined context window is bound to be limited. Despite attempts to extend the context window through methods like extrapolating the positional embedding, using recurrence, or selectively retrieving essential parts of the long sequence, long-text understanding continues to be a challenge. We propose an alternative approach which instead treats the LLM as an interactive agent, allowing it to decide how to read the text via iterative prompting. We introduce MEMWALKER, a method that first processes the long context into a tree of summary nodes. Upon receiving a query, the model navigates this tree in search of relevant information, and responds once it gathers sufficient information. On long-text question answering tasks our method outperforms baseline approaches that use long context windows, recurrence, and retrieval. We show that, beyond effective reading, MEMWALKER enhances explainability by highlighting the reasoning steps as it interactively reads the text; pinpointing the relevant text segments related to the query.</p>
<h2>1 INTRODUCTION</h2>
<p>Large language models (LLMs) have witnessed significant advancements due to the increased model size, expanded pretraining data, and the adoption of the Transformer architecture with self-attention (Vaswani et al., 2017). As LLMs evolve in capability, users increasingly seek to use longer input sequences during inference. This results in a growing demand in querying for information in long documents, analyzing legal or scientific papers, and managing extended conversational dialogues. These tasks involve consuming large amount of information, highlighting the importance of longer context processing.</p>
<p>Despite the rapid development, the limitation of the self-attention mechanism becomes apparent as its memory usage increases with longer sequences, consequently limiting the size of the context window. To address this, different approaches have been employed, such as designing lighter and more efficient attention schemes (Zaheer et al., 2020), finetuning with extrapolated or interpolated positional embeddings (Press et al., 2022; Chen et al., 2023), incorporating recurrence to bring forward information from preceding text segments into the next (Rae et al., 2019; Fan et al., 2020; Xu et al., 2022), or retrieving relevant parts of the text (Lewis et al., 2020; Izacard \&amp; Grave, 2020). However, these approaches are still limited by design. The context window, no matter how long it is extended, assumes a fixed size, and not all positions within it hold equivalent significance (Liu et al., 2023). While recurrence can manage infinite-length sequences, it often misses out on retaining information from earlier segments. Additionally, retrieving segments from the coherent long-text might be ineffective, given that many retrieval systems are tailored to distinguish similar but distinct documents (Chen et al., 2017).</p>
<p>To address these issues, we develop a fundamentally different approach which treats the model with a finite context window as an interactive agent, rather than simply processing the entire sequence in one go. To this end, we introduce MEMWALKER, a method that enables the model to read the long-text interactively via iterative LLM prompting. MEMWALKER operates through a two-stage</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>Stage 1: Memory Tree Construction</h1>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The two-stage procedure of MEMWALKER. Top (stage 1): the memory tree is constructed. The long text is split into segments of a predetermined size and each segment is first summarized into a summary node. The summary nodes are recursively summarized into higher level nodes until it reaches the root. Bottom (stage 2): Given a query, the LLM navigates the tree structure via iterative prompting and finds the node that contains relevant segment to form the answer. At each node, the LLM decides the action by first reasoning about the child summary nodes by sampling from the distribution $\operatorname{LLM}($ reasoning, action $\mid$ summ, query). The LLM can choose the revert action to return to the parent node if it chose the wrong path or the segment at hand is irrelevant (dashed red arrow). See Table 1 for a detailed example showing the LLM prompts that enable navigation.
approach: 1) memory tree construction and 2) navigation. During the first stage, the long-text is segmented into small chunks that fit within the LLM's context window. The LLM then subsequently summarizes each segment into a textual summary node. These summary nodes are progressively further summarized into higher-level summary nodes, thus building a tree structure (Figure 1). To answer a user query, the LLM begins navigation from the tree's root node. It traverses the tree, inspecting various parts of the text to identify the path and segment relevant to answer the query. As a result, MEMWALKER can go beyond the context limit, efficiently processing texts and localizing the important segments of the long-text, without additional finetuning.</p>
<p>We evaluate MEMWALKER on three long context question answering tasks and show superior performance against recurrence, retrieval, and vanilla LLM baselines. MEMWALKER also outperforms other open long context systems that can take 8,000 to 16,000 tokens. We provide an analysis of the effectiveness of MEMWALKER, and show it can reason about navigation decisions, incorporate working memory during traversal, and recover from errors made in early navigational steps.</p>
<h2>2 Related Work</h2>
<p>Context window scaling. A straightforward approach to enable a longer context sequence is to tune the pre-trained language models and extrapolate their positional embeddings on longer text sequences (Press et al., 2022; Chen et al., 2023). Another direction is modified self-attention (Beltagy et al., 2020; Zaheer et al., 2020; Guo et al., 2022; Ainslie et al., 2023). This approach has advanced in large strides thanks to training techniques such as Flash Attention (Dao et al., 2022) that greatly</p>
<p>reduce the memory footprint. Despite the recent advances, this approach comes with two natural limitations: 1) to enable models on longer sequences, the model needs to be fine-tuned, incurring a non-negligible cost and 2) the attention mechanism may become less effective due to positional biases as the sequence length becomes very long (Liu et al., 2023).</p>
<p>Recurrence. Recurrent architectures have been extensively studied to tackle long sequence problems, from recurrent neural network based models Hochreiter \&amp; Schmidhuber (1997); Miller et al. (2016) to the modern Transformer based models (Dai et al., 2019; Rae et al., 2019; Fan et al., 2020; Xu et al., 2022; Bulatov et al., 2023; Chevalier et al., 2023). However, each recurrence step incurs information loss and the training objective does not guide "how to compress" with regard to downstream tasks. Typically this compression means that recall of older sequence information is weaker compared to recent information.</p>
<p>Retrieval. Retrieval systems are commonly used to select relevant documents from a large pool of documents, and have been incorporated into neural models in various ways (Chen et al., 2017; Dinan et al., 2018; Lewis et al., 2020). For long sequence reading, retrieval based methods typically first embed the text segments into vector representations and retrieve them based on the query instead of feeding the entire sequence into the model such as in Fusion-in-Decoder Izacard \&amp; Grave (2020) or kNN variants that attend to external memory such as Memorizing Transformers (Wu et al., 2022).</p>
<p>Reasoning agents. Instead of taking the long text as a single monolithic input, a model can act as an agent that reads part of the text and takes flexible actions. Work such as WebGPT (Nakano et al., 2021) and WebShop (Yao et al., 2022) allow the model to scroll through the internet and search for the requested answer or item. While their atomic actions allow for interactive search for relevant content, the models were not designed for understanding long and coherent texts. On the other hand, PEARL (Sun et al., 2023) prompts the model to generate pseudo APIs for the model to call in order to focus on the right parts of the long text. However, the method operates within the LLM's context window, rather than being a memory-access approach that goes beyond the context limit. Other works leveraged iterative prompting to reason and plan for long text generation tasks such as Re3 (Yang et al., 2022) and RecurrentGPT (Zhou et al., 2023). Self-Notes (Lanchantin et al., 2023) interleaved self-generating notes and the input data to perform better reasoning. Prior to current LLMs, LSTMs were also applied to searching through document structures (titles, subsections) Geva \&amp; Berant (2018). Recursive tree structure has also been explored in the context of summarization of long text such as books in (Wu et al., 2021), but was not used for memory navigation in that work.</p>
<h1>3 MEMWALKER: AN INTERACTIVE READER</h1>
<p>We study tasks related to long-context question answering - given a long-text $x$ and a query $q$, the model aims to generate the response $r$.</p>
<p>MEMWALKER follows two steps: 1) memory tree construction, where the long-context is broken down into a tree data structure. This construction does not depend on the query, and can hence be computed in advance if the sequence data is available beforehand. 2) navigation, in which the model navigates this structure upon receiving a query, gathering information to craft a suitable response. MEMWALKER assumes access to an underlying LLM, and both construction and navigation are achieved through iterative LLM prompting.</p>
<p>Memory tree construction. MEMWALKER first creates a tree data structure, $\mathcal{T}(x)$, from the long-text $x$. Each node is represented by text that encapsulates the summaries of all its child nodes below it. Specifically, the long-text $x$ is divided into segments $\left(c_{1}, \ldots, c_{n}\right)$. The LLM then summarizes each segment into a summary at the first level, represented as $s_{i}^{l+1}=\operatorname{LLM}\left(c_{i}:\right)$, $i=1 . . n$. The initial summary nodes are subsequently summarized further into higher level nodes, $s_{j}^{l+1}=\operatorname{LLM}\left(s_{i}^{l}, \ldots, s_{i+M_{t}}^{l}\right)$ where $M_{t}$ denotes the number of nodes in the $t$-th grouping at level $l$. This process continues until the topmost root node, $s^{L}$ is generated. The complete tree generation process is illustrated in Figure 1. Summarization is performed using LLM prompting. We include the prompts for memory tree construction in Appendix A.1.</p>
<p>Table 1: Example trajectory from the QuALITY dataset. The LLM first sees the content of the children nodes at the root node (summ 9 in Figure 1) and generates the response (takes action 0 to enter summ 7). When arriving at the leaf node (summ 2), the LLM determines that there is not enough information, therefore takes the action to revert (action -1) to the parent node. After hopping back-and-forth between nodes, the LLM commits to a Leaf node (summ 3) and answers the question. Yellow indicates triage prompt and purple indicates leaf prompt described in $\S 3$. Text after // denotes comments that are not processed by the LLM.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Navigation Trajectory</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">summ 9</td>
<td style="text-align: center;">The following passage(s) are the summaries of the different parts of a story. <br> To answer the question: Why did Ro change his mind about the people on Mars being backwards? <br> Which of the following summary is MOST LIKELY to contain information about the answer? <br> First provide reasoning to compare the summaries before you make the decision. <br> Summary 0: The story is set on Mars and follows the adventures of Ro, [...] // summ 7 <br> Summary 1: Ro, a young Martian, is climbing down a cliff to rescue [...] // summ 8 <br> Relpy with the passage number as your action. <br> You MUST choose one summary number and you should reply with the following format: <br> ####################################################################################################\</td>
</tr>
</tbody>
</table>
<ul>
<li>choosing one of the child nodes to further inspect, or to revert to the parent node. At leaf node $s_{i}^{l=1}$, the LLM can decide one of two actions: commit to the leaf node and respond to the query or revert to the parent node $\left(s_{j}^{l+1}\right)$ if the information in the leaf node (i.e., $c_{i}$ ) is insufficient. To make a navigation decision, we can also ask the LLM (via prompting) to first generate a reason in natural language to justify the action, followed by the action choice itself. Specifically, at each node, the model generates a response $r \sim \operatorname{LLM}(r \mid s, q)$ where the response is either of the two tuples: 1) $r=$ (reasoning, action, answer) when the LLM is at a leaf node or 2) $r=$ (reasoning, action) when the LLM is at non-leaf nodes.</li>
</ul>
<p>Navigational prompt design. We enable LLM navigation through zero-shot prompting. Our method requires two types of prompt: 1) triage prompt and 2) leaf prompt (highlighted in Table 1). Triage prompt contains the the query, the summaries of the children nodes, and instructions for the LLM to follow. Triage prompt is used at non-leaf nodes. Leaf prompt contains the content of the segment, the query (and options), and instructions that ask the LLM to either generate the answer or revert to the parent node. Both the triage prompt and leaf prompt specify an output format that the LLM needs to follow. Failure to conform to the format results in invalid actions and the LLM is required to regenerate. If the LLM fails to generate parsable output three consecutive times, the navigation terminates and returns "no answer".</p>
<p>Working memory. As the LLM traverses the tree, it can keep information throughout the navigation trajectory and add it to the context. Formally, the LLM generates the response $r \sim \operatorname{LLM}(r \mid$ $s, q, m)$ where the extra working memory $m \in{\varnothing} \cup\left{\left(s_{i}, s_{i+1}, \ldots\right)\right}$ is either empty or consists of contents from previously visited nodes. We truncate the working memory such that they can fit in the LLM's context window.* Table 1 illustrates the way working memory is added via [WORKING_MEMORY] in the prompt.</p>
<h1>4 EXPERIMENTAL SETUP</h1>
<h3>4.1 Datasets \&amp; Evaluation</h3>
<p>We use three datasets: QuALITY, SummScreenFD, and GovReport from the SCROLLS benchmark (Shaham et al., 2022). We report accuracy for all datasets.</p>
<p>QuALITY. QuALITY is a multiple choice question answering dataset collected by Pang et al. (2022). The dataset contains long-form stories sourced from Project Gutenberg and questions annotated by human annotators. We use a subset of 187 examples for our experiments.</p>
<p>SummScreenFD. SummScreenFD (Chen et al., 2022) is a dataset of TV and movie scripts in the form of dialogues among actors originally designed for summarization. We repurpose the dataset into a question answering task where the original provided ground truth summary text is used to generate a "who" question using Stable Beluga 2, with answers then checked by a human expert. The question paired with the original long text becomes the repurposed QA task of 306 examples.</p>
<p>GovReport. The GovReport dataset aggregates documents from Congressional Research Service and the U.S. Government Accountability Office together with summaries provided by experts (Huang et al., 2021). We repurpose the dataset into a question answering dataset of 101 examples the same way as for SummScreenFD.</p>
<p>All three datasets feature long contexts per example of varying length - some shorter examples, and some longer sequences. We therefore both report results on the original dataset, and also report on a subset of each task containing only longer sequences, to better evaluate memory access in the harder, longer context case. The thresholds are above 8, 000 tokens for QuALITY, 6, 000 tokens for SummScreenFD, and 12, 000 tokens for GovReport.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2: Results on the three question answering tasks, reporting test accuracy. Orig. denotes using the entire dataset and Long denotes the subset of longer sequences. Top: comparison to open long context models. Bottom: baselines and MEMWALKER performance, with all methods using the underlying Stable Beluga 2 LLM with a maximum 4,096 -token context length. MEMWALKER outperforms all other systems on longer sequences.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">QuALITY <br> Orig. / Long</th>
<th style="text-align: center;">SummScreenFD <br> Orig. / Long</th>
<th style="text-align: center;">GovReport <br> Orig. / Long</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MPT 13B (8k)</td>
<td style="text-align: center;">$44.4 / 47.3$</td>
<td style="text-align: center;">$65.0 / 63.5$</td>
<td style="text-align: center;">$44.6 / 43.8$</td>
</tr>
<tr>
<td style="text-align: left;">LongChat 13B (16k)</td>
<td style="text-align: center;">$43.3 / 48.4$</td>
<td style="text-align: center;">$62.4 / 61.1$</td>
<td style="text-align: center;">$54.5 / 52.1$</td>
</tr>
<tr>
<td style="text-align: left;">Recurrence</td>
<td style="text-align: center;">$51.3 / 56.0$</td>
<td style="text-align: center;">$47.7 / 45.4$</td>
<td style="text-align: center;">$35.6 / 33.8$</td>
</tr>
<tr>
<td style="text-align: left;">Retrieval</td>
<td style="text-align: center;">$63.1 / 64.8$</td>
<td style="text-align: center;">$63.7 / 62.2$</td>
<td style="text-align: center;">$54.0 / 52.1$</td>
</tr>
<tr>
<td style="text-align: left;">Full Context (keep left)</td>
<td style="text-align: center;">$56.7 / 64.8$</td>
<td style="text-align: center;">$62.7 / 62.7$</td>
<td style="text-align: center;">$\mathbf{5 9 . 4 / 5 6 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">Full Context (keep right)</td>
<td style="text-align: center;">$\mathbf{7 0 . 1 / 7 2 . 5}$</td>
<td style="text-align: center;">$64.7 / 63.1$</td>
<td style="text-align: center;">$50.5 / 50.0$</td>
</tr>
<tr>
<td style="text-align: left;">MEMWALKER</td>
<td style="text-align: center;">$67.4 / \mathbf{7 3 . 6}$</td>
<td style="text-align: center;">$\mathbf{6 7 . 3 / 6 4 . 5}$</td>
<td style="text-align: center;">$\mathbf{5 9 . 4 / 6 0 . 4}$</td>
</tr>
</tbody>
</table>
<h1>4.2 MODEL</h1>
<p>We use Stable Beluga 2 (Mahan et al.) as the base LLM for the majority of our experiments, as it provides state-of-the-art performance compared to several other LLM variants, as we will show. Stable Beluga 2 is an instruction-tuned model built on top of 70B LLaMA-2(Touvron et al., 2023), where the finetuning does not overlap with our evaluation tasks. It has a maximum 4,096 token context length. We use the model in a zero-shot prompting fashion without further fine-tuning or incontext few shot examples for our tasks. We use top- $p$ sampling for both memory tree construction as well as generating action and reasoning for navigation. We set the maximum number of nodes $\max <em t="t">{t} M</em>=8,5,8$ and segment size $|c|=1000,1000,1200$ for QuALITY, SummScreenFD, and GovReport respectively.</p>
<h3>4.3 BASELINES</h3>
<p>We compare with three baselines memory techniques all based on the same underlying LLM, Stable Beluga 2: 1) full context window, 2) recurrence, and 3) retrieval. The full context window baselines utilize the full 4,096 tokens to process both the long input text and generation. Since the instances in the dataset often exceed the context limit, we perform truncation of the length to the right (most recent) or left (least recent) of the text as the input, as evaluate both approaches. For retrieval, we use Contriever (Izacard et al., 2022) to select segments from the long context based on the query. The highest scored segments are concatenated as the input context to the LLM until they fill the context. Finally, we implement a baseline that recurrently carries information from previous segment tokens to the current one through summarization (Xu et al., 2022), where each segment is 2,500 tokens and the maximum summary size is 500 tokens.</p>
<h2>5 ReSULTS \&amp; ANALYSIS</h2>
<p>Main results. Table 2 shows comparisons between MEMWALKER and other baselines. MEMWALKER outperforms both the recurrence baseline across all tasks by a large margin. This shows the limitation of recurrence, where relevant information to the query is lost after several steps. MEMWALKER also outperforms retrieval where the segments are from a coherent long story instead of separate documents. On these tasks, the full context baselines can perform well in the "Original" task setting, which can contain relatively shorter sequences, although choosing either left or right truncate for best performance seems to be dataset dependent. Still, MEMWALKER achieves higher performance in the Original setting against the Full Context baselines except for the keep right variant on QuALITY and the keep left variant on GovReport, likely due to the positional bias in the dataset where relevant segment often appears at the beginning or the end of the text. However, on the Long version of all three tasks MEMWALKER outperforms all baselines, that is it shows strong performance when memory access becomes more critical. MEMWALKER also outperforms other publicly available models, including LongChat (Li et al., 2023) and MPT (MosaicML, 2023).</p>
<p>Table 3: MemWALKER performance using different underlying LLMs with different reasoning capabilities, and an ablation on their reason justification component when making a navigation decision ("w/o reasoning" simply predicts the action, with no reason generated, see e.g. Table 1). Valid Action shows the percent of generated actions that are a valid navigation action. We find that the strongest performing LLM (Stable Beluga 2) benefits from reasoning with improved accuracy, while weaker performing LLMs do not (get worse in terms of accuracy and valid actions).</p>
<table>
<thead>
<tr>
<th></th>
<th>QuALITY</th>
<th>SummScreenFD</th>
<th>GovReport</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Acc. / Valid Action (\%)</td>
<td>Acc. / Valid Action (\%)</td>
<td>Acc. / Valid Action (\%)</td>
<td></td>
</tr>
<tr>
<td>LLaMA 2 Chat (13B)</td>
<td>$39.6 / 73.2$</td>
<td>$20.9 / 75.5$</td>
<td>$15.8 / 69.0$</td>
<td></td>
</tr>
<tr>
<td>w/o reasoning</td>
<td>$48.1 / 97.4$</td>
<td>$25.8 / 95.8$</td>
<td>$21.8 / 93.1$</td>
<td></td>
</tr>
<tr>
<td>LLaMA 2 Chat (70B)</td>
<td>$52.0 / 86.1$</td>
<td>$55.6 / 99.5$</td>
<td>$41.6 / 97.8$</td>
<td></td>
</tr>
<tr>
<td>w/o reasoning</td>
<td>$59.9 / 100.0$</td>
<td>$58.5 / 100.0$</td>
<td>$42.6 / 100.0$</td>
<td></td>
</tr>
<tr>
<td>Stable Beluga 2 (70B)</td>
<td>$67.4 / 92.5$</td>
<td>$67.3 / 95.1$</td>
<td>$59.4 / 97.0$</td>
<td></td>
</tr>
<tr>
<td>w/o reasoning</td>
<td>$66.8 / 100.0$</td>
<td>$64.1 / 90.5$</td>
<td>$52.5 / 98.2$</td>
<td></td>
</tr>
</tbody>
</table>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Performance breakdown by context length (in tokens). Each dataset is thresholded into two bucket of equal sizes. MEMWALKER outperforms full context baselines (truncated either left or right, when the sequence does not fit) on longer context sequences, for all three tasks.</p>
<p>MEMWALKER improves performance on long sequences. We provide a breakdown of performance by input sequence length for each task in Figure 2. MEMWALKER is not advantageous over Full Context (with truncation left or right) baselines when the text length is short, but outperforms both types of truncation for all tasks for longer sequences. The benefit of interactive reading emerges after the text length is suitably large, i.e. showing better performance once the sequence length is sufficiently larger than the LLM context length of 4,096 .</p>
<p>Reasoning capability is essential for memory tree navigation. The effectiveness of MEMWALKER is highly dependent on the underlying LLM's reasoning capability. For each navigation decision, we employ an LLM prompt that requires the LLM to first generate a reason in natural language that justifies the following predicted action, see Table 1. We show in Table 3 how reasoning impacts performance by comparing Llama 2 Chat (13B and 70B parameter variants) and Stable Beluga 2 (70B) with and without the reasoning justification by removing the line "First provide reasoning ...before you make your decision" from the prompt. With the smaller, less capable models (13B), the performance lags behind 70B models by a large margin due to its inability to follow instructions. In fact, asking for reasoning justifications for weaker models decreases performance, presumably due to their inability to generate and make use of such reasons. Stable Beluga 2 outperforms Llama 2 Chat for the same LLM size, and also displays heightened reasoning ability. For Stable Beluga 2, asking for reasoning justification improves performance across all tasks. This highlights the main characteristic of MEMWALKER: if an LLM passes a critical reasoning ability threshold, it can reason about a long input in multiple rounds without errors cascading quickly across rounds. For weaker LLMs that cannot make good navigation decisions, errors could compound and</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: MemWalker performance comparisons between using working memory and without (i.e., the LM only looks at the content of the children memory tree nodes, rather than memory from all the nodes it has traversed). Inclusion of working memory yields large gains.</p>
<p>Table 4: MEMWALKER navigation analysis. Stray ratio: percentage of paths that contain the revert action. Recovery Rate: percentage of stray paths that recover and answer the query correctly.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Stray <br> Ratio</th>
<th style="text-align: center;">Recovery <br> Rate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">QuALITY</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">70.0</td>
</tr>
<tr>
<td style="text-align: left;">SummScreenFD</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">59.6</td>
</tr>
<tr>
<td style="text-align: left;">GovReport</td>
<td style="text-align: center;">18.8</td>
<td style="text-align: center;">79.0</td>
</tr>
</tbody>
</table>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Percentage comparison of total tokens processed against the tokens of the original example between all paths vs. successful paths.
overall performance suffers. As LLMs will only improve in reasoning ability over the coming years, we expect methods like MEMWALKER will become more and more effective.</p>
<p>Navigating the memory tree requires working memory. As MEMWALKER makes decisions to traverse the memory tree and read relevant segments, it might lose sight of the overall context. The model thus carries information from the nodes along the navigation path as working memory, where the content of the working memory updates as the model selects the next path. We evaluate the performance of MEMWALKER with and without working memory, with results given in Figure 3. We find a significant performance degradation without working memory across all tasks, with a $5-13 \%$ drop in accuracy, showing the importance of this component.</p>
<p>MEMWALKER can recover from stray paths. As MEMWALKER navigates the memory tree, it needs to not only find the path towards the most pertinent segments, but also potentially to recover from traversal errors should they occur. We report recovery statistics in Table 4. MEMWALKER executes a revert navigation action (and hence changes path) for around $15 \%-20 \%$ of examples, but of those examples can recover and get those examples correct $70 \%$ of the time for QuALITY, $\sim 60 \%$ for SummScreenFD, and $\sim 80 \%$ for GovReport.</p>
<p>MEMWALKER enables efficient reading. Since MEMWALKER determines which parts of the long text it needs to read, the effective content that needs to be read may be smaller than the entire sequence. We report the percentage of the long context read averaged over all examples, for each of the three tasks, in Figure 4. We find that between only $63 \%-69 \%$ of the text on average needs to</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Performance trade-off of different memory construction configurations on QuALITY. x-axis: maximum number of nodes that can be connected to a parent node. Red: summarizing 1, 000 -token segments. Blue: summarizing 500 -token segments.
be read to answer the question including the content of the tree nodes. Among successful paths, the reading required further reduces to $59 \%-64 \%$.</p>
<p>Memory tree construction trade-off. A fundamental trade-off arises as we construct the memory tree - summarizing larger segments compresses more information into a node to reduce the depth of the tree, but risks losing fidelity of the content. Similarly, connecting many lower level nodes to the upper one can help flatten the tree, yet render the navigation task harder for the LLM at each node. Figure 5 shows the performance of different configurations of the memory tree on QuALITY. Summarizing larger segments is generally more beneficial than smaller segments as well as connecting more children nodes to the parent. However, the performance plateaus as the maximum number of nodes increases, showing the trade-off with respect to how much information can be packed into the nodes during memory tree construction.</p>
<h1>6 CONCLUSION</h1>
<p>We propose MEMWALKER, an interactive reading agent which uses iterative LLM prompting to decide which part of the content should be read closely based on its own reasoning. Our approach first builds a structured memory given long context sequence data, and then makes navigation decisions of the pertinent parts to read given a query. Our method shows superior performance against a number of baselines including various long context length models, retrieval and recurrence baselines, in particular for longer sequence tasks. Detailed analysis highlights a number of important factors, including our method's ability to reason about navigation decisions, ability to revert navigation to a different path when necessary, and incorporation of a working memory. Future work should explore many new directions that MEMWALKER opens up, in particular its application to different data structures other than trees, and finetuning its performance specific to the interactive reading goal.</p>
<h2>7 LIMITATIONS</h2>
<p>MEMWALKER exhibits three major limitations. First, the memory tree generation might not scale too well if the sequence's length becomes extremely long. The increase in sequence length entails more nodes in the tree and hence renders the tree construction process onerous. Workaround such as trading off the granularity of the summary in exchange for speed might be viable. Nonetheless, the issue of scaling remains a limit. In this setting it may make sense to generalize MEMWALKER to a combination of tree and hash Bawa et al. (2005) or other alternative data structure, whilst retaining its travesersal ability via LLM prompting. Second, MEMWALKER only works when the LLM exhibits a strong enough reasoning capability, which according to our experiments is required to be large (over 70B) and instruction-tuned. If the reasoning capability falls short, the error compounds</p>
<p>and the method would fail. Enabling a smaller model that can perform a similar instruction following procedure could be useful for scaling the method. This could be made possible by removing the following third limitation. Third, MEMWALKER only uses zero-shot prompting and does not leverage fine-tuning to further improve the interactive reading capability. This could be done, for example, by performing interactive reading and collect the successful paths for further fine-tuning.</p>
<h1>REFERENCES</h1>
<p>Joshua Ainslie, Tao Lei, Michiel de Jong, Santiago Ontañón, Siddhartha Brahma, Yury Zemlyanskiy, David Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, Yun-Hsuan Sung, and Sumit Sanghai. Colt5: Faster long-range transformers with conditional computation. In preprint, 2023.</p>
<p>Mayank Bawa, Tyson Condie, and Prasanna Ganesan. Lsh forest: self-tuning indexes for similarity search. In Proceedings of the 14th international conference on World Wide Web, pp. 651-660, 2005.</p>
<p>Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. In preprint, 2020.</p>
<p>Aydar Bulatov, Yuri Kuratov, and Mikhail S Burtsev. Scaling transformer to 1m tokens and beyond with rmt. In preprint, 2023.</p>
<p>Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer opendomain questions. arXiv preprint arXiv:1704.00051, 2017.</p>
<p>Mingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. Summscreen: A dataset for abstractive screenplay summarization. In Association for Computational Linguistics (ACL), 2022.</p>
<p>Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. preprint, 2023.</p>
<p>Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts. In preprint, 2023.</p>
<p>Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. In Association for Computational Linguistics (ACL), 2019.</p>
<p>Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. In Advances in Neural Information Processing Systems (NeurIPS), 2022.</p>
<p>Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of wikipedia: Knowledge-powered conversational agents. arXiv preprint arXiv:1811.01241, 2018.</p>
<p>Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, and Sainbayar Sukhbaatar. Addressing some limitations of transformers with feedback memory. In preprint, 2020.</p>
<p>Mor Geva and Jonathan Berant. Learning to search in long documents using document structure. arXiv preprint arXiv:1806.03529, 2018.</p>
<p>Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang. Longt5: Efficient text-to-text transformer for long sequences. In North American Association for Computational Linguistics (NAACL), 2022.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. In Neural Computation, 1997.
Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient attentions for long document summarization. In North American Association for Computational Linguistics (NAACL), 2021.</p>
<p>Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain question answering. In preprint, 2020.</p>
<p>Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning. 2022.</p>
<p>Jack Lanchantin, Shubham Toshniwal, Jason Weston, Arthur Szlam, and Sainbayar Sukhbaatar. Learning to reason and memorize with self-notes. In preprint, 2023.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33: $9459-9474,2020$.</p>
<p>Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can open-source llms truly promise on context length? 2023. URL https://lmsys.org/blog/2023-06-29-longchat.</p>
<p>Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. In preprint, 2023.</p>
<p>Dakota Mahan, Ryan Carlow, Louis Castricato, Nathan Cooper, and Christian Laforte. Stable beluga models. URL https://huggingface.co/stabilityai/StableBeluga2.</p>
<p>Alexander Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, and Jason Weston. Key-value memory networks for directly reading documents. In Association for Computational Linguistics (ACL), 2016.</p>
<p>MosaicML. Introducing mpt-7b: A new standard for open-source, commercially usable llms. 2023. Accessed: 2023-05-05.</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt: Browser-assisted question-answering with human feedback. In preprint, 2021.</p>
<p>Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Samuel R Bowman. Quality: Question answering with long input texts, yes! In North American Association for Computational Linguistics (NAACL), 2022.</p>
<p>Ofir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations (ICLR), 2022.</p>
<p>Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019.</p>
<p>Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy. Scrolls: Standardized comparison over long language sequences. In Empirical Methods in Natural Language Processing (EMNLP), 2022.</p>
<p>Simeng Sun, Yang Liu, Shuohang Wang, Chenguang Zhu, and Mohit Iyyer. Pearl: Prompting large language models to plan and execute actions over long documents. In preprint, 2023.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen</p>
<p>Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5998-6008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ 3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.</p>
<p>Jeff Wu, Long Ouyang, Daniel M. Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Christiano. Recursively summarizing books with human feedback. In preprint, 2021.</p>
<p>Yuhuai Wu, Markus N. Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. In International Conference on Learning Representations (ICLR), 2022.</p>
<p>Jing Xu, Arthur Szlam, and Jason Weston. Beyond goldfish memory: Long-term open-domain conversation. In Association for Computational Linguistics (ACL), 2022.</p>
<p>Kevin Yang, Yuandong Tian, Nanyun Peng, and Dan Klein. Re3: Generating longer stories with recursive reprompting and revision. In Empirical Methods in Natural Language Processing (EMNLP), 2022.</p>
<p>Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. In Advances in Neural Information Processing Systems (NeurIPS), 2022.</p>
<p>Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Transformers for longer sequences. In Advances in Neural Information Processing Systems (NeurIPS), 2020.</p>
<p>Wangchunshu Zhou, Yuchen Eleanor Jiang, Peng Cui, Tiannan Wang, Zhenxin Xiao, Yifan Hou, Ryan Cotterell, and Mrinmaya Sachan. Recurrentgpt: Interactive generation of (arbitrarily) long text. In preprint, 2023.</p>
<h1>A APPENDIX</h1>
<h2>A. 1 Prompts</h2>
<p>We provide full prompts for both memory tree construction and navigation described in $\S 3$.</p>
<h2>A.1.1 MEMORY TREE CONSTRUCTION Prompts</h2>
<p>We use two prompts for memory tree construction (the construction component of Table 5). The first (leaf) instructs the LLM to summarize the text segment into a comprehensive summary. After this step, the segments are grouped and summarized into non-leaf node summaries. The summaries ([CHILD_SUMM_NODE_0], [CHILD_SUMM_NODE_1],..., [CHILD_SUMM_NODE_N]) are grouped and concatenated as the summary content of their parent node. During this process, if the concatenated summaries exceed the predetermined length, the second construction prompt is used to further summarize the text (i.e., [SUMMARIES]) for the parent node.</p>
<h2>A.1.2 Navigation Prompts</h2>
<p>We use two navigation prompts (triage and leaf) as described in $\S 3$. We show the general prompt template in the navigation stage of Table 5.</p>
<h2>A. 2 EXAMPLES</h2>
<p>We provide an extra navigation example in Table 6.</p>
<p>Table 5: Prompts used for the memory tree construction stage and the navigation stage. For the memory construction stage, [TEXT_OF_SEGMENT] is filled with the segment text at the leaf nodes. [SUMMARIES] is the concatenated summaries from the child nodes and will be further summarized if it exceeds the predetermined length. For navigation, [QUERY] is the query, [OPTIONS] are the multi-choice options (only in QuALITY), [CHILD_SUMM_NODE_n] represents the summary text of the $n$-th child node, and [WORKING_MEMORY] is the information carried from previous nodes. Yellow indicates triage prompt and purple indicates leaf prompt, as described in $\S 3$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Stage</th>
<th style="text-align: center;">Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Construction (leaf)</td>
<td style="text-align: center;">[TEXT_OF_SEGMNET] . Summarize the above text comprehensively into a fluent passage.</td>
</tr>
<tr>
<td style="text-align: center;">Construction (non-leaf)</td>
<td style="text-align: center;">[SUMMARIES]. Compress each summary into a much shorter summary.</td>
</tr>
<tr>
<td style="text-align: center;">Navigation (triage)</td>
<td style="text-align: center;">The following passage(s) are the summaries of the different parts of a story. <br> To answer the question: [QUERY] <br> Which of the following summary is MOST LIKELY to contain information about the answer? <br> First provide reasoning to compare the summaries before you make the decision. <br> Summary 0: [CHILD_SUMM_NODE_S] <br> Summary 1: [CHILD_SUMM_NODE_1] <br> Summary N: [CHILD_SUMM_NODE_N]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Relpy with the passage number as your action. <br> You MUST choose one summary number and you should reply with the following format: <br> $88888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888</td>
</tr>
</tbody>
</table>
<p>Table 6: Example trajectory from the SummScreenFD dataset.</p>
<h1>Navigation Trajectory</h1>
<p>Node 1 The following passage(s) are the summaries of the different parts of a story.
To answer the question: Who invited Michael to his business school as a guest speaker?
Which of the following summary is MOST LIKELY to contain information about the answer?
First provide reasoning to compare the summaries before you make the decision.
Summary 0: In the text, Michael and Ryan are on their way to give a speech at a business school. [...]
Summary 1: Michael is reminiscing about his college days and suggests playing Frisbee with a college student. [...]
Summary 2: Michael Scott is giving a presentation to a group of business students, attempting to explain [...]
Summary 3: In the text, there is a scene where a bat is found in the office and employees react differently to its presence. [...]
Summary 4: Pam, an artist, has an art show featuring her paintings. Roy compliments her art [...]
Relpy with the passage number as your action.
You MUST choose one summary number and you should reply with the following format:
####################################################################################################\</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Further summarizing the working memory as it accumulates would be an alternative approach, which we have not explored in this study.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
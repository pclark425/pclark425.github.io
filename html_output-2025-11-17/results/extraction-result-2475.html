<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2475 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2475</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2475</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-231924506</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2102.07647v1.pdf" target="_blank">Uncertainty quantification and exploration-exploitation trade-off in humans</a></p>
                <p><strong>Paper Abstract:</strong> The main objective of this paper is to outline a theoretical framework to analyse how humans' decision-making strategies under uncertainty manage the trade-off between information gathering (exploration) and reward seeking (exploitation). A key observation, motivating this line of research, is the awareness that human learners are amazingly fast and effective at adapting to unfamiliar environments and incorporating upcoming knowledge: this is an intriguing behaviour for cognitive sciences as well as an important challenge for Machine Learning. The target problem considered is active learning in a black-box optimization task and more specifically how the exploration/exploitation dilemma can be modelled within Gaussian Process based Bayesian Optimization framework, which is in turn based on uncertainty quantification. The main contribution is to analyse humans' decisions with respect to Pareto rationality where the two objectives are improvement expected and uncertainty quantification. According to this Pareto rationality model, if a decision set contains a Pareto efficient (dominant) strategy, a rational decision maker should always select the dominant strategy over its dominated alternatives. The distance from the Pareto frontier determines whether a choice is (Pareto) rational (i.e., lays on the frontier) or is associated to"exasperate"exploration. However, since the uncertainty is one of the two objectives defining the Pareto frontier, we have investigated three different uncertainty quantification measures and selected the one resulting more compliant with the Pareto rationality model proposed. The key result is an analytical framework to characterize how deviations from"rationality"depend on uncertainty quantifications and the evolution of the reward seeking process.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2475.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2475.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BO (GP-based)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Optimization with Gaussian Process surrogate</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequential global optimization framework using a Gaussian Process surrogate to model an expensive black-box objective and an acquisition function to allocate evaluations that trade off exploration and exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Gaussian Process Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Fits a Gaussian Process (GP) posterior to observations (computational cost dominated by O(n^3) matrix inversion) and chooses next query points by optimizing an acquisition function combining GP posterior mean μ(x) and uncertainty σ(x) (or other uncertainty measures). Acquisition functions include improvement-based (PI, EI, UCB/GP-UCB) and information-based (ES, PES, MES) criteria; the framework is applied iteratively under a fixed trial budget to maximize cumulative reward or find the optimizer.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General black-box optimization (engineering design, hyperparameter tuning, automated machine learning, robotics, experimental design)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>At each iteration the GP posterior is computed and an acquisition function (e.g., EI, UCB, MES) is optimized to pick the next experiment; the acquisition function encodes the allocation trade-off between expected improvement (exploitation) and uncertainty/information gain (exploration).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of GP posterior updates and cost of GP conditioning/inference (matrix inversion ~ O(n^3) in number of observations n), plus extra costs for sampling from GP posterior when using information-based acquisition functions; dimensionality-driven increases in sampling cost are noted.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Depends on acquisition: EI/PI use expected improvement; UCB uses confidence bound (μ ± β^0.5 σ); ES/PES use reductions in differential entropy / mutual information about x*; MES measures information about the max-value y*.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Encoded in the acquisition function: improvement-based mix μ(x) and σ(x) (EI, PI), UCB adds an uncertainty bonus √β σ(x), and information-based methods select points that maximize expected information about the optimizer (ES/PES/MES). Hyperparameters (e.g., β in GP-UCB) or schedules/randomization can tune the balance.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Indirect: different acquisition functions and sampling-based methods (Thompson Sampling or Pareto-frontier sampling) produce more diverse queries; BO itself does not enforce explicit diversity beyond the chosen acquisition.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of expensive evaluations / trial budget (the paper's experiments use a fixed shot budget per human subject)</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Acquisition functions aim to maximize information/value per expensive evaluation; information-based methods are favored when evaluations are very expensive despite higher computational overhead for selection.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Expected improvement (EI), max-value y* (MES) or direct improvement to best observed value; Pareto approaches use expected improvement as one objective paired with uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative: ability to identify x* and maximize Average Cumulative Reward (ACR). The paper reports BO's estimated x* closer to human solutions in prior work (Fig.1) and uses ACR for human-analysis; no method-level numeric performance from this paper beyond references to prior empirical comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Random Search, DIRECT, Genetic Algorithms, PSO, Simulated Annealing, and various acquisition functions compared across literature</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Referenced: BO's estimated x* was closer to humans than other global optimization methods; literature reports MES and some Pareto sampling approaches outperforming some traditional acquisition functions on several tasks (no new numeric values provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Qualitative sample-efficiency advantage for BO in expensive-evaluation regimes; gains depend on acquisition and surrogate fidelity. No explicit percentage/time numbers provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper emphasizes the tradeoff between computational cost of computing information-based acquisitions (sampling from GP, dimensionality costs) versus their information gain; notes that ES/PES are computationally expensive and suitable only when function evaluations are extremely costly, motivating MES and Pareto-based approaches as computationally friendlier options.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Paper finds that using predictive standard deviation as an uncertainty measure increases alignment with a Pareto-rational model of human decisions and that acquisition choice must account for model uncertainty estimation and computational costs; recommends selecting allocation strategies that balance GP modelling reliability, uncertainty quantification choice, and computational feasibility given evaluation budget.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Uncertainty quantification and exploration-exploitation trade-off in humans', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2475.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2475.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ES / PES</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Entropy Search / Predictive Entropy Search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Information-theoretic acquisition functions that choose queries to maximally reduce entropy (uncertainty) about the optimizer location x* (ES) or about the predictive distribution conditioned on x* (PES).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Entropy search for information-efficient global optimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Entropy Search (ES) and Predictive Entropy Search (PES)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Compute the expected reduction in differential entropy of the posterior over the optimizer x* (ES) or use an alternative formulation integrating over y* (PES). Both require sampling function realizations from the GP posterior and estimating optima to approximate entropy changes, making them computationally intensive and often intractable in high dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Black-box global optimization where evaluations are extremely expensive and information per evaluation is critical</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Select next experiment to maximize expected information gain about the optimizer location (mutual information / reduction in differential entropy).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of posterior samples (GP paths) and optimization of sampled paths per BO iteration; cost grows steeply with search dimensionality; GP conditioning cost (O(n^3)) also applies.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected reduction in differential entropy of p(x*|D) (mutual information between candidate observation and optimizer location).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Purely information-seeking: tends to choose points that reduce uncertainty about x* even if immediate improvement is small (strong exploration bias guided by information gain).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Implicit via information-maximization which favors queries that resolve different hypotheses about x*; no explicit diversity constraint besides entropy objective.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed evaluation budget; intended for extremely expensive evaluations where computational selection cost is negligible relative to experiment cost.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Compute queries that maximize information per expensive evaluation; high internal compute cost accepted only when experimental evaluations dominate cost.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Reduction of uncertainty about x* (probability mass concentrating on high-value regions); success assessed by locating optimizer with few evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in literature as sample-efficiency (number of function evaluations to locate optimizer) but ES/PES are often outperformed by MES in practice due to computational approximations; no numeric values in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared in literature to EI, PI, UCB, MES and other acquisitions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Paper notes ES/PES are analytically intractable and expensive; MES was proposed as a more efficient alternative and has empirically outperformed ES, UCB and PI on several tasks (as reported in cited works).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Noted trade-off: high information-per-query but high computational cost for selection; effective when experiment cost >> selection cost.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper explicitly discusses ES/PES computational expense vs information gain and dimensionality scaling, recommending MES when selection cost must be reduced.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Information-maximizing allocations (ES/PES) are theoretically attractive for maximizing information per trial, but practical use requires approximations (or switching to MES) due to high sampling and optimization cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Uncertainty quantification and exploration-exploitation trade-off in humans', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2475.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2475.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MES</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Max-value Entropy Search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An information-based acquisition that targets the entropy of the maximum value y* (max f(x)), making information estimation one-dimensional and more computationally efficient than ES/PES.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Max-value Entropy Search for Efficient Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Max-value Entropy Search (MES)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Measures mutual information between a candidate observation {x,y} and the maximum function value y*; approximates information gain by sampling y* (rather than x*), enabling more efficient estimation because y* is scalar. Sampling can be done from GP posterior or approximated via a Gumbel distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Black-box optimization problems where function evaluations are very expensive and entropy-based selection is desired but with limited selection-time compute budget.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Pick x that maximizes expected information about the max-value y*, thereby favoring points that are both informative and potentially high-yield for locating the optimum.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Cost of sampling y* from GP posterior (cheaper than sampling full function-path optima) plus GP conditioning cost; computational cost lower than ES/PES but still higher than simple EI/UCB.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Mutual information I({x,y}; y* | D) = H(p(y|D,x)) - E[H(p(y|D,x,y*))].</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Information-guided exploration with built-in focus on the value-of-interest (the max); implicitly balances exploration of uncertain regions that affect y* and exploitation of promising regions.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Implicit through information gain criterion; not an explicit diversity-regularizer but sampling-based selection tends to explore multiple hypotheses affecting y*.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of evaluations; intended when evaluation cost >> selection cost but selection-time compute still constrained.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Reduces selection-time computation compared to ES/PES by focusing on scalar y*; appropriate when needing information-rich queries with manageable selection overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Information about y* and expected improvement in the best observed value; success measured as fewer evaluations to find high-value y.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Cited literature reports MES empirically outperforming ES, UCB and PI on several optimization tasks; no numeric detail provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>ES, PES, UCB, PI, EI and other acquisition functions (in cited studies).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Paper cites results showing MES underperformed/overperformed relative to different acquisitions depending on tasks but notes MES often empirically outperforms many traditional acquisition functions.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Lower selection-time computational cost than ES/PES while retaining information-theoretic focus; qualitative efficiency gain when selection cost is a concern.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>MES presented as a pragmatic compromise between information gain and computational cost for acquisition selection; paper highlights the importance of such trade-offs in method choice.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Using a scalar-information target (y*) can capture much of the value of full ES/PES while offering tractable computation; thus MES is recommended when wanting information-driven allocation without prohibitive compute.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Uncertainty quantification and exploration-exploitation trade-off in humans', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2475.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2475.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pareto bi-objective</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pareto-frontier bi-objective acquisition framework (mean vs uncertainty)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Formulates acquisition as a bi-objective optimization between predicted improvement (mean) and uncertainty (std or other measures), and selects Pareto-efficient points as candidate allocations to balance exploitation and exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bi-objective decision making in global optimization based on statistical models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Pareto-frontier acquisition (mean vs uncertainty)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Given GP posterior, map each candidate x to a 2D objective vector (expected improvement or μ-based improvement, and an uncertainty measure σ(x) or alternative). Compute Pareto frontier over a dense grid to identify non-dominated (Pareto-efficient) candidate queries; decision-makers can sample from or choose on the frontier, providing a spectrum of exploration/exploitation trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Black-box optimization and active learning where explicit multi-objective trade-off between improvement and uncertainty is useful (human decision modelling, BO variants, experimental design).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Generate set of Pareto-efficient candidate experiments balancing expected improvement and uncertainty; selection among them can be made deterministically (dominance) or stochastically (random sampling from Pareto set) to allocate limited experimental budget.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Cost to evaluate μ(x) and chosen uncertainty measure at grid points; GP inference cost for conditioning (O(n^3)) dominates; Pareto-frontier approximation cost is primarily cost of making predictions over a dense grid (cheap relative to conditioning).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Uncertainty measure used as one objective (could be σ(x), entropy h(x), or distance-based z(x)); information gain is implicit via the chosen uncertainty metric rather than mutual information unless entropy-based uncertainty is used.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Provides the full Pareto set of trade-offs between exploitation (improvement) and exploration (uncertainty); choice among Pareto-efficient points determines the balance. The model interprets human choices relative to Pareto rationality (distance to frontier).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Selecting different points along the Pareto frontier naturally yields diverse query choices spanning exploration-exploitation spectrum; random sampling from the frontier (De Ath) can increase diversity in selected experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of trials; also applicable when many acquisition choices are cheap to compute but experiments are costly.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>By exposing the Pareto set, the method allows allocating limited experimental budget to points that best match the chosen trade-off (e.g., pick more exploratory Pareto points early, exploitative later), and supports randomization or focused subregion sampling for batch allocation.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Expected improvement (improvement over current best) used as the exploitation objective; 'breakthrough' implied by high improvement values on Pareto frontier.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In this paper, Pareto-frontier analysis is used to interpret human decisions (number of Pareto-rational choices, distances to frontier) rather than to report optimization performance; literature shows sampling from Pareto frontier can outperform single-acquisition strategies in some settings (cited works).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Traditional single-objective acquisition functions (EI, PI, UCB) and random search; Pareto approaches can reproduce EI/PI as frontier points per Žilinskas & Calvin.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Cited results indicate Pareto sampling can produce a richer set of candidate decisions and random sampling from the frontier can outperform some acquisitions; the paper reports that treating mean and uncertainty as bi-objective reveals more candidate strategies than traditional acquisitions.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Not quantified here; conceptual gain is increased flexibility and potential to match human-like strategies; sampling from a precomputed Pareto set can be efficient since prediction cost is lower than conditioning cost.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper's central analytic contribution: frames human decisions as Pareto-rational on the improvement-uncertainty plane and investigates how different uncertainty metrics change Pareto sets and explain human exploratory deviations; finds predictive standard deviation often increases alignment with Pareto rationality.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Key insight: exposing the Pareto frontier lets a rational agent choose dominant trade-offs; choice of uncertainty quantification (σ, entropy-based h(x), or distance z(x)) crucially affects which actions are Pareto-efficient and thus optimal under this framework; predictive σ(x) matched human behaviour best in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Uncertainty quantification and exploration-exploitation trade-off in humans', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2475.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2475.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pareto sampling (De Ath)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random sampling from Pareto frontier for acquisition</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An acquisition approach that randomly samples candidate points from the Pareto frontier of mean vs uncertainty to select experiments, providing a wider and potentially more diverse set of choices than single acquisition functions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Greed is Good: Exploration and Exploitation Tradeoffs in Bayesian Optimisation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Random Pareto-frontier sampling</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Approximate Pareto frontier over GP predictions, then randomly sample a point from the frontier (or from a subregion) to obtain the next query; randomization can help avoid bias toward particular trade-offs and increase chance of discovering rewarding regions.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Bayesian optimization and active learning where promoting diversity among candidate queries can improve exploration and avoid local optima.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocation is done by stochastic selection from the Pareto set; this spreads resources across different compromise points between exploration and exploitation rather than committing to a single heuristic.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Cost to compute Pareto frontier (predictions over grid) and random sampling; dominated by GP conditioning cost but sampling is cheap relative to conditioning; no heavy sampling of posterior paths required.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Implicit via the uncertainty objective used to define the Pareto frontier (σ, entropy, or distance-based measures); not explicitly mutual information unless entropy is used.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Random selection from Pareto frontier yields different balances across iterations; increases exploration diversity while still restricting choices to Pareto-efficient trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Explicit: randomness in selecting among frontier points promotes diversity of experiments and avoids repetitive exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed-trial budget; can be used in sequential or batched settings.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>By randomizing among Pareto-efficient candidates, resources are spread across a diverse set of promising points, potentially improving robustness under tight budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>High expected improvement values among frontier points; sampling can increase probability of selecting a breakthrough candidate by covering different trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Cited work indicates random Pareto sampling can outperform some traditional acquisitions on benchmark tasks; paper references these results qualitatively without numeric details.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared in cited literature to EI, PI, UCB and other single-acquisition strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Reported in citations that random Pareto-frontier sampling can outperform other methods on certain problems, especially by improving exploration diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Qualitative: increased chance of locating good regions under same evaluation budget due to diversified allocation; no numeric gain provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Highlights trade-off between restricting choices to Pareto-efficient points (preserving rationality) and introducing randomness to promote exploration diversity; balances computational tractability with exploratory coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Stochastic sampling from the Pareto frontier is a practical strategy to diversify allocations and can empirically yield better exploration-exploitation outcomes than committing to a single acquisition heuristic.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Uncertainty quantification and exploration-exploitation trade-off in humans', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2475.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2475.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Paria RS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random scalarizations framework for multi-objective BO</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A flexible framework that uses random scalarizations to sample different parts of a multi-objective Pareto frontier, enabling focused or uniform exploration of trade-offs between objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A flexible framework for multi-objective bayesian optimization using random scalarizations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Random scalarizations for multi-objective Bayesian optimization</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Creates scalar acquisition objectives by randomly sampling scalarization weights (or otherwise focusing on subregions) to convert a multi-objective Pareto problem into scalarized single-objective problems; running BO on these scalarizations yields points across the Pareto frontier and supports targeted exploration of subregions.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Multi-objective optimization and active learning where multiple conflicting goals (e.g., improvement vs uncertainty, multiple tasks) must be balanced.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Sample scalarization weights (possibly biased toward a subregion) and optimize the resulting scalar objective to select next point; repeated sampling allocates resources across different Pareto trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Cost per scalarized BO run (GP conditioning + scalar acquisition optimization); multiple scalarizations increase compute proportional to number of scalar samples but can be parallelized or batched.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Depends on the scalarized acquisition used (could be EI, UCB, or information-based objectives); no single fixed information metric—flexible by design.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploration/exploitation balance controlled by the choice of scalarization and the underlying acquisition function used per scalarization; randomness in scalarization promotes exploration across trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Explicit via random scalarizations which force consideration of diverse Pareto trade-offs; can focus sampling in specific regions to control diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed-trial budget; computational budget if many scalarizations are sampled.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Control number and focus of scalarizations to respect computational and experimental budgets; sample more coarse-grained scalarizations when budget is tight.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Scalarized objective value or Pareto dominance; breakthroughs correspond to points with large scalarized improvement or novel Pareto-optimal performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in cited work as effective at covering Pareto front and enabling targeted search; no numeric details in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared in literature to single-objective acquisitions and Pareto sampling methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Claimed flexibility and improved coverage of Pareto frontier versus single fixed scalarization; empirical benefits depend on scalarization sampling design.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Enables focused allocation of evaluations to particular subregions of interest, potentially improving discovery of high-impact trade-offs under limited budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Provides a practical mechanism to trade off computational cost (number of scalarizations) versus diversity/coverage of Pareto frontier; can bias allocation toward promising regions if prior preferences exist.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Random scalarizations offer a tunable way to allocate experiments across Pareto trade-offs; choose sampling density and focus to match experimental/resource constraints and discovery priorities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Uncertainty quantification and exploration-exploitation trade-off in humans', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2475.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2475.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Thompson Sampling (TS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Thompson Sampling via sampling from GP posterior</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A randomized sequential strategy that samples a function realization from the GP posterior and selects its optimizer as the next query, implicitly trading off exploration and exploitation by posterior sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An information-theoretic analysis of Thompson sampling</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Thompson Sampling (GP posterior sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>At each iteration draw a sample function from the GP posterior, optimize that sampled function to get candidate x, then query the true function at x; repeat. TS can be biased toward exploitation of high-probability optima in the posterior; -greedy variants add uniform random exploration with probability ε.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Bandit problems, sequential optimization, active learning where randomized exploration is acceptable or beneficial.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocation is driven by sampled posterior hypotheses: points that are optimal under a plausible posterior draw are evaluated, implicitly allocating resources to hypotheses proportional to posterior probability.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Cost to sample functions from GP posterior (sampling methods for scalable sampling cited) and to optimize sampled function; cost scales with number of samples drawn and dimensionality.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Implicit via posterior sampling; not explicitly optimizing mutual information but serves to explore according to posterior uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Randomized: exploration emerges from variability in posterior samples; exploitation occurs when posterior concentrates; can incorporate ε-greedy to force more exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Randomness in posterior samples yields diversity over iterations; no explicit diversity regularizer.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of queries; sampling cost affects computational budget for selection.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Adjust number of posterior samples or adopt approximate sampling to limit selection-time compute; ε-greedy variants can tune allocation under evaluation budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Probability of sampling a posterior draw that yields a high-improvement candidate; breakthroughs occur when a sampled hypothesis points to a previously untested high-value region.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Literature notes TS can be biased toward exploitation; -greedy TS variants recommended to improve exploration; no new numerical metrics in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to UCB, EI, random search and other BO acquisition strategies in the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Cited analyses indicate TS may be biased toward exploitation and may benefit from injected random exploration; performance depends on sampling fidelity and problem structure.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Potentially efficient with low parameter tuning, but requires scalable GP sampling methods to be computationally practical (cited works propose efficient samplers).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>TS offers a principled randomized exploration strategy whose exploration-exploitation balance depends on posterior uncertainty; computational cost of accurate sampling vs benefits of stochastic exploration is noted.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>TS allocates resources proportional to posterior probability of being optimal; for balanced exploration, augmentations (ε-greedy or other) or careful sampling are recommended.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Uncertainty quantification and exploration-exploitation trade-off in humans', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2475.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2475.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GP-UCB / Randomized GP-UCB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gaussian Process Upper Confidence Bound and Randomized GP-UCB</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An optimistic acquisition (GP-UCB) that selects points by μ(x)+√β σ(x), with β controlling exploration, and recent randomized variants sample β from a distribution to diversify exploration-exploitation behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Randomised Gaussian Process Upper Confidence Bound for Bayesian Optimisation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GP-UCB and Randomized GP-UCB</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>GP-UCB selects next point maximizing an upper confidence bound combining mean and scaled standard deviation; β can be scheduled or tuned. Randomized GP-UCB approaches sample β randomly per iteration (or from a distribution) to explore a range of optimistic trade-offs and avoid poor fixed β choices.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Sequential decision-making and BO where controlling the optimism/exploration parameter is crucial (human modelling, engineering optimization).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates experiments to points with high optimistic estimates; randomized β spreads allocation across different optimism levels across iterations, improving robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Cost of evaluating μ and σ across candidate set and optimizing UCB; dominated by GP conditioning O(n^3). Randomized β sampling adds negligible cost.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Not explicitly mutual information; uses uncertainty bonus (σ) as heuristic proxy for information value.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Explicit via the β parameter: larger β increases exploration weight on σ(x); scheduling or random sampling of β changes allocation dynamically.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Randomized β induces diversity in selected points across iterations by varying optimism level; no explicit diversity penalty.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of queries; β scheduling can be adapted to budget (explore early, exploit later).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>GP-UCB can use scheduled β to manage exploration over the given budget; randomized β can be used to search for good β values without manual tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>High upper-confidence predicted value (μ + √β σ) indicating potentially high-yield or high-uncertainty high-yield regions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Cited works show GP-UCB models human strategies well in some tasks; randomized GP-UCB reported to outperform traditional GP-UCB on a range of problems in cited literature (no numerical specifics in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to EI, PI, TS, and other acquisition schemes in cited works.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>References indicate randomized GP-UCB can lead to improved performance by effectively exploring parameter settings and avoiding local optima; no concrete numeric comparisons provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Randomization can improve identification of high-value regions under the same budget by diversifying exploration-exploitation trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper highlights that GP-UCB's β controls trade-off and that scheduling or random sampling of β is important to balance exploration cost vs exploitation benefit under finite budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Tuning or randomizing β is crucial for effective allocation; using an adaptive or randomized optimism parameter helps avoid poor fixed-policy outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Uncertainty quantification and exploration-exploitation trade-off in humans', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2475.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2475.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sandholtz IBO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inverse Bayesian Optimization (IBO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A probabilistic framework to infer a human (or agent) acquisition function from observed search paths in black-box optimization tasks, enabling reverse-engineering of allocation strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Modeling human decision-making in spatial and temporal systems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Inverse Bayesian Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Given an observed sequence of queries and outcomes (a human search path), infer the underlying acquisition function by probabilistic (non-parametric Bayesian) inference over a family of candidate acquisition functions. Requires assumptions (e.g., convexity, smoothness) on the objective in prior formulations.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Cognitive modelling of human active search, and potentially automated imitation or transfer of human allocation strategies to algorithmic agents.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Not an allocation algorithm per se, but infers the allocation heuristic used by an agent so that one can replicate or analyze resource allocation decisions; the inferred acquisition can then be used to allocate experiments similarly.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Cost of Bayesian inference over acquisition-function space, which can be large; also depends on assumptions and the parametrization of candidate acquisition functions.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Inference objective maximizes posterior likelihood of observed sequence under candidate acquisition functions; not directly an information-gain-based allocation metric.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Reveals whether the observed agent used directed exploration, random exploration, UCB-like bonuses, EI-like improvement focus, or Pareto-like multi-objective trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Observation-based (human limited by experimental shots); inference must account for limited-horizon behaviours.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Inference conditions on observed finite sequences; requires restrictive function assumptions to make the inverse problem tractable as noted in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not directly applicable; can identify whether agent prioritized breakthrough (high improvement) or information-gathering in allocation decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>IBO provides posterior over acquisition functions; Sandholtz notes restrictive assumptions required; no numeric performance reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Approach contrasted with assuming a known acquisition function (forward BO) or other behaviorist models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not assessed quantitatively in this paper; cited work reports methodological constraints limiting generality.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Enables understanding/human-model transfer which may improve algorithmic allocation strategies when human heuristics are effective, but inference can be computationally demanding and assumption-sensitive.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Highlights inference cost and model assumptions trade-offs; notes inverse problem is hard in general black-box settings and often requires smoothness/convexity assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Inference can identify allocation heuristics (bias toward exploration or exploitation) used by humans, but practical application is limited by required assumptions and inference complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Uncertainty quantification and exploration-exploitation trade-off in humans', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Entropy search for information-efficient global optimization <em>(Rating: 2)</em></li>
                <li>Predictive entropy search for efficient global optimization of black-box functions <em>(Rating: 2)</em></li>
                <li>Max-value Entropy Search for Efficient Bayesian Optimization <em>(Rating: 2)</em></li>
                <li>Bi-objective decision making in global optimization based on statistical models <em>(Rating: 2)</em></li>
                <li>Greed is Good: Exploration and Exploitation Tradeoffs in Bayesian Optimisation <em>(Rating: 1)</em></li>
                <li>A flexible framework for multi-objective bayesian optimization using random scalarizations <em>(Rating: 2)</em></li>
                <li>Randomised Gaussian Process Upper Confidence Bound for Bayesian Optimisation <em>(Rating: 2)</em></li>
                <li>An information-theoretic analysis of Thompson sampling <em>(Rating: 2)</em></li>
                <li>Active learning for multi-objective optimization <em>(Rating: 2)</em></li>
                <li>Modeling human decision-making in spatial and temporal systems <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2475",
    "paper_id": "paper-231924506",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "BO (GP-based)",
            "name_full": "Bayesian Optimization with Gaussian Process surrogate",
            "brief_description": "A sequential global optimization framework using a Gaussian Process surrogate to model an expensive black-box objective and an acquisition function to allocate evaluations that trade off exploration and exploitation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Gaussian Process Bayesian Optimization",
            "system_description": "Fits a Gaussian Process (GP) posterior to observations (computational cost dominated by O(n^3) matrix inversion) and chooses next query points by optimizing an acquisition function combining GP posterior mean μ(x) and uncertainty σ(x) (or other uncertainty measures). Acquisition functions include improvement-based (PI, EI, UCB/GP-UCB) and information-based (ES, PES, MES) criteria; the framework is applied iteratively under a fixed trial budget to maximize cumulative reward or find the optimizer.",
            "application_domain": "General black-box optimization (engineering design, hyperparameter tuning, automated machine learning, robotics, experimental design)",
            "resource_allocation_strategy": "At each iteration the GP posterior is computed and an acquisition function (e.g., EI, UCB, MES) is optimized to pick the next experiment; the acquisition function encodes the allocation trade-off between expected improvement (exploitation) and uncertainty/information gain (exploration).",
            "computational_cost_metric": "Number of GP posterior updates and cost of GP conditioning/inference (matrix inversion ~ O(n^3) in number of observations n), plus extra costs for sampling from GP posterior when using information-based acquisition functions; dimensionality-driven increases in sampling cost are noted.",
            "information_gain_metric": "Depends on acquisition: EI/PI use expected improvement; UCB uses confidence bound (μ ± β^0.5 σ); ES/PES use reductions in differential entropy / mutual information about x*; MES measures information about the max-value y*.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Encoded in the acquisition function: improvement-based mix μ(x) and σ(x) (EI, PI), UCB adds an uncertainty bonus √β σ(x), and information-based methods select points that maximize expected information about the optimizer (ES/PES/MES). Hyperparameters (e.g., β in GP-UCB) or schedules/randomization can tune the balance.",
            "diversity_mechanism": "Indirect: different acquisition functions and sampling-based methods (Thompson Sampling or Pareto-frontier sampling) produce more diverse queries; BO itself does not enforce explicit diversity beyond the chosen acquisition.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Fixed number of expensive evaluations / trial budget (the paper's experiments use a fixed shot budget per human subject)",
            "budget_constraint_handling": "Acquisition functions aim to maximize information/value per expensive evaluation; information-based methods are favored when evaluations are very expensive despite higher computational overhead for selection.",
            "breakthrough_discovery_metric": "Expected improvement (EI), max-value y* (MES) or direct improvement to best observed value; Pareto approaches use expected improvement as one objective paired with uncertainty.",
            "performance_metrics": "Qualitative: ability to identify x* and maximize Average Cumulative Reward (ACR). The paper reports BO's estimated x* closer to human solutions in prior work (Fig.1) and uses ACR for human-analysis; no method-level numeric performance from this paper beyond references to prior empirical comparisons.",
            "comparison_baseline": "Random Search, DIRECT, Genetic Algorithms, PSO, Simulated Annealing, and various acquisition functions compared across literature",
            "performance_vs_baseline": "Referenced: BO's estimated x* was closer to humans than other global optimization methods; literature reports MES and some Pareto sampling approaches outperforming some traditional acquisition functions on several tasks (no new numeric values provided here).",
            "efficiency_gain": "Qualitative sample-efficiency advantage for BO in expensive-evaluation regimes; gains depend on acquisition and surrogate fidelity. No explicit percentage/time numbers provided in this paper.",
            "tradeoff_analysis": "Paper emphasizes the tradeoff between computational cost of computing information-based acquisitions (sampling from GP, dimensionality costs) versus their information gain; notes that ES/PES are computationally expensive and suitable only when function evaluations are extremely costly, motivating MES and Pareto-based approaches as computationally friendlier options.",
            "optimal_allocation_findings": "Paper finds that using predictive standard deviation as an uncertainty measure increases alignment with a Pareto-rational model of human decisions and that acquisition choice must account for model uncertainty estimation and computational costs; recommends selecting allocation strategies that balance GP modelling reliability, uncertainty quantification choice, and computational feasibility given evaluation budget.",
            "uuid": "e2475.0",
            "source_info": {
                "paper_title": "Uncertainty quantification and exploration-exploitation trade-off in humans",
                "publication_date_yy_mm": "2021-02"
            }
        },
        {
            "name_short": "ES / PES",
            "name_full": "Entropy Search / Predictive Entropy Search",
            "brief_description": "Information-theoretic acquisition functions that choose queries to maximally reduce entropy (uncertainty) about the optimizer location x* (ES) or about the predictive distribution conditioned on x* (PES).",
            "citation_title": "Entropy search for information-efficient global optimization",
            "mention_or_use": "mention",
            "system_name": "Entropy Search (ES) and Predictive Entropy Search (PES)",
            "system_description": "Compute the expected reduction in differential entropy of the posterior over the optimizer x* (ES) or use an alternative formulation integrating over y* (PES). Both require sampling function realizations from the GP posterior and estimating optima to approximate entropy changes, making them computationally intensive and often intractable in high dimensions.",
            "application_domain": "Black-box global optimization where evaluations are extremely expensive and information per evaluation is critical",
            "resource_allocation_strategy": "Select next experiment to maximize expected information gain about the optimizer location (mutual information / reduction in differential entropy).",
            "computational_cost_metric": "Number of posterior samples (GP paths) and optimization of sampled paths per BO iteration; cost grows steeply with search dimensionality; GP conditioning cost (O(n^3)) also applies.",
            "information_gain_metric": "Expected reduction in differential entropy of p(x*|D) (mutual information between candidate observation and optimizer location).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Purely information-seeking: tends to choose points that reduce uncertainty about x* even if immediate improvement is small (strong exploration bias guided by information gain).",
            "diversity_mechanism": "Implicit via information-maximization which favors queries that resolve different hypotheses about x*; no explicit diversity constraint besides entropy objective.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed evaluation budget; intended for extremely expensive evaluations where computational selection cost is negligible relative to experiment cost.",
            "budget_constraint_handling": "Compute queries that maximize information per expensive evaluation; high internal compute cost accepted only when experimental evaluations dominate cost.",
            "breakthrough_discovery_metric": "Reduction of uncertainty about x* (probability mass concentrating on high-value regions); success assessed by locating optimizer with few evaluations.",
            "performance_metrics": "Reported in literature as sample-efficiency (number of function evaluations to locate optimizer) but ES/PES are often outperformed by MES in practice due to computational approximations; no numeric values in this paper.",
            "comparison_baseline": "Compared in literature to EI, PI, UCB, MES and other acquisitions.",
            "performance_vs_baseline": "Paper notes ES/PES are analytically intractable and expensive; MES was proposed as a more efficient alternative and has empirically outperformed ES, UCB and PI on several tasks (as reported in cited works).",
            "efficiency_gain": "Noted trade-off: high information-per-query but high computational cost for selection; effective when experiment cost &gt;&gt; selection cost.",
            "tradeoff_analysis": "Paper explicitly discusses ES/PES computational expense vs information gain and dimensionality scaling, recommending MES when selection cost must be reduced.",
            "optimal_allocation_findings": "Information-maximizing allocations (ES/PES) are theoretically attractive for maximizing information per trial, but practical use requires approximations (or switching to MES) due to high sampling and optimization cost.",
            "uuid": "e2475.1",
            "source_info": {
                "paper_title": "Uncertainty quantification and exploration-exploitation trade-off in humans",
                "publication_date_yy_mm": "2021-02"
            }
        },
        {
            "name_short": "MES",
            "name_full": "Max-value Entropy Search",
            "brief_description": "An information-based acquisition that targets the entropy of the maximum value y* (max f(x)), making information estimation one-dimensional and more computationally efficient than ES/PES.",
            "citation_title": "Max-value Entropy Search for Efficient Bayesian Optimization",
            "mention_or_use": "mention",
            "system_name": "Max-value Entropy Search (MES)",
            "system_description": "Measures mutual information between a candidate observation {x,y} and the maximum function value y*; approximates information gain by sampling y* (rather than x*), enabling more efficient estimation because y* is scalar. Sampling can be done from GP posterior or approximated via a Gumbel distribution.",
            "application_domain": "Black-box optimization problems where function evaluations are very expensive and entropy-based selection is desired but with limited selection-time compute budget.",
            "resource_allocation_strategy": "Pick x that maximizes expected information about the max-value y*, thereby favoring points that are both informative and potentially high-yield for locating the optimum.",
            "computational_cost_metric": "Cost of sampling y* from GP posterior (cheaper than sampling full function-path optima) plus GP conditioning cost; computational cost lower than ES/PES but still higher than simple EI/UCB.",
            "information_gain_metric": "Mutual information I({x,y}; y* | D) = H(p(y|D,x)) - E[H(p(y|D,x,y*))].",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Information-guided exploration with built-in focus on the value-of-interest (the max); implicitly balances exploration of uncertain regions that affect y* and exploitation of promising regions.",
            "diversity_mechanism": "Implicit through information gain criterion; not an explicit diversity-regularizer but sampling-based selection tends to explore multiple hypotheses affecting y*.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed number of evaluations; intended when evaluation cost &gt;&gt; selection cost but selection-time compute still constrained.",
            "budget_constraint_handling": "Reduces selection-time computation compared to ES/PES by focusing on scalar y*; appropriate when needing information-rich queries with manageable selection overhead.",
            "breakthrough_discovery_metric": "Information about y* and expected improvement in the best observed value; success measured as fewer evaluations to find high-value y.",
            "performance_metrics": "Cited literature reports MES empirically outperforming ES, UCB and PI on several optimization tasks; no numeric detail provided in this paper.",
            "comparison_baseline": "ES, PES, UCB, PI, EI and other acquisition functions (in cited studies).",
            "performance_vs_baseline": "Paper cites results showing MES underperformed/overperformed relative to different acquisitions depending on tasks but notes MES often empirically outperforms many traditional acquisition functions.",
            "efficiency_gain": "Lower selection-time computational cost than ES/PES while retaining information-theoretic focus; qualitative efficiency gain when selection cost is a concern.",
            "tradeoff_analysis": "MES presented as a pragmatic compromise between information gain and computational cost for acquisition selection; paper highlights the importance of such trade-offs in method choice.",
            "optimal_allocation_findings": "Using a scalar-information target (y*) can capture much of the value of full ES/PES while offering tractable computation; thus MES is recommended when wanting information-driven allocation without prohibitive compute.",
            "uuid": "e2475.2",
            "source_info": {
                "paper_title": "Uncertainty quantification and exploration-exploitation trade-off in humans",
                "publication_date_yy_mm": "2021-02"
            }
        },
        {
            "name_short": "Pareto bi-objective",
            "name_full": "Pareto-frontier bi-objective acquisition framework (mean vs uncertainty)",
            "brief_description": "Formulates acquisition as a bi-objective optimization between predicted improvement (mean) and uncertainty (std or other measures), and selects Pareto-efficient points as candidate allocations to balance exploitation and exploration.",
            "citation_title": "Bi-objective decision making in global optimization based on statistical models",
            "mention_or_use": "use",
            "system_name": "Pareto-frontier acquisition (mean vs uncertainty)",
            "system_description": "Given GP posterior, map each candidate x to a 2D objective vector (expected improvement or μ-based improvement, and an uncertainty measure σ(x) or alternative). Compute Pareto frontier over a dense grid to identify non-dominated (Pareto-efficient) candidate queries; decision-makers can sample from or choose on the frontier, providing a spectrum of exploration/exploitation trade-offs.",
            "application_domain": "Black-box optimization and active learning where explicit multi-objective trade-off between improvement and uncertainty is useful (human decision modelling, BO variants, experimental design).",
            "resource_allocation_strategy": "Generate set of Pareto-efficient candidate experiments balancing expected improvement and uncertainty; selection among them can be made deterministically (dominance) or stochastically (random sampling from Pareto set) to allocate limited experimental budget.",
            "computational_cost_metric": "Cost to evaluate μ(x) and chosen uncertainty measure at grid points; GP inference cost for conditioning (O(n^3)) dominates; Pareto-frontier approximation cost is primarily cost of making predictions over a dense grid (cheap relative to conditioning).",
            "information_gain_metric": "Uncertainty measure used as one objective (could be σ(x), entropy h(x), or distance-based z(x)); information gain is implicit via the chosen uncertainty metric rather than mutual information unless entropy-based uncertainty is used.",
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Provides the full Pareto set of trade-offs between exploitation (improvement) and exploration (uncertainty); choice among Pareto-efficient points determines the balance. The model interprets human choices relative to Pareto rationality (distance to frontier).",
            "diversity_mechanism": "Selecting different points along the Pareto frontier naturally yields diverse query choices spanning exploration-exploitation spectrum; random sampling from the frontier (De Ath) can increase diversity in selected experiments.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed number of trials; also applicable when many acquisition choices are cheap to compute but experiments are costly.",
            "budget_constraint_handling": "By exposing the Pareto set, the method allows allocating limited experimental budget to points that best match the chosen trade-off (e.g., pick more exploratory Pareto points early, exploitative later), and supports randomization or focused subregion sampling for batch allocation.",
            "breakthrough_discovery_metric": "Expected improvement (improvement over current best) used as the exploitation objective; 'breakthrough' implied by high improvement values on Pareto frontier.",
            "performance_metrics": "In this paper, Pareto-frontier analysis is used to interpret human decisions (number of Pareto-rational choices, distances to frontier) rather than to report optimization performance; literature shows sampling from Pareto frontier can outperform single-acquisition strategies in some settings (cited works).",
            "comparison_baseline": "Traditional single-objective acquisition functions (EI, PI, UCB) and random search; Pareto approaches can reproduce EI/PI as frontier points per Žilinskas & Calvin.",
            "performance_vs_baseline": "Cited results indicate Pareto sampling can produce a richer set of candidate decisions and random sampling from the frontier can outperform some acquisitions; the paper reports that treating mean and uncertainty as bi-objective reveals more candidate strategies than traditional acquisitions.",
            "efficiency_gain": "Not quantified here; conceptual gain is increased flexibility and potential to match human-like strategies; sampling from a precomputed Pareto set can be efficient since prediction cost is lower than conditioning cost.",
            "tradeoff_analysis": "Paper's central analytic contribution: frames human decisions as Pareto-rational on the improvement-uncertainty plane and investigates how different uncertainty metrics change Pareto sets and explain human exploratory deviations; finds predictive standard deviation often increases alignment with Pareto rationality.",
            "optimal_allocation_findings": "Key insight: exposing the Pareto frontier lets a rational agent choose dominant trade-offs; choice of uncertainty quantification (σ, entropy-based h(x), or distance z(x)) crucially affects which actions are Pareto-efficient and thus optimal under this framework; predictive σ(x) matched human behaviour best in this study.",
            "uuid": "e2475.3",
            "source_info": {
                "paper_title": "Uncertainty quantification and exploration-exploitation trade-off in humans",
                "publication_date_yy_mm": "2021-02"
            }
        },
        {
            "name_short": "Pareto sampling (De Ath)",
            "name_full": "Random sampling from Pareto frontier for acquisition",
            "brief_description": "An acquisition approach that randomly samples candidate points from the Pareto frontier of mean vs uncertainty to select experiments, providing a wider and potentially more diverse set of choices than single acquisition functions.",
            "citation_title": "Greed is Good: Exploration and Exploitation Tradeoffs in Bayesian Optimisation",
            "mention_or_use": "mention",
            "system_name": "Random Pareto-frontier sampling",
            "system_description": "Approximate Pareto frontier over GP predictions, then randomly sample a point from the frontier (or from a subregion) to obtain the next query; randomization can help avoid bias toward particular trade-offs and increase chance of discovering rewarding regions.",
            "application_domain": "Bayesian optimization and active learning where promoting diversity among candidate queries can improve exploration and avoid local optima.",
            "resource_allocation_strategy": "Allocation is done by stochastic selection from the Pareto set; this spreads resources across different compromise points between exploration and exploitation rather than committing to a single heuristic.",
            "computational_cost_metric": "Cost to compute Pareto frontier (predictions over grid) and random sampling; dominated by GP conditioning cost but sampling is cheap relative to conditioning; no heavy sampling of posterior paths required.",
            "information_gain_metric": "Implicit via the uncertainty objective used to define the Pareto frontier (σ, entropy, or distance-based measures); not explicitly mutual information unless entropy is used.",
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Random selection from Pareto frontier yields different balances across iterations; increases exploration diversity while still restricting choices to Pareto-efficient trade-offs.",
            "diversity_mechanism": "Explicit: randomness in selecting among frontier points promotes diversity of experiments and avoids repetitive exploitation.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed-trial budget; can be used in sequential or batched settings.",
            "budget_constraint_handling": "By randomizing among Pareto-efficient candidates, resources are spread across a diverse set of promising points, potentially improving robustness under tight budgets.",
            "breakthrough_discovery_metric": "High expected improvement values among frontier points; sampling can increase probability of selecting a breakthrough candidate by covering different trade-offs.",
            "performance_metrics": "Cited work indicates random Pareto sampling can outperform some traditional acquisitions on benchmark tasks; paper references these results qualitatively without numeric details.",
            "comparison_baseline": "Compared in cited literature to EI, PI, UCB and other single-acquisition strategies.",
            "performance_vs_baseline": "Reported in citations that random Pareto-frontier sampling can outperform other methods on certain problems, especially by improving exploration diversity.",
            "efficiency_gain": "Qualitative: increased chance of locating good regions under same evaluation budget due to diversified allocation; no numeric gain provided here.",
            "tradeoff_analysis": "Highlights trade-off between restricting choices to Pareto-efficient points (preserving rationality) and introducing randomness to promote exploration diversity; balances computational tractability with exploratory coverage.",
            "optimal_allocation_findings": "Stochastic sampling from the Pareto frontier is a practical strategy to diversify allocations and can empirically yield better exploration-exploitation outcomes than committing to a single acquisition heuristic.",
            "uuid": "e2475.4",
            "source_info": {
                "paper_title": "Uncertainty quantification and exploration-exploitation trade-off in humans",
                "publication_date_yy_mm": "2021-02"
            }
        },
        {
            "name_short": "Paria RS",
            "name_full": "Random scalarizations framework for multi-objective BO",
            "brief_description": "A flexible framework that uses random scalarizations to sample different parts of a multi-objective Pareto frontier, enabling focused or uniform exploration of trade-offs between objectives.",
            "citation_title": "A flexible framework for multi-objective bayesian optimization using random scalarizations",
            "mention_or_use": "mention",
            "system_name": "Random scalarizations for multi-objective Bayesian optimization",
            "system_description": "Creates scalar acquisition objectives by randomly sampling scalarization weights (or otherwise focusing on subregions) to convert a multi-objective Pareto problem into scalarized single-objective problems; running BO on these scalarizations yields points across the Pareto frontier and supports targeted exploration of subregions.",
            "application_domain": "Multi-objective optimization and active learning where multiple conflicting goals (e.g., improvement vs uncertainty, multiple tasks) must be balanced.",
            "resource_allocation_strategy": "Sample scalarization weights (possibly biased toward a subregion) and optimize the resulting scalar objective to select next point; repeated sampling allocates resources across different Pareto trade-offs.",
            "computational_cost_metric": "Cost per scalarized BO run (GP conditioning + scalar acquisition optimization); multiple scalarizations increase compute proportional to number of scalar samples but can be parallelized or batched.",
            "information_gain_metric": "Depends on the scalarized acquisition used (could be EI, UCB, or information-based objectives); no single fixed information metric—flexible by design.",
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Exploration/exploitation balance controlled by the choice of scalarization and the underlying acquisition function used per scalarization; randomness in scalarization promotes exploration across trade-offs.",
            "diversity_mechanism": "Explicit via random scalarizations which force consideration of diverse Pareto trade-offs; can focus sampling in specific regions to control diversity.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed-trial budget; computational budget if many scalarizations are sampled.",
            "budget_constraint_handling": "Control number and focus of scalarizations to respect computational and experimental budgets; sample more coarse-grained scalarizations when budget is tight.",
            "breakthrough_discovery_metric": "Scalarized objective value or Pareto dominance; breakthroughs correspond to points with large scalarized improvement or novel Pareto-optimal performance.",
            "performance_metrics": "Reported in cited work as effective at covering Pareto front and enabling targeted search; no numeric details in this paper.",
            "comparison_baseline": "Compared in literature to single-objective acquisitions and Pareto sampling methods.",
            "performance_vs_baseline": "Claimed flexibility and improved coverage of Pareto frontier versus single fixed scalarization; empirical benefits depend on scalarization sampling design.",
            "efficiency_gain": "Enables focused allocation of evaluations to particular subregions of interest, potentially improving discovery of high-impact trade-offs under limited budgets.",
            "tradeoff_analysis": "Provides a practical mechanism to trade off computational cost (number of scalarizations) versus diversity/coverage of Pareto frontier; can bias allocation toward promising regions if prior preferences exist.",
            "optimal_allocation_findings": "Random scalarizations offer a tunable way to allocate experiments across Pareto trade-offs; choose sampling density and focus to match experimental/resource constraints and discovery priorities.",
            "uuid": "e2475.5",
            "source_info": {
                "paper_title": "Uncertainty quantification and exploration-exploitation trade-off in humans",
                "publication_date_yy_mm": "2021-02"
            }
        },
        {
            "name_short": "Thompson Sampling (TS)",
            "name_full": "Thompson Sampling via sampling from GP posterior",
            "brief_description": "A randomized sequential strategy that samples a function realization from the GP posterior and selects its optimizer as the next query, implicitly trading off exploration and exploitation by posterior sampling.",
            "citation_title": "An information-theoretic analysis of Thompson sampling",
            "mention_or_use": "mention",
            "system_name": "Thompson Sampling (GP posterior sampling)",
            "system_description": "At each iteration draw a sample function from the GP posterior, optimize that sampled function to get candidate x, then query the true function at x; repeat. TS can be biased toward exploitation of high-probability optima in the posterior; -greedy variants add uniform random exploration with probability ε.",
            "application_domain": "Bandit problems, sequential optimization, active learning where randomized exploration is acceptable or beneficial.",
            "resource_allocation_strategy": "Allocation is driven by sampled posterior hypotheses: points that are optimal under a plausible posterior draw are evaluated, implicitly allocating resources to hypotheses proportional to posterior probability.",
            "computational_cost_metric": "Cost to sample functions from GP posterior (sampling methods for scalable sampling cited) and to optimize sampled function; cost scales with number of samples drawn and dimensionality.",
            "information_gain_metric": "Implicit via posterior sampling; not explicitly optimizing mutual information but serves to explore according to posterior uncertainty.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Randomized: exploration emerges from variability in posterior samples; exploitation occurs when posterior concentrates; can incorporate ε-greedy to force more exploration.",
            "diversity_mechanism": "Randomness in posterior samples yields diversity over iterations; no explicit diversity regularizer.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed number of queries; sampling cost affects computational budget for selection.",
            "budget_constraint_handling": "Adjust number of posterior samples or adopt approximate sampling to limit selection-time compute; ε-greedy variants can tune allocation under evaluation budgets.",
            "breakthrough_discovery_metric": "Probability of sampling a posterior draw that yields a high-improvement candidate; breakthroughs occur when a sampled hypothesis points to a previously untested high-value region.",
            "performance_metrics": "Literature notes TS can be biased toward exploitation; -greedy TS variants recommended to improve exploration; no new numerical metrics in this paper.",
            "comparison_baseline": "Compared to UCB, EI, random search and other BO acquisition strategies in the literature.",
            "performance_vs_baseline": "Cited analyses indicate TS may be biased toward exploitation and may benefit from injected random exploration; performance depends on sampling fidelity and problem structure.",
            "efficiency_gain": "Potentially efficient with low parameter tuning, but requires scalable GP sampling methods to be computationally practical (cited works propose efficient samplers).",
            "tradeoff_analysis": "TS offers a principled randomized exploration strategy whose exploration-exploitation balance depends on posterior uncertainty; computational cost of accurate sampling vs benefits of stochastic exploration is noted.",
            "optimal_allocation_findings": "TS allocates resources proportional to posterior probability of being optimal; for balanced exploration, augmentations (ε-greedy or other) or careful sampling are recommended.",
            "uuid": "e2475.6",
            "source_info": {
                "paper_title": "Uncertainty quantification and exploration-exploitation trade-off in humans",
                "publication_date_yy_mm": "2021-02"
            }
        },
        {
            "name_short": "GP-UCB / Randomized GP-UCB",
            "name_full": "Gaussian Process Upper Confidence Bound and Randomized GP-UCB",
            "brief_description": "An optimistic acquisition (GP-UCB) that selects points by μ(x)+√β σ(x), with β controlling exploration, and recent randomized variants sample β from a distribution to diversify exploration-exploitation behavior.",
            "citation_title": "Randomised Gaussian Process Upper Confidence Bound for Bayesian Optimisation",
            "mention_or_use": "mention",
            "system_name": "GP-UCB and Randomized GP-UCB",
            "system_description": "GP-UCB selects next point maximizing an upper confidence bound combining mean and scaled standard deviation; β can be scheduled or tuned. Randomized GP-UCB approaches sample β randomly per iteration (or from a distribution) to explore a range of optimistic trade-offs and avoid poor fixed β choices.",
            "application_domain": "Sequential decision-making and BO where controlling the optimism/exploration parameter is crucial (human modelling, engineering optimization).",
            "resource_allocation_strategy": "Allocates experiments to points with high optimistic estimates; randomized β spreads allocation across different optimism levels across iterations, improving robustness.",
            "computational_cost_metric": "Cost of evaluating μ and σ across candidate set and optimizing UCB; dominated by GP conditioning O(n^3). Randomized β sampling adds negligible cost.",
            "information_gain_metric": "Not explicitly mutual information; uses uncertainty bonus (σ) as heuristic proxy for information value.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Explicit via the β parameter: larger β increases exploration weight on σ(x); scheduling or random sampling of β changes allocation dynamically.",
            "diversity_mechanism": "Randomized β induces diversity in selected points across iterations by varying optimism level; no explicit diversity penalty.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed number of queries; β scheduling can be adapted to budget (explore early, exploit later).",
            "budget_constraint_handling": "GP-UCB can use scheduled β to manage exploration over the given budget; randomized β can be used to search for good β values without manual tuning.",
            "breakthrough_discovery_metric": "High upper-confidence predicted value (μ + √β σ) indicating potentially high-yield or high-uncertainty high-yield regions.",
            "performance_metrics": "Cited works show GP-UCB models human strategies well in some tasks; randomized GP-UCB reported to outperform traditional GP-UCB on a range of problems in cited literature (no numerical specifics in this paper).",
            "comparison_baseline": "Compared to EI, PI, TS, and other acquisition schemes in cited works.",
            "performance_vs_baseline": "References indicate randomized GP-UCB can lead to improved performance by effectively exploring parameter settings and avoiding local optima; no concrete numeric comparisons provided here.",
            "efficiency_gain": "Randomization can improve identification of high-value regions under the same budget by diversifying exploration-exploitation trade-offs.",
            "tradeoff_analysis": "Paper highlights that GP-UCB's β controls trade-off and that scheduling or random sampling of β is important to balance exploration cost vs exploitation benefit under finite budgets.",
            "optimal_allocation_findings": "Tuning or randomizing β is crucial for effective allocation; using an adaptive or randomized optimism parameter helps avoid poor fixed-policy outcomes.",
            "uuid": "e2475.7",
            "source_info": {
                "paper_title": "Uncertainty quantification and exploration-exploitation trade-off in humans",
                "publication_date_yy_mm": "2021-02"
            }
        },
        {
            "name_short": "Sandholtz IBO",
            "name_full": "Inverse Bayesian Optimization (IBO)",
            "brief_description": "A probabilistic framework to infer a human (or agent) acquisition function from observed search paths in black-box optimization tasks, enabling reverse-engineering of allocation strategies.",
            "citation_title": "Modeling human decision-making in spatial and temporal systems",
            "mention_or_use": "mention",
            "system_name": "Inverse Bayesian Optimization",
            "system_description": "Given an observed sequence of queries and outcomes (a human search path), infer the underlying acquisition function by probabilistic (non-parametric Bayesian) inference over a family of candidate acquisition functions. Requires assumptions (e.g., convexity, smoothness) on the objective in prior formulations.",
            "application_domain": "Cognitive modelling of human active search, and potentially automated imitation or transfer of human allocation strategies to algorithmic agents.",
            "resource_allocation_strategy": "Not an allocation algorithm per se, but infers the allocation heuristic used by an agent so that one can replicate or analyze resource allocation decisions; the inferred acquisition can then be used to allocate experiments similarly.",
            "computational_cost_metric": "Cost of Bayesian inference over acquisition-function space, which can be large; also depends on assumptions and the parametrization of candidate acquisition functions.",
            "information_gain_metric": "Inference objective maximizes posterior likelihood of observed sequence under candidate acquisition functions; not directly an information-gain-based allocation metric.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Reveals whether the observed agent used directed exploration, random exploration, UCB-like bonuses, EI-like improvement focus, or Pareto-like multi-objective trade-offs.",
            "diversity_mechanism": null,
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Observation-based (human limited by experimental shots); inference must account for limited-horizon behaviours.",
            "budget_constraint_handling": "Inference conditions on observed finite sequences; requires restrictive function assumptions to make the inverse problem tractable as noted in the paper.",
            "breakthrough_discovery_metric": "Not directly applicable; can identify whether agent prioritized breakthrough (high improvement) or information-gathering in allocation decisions.",
            "performance_metrics": "IBO provides posterior over acquisition functions; Sandholtz notes restrictive assumptions required; no numeric performance reported here.",
            "comparison_baseline": "Approach contrasted with assuming a known acquisition function (forward BO) or other behaviorist models.",
            "performance_vs_baseline": "Not assessed quantitatively in this paper; cited work reports methodological constraints limiting generality.",
            "efficiency_gain": "Enables understanding/human-model transfer which may improve algorithmic allocation strategies when human heuristics are effective, but inference can be computationally demanding and assumption-sensitive.",
            "tradeoff_analysis": "Highlights inference cost and model assumptions trade-offs; notes inverse problem is hard in general black-box settings and often requires smoothness/convexity assumptions.",
            "optimal_allocation_findings": "Inference can identify allocation heuristics (bias toward exploration or exploitation) used by humans, but practical application is limited by required assumptions and inference complexity.",
            "uuid": "e2475.8",
            "source_info": {
                "paper_title": "Uncertainty quantification and exploration-exploitation trade-off in humans",
                "publication_date_yy_mm": "2021-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Entropy search for information-efficient global optimization",
            "rating": 2,
            "sanitized_title": "entropy_search_for_informationefficient_global_optimization"
        },
        {
            "paper_title": "Predictive entropy search for efficient global optimization of black-box functions",
            "rating": 2,
            "sanitized_title": "predictive_entropy_search_for_efficient_global_optimization_of_blackbox_functions"
        },
        {
            "paper_title": "Max-value Entropy Search for Efficient Bayesian Optimization",
            "rating": 2,
            "sanitized_title": "maxvalue_entropy_search_for_efficient_bayesian_optimization"
        },
        {
            "paper_title": "Bi-objective decision making in global optimization based on statistical models",
            "rating": 2,
            "sanitized_title": "biobjective_decision_making_in_global_optimization_based_on_statistical_models"
        },
        {
            "paper_title": "Greed is Good: Exploration and Exploitation Tradeoffs in Bayesian Optimisation",
            "rating": 1,
            "sanitized_title": "greed_is_good_exploration_and_exploitation_tradeoffs_in_bayesian_optimisation"
        },
        {
            "paper_title": "A flexible framework for multi-objective bayesian optimization using random scalarizations",
            "rating": 2,
            "sanitized_title": "a_flexible_framework_for_multiobjective_bayesian_optimization_using_random_scalarizations"
        },
        {
            "paper_title": "Randomised Gaussian Process Upper Confidence Bound for Bayesian Optimisation",
            "rating": 2,
            "sanitized_title": "randomised_gaussian_process_upper_confidence_bound_for_bayesian_optimisation"
        },
        {
            "paper_title": "An information-theoretic analysis of Thompson sampling",
            "rating": 2,
            "sanitized_title": "an_informationtheoretic_analysis_of_thompson_sampling"
        },
        {
            "paper_title": "Active learning for multi-objective optimization",
            "rating": 2,
            "sanitized_title": "active_learning_for_multiobjective_optimization"
        },
        {
            "paper_title": "Modeling human decision-making in spatial and temporal systems",
            "rating": 1,
            "sanitized_title": "modeling_human_decisionmaking_in_spatial_and_temporal_systems"
        }
    ],
    "cost": 0.02320775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Uncertainty quantification and exploration-exploitation trade-off in humans</p>
<p>Antonio Candelieri 
Department of Economics, Management and Statistics
University of Milano-Bicocca
Italy</p>
<p>Andrea Ponti 
Department of Computer Science
Systems and Communication
University of Milano-Bicocca
Italy</p>
<p>Francesco Archetti 
Department of Computer Science
Systems and Communication
University of Milano-Bicocca
Italy</p>
<p>Uncertainty quantification and exploration-exploitation trade-off in humans
024B333455747C54A16988F120E686E3Active learningPareto analysisuncertainty quantificationhuman learningexploration/exploitation dilemma
The main objective of this paper is to outline a theoretical framework to analyse how humans' decision-making strategies under uncertainty manage the trade-off between information gathering (exploration) and reward seeking (exploitation).A key observation, motivating this line of research, is the awareness that human learners are amazingly fast and effective at adapting to unfamiliar environments and incorporating upcoming knowledge: this is an intriguing behaviour for cognitive sciences as well as an important challenge for Machine Learning.The target problem considered is active learning in a black-box optimization task and more specifically how the exploration/exploitation dilemma can be modelled within Gaussian Process based Bayesian Optimization framework, which is in turn based on uncertainty quantification.The main contribution is to analyse humans' decisions with respect to Pareto rationality where the two objectives are improvement expected and uncertainty quantification.According to this Pareto rationality model, if a decision set contains a Pareto efficient (dominant) strategy, a rational decision maker should always select the dominant strategy over its dominated alternatives.The distance from the Pareto frontier determines whether a choice is (Pareto) rational (i.e., lays on the frontier) or is associated to "exasperate" exploration.However, since the uncertainty is one of the two objectives defining the Pareto frontier, we have investigated three different uncertainty quantification measures and selected the one resulting more compliant with the Pareto rationality model proposed.The key result is an analytical framework to characterize how deviations from "rationality" depend on uncertainty quantifications and the evolution of the reward seeking process.</p>
<p>Introduction 1.1 Motivation</p>
<p>When a humanas well as an algorithmis asked to search for a target under limited resources (trials, time, effort, or money), he/she has to sequentially perform queries in a decision/action space and observe the associated outcomes or rewards.This activity at all levels of behaviour and time scales of decision-making requires dealing with the exploration-exploitation dilemma: exploitation means using the knowledge collected so far to get closer to the target (i.e., maximizing immediate reward), while exploration means investing resources to acquire more knowledge to update one's beliefs and potentially upset the current belief (i.e., maximizing immediate information gain).The dilemma arises because of the need to make decisions under uncertainty: decisions allowing for increasing knowledge do not necessarily lead to the greatest immediate reward (Wilson et al., 2020a;Wilson et al., 2014).The trade-off between explorative and exploitative behaviours characterizes many disciplines (Berger-Tal et al., 2014) and has originated a multidisciplinary framework that applies to humans, animals, and organizations.The analysis of the strategies implemented by humans in dealing with uncertainty has been an actively researched topic (Schulz et al., 2015;Gershman, 2018;Schulz and Gershman, 2019).A key observation, motivating this line of research, is also the awareness that human learners are amazingly fast and effective at adapting to unfamiliar environments and incorporating upcoming knowledge: this is an intriguing behaviour for cognitive sciences as well as an important challenge for Machine Learning.The reference task considered in this paper is the optimization problem:
x * = argmax 𝑥∈Ω⊂ℜ 𝑑 𝑓(𝑥)(1)
with () is black box, meaning that its analytical form is not given, no derivatives are available and the value of () can be only known pointwise through expensive and noisy evaluations.Finally, Ω denotes the search space, usually box bounded.</p>
<p>We consider sequential optimization to solve (1).At each iteration , the agent/algorithm chooses a location  () and the associated function value is observed, possibly perturbed by noise,  () = ( () ) + .The goal is to get close to  * within a limited number, , of trials.A related goal is to maximize the Average Cumulative Reward (ACR) over the , of trials, that is  () = 1  ∑  ()    =1</p>
<p>. Recently, the Bayesian optimization framework (BO) (Shahriari, et al., 2015;Frazier, 2018;Archetti and Candelieri, 2019) has become one of the most efficient method for solving (1), which is a common problem in many application domains ranging from robotics and engineering design to biomedicine and Automated Machine Learning (Archetti and Candelieri, 2019).BO is based on a probabilistic surrogate model approximating (), usually a Gaussian Process (GP), and an acquisition function (aka infill criterion or utility function) which balances exploration/exploitation to implement sample efficiency.Moreover, BO is linked to the ongoing discussion in cognitive science as to whether also humans' strategies are sample efficient: (Borji and Itti, 2013;Candelieri et al., 2020) have been arguing, based on empirical evidence, that strategies adopted by humans in solving global optimization problems can be associated to BO.Evidence of this is captured in Figure 1: compared to other global optimization methods, the estimated location of  * provided by BO is the closest to the humans' ones.Candelieri et al., 2020).</p>
<p>A caveat is that although the BO is compliant with humans' strategies over the entire sequential process, as shown empirically in (Candelieri et al., 2020), it is not necessarily sufficient to capture the explorationexploitation balance performed by humans at each decision step.The working hypothesis of this paper is that this misalignment between Bayesian model of active learning in optimization and humans' strategies might be due to some shortcomings in the general BO's modelling framework: first, the approximation of () depending on decisions and associated outcomes, then the uncertainty quantification.</p>
<p>GP modelling and Bayesian learning, first proposed in (Kruschke, 2008;Griffiths et al., 2008) have emerged as central paradigms in modelling human learning, where the GP model is used to approximate the outcome of the next decision conditioned on previous decisions and observed outcomes.Fitting a GP requires to choose, a priori, a kernel as covariance function; different kernels are available, each one implying a different characterization for the approximation of ().As already stated in (Wilson et al., 2015), it was demonstrated that "GPs with standard kernels struggle on function extrapolation problems that are trivial for human learners".Indeed, they proposed a kernel learning framework to reverse engineer the inductive biases of human learners across a set of behavioural experiments, gaining psychological insights and extrapolating in humanlike ways that go beyond traditional kernels.Different approximations of () can lead to completely different decisions, due to the optimization of the acquisition function whose value depends on the GP model.Many acquisition functions are available; a basic differentiation can be between two "families": the improvement-based acquisition functions, searching for the optimum value  * = ( * ), and the information-based acquisition functions, searching for the  * .This distinction is critically important because the two families relate to different quantifications of the uncertaintyas discussed later in the paperwhich has been proved to be a key concept also in theories of cognition and emotion (Gershman, 2019).A recent contribution (Bertram et al., 2020) investigates the relationship between entropy and the emotional state and the perception of uncertainty.</p>
<p>Uncertainty quantification is not the only modelling issue related to acquisition functions.Recently, some papers proposed to generalize the acquisition mechanism by considering the exploration-exploitation in the framework of the theory of rational decision making under uncertainty (Žilinskas and Calvin, 2019).The analysis of the Pareto frontier in the space of the GP's predictive mean and standard deviation offers a set of Pareto-efficient decisions which can be significantly more than those selected through "traditional" acquisition functions.According to Pareto-based rationality model, if a decision set contains a Pareto efficient (dominant) strategy, a rational decision maker should always select the dominant strategy over its dominated alternatives.</p>
<p>Still, according to a famous Schumpeter quotation (Schumpeter, 1954) traditional decision making under risk "has a much better claim to being called a logic of choice than a psychology of value" and indeed deviations from Pareto rational behaviour have been documented in domains like economics, business, but also Reinforcement Learning.</p>
<p>The analysis of violations of dominance in decision-making has become mainstream economics under the name of behavioural economics and prospect theory (Kahneman, 2011): rather than being labelled "irrational", they are just not well described by the rational-agent model.Would a different uncertainty quantification restore rationality?Another key point addressed in this paper is that although BO is the most compliant approach to humans' searching strategies over an entire search task, it could be not sufficiently representative of the exploration-exploitation balance performed by humans at each decision step.</p>
<p>Contributions of this paper</p>
<p>The main contribution of this paper is a methodological framework to analyse how humans' decision-making strategies under uncertainty balance information gathering (exploration) and reward seeking (exploitation).</p>
<p>The target problem considered in this paper is active learning in a black-box optimization task and more specifically how this balancing can be represented by different uncertainty quantifications and exploration/exploitation trade-off in the framework of Gaussian Process modelling.This required a critical analysis of a large body of results from cognitive science and their relationship with learning and optimization.This has also spawned a more ambitious task: while most of the previous works addressed how people assess the information value of possible queries, in this paper we rather address the issue of the perception of probabilistic uncertainty itself.This objective has required the development of a software environment for gathering data about human behaviour and analysing them, whose use can be helpful, beyond the specific case, to analyse human strategies in learning problems.</p>
<p>The computational results and their analysis allow to formulate at least a tentative answer to the following research questions:</p>
<p>• Do humans always make "rational" choices (i.e., Pareto optimal decisions between the improvement expected and uncertainty) or, in some cases, they "exasperate" exploration?• Do different uncertainty quantification measures lead to different classifications of humans' decisions?</p>
<p>And which uncertainty quantification measure make humans "more rational"?• Do deviations from (Pareto) "rationality" and switches towards "exasperated" exploration depend on the evolution of the optimization process measured as the reward collected over the limited number of trials available?</p>
<p>Related works</p>
<p>In Sect.1.1 we have briefly introduced the issue of uncertainty quantification in humans and its relationship with learning and optimization.Coherently with the centrality of this issue, several research lines have emerged.Here we provide a more specific analysis of the prior work and significant recent results.</p>
<p>An early contribution (Cohen et al., 2007) analyses how humans manage the trade-off between exploration and exploitation in non-stationary environments and links the issue to the Multi Armed Bandit (MAB) problem and Reinforcement Learning.Successively, (Wilson et al., 2014) demonstrates that humans use both random and directed exploration in a two-arms bandit task.More recently, (Gershman and Uchida, 2019) show how directed exploration in humans amounts to adding an "uncertainty bonus" to estimated reward values and how this brings to the Upper Confidence Bound acquisition function in MAB (Auer et al., 2002) and BO (Srinivas et al., 2012).The same approach is elaborated in (Schulz and Gershman, 2019), who distinguish between irreducible uncertainty related to the reward stochasticity and uncertainty which can be reduced through information gathering.In the former the decision strategy is random search while for the latter is directed exploration which attaches an uncertainty bonus to each decision value.This distinction mirrors the one in Machine Learning between aleatoric uncertaintydue to the stochastic variability inherent in querying () and epistemic uncertaintydue to the lack of knowledge about the actual structure of ()which can be reduced by collecting more information.The same point is argued in (Gershman, 2019) which associates random exploration to Thompson Sampling, which consists in drawing a sample of () from the GP model and then make the next decision according to the optimization of that sample (Wilson et al., 2020b).</p>
<p>Recent results in the line of research related to brain science are discussed in (Gershman, 2017;Friston et al., 2014).The former analyses the dopamine response in terms of Bayesian Reinforcement Learning, while the second analyses how entropy and expected utility account, respectively, for exploratory and exploitative behaviour, arguing that the dynamics of beliefs updates are consistent with the psychology and anatomy of the dopaminergic system.Moreover, it has been explored how the neuromodulator dopamine plays a central role in encoding and updating of beliefs: "the level of dopamine is related to the discrepancy between observed and expected reward, known as the reward prediction error (RPE), which serves as a learning signal for updating reward expectations.On the other hand, dopamine also appears to participate in various probabilistic computations, including the encoding of uncertainty and the control of uncertainty-guided exploration" (Gershman and Uchida, 2019).These results have been correlated with molecular analysis (Blanco et al., 2015) where it is empirically demonstrated that the carrier of the MET allele in COMT gene will be advantaged in managing the exploration/exploitation dilemma, especially in making choices that maximize long term payoffs.</p>
<p>In the BO research community, recent papers proposed to generalize the acquisition mechanism by considering the exploration-exploitation dilemma as a bi-objective optimization problem: minimizing the predictive mean (associated to exploitation) while maximizing uncertainty, typically the predictive standard deviation (associated to exploration).For instance, in (Žilinskas and Calvin, 2019) the important result is that two wellknown acquisition functions, specifically Probability of Improvement (PI) and Expected Improvement (EI), are special cases of this bi-optimization framework, because they lay on the Pareto frontier of all the predictive mean and standard deviation pairs computed fortheoreticallyevery possible decision.The mean-variance framework has been also considered in (Iwazaki et al., 2020), for multi-task, multi-objective and constrained optimization scenarios.(De Ath et al., 2019;De Ath et al., 2020) show that taking a decision by randomly sampling from the Pareto frontier can outperform other acquisition functions.The main motivation is that the Pareto frontier offers a set of Pareto-efficient decisions wider than that allowed by "traditional" acquisition functions.(Paria et al., 2020) introduce a sampling which can be focused on a specific subregion of the Pareto frontier.</p>
<p>The issue of deviations from Pareto optimality has become a central topic in behavioural economics from the seminal work in (Tversky and Kahneman, 1989) to (Kourouxous and Bauer, 2019) which identifies the most common causes for violations of dominance, namely framing (i.e., presentation of a decision problem), reference points (i.e., a form of prior expectation), bounded rationality and emotional responses.Emotions impact decisions by influencing preferences, triggering ad hoc heuristics, or causing attention shifts to catastrophic outcomes.</p>
<p>A recent important contribution is (Sandholtz, 2020) which tackles the problem to infer, given the observed search path generated by a human subject in the execution of a black box optimization task, the unknown acquisition function underlying the sequence.It is to be remarked that this analysis requires restrictive assumptions on the objective function like convexity and smoothness which cannot be assumed in black box problems.For the solution of this problem, referred to as Inverse Bayesian Optimization (IBO), a probabilistic framework for the non-parametric Bayesian inference of the acquisition function is proposed, performed on a set of possible acquisition functions.</p>
<p>This paper is organized as follows.Section 2 introduces the basic definitions about Gaussian Process regression and how different acquisition functions deal with the exploration/exploitation dilemma and use different uncertainty quantification.Section 3 introduces three specific uncertainty quantification measures, the definition of Pareto optimality and the differences induced by different Gaussian Process modelling options (i.e., kernels) and uncertainty quantifications measures.Section 4 introduces the experimental framework used for data collection about the strategies applied by humans and the proposed analytical framework.Section 5 describes the relevant results obtained by the application of the analytical framework.Finally, Section 6 outlines the conclusions about this study and the perspective of future works.</p>
<p>Materials and methods</p>
<p>Gaussian Process regression</p>
<p>A GP is a random distribution over functions :  ⊂ ℜ  → ℜ denoted with ()~((), (,  ′ )) where () = (()): Ω → ℜ is the mean function of the GP and (, '): Ω × Ω → ℜ is the kernel or covariance function.One way to interpret a GP is as a collection of correlated random variables, any finite number of which have a joint Gaussian distribution, so () can be considered as a sample drawn from a multivariate normal distribution.In Machine Learning, GP modelling is largely used for both classification and regression tasks (Williams and Rasmussen, 2006;Gramacy, 2020), providing probabilistic predictions by conditioning () and  2 () on a set of available data/observations.Let denote with X 1: = { () } =1,…, a set of  locations in Ω ⊂ ℜ  and with  1: = {( () ) + } =1,.., the associated function values, possibly noisy with  a zero-mean Gaussian noise ~(0,  2 ).Then () and  2 () are the GP's posterior predictive mean and standard deviation, conditioned on X 1: and  1: according to the following equations:
𝜇(𝑥) = k(𝑥, X 1:𝑛 ) [K + 𝜆 2 𝐼] −1 𝑦 1:𝑛 (2) 𝜎 2 (𝑥) = 𝑘(𝑥, 𝑥) − k(𝑥, X 1:𝑛 ) [K + 𝜆 2 𝐼] −1 k(X 1:𝑛 , 𝑥)(3)
where k(, X 1: ) = {(,  () )} =1,…, and K ∈ ℜ × with entries K  = ( () ,  () ).</p>
<p>The choice of the kernel establishes prior assumptions over the structural properties of the underlying (aka latent) function (), specifically its smoothness.However, almost every kernel has its own hyperparameters to tuneusually via Maximum Log-likelihood Estimation (MLE) or Maximum A Posteriori (MAP)for reducing the potential mismatches between prior smoothness assumptions and the observed data.Common kernels for GP regressionconsidered in this paperare:
• Squared Exponential: 𝑘 𝑆𝐸 (𝑥, 𝑥′) = 𝑒 − ‖𝑥−𝑥′‖ 2 2ℓ 2 • Exponential: 𝑘 𝐸𝑋𝑃 (𝑥, 𝑥′) = 𝑒 − ‖𝑥−𝑥′‖ ℓ • Power-exponential: 𝑘 𝑃𝐸 (𝑥, 𝑥′) = 𝑒 − ‖𝑥−𝑥′‖ 𝑝 ℓ 𝑝 • Matérn3/2: 𝑘 𝑀3/2 (𝑥, 𝑥′) = (1 + √3 ‖𝑥−𝑥′‖ ℓ ) 𝑒 − √3 ‖𝑥−𝑥′‖ ℓ • Matérn5/2: 𝑘 𝑀5/2 (𝑥, 𝑥′) = [1 + √5 ‖𝑥−𝑥′‖ ℓ + 5 3 ( ‖𝑥−𝑥′‖ ℓ ) 2 ] 𝑒 − √5 ‖𝑥−𝑥′‖ ℓ
The main well-known disadvantage of GP modelling is its cubic complexity due to the inversion of the matrix
[K + 𝜆 2 𝐼].</p>
<p>Dealing with the exploration-exploitation dilemma</p>
<p>Global optimization methods differ one from another in how they generate the next decision (i.e., location)  (+1) .To do this, BO fits a GP according to (2-3) and where X 1: = { () } =1,…, and  1: = { () } =1,.., are the two sequences of, respectively, decisions made and associated observed outcomes.Then, an acquisition function, combining GP's () and (), is optimized to obtain  (+1) , while dealing with the explorationexploitation trade-off.</p>
<p>Improvement-based acquisition functions</p>
<p>Acquisition functions belonging to this "family" are aimed at searching for  * = max ∈Ω⊂ℜ  ()instead of searching for  * = argmax ∈Ω⊂ℜ  () -and are characterized by "mixing" GP's mean and standard deviation to balance between exploitation and exploration in the choice of  (+1) .Common acquisition functions from this family are Probability of Improvement (PI) (Kushner, 1964), Expected Improvement (EI) (Močkus, 1975) and GP Confidence Bound (i.e., Upper Confidence Bound, UCB, for minimization) (Srinivas et al., 2012):
𝑃𝐼(𝑥) = 𝚽 ( 𝜇(𝑥) − 𝑦 + 𝜎(𝑥) ) 𝐸𝐼(𝑥) = (𝜇(𝑥) − 𝑦 + ) 𝚽 ( 𝜇(𝑥) − 𝑦 + 𝜎(𝑥) ) + 𝜎(𝑥)𝜙 ( 𝜇(𝑥) − 𝑦 + 𝜎(𝑥) ) 𝑈𝐶𝐵(𝑥) = 𝜇(𝑥) + √𝛽𝜎(𝑥)
where  and  are the standard normal cumulative distribution function (cdf) and the standard normal probability density function (pdf).Since () and () are biased to exploration, an additional parameter  can be included in the numerator of the arguments of  and  to increase exploration (Brochu et al., 2010).</p>
<p>Alternatively, an exploration enhanced EI has been recently (Berk et al., 2018) while (Preuss and Von Toussaint, 2018) (deterministically) alternates between maximization of EI and maximization of GP's predictive variance to switch between exploitative and explorative decisions.GP-UCB, it is also classified as an optimistic policy, because it chooses  (+1) depending on the most optimistic value for () under the current GP.From a cognitive point of view, (Wu et al., 2018) analysed the human search strategy, under a limited number of trials, concluding that GP-UCB offers the best option for modelling the exploitation-exploration trade-off adopted by the humans.Furthermore, contrary to () and ()at least to their original formulations -GP-UCB is more flexible, thanks to its own hyperparameter , whose value can be set up to give a different relevance to exploitation and exploration in choosing  (+1)  or it can be scheduled to adapt the balance between exploitation and exploration along the optimization process.</p>
<p>While it is empirically suggested to apply a decreasing schedule for  (i.e., preferring exploration at the beginning and then moving towards exploitation), in (Srinivas et al., 2012) a convergence proof is given for an increasing scheduling of , aimed at avoiding to getting stuck at local optima.However, (Berk et al., 2020), has recently obtained better performance by randomly sampling  from a given distribution.They proved that this allows to identify more suitable  values and to outperform "traditional" GP-CB on a range of synthetic and real-world problems.</p>
<p>Information-based acquisition functions</p>
<p>Information-based acquisition functions (Hennig and Schuler, 2012) relies on an information-theoretic perspective, that is choosing  (+1) , given  1: = ( 1: ,  1: ), to maximize the information about the location of  * = argmax ∈Ω⊂ℜ  ().</p>
<p>Information gain measures how informative is a set of observations,  1: = ( 1: ,  1: ), and it is defined as the mutual information between  (+1) and  1: :
𝐼(𝑦 1:𝑛 ; 𝑦 (𝑛+1) ) = 𝐻(𝑦 1:𝑛 ) − 𝐻(𝑦 1:𝑛 |𝑦 (𝑛+1) )
and where (()) = − ∫ () log ()  is the differential entropy of a generic distribution () and measures the amount of uncertainty in ().In the discrete case, that is related to a discrete random variable Α, differential entropy is defined as () = ∑ () log 1 () ∈Α .Two important acquisition functions from this family are Entropy Search (ES) (Hennig and Schuler, 2012) and Predictive Entropy Search (PES) (Hernandez-Lobato et al., 2014).Both use differential entropy to characterize the uncertainty about the location of the optimizer,  * .More specifically, the aim is to choose the next decision  which maximizes the expected uncertainty reduction:
𝐸𝑆(𝑥) = 𝐻(𝑝(𝑥 * |𝐷 1:𝑛 )) − 𝔼[𝐻(𝑝(𝑥 * |𝐷 1:𝑛 ∪ {𝑥, 𝑦}))] 𝑃𝐸𝑆(𝑥) = 𝐻(𝑝(𝑦|𝐷 1:𝑛 , 𝑥)) − 𝔼[𝐻(𝑝(𝑦|𝐷 1:𝑛 , 𝑥, 𝑥 * ))]
The main difference is that ES uses the expectation over ( * | 1: ), while PES uses expectation over (| 1: , ).They are anyway analytically intractable and are approximated via expensive computations which requires to sample a set of paths from the GP posterior, at each BO iteration, and compute their optima to estimate the differential entropy.Moreover, computational cost drastically increases with the dimensionality of the search space.Therefore, ES and PES are useful just in the case that () is extremely expensive to evaluate, so that the cost for sampling from GP can be considered negligible.Due to these limitations, the Max-value Entropy Search (MES) acquisition function has been recently proposed (Wang and Jegelka, 2017), where the uncertainty about  * is replaced with the uncertainty about  * :
𝑀𝐸𝑆(𝑥) = 𝐼({𝑥, 𝑦}; 𝑦 * |𝐷 1:𝑛 ) = 𝐻(𝑝(𝑦)|𝐷 1:𝑛 , 𝑥) − 𝔼[𝐻(𝑝(𝑦|𝐷 1:𝑛 , 𝑥, 𝑦 * ))]
MES requires to sample  * (instead of  * ) which can be done by sampling from the GP posterior or from a Gumbel distribution, as also proposed in (Wang and Jegelka, 2017).In (Wang et al., 2016) the relation between MES and other popular acquisition functions has been demonstrated, including ES, UCB and PI, which have anyway empirically underperformed MES on several optimization tasks.Although MES is still based on entropy, the estimation of its information gain via sampling is more efficient, because  * lays in a onedimensional space.Linked to sampling from GP posterior is Thompson Sampling (TS), which can be also considered as a sequential optimization strategy per-se.Iteratively, TS draws a path by sampling from the GP posterior and then minimize it to obtain  (+1) as a possible estimation of the location of  * .After  (+1) is observed, the GP is updated, and TS continues until a termination criterion is met.An analysis on TS has been recently proposed in (Russo &amp; Van Roy 2016), concluding that TS is biased towards exploitation and suggesting that an -greedy version of TS can lead to a better performance (i.e., randomly selecting  (+1) within the search space, with probability , or performing TS with probability 1 − ).An efficient sampling procedure has been recently proposed in (Hahn et al., 2019) (Wilson et al., 2020b).Sampling from GP posterior is at the basis of information-based acquisition functions, described in the following section.The distinction between the two families of acquisition functions is relevant in terms of computational cost but, more relevant at least in this paper, is their difference in terms of the uncertainty quantification, providing more options for modelling the uncertainty quantification made by a human.</p>
<p>The problem of uncertainty quantification</p>
<p>From the viewpoint of Machine Learning, uncertainty quantification plays a pivotal role in reduction of errors during learning, optimization and decision making.In (Abdar et al., 2020) a wide survey of different uncertainty quantification methods is provided, considering many application fields, such as computer vision (e.g., self-driving cars and object detection), image processing (e.g., image restoration), medical image analysis (e.g., medical image classification and segmentation), natural language processing (e.g., text classification, social media texts and recidivism risk-scoring).In decision making, uncertainty is usually associated to exploration: when the uncertainty is "larger" than the possible estimated improvement, then it could be more profitable to adopt an explorative behaviour and acquire more knowledge about ().From a modelling perspective, uncertainty can be split in aleatoric and epistemic (Der Kiureghian and Ditlevsen, 2009;Kendall and Gal, 2017), where the aleatoric uncertainty is randomness proper in the evaluation of () (usually named "noise") and cannot be reduced, while the epistemic uncertainty depends on the model and can be reduced by collecting more data.In many applications it could be interesting to separate the two types of uncertainty: in (Depeweg et al., 2018) two possible decompositions are described, based on two uncertainty quantification metrics that are variance and entropy.This means that informationbased and entropy-based acquisition functions use two different metrics to quantify uncertainty, leading to different trade-off between exploitation and exploration, given the same approximation of the objective function.</p>
<p>From an informalyet more intuitivepoint of view, uncertainty about a decision is the amount of lack of knowledge about it, increasing with the "distance" from decisions already performed and where "distance" can be any suitable metric to compare two decisions.When decisions are locations in a search space, as in this paper, any spatial distance can be considered: an example of this uncertainty quantification has been recently proposed in (Bemporad, 2020) which uses Radial Basis Functions (RBF) as surrogate model and an inverse distance weighting such that the proposed distance is zero at sampled points and grows in between.Although GP's predictive standard deviation usually shows a similar behaviour,it exhibit , in some situations , variance starvation (Wang et al., 2018), consisting in an underestimation of variance scale compared to mean scale which can significantly reduce exploration chances in some portions of the search space.Moreover, GP modelling could be also drastically affected by wrong choices about its priorkernel type, in primisresulting in possible misleading uncertainty quantification and, consequently, suboptimal exploration.This issue has been recently addressed in (Neiswanger and Ramdas, 2020), in which authors do not assume correctness of the GP prior and generate a confidence sequence for () function using martingale techniques.Cognitive theories of emotion define uncertainty as a cognitive component characterizing emotional states.Finally, when humans' decisions are analysed, there is still another relevant lack in mathematical methods for uncertainty quantification as recently demonstrated in (Schultz et al., 2019) and (Bertram et al., 2020), which have investigated the role of emotion in judgment, risk assessment, and decision making under uncertainty and the different kinds of entropy which can be used to quantify uncertainty in the Sharma-Mittal space of entropy measures.Emotional states are significantly connected with subjective uncertainty estimation.While emotions such as anger and pride are associated to low uncertainty, anxiety and curiosity are associated to high uncertainty.There is not -at the authors' knowledgeany mathematical "trick" to implement an emotionrelated uncertainty quantification in BO (whichever might be the analogous of "emotion" for an algorithm).Indeed, their conclusion is that emotional conditions have no effect on uncertainty appraisal.Sharma-Mittal entropy uses a parametrised family of surprise functions but effect on the entropy parameters driven by the difference between control and emotional conditions.</p>
<p>Analytical Framework</p>
<p>Definition of uncertainty quantification measures</p>
<p>Let  denotes the set of kernels to choose as GP's prior.In this study  = {  ,   ,   ,  3/2 ,  5/2 }.Let () denotes the improvement expected by querying the objective function at location , depending on the GPs' posterior (i.e., one GP for each kernel in ).Formally, () = () −  + , where  + = max =1,…, { () } because we are considering max ∈Ω⊂ℝ  ().Then, let denote with  the set of possible uncertainty quantification measures.In this paper we consider the following three alternatives:</p>
<p>• GP's predictive standard deviation, namely ().Typically adopted as uncertainty measure in the improvement-based acquisition functions.</p>
<p>• GP's differential entropy.For a GP it is given by (| 1: ) = 1 2 log det(K) +  2 log det(2), where K ∈ ℜ × with entries K  = ( () ,  () ), ∀  () ,  () ∈  1: (Williams and Rasmussen, 2006).However, the GP's differential entropy does not depend on the location , but it is just a scalar measure of the uncertainty of the overall GP posterior distribution.Thus, we introducefollowing this bullet lista location-dependent measure of entropy, denoted with ℎ(), as a possible entropy-based location dependent uncertainty quantification.With respect to the entropy-based uncertainty quantification, and starting from the GP's differential entropy formula, we define:
ℎ(𝑥) = 𝐻(𝑦|{𝑋 1:𝑛 ∪ {𝑥}}) = 1 2 log det(K′) + 𝑑 2 log det(2𝜋𝑒)(4)
where K′ ∈ ℜ (+1)×(+1) with entries K′  = ( () ,  () ), ∀  () ,  () ∈ { 1: ∪ {}}.This allows us to estimate the entropy-based uncertainty at any location  depending on all the previous decisions  1: .It is important to remark that, analogously to (), also ℎ() dependsindirectlyon both decisions and outcomes through the kernel function, whose hyperparameters are tuned depending on  1: .</p>
<p>• Distance from previous decisions, inspired from (Bemporad, 2020) and denoted by () :
𝑧(𝑥) = { 0 if ∃ 𝑥 (𝑖) ∈ 𝑋 1:𝑛 ∶ ‖𝑥 − 𝑥 (𝑖) ‖ 2 2 = 0 2 𝜋 tan −1 ( 1 ∑ 𝑤 𝑗 (𝑥) 𝑛 𝑗=1
) otherwise ( 5)
with 𝑤 𝑗 (𝑥) = 𝑒 −‖𝑥−𝑥 (𝑗) ‖ 2 2 ‖𝑥−𝑥 (𝑗) ‖ 2 2 .
Thus, () is zero at sampled points and grows in between; tan −1 is introduced to damp the growth of () when  is located far away from all sampled points.Contrary to () and ℎ(), which depend also on , the uncertainty quantification measure () depends only on  1: , that is it depends only on how decisions "cover" the search space, irrespectively to their outcomes  1: .Although () intentionally ignores a portion of collected knowledge (i.e., outcomes of decisions), as main advantage it does not suffer, "by design", from variance starvation.</p>
<p>In Figure 2 a simple 1D example is reported to show the differences between the three uncertainty quantification measures, given the same set of previous decisions and GP model.Just for visualization purposes, each uncertainty quantification measure has been scaled in [0,1].In this specific case, almost all the intervals between two successive decisions are "equally uncertain", according to () and ℎ(), where "equally uncertain" means that () ≈ (′)as well as ℎ() ≈ ℎ(′) -∀ ,  ′ ∉  1: .On the contrary, the value of () changes over the search space providing a different quantification of uncertainty at every location.</p>
<p>Pareto rationality</p>
<p>Given the GP conditioned on the decisions performed so far, it is possible to map the next decision  (+1) ∈ Ω whichever it isas a bi-objective choice, with objectives () and () ∈  (both to be maximized).</p>
<p>Pareto rationality is the theoretical framework to analyse multi-objective optimization problems where  objective functions  1 (), … ,   () where   (): → ℝ are to be simultaneously optimized in Ω ⊆ ℝ  .We use the notation () = ( 1 (), … ,   ()) to refer to the vector of all objectives evaluated at a location .The goal in multi-objective optimization is to identify the Pareto frontier of ().</p>
<p>To do this we need an ordering relation in ℝ  :  = ( 1 , … ,   ) ≼  ′ = ( 1 ′ , … ,   ′ ) if and only if   ≤   ′ for  = 1, … , .This ordering relation induces an order in Ω:  ≼ ′ if and only if () ≼ ( ′ ).We also say that ′ dominates  (strongly if ∃  = 1, … ,  for which   &lt;   ′ ).The optimal non-dominated solutions lay on the so-called Pareto frontier.The interest in finding locations  having the associated () on the Pareto frontier is clear: they represent the trade-off between conflicting objectives and are the only ones, according to the Pareto rationality, to be considered.In this paper  = 2, with  1 ()=() and  2 () = () ∈ .Both the objectives are not expensive to evaluate, therefore the Pareto frontier can be easily approximated by considering a fine grid of locations in Ω without the need to resort to methods approximating expensive Pareto frontiers within a limited number of evaluations, such as in (Zuluaga et al., 2013).Thus, we approximate our Pareto frontier by sampling a grid of  points in Ω, denoted by  ̂1: = { () } =1,…, , and then computing the associated pairs Ψ 1: = {(( () ), ( () ))} =1,…, .</p>
<p>It is important to remark that a large value of  is needed to have a good approximation of the Pareto frontier but this is not an issue because the computational cost is dominated by conditioning the GP on observations (i.e., ( 3 ), with  ≪ ) instead of making predictions (i.e., inference).The Pareto frontier can be approximated as:
𝒫(Ψ 1:𝑚 ) = {𝜓 ∈ Ψ 1:𝑚 : ∀ 𝜓′ ∈ Ψ 1:𝑚 𝜓 ≻ 𝜓 ′ }
where  = ((), ()) and ′ = ((′), (′)), and  ≻  ′ ⟺ () &gt; ( ′ ) ∧ () &gt; (′).Similar charts are reported in Figure 4 and Figure 5 for the other two uncertainty quantification measures.The only way to analyse how different uncertainty quantification measures can lead to completely different decisionseven if anyway Pareto rationalis to localize, within the search space Ω ⊂ ℜ  , the locations whose associated objectives lays on the Pareto frontier (namely, the Pareto set). Figure 6 reports just a 2D example, considering ten previous decisions (bold black crosses), five different kernels and three alternative uncertainty quantification measure.The black box function considered is Branin-Hoo.</p>
<p>Figure 6.Next decision depending on: (i) ten previous observations (bold crosses), (ii) uncertainty quantification measures (rows: (), ℎ() and ()), and kernels (columns: "gauss" for   , "exp" for   , "powexp" for   , "matern3_2" for  3/2 and "matern5_2" for  5/2 )</p>
<p>From the figure it is possible to notice that the region of locations associated to Pareto-rational decisions does not change so much depending on kernel, as well as by using () or ℎ().The most evident difference arises by using () as uncertainty quantification measure, because it allows to consider as Pareto-rational also decisions in the area around, approximately, the location ( 1 = 1;  2 = 11).This area is associated to a more explorative behaviour compared to the otherwhich is also identified by using the other two uncertainty quantification measuresmeaning that some explorative choice could be still considered Pareto rational when () = ().This is just an example for explanatory purposes, the hypothesis is investigated and validated in our analysis.Moreover, we have also to consider that humans, (Kahneman, 2011) could take non-Pareto-rational decisions, and it is therefore important to measure how much a decision can be considered "far from a Pareto-rational one".This issue is addressed and formalized in the next section.</p>
<p>Distance from the Pareto rationality</p>
<p>Every next decision,  (+1) , can be analysed according to the distance of its "image" (( (+1) ), ( (+1) )) from the Pareto frontier, computed as follows:
𝑑(𝜓 ̅ , 𝒫 ̅ ) = min 𝜓∈𝒫 ̅ {‖𝜓 ̅ − 𝜓‖ 2 2 }
where  ̅ = (( (+1) ), ( (+1) )) and  ̅ = (Ψ 1: ) ∪ { ̅ }.</p>
<p>This distance is computed for every choice among the five kernels and the three uncertainty quantification measures previously presented.The hypothesis is that humans are mostly Pareto rational, and they should therefore make decisions laying onor close tothe Pareto frontier.</p>
<p>We analyse the distances from all the 15 possible Pareto frontiers (5 kernels × 3 uncertainty quantification measures) and how they change along the optimization process.Figure 7 shows an example taken from our experimental results and anticipated here just for explanatory purposes.The 10 charts refer to as many black box optimization problems solved by a single human subject.At each iteration, and for each uncertainty quantification measure, the minimum distance between the associated Pareto frontier and the human decision is reported, irrespectively to the kernel.It is important to remark that distances cannot be compared in absolute terms, because the three different uncertainty measures can vary in very different ranges.However, it is possible to observe that distances result correlated in some cases and uncorrelated in others.</p>
<p>Finally, from the charts it is possible to notice that: (a) in some cases Pareto rationality is independent on the uncertainty quantification, such as for the problems: bukin6, goldpr, rastr, stybtang, but not in general; (b) a higher number of decisions are considered Pareto rational if () = (); (c) in some cases it is possible to observe a shift from Pareto-rationality to not-Pareto rationality (e.g., this is evident in for beale, goldpr and rastr).</p>
<p>Experimental setup 4.1 Data collection</p>
<p>To collect data about humans' strategies we have used a gaming application based on the implementation used in (Candelieri et al., 2020).Figure 8 shows the web-based Graphical User Interface (GUI) of our game, with a game play example.The game field, with previous decisions and observations, as well as the score and remaining "shots", are reported.The game can be arranged according to different goals/conditions.</p>
<p>• Game mode #1: searching for the location having the highest score;</p>
<p>• Game mode #2: searching for the location having the highest score, but given the additional information about its value; • Game mode #3: maximizing the cumulative score (sum of the scores of all the choices).This paper focuses only on the first game mode (i.e., Game mode #1), with the aim to formalize and validate an analytical framework which could be successively adopted to investigate how the different goals of the other two game modes can imply different the humans' strategies.Fourteen volunteers have been enrolled (among families and friends, which had no competences in computer science and/or optimization), asking for solving ten different tasks each (only for Game mode #1).Each task refers to a global optimization test function, which subjects "learn and optimize" by clicking at a location and observing the associated score (aka reward).For each task, every player has a maximum number of 20 clicks (decisions) available.The 10 global optimization test functions adopted are depicted in Appendix (A1).Since these functions are related to minimization problems, the score returned to the player is −(), translating them into maximization tasks.Finally, the game has been developed in R, specifically R-shiny for the web-based GUI.All the analytical components, described in the following, have been also developed in R as backend of the application.</p>
<p>Data analysis</p>
<p>This study analyses every single decision performed by every volunteer, and how it can be explained in terms of uncertainty quantification and Pareto rationality.The analysis is organized in three consecutive steps:</p>
<p>• Step 1: computing the number of Pareto rational decisions, depending on the uncertainty quantification measure, and comparing them.A decision is considered Pareto rational if the distance from the Pareto frontier is less than 10 −4 .This analysis step is summarized as follows:</p>
<p>1.For each player and each test problem do: 2. Initialize  = 3 (i.e., he first three decisions of each user and for each test problem cannot be analysed, because fitting a GP over a 2-dimensional search space requires at least three observations).3. Condition a GP for each one the kernels in  to the previous  decisions and observations performed by that player for that test problem.4. For each () ∈  do: 5. Use each one of the conditioned GPs to approximate the associated Pareto frontier by sampling a grid of  = 30 × 30 = 900 locations in Ω and then computing the associated locations in the - space.6. Map  (+1) into five associated locations  = (( (+1) ), ( (+1) ))one for each kernel and compute its minimum distance from the five Pareto frontiers and store it.7.  ←  + 1 and go to Step 3 8.End For each () ∈  9. End For each player and each test problem All the results are stored into a data table with columns: user_id, problem_id, n+1, uncertainty_measure, min_dist_from_Pareto_frontier.Finally, numbers of Pareto rational decisions are separately computed for each uncertainty quantification measure and aggregated by (a) players and (b) test problems.</p>
<p>• Step 2: computing the length of consecutive Pareto-rational decisions, depending on the uncertainty quantification measure, and comparing them.This analysis step uses the same data table previously computed but, instead of the number, the length of consecutive Pareto rational decisions is computed, separately for the three uncertainty measures, and aggregated by (a) players and (b) test problems.</p>
<p>• Step 3: depending on results from the two previous steps, the uncertainty quantification measure which allows to more frequently classify the humans' choices as Pareto rational is selected.Then, the relationship between the fact that the decision is Pareto rational and the reward collected so far by the user is investigated, with the aim to identify a possible motivation for over-explorative decisions (i.e., not-Pareto rational decisions).In our analysis reward is represented by the score immediately observed by the player implied by his/her own decision.Cumulative reward is therefore the sum of scores collected up to a given decision.Finally, the Average Cumulated Reward (ACR), up to a given decision, is computed as the arithmetic mean of the cumulated reward up to that decision:
𝐴𝐶𝑅 (𝑛+1) = 1 𝑛 ∑ 𝑦 (𝑖) 𝑛 𝑖=1
we use  (+1) to denote the average reward collected up to  just to be coherent with indexing, since this value is analysed in relation with the Pareto distance of decision  (+1) .The idea is that ACR could quantify the amount of "gratification" (high values of ACR) or "stress" (low values of ACR) experienced by the player in solving the test problem.The hypothesis is that lower values of ACR could be associated to not-Pareto rational decisions, induced by a sense of stress for the incapability to (further) improve the score.</p>
<p>Experimental results and their analysis</p>
<p>Results about analysis step 1</p>
<p>The main result from analysis step 1 is that using () as uncertainty quantification measure increases the number of Pareto rational decisions for some problems, over all the players.Figure 9 shows the number of players with respect to the percentage of Pareto rational decisions.A stacked histogram is provided for each test problem, comparing the distributions obtained for each one of the three uncertainty quantification measures.The increase in terms of number of Pareto rational decisions, by using () = (), is more evident for the test problems schwef and ackley.Moreover, the higher number of Pareto rational decisions, obtained by using () = (), is spread over all the players.Indeed, Figure 10 shows the distributions of the number of test problems with respect to the percentage of Pareto rational decisions.A stacked histogram is provided for each player, comparing the distributions obtained considering each one of the three uncertainty quantification measures.For almost all the subjects a higher number of test problems is solved by using a high percentage of Pareto rational decisions when () = ().</p>
<p>Results about analysis step 2</p>
<p>Results of analysis step 2 confirm those from the previous step.Choosing () = () leads to longer sequences of consecutive Pareto rational decisions, according to both the number of players for each test function (Figure 11) and the number of test functions for each player (Figure 12).</p>
<p>Results about analysis step 3</p>
<p>According to the results from the two previous steps, we can conclude that () is, among the uncertainty quantification measures considered, the one inducing the Pareto model with the highest representation power, that is the one maximizing the number of Pareto optimal decisions.Indeed, if we assume that every human's decision is Pareto rational, then () is the only optionat least among those consideredallowing us to get close to this rationality model.Therefore, we have selected () = () to perform the analysis step 3.</p>
<p>As the main result of this step, the value of the ACR can help to determine if the next decision  (+1) will be Pareto rational or not.More specifically, ACR resulted, on average, higher in the case of a Pareto rational decision on 8 out of the 10 test problems (in 4 cases, this difference is statistically significant, p-value&lt;0.05,U Mann-Whitney test).Only in one case (i.e., stybtang) the ACR is significantly higher for not-Pareto decisions (p-value&lt;0.001,U Mann-Whitney test).Results are reported in Table 1 and, for a more immediate comparison, also as boxplots in Figure 13.</p>
<p>Conclusions and perspectives</p>
<p>The main result of this paper is a methodological framework to collect and analyse data related to humans' decision-making strategies under uncertainty, specifically about how they balance gathering new information (exploration) and reward seeking (exploitation).To better model this balance, we have used a bi-objective setting and assumed that humans' choices might be more frequently located on the Pareto frontier (Pareto rational choices).Since one of the two objectives is uncertainty, we have analysed three uncertainty quantification measures to investigate which one would offer the best fit with the Pareto rationality model (i.e., the one maximizing the number of choices laying on the associated Pareto frontier).Thus, while most of previous research studies has investigated how people assess the information value of possible queries, we rather addressed the issue of the perception of probabilistic uncertainty itself.This problem is still an open question in Machine Learning and cognitive sciences and neither our results nor those prevailing in the rich literature about this issue provide unequivocal evidence about the underlying algorithms used by humans.Humans do not always make "rationale" choices (i.e., Pareto optimal decisions in the space of expected improvement and uncertainty) and in some cases, they "exasperate" exploration.The computational results and their analysis allow to formulate at least a tentative answer to why or rather in which conditions we observe deviations from "rationality" and switches towards "exasperated" exploration depending on the dynamics of the optimization process as represented by Average Cumulative Reward.</p>
<p>Next steps should be a probabilistic characterization of the sequence of decisions and a close analysis of the dynamics of how people change their behaviour.A big question, which we have only partially addressed in this paper, is whether this analysis sits well with the Paretian expected utility theory or should rather be developed along entirely different lines of inquiry as for instance those proposed in (Peters, 2019) bringing about new uncertainty quantification measures and a new family of Bayesian Optimization acquisition functions.</p>
<p>A2. Distances from Pareto frontiers for each player, by test function</p>
<p>The following 10 figuresone for each test functionreport the distances of each decision from the Pareto frontiers and for each player.</p>
<p>A3. Distances from Pareto frontiers for each test functions, by player</p>
<p>The following 14 figuresone for each playerreport the distances of each decision from the Pareto frontiers and with respect to each test function.</p>
<p>Figure 1 .
1
Figure 1.Density plot of the distance between the estimated location of  * provided by humans and those provided by different global optimization strategies: Random Search (RS), DIRECT, Generic Algorithms (GA), Particle Swarm Optimization (PSO), Simulated Annealing (SA) and Bayesian Optimization (from Candelieri et al., 2020).</p>
<p>Figure 2 .
2
Figure 2. Differences between the three uncertainty quantification measures considered on a simple 1D example.Top: a black box function (), eight observations, the GP's posterior mean and standard deviation.Bottom: the amount of location-dependent uncertainty given by the three uncertainty quantification measures (values are scaled in [0,1] for visualization purposes.</p>
<p>Figure 3
3
Figure 3 shows an example of Pareto frontier for () and () = ().First five charts, top-left to bottomright, depict Ψ 1: and the associated (Ψ 1: ) for each kernel in  = {  ,   ,   ,  3/2 ,  5/2 }, separately.The last chart (bottom-right) compares only the five Pareto frontiers, better highlighting the role of the GP kernel.For this example, () is the Branin-Hoo (Jekel and Haftka, 2019) function in Ω: [−5; 10] × [0; 15],  = 1976 (related to a grid 76 × 26, obtained by using a step of 0.2 on each dimension).</p>
<p>Figure 3 .
3
Figure 3. Pareto frontiers obtained by using the GP's posterior standard deviation as uncertainty quantification (i.e., () = ()).Five different kernels are used to fit as many GPs, leading to as many Pareto frontiers.Last chart (bottom-right) depicts the five frontiers all together for an easier comparison.</p>
<p>Figure 4 .
4
Figure 4. Pareto frontiers obtained by using an entropy-based uncertainty quantification (i.e., () = ℎ()).Five different kernels are used to fit as many GPs, leading to as many Pareto frontiers.Last chart (bottom-right) depicts the five frontiers all together for an easier comparison.</p>
<p>Figure 5 .
5
Figure 5. Pareto frontiers obtained by using the distance-based uncertainty quantification (i.e., () = ()).Five different kernels are used to fit as many GPs, leading to as many Pareto frontiers.Last chart (bottom-right) depicts the five frontiers all together for an easier comparison.</p>
<p>Figure 7 .
7
Figure 7. Distance, at each iteration, of the next decision from three different Pareto frontiers, one for each uncertainty quantification measure (), ℎ() and ().All the fifteen charts are related to as many black box optimization tasks performed by a single human subject.</p>
<p>Figure 8 .
8
Figure 8. Web-based Graphical User Interface (GUI) of our game: a game play example</p>
<p>Figure 9 .
9
Figure 9. Number of players with respect to percentage of decisions classified as Pareto rational, separately for the three uncertainty quantification measures.One chart for each test problem.</p>
<p>Figure 10 .
10
Figure 10.Number of test problems with respect to percentage of decisions classified as Pareto rational, separately for the three uncertainty quantification measures.One chart for each player.</p>
<p>Figure 11 .
11
Figure 11.Number of players with respect to length of consecutive Pareto rational decisions, separately for the three uncertainty quantification measures.One chart for each test function.</p>
<p>Figure 12 .
12
Figure 12.Number of test functions with respect to length of consecutive Pareto rational decisions, separately for the three uncertainty quantification measures.One chart for each player.</p>
<p>Figure 13 .
13
Figure 13.Comparing ACR between Pareto and not-Pareto rational decisions: a boxplot for each test problem, data aggregated over the decisions of all the players.</p>
<p>Table 1
1
Results: comparing ACR between Parto and not-Pareto rational decisions.Results are per test function, over all players and decisions.
ACR ParetoACR not-ParetoU Mann-Whitney testtest functionmean (sd)mean (sd)p-valueackley-178.916 (95.209)-188.259 (96.440)0.409beale-54603.570 (111582.400)-53695.820 (111111.300)0.170branin-209.413 (224.193)-380.112 (268.029)&lt;0.001<em>bukin6-482.495 (203.231)-995.122 (473.103)&lt;0.001</em>goldpr-20.601 (21.869)-24.551 (14.557)0.030<em>griewank-7.791 (4.122)-8.485 (5.610)0.9015levy-92.548 (69.508)-114.750 (88.185)0.1363rastr-309.380 (145.268)-421.085 (178.801)&lt;0.001</em>schwef-8859.593 (3905.512)-9426.500 (2749.823)0.816stybtang157.231 (100.050)319.434 (227.555)&lt;0.001*
AcknowledgementsWe greatly acknowledge the DEMS Data Science Lab, Department of Economics Management and Statistics (DEMS), for supporting this work by providing computational resources.Availability of data and material (data transparency):Both data and code for reproducing analysis and results of this paper are available at the following link: https://github.com/acandelieri/humans_strategies_analysis.Conflicts of interest/Competing interests (include appropriate disclosures)Ethics approval (include appropriate approvals or waivers)Informed consent was given in accordance with the Helsinki declaration.Appendix A A1. The ten test problemsThe ten global optimization test functions used in this study, including their analytical formulations, search spaces and information about optimums and optimizers, can be found at the following link: https://www.sfu.ca/~ssurjano/optimization.htmlSince they are minimization test functions, we have returned −() as score in order to translate them into the maximization problems depicted in Figure14.
M Abdar, F Pourpanah, S Hussain, D Rezazadegan, L Liu, M Ghavamzadeh, . . Nahavandi, S , arXiv:2011.06225A Review of Uncertainty Quantification in Deep Learning: Techniques, Applications and Challenges. 2020arXiv preprint</p>
<p>Bayesian Optimization and Data Science. F Archetti, A Candelieri, 2019Springer International Publishing</p>
<p>Finite-time analysis of the multiarmed bandit problem. P Auer, N Cesa-Bianchi, P Fischer, Machine learning. 472-32002</p>
<p>Global optimization via inverse distance weighting and radial basis functions. A Bemporad, Computational Optimization and Applications. 7722020</p>
<p>The exploration-exploitation dilemma: a multidisciplinary framework. O Berger-Tal, J Nathan, E Meron, D Saltz, PloS one. 94e956932014</p>
<p>J Berk, S Gupta, S Rana, S Venkatesh, arXiv:2006.04296Randomised Gaussian Process Upper Confidence Bound for Bayesian Optimisation. 2020arXiv preprint</p>
<p>Exploration enhanced expected improvement for bayesian optimization. J Berk, V Nguyen, S Gupta, S Rana, S Venkatesh, Joint European Conference on Machine Learning and Knowledge Discovery in Databases. ChamSpringer2018, September</p>
<p>Emotion, entropy evaluations and subjective uncertainty. L Bertram, E Schulz, M Hofer, J D Nelson, 42nd Annual Virtual Meeting of the Cognitive Science Society. PsychArchives2020. 2020</p>
<p>A frontal dopamine system for reflective exploratory behavior. N J Blanco, B C Love, J A Cooper, J E Mcgeary, V S Knopik, W T Maddox, Neurobiology of learning and memory. 1232015</p>
<p>Bayesian optimization explains human active search. A Borji, L Itti, Advances in neural information processing systems. 2013</p>
<p>A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. E Brochu, V M Cora, N De Freitas, arXiv:1012.25992010arXiv preprint</p>
<p>Modelling human active search in optimizing black-box functions. A Candelieri, R Perego, I Giordani, A Ponti, F Archetti, 10.1007/s00500-020-05398-2Soft Comput. 242020</p>
<p>Should I stay or should I go? How the human brain manages the tradeoff between exploitation and exploration. J D Cohen, S M Mcclure, A J Yu, Philosophical Transactions of the Royal Society B: Biological Sciences. 3622007. 1481</p>
<p>G De Ath, R M Everson, J E Fieldsend, A A Rahat, arXiv:2002.01873$\epsilon $-shotgun: $\epsilon $-greedy Batch Bayesian Optimisation. 2020arXiv preprint</p>
<p>G De Ath, R M Everson, A A Rahat, J E Fieldsend, arXiv:1911.12809Greed is Good: Exploration and Exploitation Tradeoffs in Bayesian Optimisation. 2019arXiv preprint</p>
<p>Decomposition of uncertainty in Bayesian deep learning for efficient and risk-sensitive learning. S Depeweg, J M Hernandez-Lobato, F Doshi-Velez, S Udluft, International Conference on Machine Learning. 2018</p>
<p>A Der Kiureghian, O Ditlevsen, Aleatory or epistemic? Does it matter? Structural Safety. 200931</p>
<p>Bayesian optimization. P I Frazier, Recent Advances in Optimization and Modeling of Contemporary Problems. INFORMS2018</p>
<p>The anatomy of choice: dopamine and decision-making. K Friston, P Schwartenbeck, T Fitzgerald, M Moutoussis, T Behrens, R J Dolan, Philosophical Transactions of the Royal Society B: Biological Sciences. 3692014. 1655. 20130481</p>
<p>Dopamine, inference, and uncertainty. S J Gershman, Neural Computation. 29122017</p>
<p>Deconstructing the human algorithms for exploration. S J Gershman, Cognition. 1732018</p>
<p>Uncertainty and exploration. Decision. S J Gershman, 20196277</p>
<p>Believing in dopamine. S J Gershman, N Uchida, Nature Reviews Neuroscience. 20112019</p>
<p>Surrogates: Gaussian Process Modeling, Design, and Optimization for the Applied Sciences. R B Gramacy, 2020CRC Press</p>
<p>ed) Cambridge handbook of computational cognitive modelling. T L Griffiths, C Kemp, J B Tenenbaum, Sun R (2008Cambridge University PressCambridgeBayesian models of cognition</p>
<p>Efficient sampling for Gaussian linear regression with arbitrary priors. P R Hahn, J He, H F Lopes, Journal of Computational and Graphical Statistics. 2812019</p>
<p>Entropy search for information-efficient global optimization. P Hennig, C J Schuler, Journal of Machine Learning Research. 132012</p>
<p>Predictive entropy search for efficient global optimization of black-box functions. J M Hernandez-Lobato, M W Hoffman, Z Ghahramani, Advances in Neural Information Processing Systems (NIPS). 2014</p>
<p>Mean-Variance Analysis in Bayesian Optimization under Uncertainty. S Iwazaki, Y Inatsu, I Takeuchi, arXiv:2009.081662020arXiv preprint</p>
<p>Fortified Test Functions for Global Optimization and the Power of Multiple Runs. C F Jekel, R T Haftka, arXiv:1912.105752019arXiv preprint</p>
<p>Thinking, Fast and Slow. Daniel Kahneman, 2011Farrar, Straus and GirouxNew York</p>
<p>What uncertainties do we need in Bayesian deep learning for computer vision?. A Kendall, Y Gal, Advances in Neural Information Processing Systems (NIPS). 2017</p>
<p>Violations of dominance in decision-making. T Kourouxous, T Bauer, Business Research. 1212019</p>
<p>Bayesian approaches to associative learning: From passive to active learning. J K Kruschke, Learn Behav. 3632008</p>
<p>A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise. H J Kushner, Optimization techniques IFIP technical conference. Berlin, HeidelbergSpringer1964. 1975On Bayesian methods for seeking the extremum</p>
<p>Learning and choosing in an uncertain world: An investigation of the explore-exploit dilemma in static and dynamic environments. D J Navarro, B R Newell, C Schulze, Cognitive psychology. 852016</p>
<p>Uncertainty quantification using martingales for misspecified Gaussian processes. W Neiswanger, A Ramdas, arXiv:2006.073682020arXiv preprint</p>
<p>A flexible framework for multi-objective bayesian optimization using random scalarizations. B Paria, K Kandasamy, B Póczos, Uncertainty in Artificial Intelligence. PMLR2020. August</p>
<p>The ergodicity problem in economics. O Peters, Nature Physics. 122019</p>
<p>Global optimization employing Gaussian process-based Bayesian surrogates. R Preuss, U Von Toussaint, Entropy. 2032012018</p>
<p>An information-theoretic analysis of Thompson sampling. D Russo, B Van Roy, The Journal of Machine Learning Research. 1712016</p>
<p>Modeling human decision-making in spatial and temporal systems (Doctoral dissertation. N Sandholtz, Science: Department of Statistics and Actuarial Science. 2020</p>
<p>Exploring the space of human exploration using Entropy Mastermind. E Schulz, L Bertram, M Hofer, J D Nelson, Proceedings of the 2019 Conference of the Cognitive Science Society. The Cognitive Science Society. the 2019 Conference of the Cognitive Science Society2019. 2019</p>
<p>Structured, uncertainty-driven exploration in real-world consumer choice. E Schulz, R Bhui, B C Love, B Brier, M T Todd, S J Gershman, 2019Proceedings of the National Academy of Sciences116</p>
<p>The algorithmic architecture of exploration in the human brain. E Schulz, S J Gershman, Current opinion in neurobiology. 552019</p>
<p>Assessing the perceived predictability of functions. E Schulz, J B Tenenbaum, D N Reshef, M Speekenbrink, S Gershman, 2015CogSci</p>
<p>History of economic analysis. J A Schumpeter, 1954Psychology Press</p>
<p>Taking the human out of the loop: A review of Bayesian optimization. B Shahriari, K Swersky, Z Wang, R P Adams, N De Freitas, Proceedings of the IEEE. 10412015</p>
<p>Information-theoretic regret bounds for gaussian process optimization in the bandit setting. N Srinivas, A Krause, S M Kakade, M W Seeger, IEEE Transactions on Information Theory. 5852012</p>
<p>Rational choice and the framing of decisions. A Tversky, D Kahneman, Multiple criteria decision making and risk analysis using microcomputers. Berlin, HeidelbergSpringer1989</p>
<p>Max-value Entropy Search for Efficient Bayesian Optimization. Z Wang, S Jegelka, International Conference on Machine Learning. 2017</p>
<p>Batched large-scale bayesian optimization in high-dimensional spaces. Z Wang, C Gehring, P Kohli, S Jegelka, International Conference on Artificial Intelligence and Statistics. 2018</p>
<p>Optimization as estimation with Gaussian processes in bandit settings. Z Wang, B Zhou, S Jegelka, Artificial Intelligence and Statistics. 2016. May</p>
<p>Gaussian processes for machine learning. C K Williams, C E Rasmussen, 2006MIT press24Cambridge, MA</p>
<p>Balancing exploration and exploitation with information and randomization. R C Wilson, E Bonawitz, V D Costa, R B Ebitz, Current Opinion in Behavioral Sciences. 2020a38</p>
<p>J T Wilson, V Borovitskiy, A Terenin, P Mostowsky, M P Deisenroth, arXiv:2002.09309Efficiently sampling functions from Gaussian process posteriors. 2020barXiv preprint</p>
<p>A G Wilson, C Dann, C Lucas, E P Xing, Advances in neural information processing systems. 2015The human kernel</p>
<p>Humans use directed and random exploration to solve the explore-exploit dilemma. R C Wilson, A Geana, J M White, E A Ludvig, J D Cohen, Journal of Experimental Psychology: General. 143620742014</p>
<p>Generalization guides human exploration in vast decision spaces. C M Wu, E Schulz, M Speekenbrink, J D Nelson, B Meder, Nat Hum Behav. 2122018</p>
<p>Bi-objective decision making in global optimization based on statistical models. A Žilinskas, J Calvin, Journal of Global Optimization. 7442019</p>
<p>Active learning for multi-objective optimization. M Zuluaga, G Sergent, A Krause, M Püschel, International Conference on Machine Learning. 2013. February</p>            </div>
        </div>

    </div>
</body>
</html>
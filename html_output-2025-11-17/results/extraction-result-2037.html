<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2037 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2037</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2037</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-49.html">extraction-schema-49</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-278782199</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.14970v1.pdf" target="_blank">Self-Evolving Curriculum for LLM Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Reinforcement learning (RL) has proven effective for fine-tuning large language models (LLMs), significantly enhancing their reasoning abilities in domains such as mathematics and code generation. A crucial factor influencing RL fine-tuning success is the training curriculum: the order in which training problems are presented. While random curricula serve as common baselines, they remain suboptimal; manually designed curricula often rely heavily on heuristics, and online filtering methods can be computationally prohibitive. To address these limitations, we propose Self-Evolving Curriculum (SEC) , an automatic curriculum learning method that learns a curriculum policy concurrently with the RL fine-tuning process. Our approach formulates curriculum selection as a non-stationary Multi-Armed Bandit problem, treating each problem category (e.g., difficulty level or problem type) as an individual arm. We leverage the absolute advantage from policy gradient methods as a proxy measure for immediate learning gain. At each training step, the curriculum policy selects categories to maximize this reward signal and is updated using the TD(0) method. Across three distinct reasoning domains: planning, inductive reasoning, and mathematics, our experiments demonstrate that SEC significantly improves models’ reasoning capabilities, enabling better generalization to harder, out-of-distribution test problems. Additionally, our approach achieves better skill balance when fine-tuning simultaneously on multiple reasoning domains. These findings highlight SEC as a promising strategy for RL fine-tuning of LLMs. 2</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2037.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2037.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SEC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Evolving Curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automatic, online curriculum-learning method for RL fine-tuning of LLMs that models curriculum selection as a non-stationary multi-armed bandit and uses batch-wise absolute advantage as the curriculum reward, updating arm values with TD(0) and sampling via a Boltzmann policy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>reinforcement learning-based (non-stationary MAB using absolute advantage as reward)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Training problems are partitioned into discrete categories (arms). At each RL training step the method (1) samples categories according to a Boltzmann softmax over Q_t(c) scores, (2) samples problems uniformly from the chosen categories to build a batch, (3) runs on-policy rollouts and computes advantages, (4) defines the per-category curriculum reward r(c) as the average absolute advantage |A_t| over rollouts for problems from that category, and (5) updates Q_t(c) with a TD(0)/exponential moving average: Q_{t+1}(c)=α r_t(c) + (1−α)Q_t(c). Temperature τ and learning rate α control exploration and Q updates.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>Text-based reasoning tasks: planning puzzles (Countdown, Zebra puzzles), inductive reasoning (ARC-1D), and mathematics (MATH / AMC / AIME datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Multi-step text reasoning problems requiring multi-step planning, look-ahead and backtracking (Countdown, Zebra), rule induction and generalization from examples (ARC-1D), and multi-step symbolic mathematical deduction (MATH dataset including AIME). Difficulty controlled by problem parameters (number of integers/entities/string length/dataset difficulty levels); tasks require combining sub-skills (search, deduction, induction, formatting).</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>random curriculum (uniform sampling across training problems), difficulty-ordered (fixed schedule from easy→hard or reverse), and multi-task random sampling (for multi-domain experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported metrics are pass@1 accuracy (average over 8 generations) on in-distribution (ID) and out-of-distribution (OOD) held-out splits. Key reported improvements relative to the random curriculum: ~13% relative on Countdown, ~21% on Zebra puzzles, ~22% on ARC-1D, and up to ~33% on AIME24. Concrete examples: for the Qwen2.5-7B model, Zebra OOD improved from 0.32 → 0.36 (≈11% relative) and AIME OOD from 0.14 → 0.18 (≈27% relative). Additional cross-algorithm results (Countdown): PPO random ID/OOD = 0.621 / 0.159 vs SEC PPO ID/OOD = 0.750 / 0.224; RLOO random ID/OOD = 0.821 / 0.465 vs SEC RLOO ID/OOD = 0.859 / 0.494 (Table 2). Table 1 and figures report SEC as the top or near-top method across tasks and model sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>No explicit wall-clock convergence time comparisons; reported dynamics show SEC adapts difficulty over training and avoids mid-training performance collapses in mixed-task training. Training steps used: Qwen2.5-3B experiments typically 240 gradient steps (math 240), Qwen2.5-7B 120 steps; multi-task fine-tuning for 3×240=720 steps. The paper reports that SEC yields steadier learning curves and maintains performance where random curricula can collapse mid-training (multi-task setting), but does not provide precise episode/step-to-threshold numbers beyond plotted curves.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>SEC improves generalization to harder OOD test sets across domains: the paper reports consistent OOD accuracy gains (examples above), showing better transfer to hardest difficulty levels reserved as OOD for Countdown, Zebra, ARC-1D and improved scores on held-out math test sets (MATH500, AMC22-23, AIME24).</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>In multi-task experiments (SEC-2D), SEC balances training across domains and difficulty combinations, maintaining stable performance on all tasks; random multi-task sampling showed a mid-training collapse in one task, indicating poorer balancing of task exposure.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not explicitly evaluated; SEC implicitly prioritizes problems near the model's Zone-of-Proximal-Development (success rate ≈0.5) by maximizing expected absolute advantage, but the paper does not perform an explicit prerequisite graph discovery or dependency analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>SEC does not generate new intermediate tasks; it selects from predefined categories/arms (difficulty levels or task-type × difficulty combinations). The paper notes SEC relies on predefined categories and does not produce novel bridging tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>SEC itself does not rely on an LLM for curriculum generation. The paper mentions that, when difficulty annotations are unavailable, difficulty can be estimated by prompting an expert LLM (citing prior work), but the authors do not use LLM-generated curricula and do not report LLM-specific failures in curriculum generation within their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>No precise cost numbers reported for SEC. The paper states SEC avoids the heavy overhead of online filtering (which repeatedly generates extra on-policy samples to estimate difficulty) and is therefore designed to be more computationally efficient than online filtering approaches; however, exact GPU-hours or FLOPs comparisons are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>No human-expert qualitative evaluation of curricula was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>SEC — a non-stationary MAB curriculum using absolute advantage as a local learning-gain proxy and TD(0) updates — consistently improves RL fine-tuning of LLMs across planning, inductive reasoning, and mathematics, particularly in OOD generalization and multi-task balancing. SEC outperforms fixed difficulty schedules and random sampling baselines, is robust across RL algorithms (GRPO/GRPO variants, PPO, RLOO), but relies on predefined curriculum categories and introduces a few tunable hyperparameters (α, τ).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2037.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2037.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SEC-2D</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Evolving Curriculum (2D variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-dimensional variant of SEC where each arm corresponds to a (problem type × difficulty level) pair, enabling simultaneous curriculum control across multiple reasoning domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>reinforcement learning-based (non-stationary MAB over 2D category grid)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Constructs arms for each unique combination of problem type and difficulty (e.g., 3 problem types × 3 difficulty levels → 9 arms). Uses the same absolute-advantage reward, TD(0) Q updates, and Boltzmann sampling as SEC to pick arms and form mixed-task batches during RL fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>Mixed multi-domain text reasoning (Countdown, Zebra, ARC-1D combined into a single mixed training set).</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Mixed-domain training requires the model to learn differing reasoning modalities simultaneously; each subtask retains its own multi-step and compositional complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>random multi-task curriculum (uniform sampling across tasks × difficulties)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Pass@1 accuracy on ID and OOD splits for each task; SEC-2D consistently outperforms random multi-task sampling across Countdown, Zebra, and ARC-1D (exact per-task numbers are reported in Figure 4; authors highlight better average and OOD accuracy and a more stable learning curve).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>SEC-2D maintains stable OOD performance across training steps and avoids the mid-training collapse observed with random multi-task sampling; no explicit numeric speed-to-convergence reported.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>SEC-2D improves OOD generalization across multiple tasks compared to random multi-task curriculum and provides improved overall multi-task balance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>SEC-2D dynamically shifts sampling proportions across arms to maintain task balance; authors report it better balances learning objectives across domains than random sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not evaluated separately; adaptive selection implicitly favors tasks with medium success rates but no explicit prerequisite graph extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>No generation of intermediate tasks; selection limited to predefined task×difficulty arms.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Not applicable (no LLM used to generate curricula in SEC-2D).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not explicitly reported; multi-task experiments used longer total training (3×240=720 steps for Qwen2.5-3B), but SEC-2D's overhead beyond standard SEC is primarily the same Q updates and sampling and was not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>None reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>SEC-2D demonstrates that SEC's MAB formulation generalizes to multi-dimensional curricula (task × difficulty), yielding more stable multi-task learning and higher OOD accuracy than random multi-task sampling, and effectively balancing skills across domains.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2037.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2037.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random Curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random uniform sampling curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline curriculum where training problems are sampled uniformly at random from the available training set (no adaptive ordering).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>random ordering (uniform sampling over dataset or arms)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>At each training batch, problems are sampled uniformly at random across the training set or across categories; used as the primary baseline for SEC experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>Same text reasoning domains used in experiments (planning, ARC-1D, mathematics).</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Matches the complexity of the task domains (see SEC entry).</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared directly against SEC and difficulty-ordered (fixed) curricula in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Pass@1 accuracy reported per task; used as reference for relative improvement claims (e.g., SEC reported relative % improvements over random: 13% Countdown, 21% Zebra, 22% ARC-1D, up to 33% AIME24). Also used in alternative RL algorithm comparisons (see Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>In multi-task settings, random sampling showed a mid-training collapse on some tasks (unstable multi-task learning) whereas SEC maintained stable performance; no exact convergence-step numbers beyond plotted curves.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Random curriculum gives reasonable baselines but SEC usually improves OOD generalization; random performed worse on hardest OOD splits in tested domains.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Random sampling provides uniform diversity by construction, but can fail to balance learning progress across tasks leading to instabilities in multi-task training.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Minimal additional cost beyond standard RL fine-tuning sampling; used as computational baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>None reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Random uniform sampling is a reasonable baseline but is suboptimal: SEC yields consistent gains over random in OOD generalization and multi-task stability; random curricula can show mid-training collapses in mixed-task settings.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2037.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2037.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Difficulty-Ordered Curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Difficulty-ordered (fixed schedule) curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hand-designed curriculum that presents problems in a fixed difficulty schedule (typically easy→hard or reverse), used as a baseline to evaluate adaptive curricula.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>hand-crafted by difficulty ordering (fixed schedule)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Problems are presented according to their annotated difficulty levels following a predefined schedule (e.g., easy to hard). The paper also tests a deliberately poor reverse ordering (hard→easy) to show adverse effects.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>Planning, inductive reasoning, mathematics datasets with annotated difficulty levels.</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Varies by level; a fixed schedule does not adapt to evolving model competence or to OOD evaluation needs.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared against SEC and random curricula. The paper includes an explicit 'reverse' curriculum experiment showing severe degradation when ordering is misaligned with learning progress.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported to be often suboptimal; the fixed difficulty schedule sometimes leads to poorer generalization than random sampling and SEC (e.g., reverse curriculum produced poor OOD generalization in a controlled Countdown experiment). Exact numeric values are reported in Table 1 but vary by task and model size.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Fixed schedules can cause inefficient use of training steps (e.g., spending excessive time on easy problems), limiting exposure to harder problems; no precise step counts reported.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Often worse than SEC and sometimes worse than random, particularly when schedule is ill-matched to model capability (reverse curriculum).</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not adaptive; diversity is determined by schedule and may be limited when stuck on particular difficulty levels.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not assessed.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Low (predefined ordering); no extra online computation beyond sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>Not reported (design is heuristic-driven).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Fixed difficulty-ordered curricula can be brittle: a poorly chosen schedule (e.g., reverse) substantially reduces final performance and generalization, highlighting the value of adaptive, online curricula such as SEC.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2037.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2037.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Online Filtering (Bae et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Online difficulty filtering for reasoning oriented reinforcement learning (referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recently proposed curriculum approach that filters training problems online by repeatedly generating solutions to estimate difficulty, used as a comparison point in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>online filtering by generating on-policy solutions to estimate problem difficulty</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Continuously generates solutions for candidate problems during training to measure difficulty and filter/weight problems accordingly; this requires repeated extra on-policy generation and is computationally intensive.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>Reasoning-oriented RL for LLMs (similar domains to SEC experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Targets reasoning problems where difficulty estimation via rollouts is meaningful; typically multi-step.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Described as related work and compared qualitatively to SEC; authors argue online filtering is more computationally heavy than SEC.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper (referenced as separate prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Paper notes online filtering methods are computationally prohibitive due to continuous generation of additional on-policy samples; no quantitative runtime comparison provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>No (filters existing problems rather than generating new ones).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Not discussed in this paper; the referenced method's repeated on-policy generation could be expensive and sensitive to rollout quality.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Described as substantial/high (qualitative claim), since it requires repeated extra on-policy sample generation for difficulty estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Online filtering approaches estimate per-instance difficulty via extra rollouts and can adapt curricula but at high computational cost; SEC is proposed as a cheaper alternative that uses advantage-based local rewards rather than separate heavy difficulty estimation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2037.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2037.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AdaRFT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AdaRFT (adaptive curriculum by difficulty-level ordering - referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adaptive curriculum approach that adjusts curriculum difficulty based on recent reward signals, but which relies on explicit difficulty-level ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>adaptive difficulty adjustment driven by recent model reward signals (requires ordered difficulty levels)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Adapts training difficulty according to the model's recent performance on annotated difficulty levels; relies on explicit ordering among levels rather than a general MAB formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>RL fine-tuning for reasoning LLMs (mentioned as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Targets tasks with explicit difficulty annotations (multi-step reasoning problems).</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Mentioned as contrast to SEC: AdaRFT uses explicit difficulty-ordering while SEC uses a general MAB over categories.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper (referenced as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>AdaRFT adaptively adjusts difficulty via reward signals but depends on a pre-specified difficulty ordering; SEC generalizes to arbitrary categorical arms and does not require an explicit ordered scale.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2037.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2037.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DUMP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DUMP (Automated distribution-level curriculum learning for RL-based LLM post-training) [parallel work]</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A contemporaneous method that also leverages absolute advantage as a curriculum reward within an MAB framework, applied to LLM reasoning (Knights and Knaves puzzle).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>MAB with absolute-advantage reward (parallel work)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>According to the paper, DUMP uses absolute advantage to score categories/arms and an MAB to select training distributions; the present paper contrasts scope and evaluation (DUMP focused on a single puzzle, SEC evaluates multiple domains and multi-task settings).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>LLM reasoning puzzles (Knights and Knaves in cited parallel work); SEC compares and contrasts broader-domain evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Logical reasoning puzzles; DUMP focuses on a narrower domain whereas SEC covers planning, ARC-1D, and math.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Mentioned as related/parallel work; SEC differs by broader empirical scope and multi-task validation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported here for DUMP (this paper only references DUMP qualitatively).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>DUMP independently explores absolute-advantage MAB curricula for LLMs in a narrower setting; SEC extends and validates the idea across multiple reasoning domains and multi-task scenarios.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2037.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2037.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WEBRL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WEBRL (self-evolving online curriculum RL for web agents) [referenced]</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior work that uses LLM prompting to autonomously generate new tasks (self-evolving curriculum) for training web agents; cited as related work on LLM-based curriculum/task generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>LLM-generated tasks via prompting (self-evolving online curriculum)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>WEBRL prompts an LLM to create new tasks based on previous agent failures, forming a self-evolving task distribution. The present paper contrasts SEC's MAB selection of existing categorized problems with WEBRL's generation of new tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>LLM web agents (web action environments) — different from the reasoning puzzles in SEC experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>WEBRL's generated tasks are domain-specific to web-agent behaviors and can include compositional sequences of actions; SEC's experiments focus on reasoning problems in text.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Mentioned as related work; notes difference between LLM-generated-task curricula and SEC's selection-based curricula.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported here (referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>WEBRL generates new tasks to increase diversity; SEC instead selects from predefined categories.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>WEBRL generates new tasks; SEC does not generate new tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Paper notes other work uses LLMs for task generation, but SEC does not evaluate LLM-generated curricula and thus does not report LLM-specific failures.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not discussed in detail here; task generation can incur extra prompting cost in WEBRL.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>WEBRL shows LLMs can be used to synthesize new tasks in an online self-evolving curriculum; SEC instead focuses on efficient selection from labeled categories using a MAB and absolute-advantage estimates.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2037.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2037.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TSCL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Teacher-Student Curriculum Learning (non-stationary MAB prior)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior curriculum-learning framework that framed curriculum selection as a non-stationary multi-armed bandit by measuring per-task learning progress, motivating SEC's MAB formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>non-stationary MAB maximizing measured learning progress</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>TSCL treats each task as an arm and seeks to allocate training to tasks yielding high recent learning progress (task-level improvements); SEC adapts this idea to RL fine-tuning and uses absolute advantage as a per-arm local learning proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>Originally RL/teacher-student settings; referenced as conceptual antecedent.</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Varies by original TSCL applications; referenced here as conceptual prior.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Used as related work background motivating SEC's MAB approach.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>TSCL demonstrates a MAB framing for curriculum selection based on measured learning progress; SEC builds on this idea and operationalizes an efficient local reward (absolute advantage) for RL fine-tuning of LLMs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Online difficulty filtering for reasoning oriented reinforcement learning <em>(Rating: 2)</em></li>
                <li>Dump: Automated distributionlevel curriculum learning for rl-based llm post-training <em>(Rating: 2)</em></li>
                <li>Teacher-student curriculum learning <em>(Rating: 2)</em></li>
                <li>Training llm web agents via self-evolving online curriculum reinforcement learning <em>(Rating: 2)</em></li>
                <li>Efficient reinforcement finetuning via adaptive curriculum learning <em>(Rating: 1)</em></li>
                <li>Adaptive teachers for amortized samplers <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2037",
    "paper_id": "paper-278782199",
    "extraction_schema_id": "extraction-schema-49",
    "extracted_data": [
        {
            "name_short": "SEC",
            "name_full": "Self-Evolving Curriculum",
            "brief_description": "An automatic, online curriculum-learning method for RL fine-tuning of LLMs that models curriculum selection as a non-stationary multi-armed bandit and uses batch-wise absolute advantage as the curriculum reward, updating arm values with TD(0) and sampling via a Boltzmann policy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generation_method": "reinforcement learning-based (non-stationary MAB using absolute advantage as reward)",
            "curriculum_method_description": "Training problems are partitioned into discrete categories (arms). At each RL training step the method (1) samples categories according to a Boltzmann softmax over Q_t(c) scores, (2) samples problems uniformly from the chosen categories to build a batch, (3) runs on-policy rollouts and computes advantages, (4) defines the per-category curriculum reward r(c) as the average absolute advantage |A_t| over rollouts for problems from that category, and (5) updates Q_t(c) with a TD(0)/exponential moving average: Q_{t+1}(c)=α r_t(c) + (1−α)Q_t(c). Temperature τ and learning rate α control exploration and Q updates.",
            "llm_model_used": null,
            "domain_environment": "Text-based reasoning tasks: planning puzzles (Countdown, Zebra puzzles), inductive reasoning (ARC-1D), and mathematics (MATH / AMC / AIME datasets).",
            "is_interactive_text_environment": true,
            "is_compositional": true,
            "task_complexity_description": "Multi-step text reasoning problems requiring multi-step planning, look-ahead and backtracking (Countdown, Zebra), rule induction and generalization from examples (ARC-1D), and multi-step symbolic mathematical deduction (MATH dataset including AIME). Difficulty controlled by problem parameters (number of integers/entities/string length/dataset difficulty levels); tasks require combining sub-skills (search, deduction, induction, formatting).",
            "is_curriculum_adaptive": true,
            "baseline_comparisons": "random curriculum (uniform sampling across training problems), difficulty-ordered (fixed schedule from easy→hard or reverse), and multi-task random sampling (for multi-domain experiments).",
            "performance_metrics": "Reported metrics are pass@1 accuracy (average over 8 generations) on in-distribution (ID) and out-of-distribution (OOD) held-out splits. Key reported improvements relative to the random curriculum: ~13% relative on Countdown, ~21% on Zebra puzzles, ~22% on ARC-1D, and up to ~33% on AIME24. Concrete examples: for the Qwen2.5-7B model, Zebra OOD improved from 0.32 → 0.36 (≈11% relative) and AIME OOD from 0.14 → 0.18 (≈27% relative). Additional cross-algorithm results (Countdown): PPO random ID/OOD = 0.621 / 0.159 vs SEC PPO ID/OOD = 0.750 / 0.224; RLOO random ID/OOD = 0.821 / 0.465 vs SEC RLOO ID/OOD = 0.859 / 0.494 (Table 2). Table 1 and figures report SEC as the top or near-top method across tasks and model sizes.",
            "learning_speed_comparison": "No explicit wall-clock convergence time comparisons; reported dynamics show SEC adapts difficulty over training and avoids mid-training performance collapses in mixed-task training. Training steps used: Qwen2.5-3B experiments typically 240 gradient steps (math 240), Qwen2.5-7B 120 steps; multi-task fine-tuning for 3×240=720 steps. The paper reports that SEC yields steadier learning curves and maintains performance where random curricula can collapse mid-training (multi-task setting), but does not provide precise episode/step-to-threshold numbers beyond plotted curves.",
            "generalization_performance": "SEC improves generalization to harder OOD test sets across domains: the paper reports consistent OOD accuracy gains (examples above), showing better transfer to hardest difficulty levels reserved as OOD for Countdown, Zebra, ARC-1D and improved scores on held-out math test sets (MATH500, AMC22-23, AIME24).",
            "task_diversity_analysis": "In multi-task experiments (SEC-2D), SEC balances training across domains and difficulty combinations, maintaining stable performance on all tasks; random multi-task sampling showed a mid-training collapse in one task, indicating poorer balancing of task exposure.",
            "prerequisite_identification": "Not explicitly evaluated; SEC implicitly prioritizes problems near the model's Zone-of-Proximal-Development (success rate ≈0.5) by maximizing expected absolute advantage, but the paper does not perform an explicit prerequisite graph discovery or dependency analysis.",
            "intermediate_task_generation": "SEC does not generate new intermediate tasks; it selects from predefined categories/arms (difficulty levels or task-type × difficulty combinations). The paper notes SEC relies on predefined categories and does not produce novel bridging tasks.",
            "llm_limitations_observed": "SEC itself does not rely on an LLM for curriculum generation. The paper mentions that, when difficulty annotations are unavailable, difficulty can be estimated by prompting an expert LLM (citing prior work), but the authors do not use LLM-generated curricula and do not report LLM-specific failures in curriculum generation within their experiments.",
            "computational_cost": "No precise cost numbers reported for SEC. The paper states SEC avoids the heavy overhead of online filtering (which repeatedly generates extra on-policy samples to estimate difficulty) and is therefore designed to be more computationally efficient than online filtering approaches; however, exact GPU-hours or FLOPs comparisons are not provided.",
            "human_expert_evaluation": "No human-expert qualitative evaluation of curricula was reported.",
            "key_findings_summary": "SEC — a non-stationary MAB curriculum using absolute advantage as a local learning-gain proxy and TD(0) updates — consistently improves RL fine-tuning of LLMs across planning, inductive reasoning, and mathematics, particularly in OOD generalization and multi-task balancing. SEC outperforms fixed difficulty schedules and random sampling baselines, is robust across RL algorithms (GRPO/GRPO variants, PPO, RLOO), but relies on predefined curriculum categories and introduces a few tunable hyperparameters (α, τ).",
            "uuid": "e2037.0"
        },
        {
            "name_short": "SEC-2D",
            "name_full": "Self-Evolving Curriculum (2D variant)",
            "brief_description": "A multi-dimensional variant of SEC where each arm corresponds to a (problem type × difficulty level) pair, enabling simultaneous curriculum control across multiple reasoning domains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generation_method": "reinforcement learning-based (non-stationary MAB over 2D category grid)",
            "curriculum_method_description": "Constructs arms for each unique combination of problem type and difficulty (e.g., 3 problem types × 3 difficulty levels → 9 arms). Uses the same absolute-advantage reward, TD(0) Q updates, and Boltzmann sampling as SEC to pick arms and form mixed-task batches during RL fine-tuning.",
            "llm_model_used": null,
            "domain_environment": "Mixed multi-domain text reasoning (Countdown, Zebra, ARC-1D combined into a single mixed training set).",
            "is_interactive_text_environment": true,
            "is_compositional": true,
            "task_complexity_description": "Mixed-domain training requires the model to learn differing reasoning modalities simultaneously; each subtask retains its own multi-step and compositional complexity.",
            "is_curriculum_adaptive": true,
            "baseline_comparisons": "random multi-task curriculum (uniform sampling across tasks × difficulties)",
            "performance_metrics": "Pass@1 accuracy on ID and OOD splits for each task; SEC-2D consistently outperforms random multi-task sampling across Countdown, Zebra, and ARC-1D (exact per-task numbers are reported in Figure 4; authors highlight better average and OOD accuracy and a more stable learning curve).",
            "learning_speed_comparison": "SEC-2D maintains stable OOD performance across training steps and avoids the mid-training collapse observed with random multi-task sampling; no explicit numeric speed-to-convergence reported.",
            "generalization_performance": "SEC-2D improves OOD generalization across multiple tasks compared to random multi-task curriculum and provides improved overall multi-task balance.",
            "task_diversity_analysis": "SEC-2D dynamically shifts sampling proportions across arms to maintain task balance; authors report it better balances learning objectives across domains than random sampling.",
            "prerequisite_identification": "Not evaluated separately; adaptive selection implicitly favors tasks with medium success rates but no explicit prerequisite graph extraction.",
            "intermediate_task_generation": "No generation of intermediate tasks; selection limited to predefined task×difficulty arms.",
            "llm_limitations_observed": "Not applicable (no LLM used to generate curricula in SEC-2D).",
            "computational_cost": "Not explicitly reported; multi-task experiments used longer total training (3×240=720 steps for Qwen2.5-3B), but SEC-2D's overhead beyond standard SEC is primarily the same Q updates and sampling and was not quantified.",
            "human_expert_evaluation": "None reported.",
            "key_findings_summary": "SEC-2D demonstrates that SEC's MAB formulation generalizes to multi-dimensional curricula (task × difficulty), yielding more stable multi-task learning and higher OOD accuracy than random multi-task sampling, and effectively balancing skills across domains.",
            "uuid": "e2037.1"
        },
        {
            "name_short": "Random Curriculum",
            "name_full": "Random uniform sampling curriculum",
            "brief_description": "Baseline curriculum where training problems are sampled uniformly at random from the available training set (no adaptive ordering).",
            "citation_title": "",
            "mention_or_use": "use",
            "curriculum_generation_method": "random ordering (uniform sampling over dataset or arms)",
            "curriculum_method_description": "At each training batch, problems are sampled uniformly at random across the training set or across categories; used as the primary baseline for SEC experiments.",
            "llm_model_used": null,
            "domain_environment": "Same text reasoning domains used in experiments (planning, ARC-1D, mathematics).",
            "is_interactive_text_environment": true,
            "is_compositional": true,
            "task_complexity_description": "Matches the complexity of the task domains (see SEC entry).",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "Compared directly against SEC and difficulty-ordered (fixed) curricula in experiments.",
            "performance_metrics": "Pass@1 accuracy reported per task; used as reference for relative improvement claims (e.g., SEC reported relative % improvements over random: 13% Countdown, 21% Zebra, 22% ARC-1D, up to 33% AIME24). Also used in alternative RL algorithm comparisons (see Table 2).",
            "learning_speed_comparison": "In multi-task settings, random sampling showed a mid-training collapse on some tasks (unstable multi-task learning) whereas SEC maintained stable performance; no exact convergence-step numbers beyond plotted curves.",
            "generalization_performance": "Random curriculum gives reasonable baselines but SEC usually improves OOD generalization; random performed worse on hardest OOD splits in tested domains.",
            "task_diversity_analysis": "Random sampling provides uniform diversity by construction, but can fail to balance learning progress across tasks leading to instabilities in multi-task training.",
            "prerequisite_identification": "Not applicable.",
            "intermediate_task_generation": "No.",
            "llm_limitations_observed": "Not applicable.",
            "computational_cost": "Minimal additional cost beyond standard RL fine-tuning sampling; used as computational baseline.",
            "human_expert_evaluation": "None reported.",
            "key_findings_summary": "Random uniform sampling is a reasonable baseline but is suboptimal: SEC yields consistent gains over random in OOD generalization and multi-task stability; random curricula can show mid-training collapses in mixed-task settings.",
            "uuid": "e2037.2"
        },
        {
            "name_short": "Difficulty-Ordered Curriculum",
            "name_full": "Difficulty-ordered (fixed schedule) curriculum",
            "brief_description": "A hand-designed curriculum that presents problems in a fixed difficulty schedule (typically easy→hard or reverse), used as a baseline to evaluate adaptive curricula.",
            "citation_title": "",
            "mention_or_use": "use",
            "curriculum_generation_method": "hand-crafted by difficulty ordering (fixed schedule)",
            "curriculum_method_description": "Problems are presented according to their annotated difficulty levels following a predefined schedule (e.g., easy to hard). The paper also tests a deliberately poor reverse ordering (hard→easy) to show adverse effects.",
            "llm_model_used": null,
            "domain_environment": "Planning, inductive reasoning, mathematics datasets with annotated difficulty levels.",
            "is_interactive_text_environment": true,
            "is_compositional": true,
            "task_complexity_description": "Varies by level; a fixed schedule does not adapt to evolving model competence or to OOD evaluation needs.",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "Compared against SEC and random curricula. The paper includes an explicit 'reverse' curriculum experiment showing severe degradation when ordering is misaligned with learning progress.",
            "performance_metrics": "Reported to be often suboptimal; the fixed difficulty schedule sometimes leads to poorer generalization than random sampling and SEC (e.g., reverse curriculum produced poor OOD generalization in a controlled Countdown experiment). Exact numeric values are reported in Table 1 but vary by task and model size.",
            "learning_speed_comparison": "Fixed schedules can cause inefficient use of training steps (e.g., spending excessive time on easy problems), limiting exposure to harder problems; no precise step counts reported.",
            "generalization_performance": "Often worse than SEC and sometimes worse than random, particularly when schedule is ill-matched to model capability (reverse curriculum).",
            "task_diversity_analysis": "Not adaptive; diversity is determined by schedule and may be limited when stuck on particular difficulty levels.",
            "prerequisite_identification": "Not assessed.",
            "intermediate_task_generation": "No.",
            "llm_limitations_observed": "Not applicable.",
            "computational_cost": "Low (predefined ordering); no extra online computation beyond sampling.",
            "human_expert_evaluation": "Not reported (design is heuristic-driven).",
            "key_findings_summary": "Fixed difficulty-ordered curricula can be brittle: a poorly chosen schedule (e.g., reverse) substantially reduces final performance and generalization, highlighting the value of adaptive, online curricula such as SEC.",
            "uuid": "e2037.3"
        },
        {
            "name_short": "Online Filtering (Bae et al.)",
            "name_full": "Online difficulty filtering for reasoning oriented reinforcement learning (referenced work)",
            "brief_description": "A recently proposed curriculum approach that filters training problems online by repeatedly generating solutions to estimate difficulty, used as a comparison point in related work.",
            "citation_title": "",
            "mention_or_use": "mention",
            "curriculum_generation_method": "online filtering by generating on-policy solutions to estimate problem difficulty",
            "curriculum_method_description": "Continuously generates solutions for candidate problems during training to measure difficulty and filter/weight problems accordingly; this requires repeated extra on-policy generation and is computationally intensive.",
            "llm_model_used": null,
            "domain_environment": "Reasoning-oriented RL for LLMs (similar domains to SEC experiments).",
            "is_interactive_text_environment": true,
            "is_compositional": true,
            "task_complexity_description": "Targets reasoning problems where difficulty estimation via rollouts is meaningful; typically multi-step.",
            "is_curriculum_adaptive": true,
            "baseline_comparisons": "Described as related work and compared qualitatively to SEC; authors argue online filtering is more computationally heavy than SEC.",
            "performance_metrics": "Not reported in this paper (referenced as separate prior work).",
            "learning_speed_comparison": "Paper notes online filtering methods are computationally prohibitive due to continuous generation of additional on-policy samples; no quantitative runtime comparison provided here.",
            "generalization_performance": "Not reported here.",
            "task_diversity_analysis": "Not discussed here.",
            "prerequisite_identification": "Not discussed here.",
            "intermediate_task_generation": "No (filters existing problems rather than generating new ones).",
            "llm_limitations_observed": "Not discussed in this paper; the referenced method's repeated on-policy generation could be expensive and sensitive to rollout quality.",
            "computational_cost": "Described as substantial/high (qualitative claim), since it requires repeated extra on-policy sample generation for difficulty estimation.",
            "human_expert_evaluation": "Not reported in this paper.",
            "key_findings_summary": "Online filtering approaches estimate per-instance difficulty via extra rollouts and can adapt curricula but at high computational cost; SEC is proposed as a cheaper alternative that uses advantage-based local rewards rather than separate heavy difficulty estimation.",
            "uuid": "e2037.4"
        },
        {
            "name_short": "AdaRFT",
            "name_full": "AdaRFT (adaptive curriculum by difficulty-level ordering - referenced)",
            "brief_description": "An adaptive curriculum approach that adjusts curriculum difficulty based on recent reward signals, but which relies on explicit difficulty-level ordering.",
            "citation_title": "",
            "mention_or_use": "mention",
            "curriculum_generation_method": "adaptive difficulty adjustment driven by recent model reward signals (requires ordered difficulty levels)",
            "curriculum_method_description": "Adapts training difficulty according to the model's recent performance on annotated difficulty levels; relies on explicit ordering among levels rather than a general MAB formulation.",
            "llm_model_used": null,
            "domain_environment": "RL fine-tuning for reasoning LLMs (mentioned as related work).",
            "is_interactive_text_environment": true,
            "is_compositional": true,
            "task_complexity_description": "Targets tasks with explicit difficulty annotations (multi-step reasoning problems).",
            "is_curriculum_adaptive": true,
            "baseline_comparisons": "Mentioned as contrast to SEC: AdaRFT uses explicit difficulty-ordering while SEC uses a general MAB over categories.",
            "performance_metrics": "Not reported in this paper (referenced as related work).",
            "learning_speed_comparison": "Not reported here.",
            "generalization_performance": "Not reported here.",
            "task_diversity_analysis": "Not discussed here.",
            "prerequisite_identification": "Not discussed here.",
            "intermediate_task_generation": "No.",
            "llm_limitations_observed": "Not applicable.",
            "computational_cost": "Not discussed here.",
            "human_expert_evaluation": "Not reported here.",
            "key_findings_summary": "AdaRFT adaptively adjusts difficulty via reward signals but depends on a pre-specified difficulty ordering; SEC generalizes to arbitrary categorical arms and does not require an explicit ordered scale.",
            "uuid": "e2037.5"
        },
        {
            "name_short": "DUMP",
            "name_full": "DUMP (Automated distribution-level curriculum learning for RL-based LLM post-training) [parallel work]",
            "brief_description": "A contemporaneous method that also leverages absolute advantage as a curriculum reward within an MAB framework, applied to LLM reasoning (Knights and Knaves puzzle).",
            "citation_title": "",
            "mention_or_use": "mention",
            "curriculum_generation_method": "MAB with absolute-advantage reward (parallel work)",
            "curriculum_method_description": "According to the paper, DUMP uses absolute advantage to score categories/arms and an MAB to select training distributions; the present paper contrasts scope and evaluation (DUMP focused on a single puzzle, SEC evaluates multiple domains and multi-task settings).",
            "llm_model_used": null,
            "domain_environment": "LLM reasoning puzzles (Knights and Knaves in cited parallel work); SEC compares and contrasts broader-domain evaluation.",
            "is_interactive_text_environment": true,
            "is_compositional": true,
            "task_complexity_description": "Logical reasoning puzzles; DUMP focuses on a narrower domain whereas SEC covers planning, ARC-1D, and math.",
            "is_curriculum_adaptive": true,
            "baseline_comparisons": "Mentioned as related/parallel work; SEC differs by broader empirical scope and multi-task validation.",
            "performance_metrics": "Not reported here for DUMP (this paper only references DUMP qualitatively).",
            "learning_speed_comparison": "Not reported here.",
            "generalization_performance": "Not reported here.",
            "task_diversity_analysis": "Not discussed here.",
            "prerequisite_identification": "Not discussed here.",
            "intermediate_task_generation": "No.",
            "llm_limitations_observed": "Not applicable.",
            "computational_cost": "Not reported here.",
            "human_expert_evaluation": "Not reported here.",
            "key_findings_summary": "DUMP independently explores absolute-advantage MAB curricula for LLMs in a narrower setting; SEC extends and validates the idea across multiple reasoning domains and multi-task scenarios.",
            "uuid": "e2037.6"
        },
        {
            "name_short": "WEBRL",
            "name_full": "WEBRL (self-evolving online curriculum RL for web agents) [referenced]",
            "brief_description": "A prior work that uses LLM prompting to autonomously generate new tasks (self-evolving curriculum) for training web agents; cited as related work on LLM-based curriculum/task generation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "curriculum_generation_method": "LLM-generated tasks via prompting (self-evolving online curriculum)",
            "curriculum_method_description": "WEBRL prompts an LLM to create new tasks based on previous agent failures, forming a self-evolving task distribution. The present paper contrasts SEC's MAB selection of existing categorized problems with WEBRL's generation of new tasks.",
            "llm_model_used": null,
            "domain_environment": "LLM web agents (web action environments) — different from the reasoning puzzles in SEC experiments.",
            "is_interactive_text_environment": true,
            "is_compositional": true,
            "task_complexity_description": "WEBRL's generated tasks are domain-specific to web-agent behaviors and can include compositional sequences of actions; SEC's experiments focus on reasoning problems in text.",
            "is_curriculum_adaptive": true,
            "baseline_comparisons": "Mentioned as related work; notes difference between LLM-generated-task curricula and SEC's selection-based curricula.",
            "performance_metrics": "Not reported here (referenced work).",
            "learning_speed_comparison": "Not discussed here.",
            "generalization_performance": "Not discussed here.",
            "task_diversity_analysis": "WEBRL generates new tasks to increase diversity; SEC instead selects from predefined categories.",
            "prerequisite_identification": "Not discussed here.",
            "intermediate_task_generation": "WEBRL generates new tasks; SEC does not generate new tasks.",
            "llm_limitations_observed": "Paper notes other work uses LLMs for task generation, but SEC does not evaluate LLM-generated curricula and thus does not report LLM-specific failures.",
            "computational_cost": "Not discussed in detail here; task generation can incur extra prompting cost in WEBRL.",
            "human_expert_evaluation": "Not reported here.",
            "key_findings_summary": "WEBRL shows LLMs can be used to synthesize new tasks in an online self-evolving curriculum; SEC instead focuses on efficient selection from labeled categories using a MAB and absolute-advantage estimates.",
            "uuid": "e2037.7"
        },
        {
            "name_short": "TSCL",
            "name_full": "Teacher-Student Curriculum Learning (non-stationary MAB prior)",
            "brief_description": "A prior curriculum-learning framework that framed curriculum selection as a non-stationary multi-armed bandit by measuring per-task learning progress, motivating SEC's MAB formulation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "curriculum_generation_method": "non-stationary MAB maximizing measured learning progress",
            "curriculum_method_description": "TSCL treats each task as an arm and seeks to allocate training to tasks yielding high recent learning progress (task-level improvements); SEC adapts this idea to RL fine-tuning and uses absolute advantage as a per-arm local learning proxy.",
            "llm_model_used": null,
            "domain_environment": "Originally RL/teacher-student settings; referenced as conceptual antecedent.",
            "is_interactive_text_environment": null,
            "is_compositional": null,
            "task_complexity_description": "Varies by original TSCL applications; referenced here as conceptual prior.",
            "is_curriculum_adaptive": true,
            "baseline_comparisons": "Used as related work background motivating SEC's MAB approach.",
            "performance_metrics": "Not reported here.",
            "learning_speed_comparison": "Not discussed here.",
            "generalization_performance": "Not discussed here.",
            "task_diversity_analysis": "Not discussed here.",
            "prerequisite_identification": "Not discussed here.",
            "intermediate_task_generation": "No.",
            "llm_limitations_observed": "Not applicable.",
            "computational_cost": "Not discussed here.",
            "human_expert_evaluation": "Not reported here.",
            "key_findings_summary": "TSCL demonstrates a MAB framing for curriculum selection based on measured learning progress; SEC builds on this idea and operationalizes an efficient local reward (absolute advantage) for RL fine-tuning of LLMs.",
            "uuid": "e2037.8"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Online difficulty filtering for reasoning oriented reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Dump: Automated distributionlevel curriculum learning for rl-based llm post-training",
            "rating": 2
        },
        {
            "paper_title": "Teacher-student curriculum learning",
            "rating": 2
        },
        {
            "paper_title": "Training llm web agents via self-evolving online curriculum reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Efficient reinforcement finetuning via adaptive curriculum learning",
            "rating": 1
        },
        {
            "paper_title": "Adaptive teachers for amortized samplers",
            "rating": 1
        }
    ],
    "cost": 0.0199,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Self-Evolving Curriculum for LLM Reasoning
29 May 2025</p>
<p>Xiaoyin Chen xiaoyin.chen@mila.quebec 
ServiceNow Research</p>
<p>Jiarui Lu 
ServiceNow Research</p>
<p>Minsu Kim 
ServiceNow Research</p>
<p>Dinghuai Zhang 
ServiceNow Research</p>
<p>Jian Tang 
ServiceNow Research</p>
<p>Alexandre Piché 
ServiceNow Research</p>
<p>Nicolas Gontier 
ServiceNow Research</p>
<p>Yoshua Bengio 
ServiceNow Research</p>
<p>Ehsan Kamalloo 
ServiceNow Research</p>
<p>Mila -Quebec 
ServiceNow Research</p>
<p>A I Institute 
ServiceNow Research</p>
<p>Université De Montréal 
ServiceNow Research</p>
<p>Kaist 
ServiceNow Research</p>
<p>Microsoft Research 
ServiceNow Research</p>
<p>Hec Montréal 
ServiceNow Research</p>
<p>Self-Evolving Curriculum for LLM Reasoning
29 May 20251D21B7AC3E1E80621CC68FCD46C5E39AarXiv:2505.14970v2[cs.AI]
Reinforcement learning (RL) has proven effective for fine-tuning large language models (LLMs), significantly enhancing their reasoning abilities in domains such as mathematics and code generation.A crucial factor influencing RL fine-tuning success is the training curriculum: the order in which training problems are presented.While random curricula serve as common baselines, they remain suboptimal; manually designed curricula often rely heavily on heuristics, and online filtering methods can be computationally prohibitive.To address these limitations, we propose Self-Evolving Curriculum (SEC), an automatic curriculum learning method that learns a curriculum policy concurrently with the RL fine-tuning process.Our approach formulates curriculum selection as a non-stationary Multi-Armed Bandit problem, treating each problem category (e.g., difficulty level or problem type) as an individual arm.We leverage the absolute advantage from policy gradient methods as a proxy measure for immediate learning gain.At each training step, the curriculum policy selects categories to maximize this reward signal and is updated using the TD(0) method.Across three distinct reasoning domains: planning, inductive reasoning, and mathematics, our experiments demonstrate that SEC significantly improves models' reasoning capabilities, enabling better generalization to harder, out-of-distribution test problems.Additionally, our approach achieves better skill balance when fine-tuning simultaneously on multiple reasoning domains.These findings highlight SEC as a promising strategy for RL fine-tuning of LLMs. 2 * Work done while at ServiceNow Research. 2Our code will be publicly released soon.Preprint.Under review.LLM Policy1 Expected Learning Gain 2 3 4 Curriculum Policy Training Batch Curriculum Reward Training Data Rewards Per Category RL Step RL with Verifiable Rewardsto the current MAB policy, which is subsequently updated on-the-fly using the reward obtained from the current training step via the TD(0) method[48].Our experiments demonstrate that SEC significantly improves model reasoning capabilities across three distinct domains: planning, inductive reasoning, and mathematics, particularly improving generalization to challenging out-of-distribution problems.Compared to the standard random curriculum, SEC achieves substantial relative improvements, such as 13% on Countdown, 21% on Zebra puzzles, 22% on ARC-1D, and up to 33% on the AIME24 dataset.When fine-tuned simultaneously across multiple reasoning domains, SEC effectively balances performance across tasks, underscoring its strength as an automatic curriculum learning strategy for RL fine-tuning of LLMs.</p>
<p>1 Introduction Reinforcement learning (RL) has emerged as a central technique for fine-tuning large language models (LLMs) [28,35,12], significantly improving their reasoning capabilities.Recent advances demonstrate notable success, particularly in domains where verifying generation correctness is straightforward [27], such as mathematics and code generation.By optimizing LLMs with rewards solely defined by verifiable outcomes, RL fine-tuning encourages the emergence of complex reasoning behaviors, including selfcorrection and back-tracking strategies [25,63,16], that substantially enhance reasoning performance.</p>
<p>A critical factor influencing the effectiveness of RL fine-tuning is the training curriculum [7], i.e., the or- der in which training data is presented.Since online RL inherently depends on the policy model itself to produce high-quality training trajectories, aligning the curriculum with the model's current learning progress is critical.Ideally, such an alignment enables the model to continually encounter problems that yield maximal learning outcomes [42,29,20].To illustrate this point concretely, we conduct a controlled experiment using the Countdown game, 3 deliberately employing a suboptimal (reverse) curriculum, in which problems are arranged from hard to easy.As shown in Figure 1, the resulting model performs poorly on the test set and exhibits minimal generalization to more challenging outof-distribution (OOD) problems.In contrast, when trained with a baseline random curriculum, where problems of varying difficulty are drawn uniformly at random, the model demonstrates significantly improved generalization and overall task performance.</p>
<p>Although random curriculum serves as a reasonable baseline, it naturally raises the question: Can we design more effective curriculum strategies?Curriculum learning [7] addresses precisely this challenge by seeking to optimize the sequencing of training tasks, thereby enhancing learning efficiency and efficacy.Recent approaches to curriculum learning for RL fine-tuning typically involve either manually crafted curricula designed upfront [50,47,58] or dynamic online filtering based on on-policy samples [64,3].However, manually designed curricula rely heavily on heuristics, demanding human intervention for new models or tasks; conversely, online filtering methods incur substantial computational overhead due to the continuous generation of additional on-policy samples.</p>
<p>In this paper, we propose Self-Evolving Curriculum (SEC) (Figure 2), an automatic curriculum learning [39] approach for RL fine-tuning of LLMs.Our method adaptively learns a curriculum policy concurrently with the RL fine-tuning process, formulating curriculum selection as a non-stationary Multi-Armed Bandit (MAB) problem [51,49,31].Each curriculum category (e.g., difficulty level or problem type) is treated as an individual arm, and the curriculum policy aims to select the arm that maximizes the learning outcomes.Specifically, we operationalize the concept of learning outcomes using the gradient norm, noting that, in policy gradient methods, the gradient norm is weighted by the absolute value of the advantage function.Leveraging this observation, we define the absolute advantage as the reward for each arm.At each RL training step, the curriculum is sampled according</p>
<p>Method</p>
<p>In the context of RL fine-tuning, at each training step t, the curriculum policy selects a subset D t ⊆ D from the training problem set D to be provided to the LLM.In our work, we consider scenarios where the training problems can be categorized into N distinct categories.This assumption simplifies the curriculum optimization problem into learning an expected return Q t (c) that maps category c to a real-valued score (Sec.2.1).The training batch is then constructed by first sampling categories according to the curriculum policy, followed by sampling problems uniformly within the categories.</p>
<p>The goal of the curriculum policy is to maximize the LLM's final task performance.However, directly evaluating such performance would require completing the entire RL fine-tuning process, while the curriculum policy is better to be updated along with the training steps.To resolve this, we introduce a locally measurable reward as a proxy objective for guiding the curriculum policy (Sec.2.2).</p>
<p>Curriculum Selection as Multi-Armed Bandit</p>
<p>Training datasets used for reasoning tasks can often be naturally decomposed into distinct categories.For example, if the dataset spans various reasoning domains, such as mathematics, coding, and planning, these domains naturally form distinct categories.When the dataset is homogeneous in task type or domain, a curriculum can still be constructed by categorizing examples based on in-domain levels, such as difficulty.For instance, the MATH dataset [17] categorizes problems into five distinct difficulty levels based on the guidelines provided by Art of Problem Solving (AoPS).Furthermore, in the absence of explicit difficulty annotations, problem difficulty can be estimated by either using the empirical accuracy of the training LLM or prompting an expert LLM in an additional preprocessing step, as demonstrated by Shi et al. [46].</p>
<p>Motivated by these considerations, we assume that, particularly for reasoning-focused datasets, training problems can be partitioned into N distinct categories C = {c 1 , c 2 , . . ., c N }.Conceptually, the curriculum policy optimization problem can then be viewed as a partially observable Markov decision process (POMDP): the state corresponds to the current LLM policy, actions correspond to curriculum selection, and rewards are defined by observable performance metrics, such as the on-policy performance associated with the selected curriculum.</p>
<p>This POMDP formulation naturally resembles a non-stationary MAB problem, a connection also highlighted by Matiisen et al. [31], where each arm represents a problem category c i , and the objective is to learn the expected return Q t (c) associated with selecting category c at training step t.Importantly, the MAB in this context is non-stationary: the expected reward distribution for each arm shifts as the LLM policy is updated over the course of training.To address this well-studied non-stationary bandit problem [51,49], we leverage the classic TD(0) method [48] to iteratively update Q t (c):
Q t+1 (c) = αr t (c) + (1 − α)Q t (c),(1)
where α is the learning rate, Q 0 (c) = 0 initializes the scores to zero, and r t (c) denotes the reward defined in the next section.Note that this is also known as the Exponential Moving Average.The curriculum policy can then be simply defined over Q t (c), as elaborated in Sec.2.3.</p>
<p>Measuring Learning Outcomes with Absolute Advantage</p>
<p>An ideal curriculum should maximize the LLM's final performance on the test data after an entire training episode.However, directly measuring this objective requires completing a full RL finetuning cycle.Although evaluating intermediate checkpoints can partially mitigate this issue, frequent evaluations are computationally expensive.To overcome this challenge, we introduce a proxy objective that can be efficiently computed locally at each training step.</p>
<p>An intuitive choice for such a proxy objective is to prioritize training data that maximizes the model's immediate learning outcomes [42,29,20], i.e., data that induces large parameter updates.Practically, this can be quantified by measuring the gradient norm of the loss function with respect to the selected training data.Specifically, consider a policy gradient algorithm that optimizes the LLM policy by minimizing the following loss function:
L PG (θ) = − E (st,at)∼π θ log π θ (a t | s t ) A t(2)
where π θ denotes the LLM policy and A t denotes the advantage value.Then, the per-step (s t , a t ) gradient norm is:
∥∇ θ L PG (θ, s t , a t )∥ 2 = E (st,at)∼π θ ∇ θ log π θ (a t | s t ) A t 2 ≈ | A t |∥∇ θ log π θ (a t | s t ) ∥ 2
We observe that the gradient magnitude is weighted by the absolute value of the advantage | A t |.We therefore approximate the learning gain of a curriculum c by the batch-wise expectation of | A t |:
r(c) = E (st,at)∼π θ (xi),xi∼c | A t |(3)
In other words, the reward for curriculum c at each training step is computed as the average absolute advantage across all rollouts associated with the problems drawn from curriculum category c.</p>
<p>Interpreting absolute advantage in RL with Verifiable Rewards.Recent works on RL finetuning for reasoning tasks frequently employ a binary correctness reward (0 for incorrect, 1 for correct) with the Group Relative Policy Optimization (GRPO) algorithm [44,25,12,30,57], a setting known as RL with Verifiable Rewards [27].Here, we show that under this common setting, the expected absolute advantage, as formalized in Equation 3, is maximized when the expected binary reward is 0.5.This means that the highest learning gain occurs when the problem is neither too easy nor too hard for the current model.Prioritizing training on problems with such difficulty aligns naturally with concepts from educational psychology, particularly the Zone of Proximal Development theory [54,8], and connects our approach to broader literature in curriculum learning [14,15,52,3].</p>
<p>Concretely, the GRPO algorithm estimates the advantage by sampling n rollouts from the same problem.The advantage of the i-th rollout is computed as:
A t,i = ri = ri−mean(r) std(r)
, where r i is the binary reward of the i-th rollout, and mean(r) and std(r) denote the mean and standard deviation of the rewards over all n rollouts.Since the reward is binary, it can be modeled as a Bernoulli distribution; thus, mean(r) = p and std(r) = p(1 − p), where p denotes the success rate.The expected absolute advantage can then be expressed as:
E[| A t,i |] = E[|r i |] = E r i − p p(1 − p)(4)
Because r i is Bernoulli, there are only two possible values:
ri =        1 − p p(1 − p) with probability p, −p p(1 − p) with probability 1 − p.(5)
Hence, the expected absolute advantage of the problem is
E |r i | = p 1 − p p(1 − p) + (1 − p) p p(1 − p) = 2 p(1 − p) p(1 − p) = 2 p(1 − p).(6)
We can see that the function g(p) = 2 p(1 − p) is symmetric around p = 0.5 and strictly concave on the interval [0, 1], reaching its maximum at p = 0.5.This derivation shows that maximizing the expected absolute advantage is equivalent to prioritizing problems at a success rate of 0.5.
1: Initialize Q 0 (c) ← 0 ∀ c ∈ C 2: for t ← 0 to T − 1 do 3: B t ← ∅ 4: while |B t | &lt; B do 5: Sample category c ∼ Softmax Q t (c)/τ 6:
Sample problem x uniformly from category c 7:
B t ← B t ∪ {x} 8:
end while
9:
Run π θ on each x ∈ B t to generate rollouts T and compute rewards r with R 10:</p>
<p>Estimate advantages A and update π θ with A(π θ , T , r)
11:
for all c ∈ C do 12:
B c ← { x ∈ B t | x belongs to category c } 13: r t (c) ← 1 |B c | j:xj ∈Bc 1 T j Tj t A t,j 14: Q t+1 (c) ← α r t (c) + (1 − α) Q t (c) 15:
end for 16: end for 17: return Fine-tuned LLM π θ Remarks: The derivation above aims to provide a concrete analysis of our proposed objective under a common RL Fine-tuning setting, as well as to offer intuitive insights into its general behavior.However, this does not imply that our method is limited to this specific scenario.Indeed, our experiments empirically demonstrate that SEC also achieves strong performance when applied with non-binary reward functions (Sec.3.2) and alternative RL algorithms (Sec.3.4).</p>
<p>Self-evolving Curriculum for RL Fine-tuning</p>
<p>At each RL training step, a batch of problems is generated as follows.First, categories are sampled according to a Boltzmann distribution defined by the current values of Q t (c):
p(c) = e Q t (c)/τ N i=1 e Q t (c i )/τ
, where τ is the temperature parameter controlling the exploration-exploitation trade-off.Next, problems are uniformly sampled from the selected categories.This process is repeated until the desired batch size is reached.Sampling from the Boltzmann distribution naturally balances exploration and exploitation in curriculum selection.</p>
<p>The resulting batch is then used to update the LLM policy.After the policy update at each step, we compute the reward r(c) for each sampled category c and update the corresponding Q t (c) values using Eq. 1.The complete procedure of SEC is summarized in Algorithm 1.</p>
<p>Experiments</p>
<p>This section presents experiments evaluating SEC across three reasoning domains: planning, inductive reasoning, and mathematics.We additionally investigate SEC's effectiveness with different curriculum categories and alternative RL algorithms.</p>
<p>Experimental Setup</p>
<p>We conduct our experiments using the open-weight Qwen2.5 models [62]: Qwen2.5-3B and Qwen2.5-7B.For RL fine-tuning on reasoning tasks, we employ the widely-used GRPO algorithm [44,12].We report average pass@1 accuracy from the best checkpoint, calculated over 8 independent generations per problem.Additional training and evaluation details are provided in Appendix B. Prompts and data examples for all tasks are provided in Appendix C. Our experiments cover three reasoning domains that require different abilities: (i) Planning, which requires look-ahead search and backtracking; (ii) Inductive reasoning, which involves learning general rules from observations and applying them to unseen scenarios; and (iii) Mathematics, which demands multi-step logical deduction and systematic problem solving.</p>
<p>Planning.For planning tasks, we consider two popular puzzle problems: (i) Countdown, where the goal is to use basic arithmetic operations to reach a target number from a given set of 3-6 integers.</p>
<p>In this puzzle, we control the task difficulty by increasing the number of given integers.(ii) Zebra Puzzles, a classic logic puzzle involving 3-6 entities (e.g., houses) each with 3-6 properties (e.g., color).Given a set of textual clues (constraints), the goal is to correctly assign each property to each entity.Here, we control the task difficulty by increasing the number of entities and properties.</p>
<p>Inductive reasoning.We adopt the 1D variant of the Abstraction and Reasoning Corpus (ARC) [9,61] for inductive reasoning.Each puzzle instance consists of strings of lengths 10, 20, 30, or 40 (with greater length corresponding to increased difficulty), which are defined over integers.Three input-output examples illustrating an underlying rule are provided, and the LLM is tested on an unseen case requiring generalization.</p>
<p>For the above three reasoning tasks (Countdown, Zebra, and ARC), we generate problems using the framework provided by Open-Thought [34].Specifically, our training data consists of the three easiest difficulty levels, and the most difficult level is reserved as an out-of-distribution (OOD) evaluation set.For each difficulty level, we sample 10,000 problems for training and 200 held-out samples for evaluation.During RL fine-tuning, we assign rewards of 1 for correct problems, 0.1 for incorrect answers but with correct formatting, and 0 otherwise.</p>
<p>Mathematics.We train LLMs on the training split of the MATH dataset [17], which comprises problems categorized into five difficulty levels, from 1 (easiest) to 5 (hardest), as specified in the dataset annotations.Unlike the previous three tasks, the training data for mathematics is imbalanced across these difficulty levels (Figure S1).For this task, we use a binary reward (1 for correct and 0 otherwise), without assigning a partial reward for a correct format.The models are subsequently evaluated on the MATH500, AMC22-23, and AIME24 datasets.</p>
<p>Main Results</p>
<p>First, we evaluate the effectiveness of SEC using problem difficulty as the curriculum category, i.e., each difficulty level corresponds to an arm in the MAB framework.We compare SEC against two commonly used curriculum strategies:  For the larger Qwen2.5-7Bmodel, SEC performance is competitive but more similar to the random curriculum on tasks like Countdown and ARC.This outcome aligns with expectations, as stronger base models may already possess sufficient reasoning capabilities to tackle harder problems, thus rendering explicit curriculum guidance less critical.Nevertheless, on more challenging tasks such as Zebra and mathematics, SEC continues to show clear improvements.Specifically, the OOD accuracy on Zebra improves by approximately 11% relative (0.32 → 0.36) over the random baseline.On the challenging AIME problmes, SEC achieves a 27% relative gain (0.14 → 0.18).The consistent improvements observed in these more challenging domains, together with the robust gains in the 3B model, highlight SEC's effectiveness in improving the model generalization.</p>
<p>Finally, the difficulty-ordered curriculum often yields suboptimal performance, likely due to its fixed difficulty schedule, which does not dynamically adapt to the model's current performance.As a result, models may spend excessive training time on easy problems, limiting exposure to harder ones from which models could potentially learn more.These results further underscore the necessity of adaptive, online curriculum strategies like SEC, which continuously align problem selection with the model's current learning state.</p>
<p>Curriculum analysis.Figure 3</p>
<p>SEC with Multiple Curriculum Categories</p>
<p>In this section, we demonstrate that SEC seamlessly supports drawing from multiple and diverse curriculum categories at the same time.A common scenario in RL fine-tuning involves optimizing a model's performance across multiple reasoning domains.To evaluate SEC in such a multi-task setting, we combine the training datasets from Countdown, Zebra, and ARC to create a mixed training set comprising multiple types of reasoning problems, and conduct RL fine-tuning using the Qwen2.5-3Bmodel.The goal here is to achieve a strong overall performance across all reasoning tasks.</p>
<p>Our MAB-based curriculum framework is agnostic to the semantic meaning of the curriculum categories, thus allowing categories to be defined arbitrarily.In this experiment, we define one arm for each unique combination of 3 problem types and 3 difficulty levels, resulting in a total of 9 distinct arms.We denote this variant as SEC-2D.</p>
<p>Figure 4 presents results evaluating SEC-2D when training simultaneously on multiple reasoning tasks.The table (left) demonstrates that SEC-2D consistently outperforms the random curriculum across all three reasoning tasks.The learning curve (right) provides a detailed view of OOD accuracy on Countdown as training progresses.Initially, both curricula show rapid improvement; however, the random curriculum exhibits a significant performance collapse midway through training, highlighting its inability to effectively balance multi-task learning.In contrast, SEC-2D maintains stable, robust performance, underscoring its strength in adaptively balancing multiple learning objectives.[43] and REINFORCE Leave-One-Out (RLOO) [24].Table 2 presents results on the Countdown task with Qwen2.5-3B,comparing SEC to the random curriculum under these two algorithms.Across both PPO and RLOO, SEC consistently improves performance on ID and OOD evaluation splits, demonstrating that it is effective beyond a single RL algorithm.</p>
<p>SEC with Alternative RL Algorithms</p>
<p>Related Works</p>
<p>RL fine-tuning for language models.Language models (LMs) can be naturally viewed as sequential decision-making policies, generating tokens conditioned on partial text states until reaching terminal outputs.Typically, reward signals are sparse and episodic, assigned only after the full generation, an approach termed Outcome Reward Models (ORM) [11].Some recent studies introduce Process Reward Models (PRM), assigning intermediate rewards during generation to facilitate local credit assignment [28,53].Leveraging this Markov Decision Process (MDP) framing, RL fine-tuning has demonstrated success across multiple domains, including aligning LMs with human preferences (RLHF) [10,66,4,36], enhancing mathematical reasoning via exact-match rewards [44], and self-training with internal LM distributions (e.g., Self-taught Reasoner, STaR) [65].Recently, Reinforcement Learning with Verifiable Rewards (RLVR) [12,27] has emerged as a promising paradigm for improving the reasoning abilities of LMs.</p>
<p>RL methods tailored to these MDP formulations have also played a central role.Policy-gradient methods, including REINFORCE variants (e.g., RLOO) [59,24,1] and Proximal Policy Optimization (PPO) approaches [43,21,44], are widely adopted due to their relative stability.Alternatively, off-policy and value-based algorithms such as Directed Preference Optimization (DPO) [41,32] and Generative Flow Networks (GFlowNets) [6,19,18] provide advantages in sample efficiency, diversity, and asynchronous training [33,5], although they may not always match the task-specific reward maximization capabilities of on-policy methods, instead prioritizing improved diversity.</p>
<p>Curriculum learning.Curriculum learning was introduced by Bengio et al. [7] and later refined as self-paced learning [26], showing that organizing examples from easy to hard smooths non-convex optimization and improves generalization.In RL, curricula mitigate sparse rewards and exploration hurdles: reverse-curriculum generation grows start states outward from the goal [14], Teacher-Student Curriculum Learning (TSCL) [31] also used a non-stationary MAB framework to maximize measured learning progress, defined as improvements in task performance, methods such as POET, ACCEL, and PAIRED co-evolve tasks with agents [55,38,13], and Kim et al. [22] proposed an adaptive teacher that dynamically adjusts curricula for multi-modal amortized sampling.</p>
<p>Only recently have similar curriculum learning ideas begun influencing RL fine-tuning of language models.R 3 applies reverse curricula specifically to chain-of-thought reasoning, progressively revealing longer reasoning sequences conditioned on gold demonstrations [60].Qi et al. [40] proposed WEBRL, a self-evolving online curriculum RL framework designed to train LM-based web agents by prompting another LLM to autonomously generate new tasks based on previous failures.</p>
<p>Concurrently, several studies have explored automatic curriculum learning for RL fine-tuning.Bae et al. [3] propose online filtering of training problems by repeatedly generating solutions to estimate their difficulty.AdaRFT [46] adaptively adjusts curriculum difficulty based on the model's recent reward signals but relies on explicit difficulty-level ordering.In contrast, SEC leverages a general MAB formulation to dynamically adjust the curriculum.DUMP [56], in parallel to us, also leverages absolute advantage as the curriculum reward with an MAB framework, focusing on the Knights and Knaves logical reasoning puzzle with GRPO.In contrast, our study covers multiple reasoning domains, examines a multi-task RL setting, and validates performance across various RL algorithms.</p>
<p>Conclusion</p>
<p>In this paper, we introduced Self-Evolving Curriculum (SEC), an automatic curriculum learning framework tailored for RL fine-tuning of LLMs.SEC formulates adaptive curriculum selection as a non-stationary Multi-Armed Bandit problem, dynamically adjusting problem difficulty according to the model's evolving capability.Extensive experiments across diverse reasoning tasks, including planning, inductive reasoning, and mathematics, demonstrate that SEC consistently improves generalization and effectively balances learning across multiple reasoning domains simultaneously.</p>
<p>Our framework consists of three major components: curriculum rewards, sampling methods, and update rules.In this paper, SEC employs absolute advantage as the curriculum reward, a Boltzmann distribution for sampling, and a TD(0) update method.The generalization of these components can be explored for future work.For instance, one might incorporate uncertainty measures into the curriculum selection by leveraging approaches such as Upper Confidence Bound (UCB) [2] or Thompson sampling [51].</p>
<p>Limitations.While SEC demonstrates consistent effectiveness across diverse reasoning tasks, it has some limitations.Our method currently relies on predefined curriculum categories; its effectiveness with automatically inferred or less clearly defined categories remains unexplored.Additionally, SEC introduces extra hyperparameters (e.g., temperature, learning rate) that require tuning.Future work may explore more flexible curriculum definitions, such as clustering problems based on embeddings or using lightweight models (e.g., linear regression) to directly estimate curriculum rewards.</p>
<p>Prompt for Zebra Puzzle:</p>
<p>A conversation between User and Assistant.The user asks a question, and the Assistant solves it.The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.User: This is a logic puzzle.There are 3 houses (numbered 1 on the left, 3 on the right), from the perspective of someone standing across the street from them.Each has a different person in them.They have different characteristics:</p>
<p>-Each person has a unique name: arnold, bob, alice -Everyone has a different favorite cigar: dunhill, prince, pall mall -The people keep different animals: cat, dog, bird</p>
<p>Prompt for ARC-1D:</p>
<p>A conversation between User and Assistant.The user asks a question, and the Assistant solves it.The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.User: Find the common rule that maps an input grid to an output grid, given the examples below.</p>
<p>Example 1: Input: 1 2 1 2 1 0 0 1 2 0 Output: 0 0 0 1 1 1 1 2 2 2</p>
<p>Example 2: Input: 1 2 0 0 0 0 2 0 1 2 Output: 0 0 0 0 0 1 1 2 2 2</p>
<p>Example 3: Input: 0 0 2 0 0 0 0 1 1 0 Output: 0 0 0 0 0 0 0 1 1 2</p>
<p>Below is a test input grid.Predict the corresponding output grid by applying the rule you found.Describe how you derived the rule and your overall reasoning process in detail before you submit your answer.Your final answer must be placed in \boxed{} and should be just the test output grid itself.</p>
<p>Input: 0 0 2 0 0 1 1 0 0 1 Assistant: Let me solve this step by step.</p>
<p>Prompt for math:</p>
<p>A conversation between User and Assistant.The user asks a question, and the Assistant solves it.The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.User: Find the remainder when
33818 2 +
Figure 1 :
1
Figure 1: Curriculum matters.A deliberately poor (reverse) curriculum severely limits RL fine-tuning performance.Our proposed Self-Evolving Curriculum (SEC) significantly outperforms the standard random curriculum.See Sec.3.1 for details.</p>
<p>Figure 2 :
2
Figure 2: Overview of Self-Evolving Curriculum (SEC).SEC dynamically adjusts the training curriculum according to the model's current capabilities.During preprocessing, training data is partitioned into distinct categories (indicated by colors), e.g., by difficulty level or problem type.At each RL fine-tuning step: (1) The curriculum policy samples a training batch based on categories' expected learning gains; (2) The LLM policy is updated using the sampled batch and the chosen RL algorithm; (3) Rewards for curriculum categories are computed using advantage values estimated by the RL algorithm; (4) The curriculum policy is updated accordingly, refining future data selection.</p>
<p>( 1 )Figure 3 :
13
Figure 3: Average sample difficulty over training steps.SEC adaptively adjusts task difficulty during RL fine-tuning.Blue curves represent the sampled difficulty, smoothed using a moving average, while the green dashed line indicates the mean difficulty of the dataset.Across all benchmarks (columns) and model sizes (top: Qwen2.5-3B,bottom, Qwen2.5-7B),SEC initially selects easier problems and progressively introduces more challenging ones as training proceeds, effectively aligning difficulty with model improvement.</p>
<p>Figure 4 :
4
Figure 4: Performance comparison when training on multiple tasks.Left: Test accuracy of Qwen2.5-3B on ID and OOD splits.SEC-2D is implemented by defining an arm for each combination of problem type and difficulty level.SEC-2D consistently achieves higher accuracy, showing improved generalization compared to a random curriculum across tasks.Right: Countdown OOD accuracy vs. training steps, smoothed by a moving average.The random curriculum's performance collapses mid-training, highlighting its inability to effectively balance multiple tasks.In contrast, SEC-2D maintains stable performance throughout training.</p>
<p>Figure S1 :
S1
Figure S1: Distribution of difficulty levels in the MATH training set.</p>
<p>Table 1 :
1
Evaluation across reasoning tasks and curriculum methods.Accuracy is measured by averaging pass@1 over 8 independent generations per problem.In-distribution (ID) results are averaged over test problems sampled from the same three difficulty levels used in training.The best-performing curriculum strategy for each dataset and model size is shown in bold, and the second-best is underlined.SEC consistently achieves strong performance across tasks, particularly improving generalization on challenging OOD test problems.
TaskSplitQwen2.5 3BQwen2.5 7BRandom Ordered SEC (Ours) Random Ordered SEC (Ours)CountdownID OOD0.859 0.4790.551 0.3210.866 0.5420.858 0.5660.820 0.4420.872 0.555ZebraID OOD0.517 0.2850.534 0.3290.547 0.3450.573 0.3210.572 0.3110.587 0.355ARC-1DID OOD0.501 0.3130.476 0.3630.500 0.3810.512 0.4360.526 0.4280.514 0.418MATH5000.6680.6720.6720.7740.7590.761MathAMC22-230.3450.3520.3510.4860.4770.511AIME240.0750.0540.1000.1380.1500.175</p>
<p>illustrates how SEC adaptively adjusts training difficulty across tasks and models.For each task and model, the sampled difficulty (blue curves) initially starts below or around the dataset mean difficulty (green dashed line), indicating SEC's initial emphasis on easier problems to facilitate early-stage learning.As training progresses, SEC gradually increases the
Task Countdown Zebra ARCSplit ID OOD ID OOD ID OODRandom SEC-2D 0.837 0.839 0.418 0.428 0.513 0.539 0.254 0.312 0.380 0.418 0.251 0.327Test Score (Moving Averge)0.0 0.1 0.2 0.3SEC (Ours) Random Curriculum100200300400500600700
difficulty of selected problems, aligning the training complexity with the improving capabilities of the model.Notably, SEC consistently selects harder problems for the stronger Qwen2.5-7Bmodel compared to the smaller 3B model, further confirming SEC's ability to effectively adapt its curriculum to the model's learning capacity.This adaptive pattern across tasks and models highlights SEC's strength in dynamically adjusting problem difficulty to maximize learning outcomes.</p>
<p>Table 2 :
2
SEC
with alternative RL algo-rithms on Countdown. SEC improvesRL fine-tuning performance with differ-ent RL algorithms (PPO, RLOO), com-pared to a random curriculum.RL Method SplitRandomSECPPOID OOD0.621 0.1590.750 0.224RLOOID OOD0.821 0.4650.859 0.494</p>
<p>33819 2 + 33820 2 + 33821 2 + 33822 2 is divided by 17.Put your final answer within \boxed{}.Assistant: Let me solve this step by step.</p>
<p>A puzzle game where players combine a given set of numbers using basic arithmetic operations to reach a target number.
Acknowledgments and Disclosure of FundingWe thank Dzmitry Bahdanau and Nicolas Chapados for insightful discussions and assistance with this project.The authors acknowledge funding from CIFAR, NSERC and the Future of Life Institute.Minsu Kim was supported by KAIST Jang Yeong Sil Fellowship.A Social Impact StatementThis paper introduces SEC, a curriculum-learning method designed to enhance the reasoning capabilities of language models through reinforcement learning.By improving model accuracy and generalization, SEC can potentially lead to positive societal impacts, such as more reliable AI assistants and improved accessibility in educational settings.However, as with any method enhancing language models, there are potential negative implications: stronger reasoning abilities might facilitate misuse in tasks like misinformation or deceptive content generation.It is thus essential to employ these methods alongside responsible AI practices, including robust monitoring and mitigation strategies, to manage and minimize possible harmful effects.B Implementation DetailsTraining.All models are fine-tuned with the GRPO algorithms[44]as implemented in the Volcano Engine Reinforcement Learning (verl) library[45].We train separate 3B and 7B parameters variants of Qwen2.5[62].The fine-tuning processes last in total 240 gradient steps for Qwen2.5-3B and 120 steps for Qwen2.5-7B with a batch size of 256 on each of the three puzzle tasks.Advantages are estimated by 8 rollouts.Both models are trained for 240 steps on the math task.We do not apply the Kullback-Leibler (KL) divergence loss by setting the corresponding loss weight to be 0 across our study.We limit the max prompt length to be 1,024 tokens and the max response length to be 4,096 tokens.The model parameters are optimized using Adam[23]with a learning rate of 1e-6 and beta (0.9, 0.99) without warm-up steps.All of the training experiments are conducted on 4-8 NVIDIA H100 80GB GPUs.Hyperparameters for SEC used in each experiment is summarized in TableS1.TableS1: Hyperparameter settings (learning rate α and temperature τ ) used in each experiment.For the multi-task experiment in Sec.3.3, we fine-tune the Qwen2.5-3Bmodel for 3 × 240 = 720 steps on the mixed dataset.We use α = 0.5 and τ = 0.2.In Sec.3.4, we train Qwen2.5-3B for 120 steps in all the experiments.For RLOO, we similarly use 8 rollouts for advantage estimation and α = 0.5, τ = 0.25 for SEC.For PPO, we use α = 0.5, τ = 1 for SEC, and λ = 1, γ = 1 for the GAE parameters.Consistent with our main experiments, we do not apply the KL divergence loss.Models.Below we list the models used in our experiments:• Qwen2.5-3B:https://huggingface.co/Qwen/Qwen2.5-3B• Qwen2.5-7B:https://huggingface.co/Qwen/Qwen2.5-7BMath Datasets.Below we list the data sources used in our experiments:• MATH500: https://huggingface.co/datasets/HuggingFaceH4/MATH-500• AMC22-23: https://huggingface.co/datasets/AI-MO/aimo-validation-amc• AIME: https://huggingface.co/datasets/Maxwell-Jia/AIME_2024C Data ExamplesBelow we list the prompts and data examples for each task in our study.The prompt template is adopted from Pan et al.[37].Prompt for Countdown:A conversation between User and Assistant.The user asks a question, and the
Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, Sara Hooker, arXiv:2402.147402024arXiv preprint</p>
<p>Using confidence bounds for exploitation-exploration trade-offs. Peter Auer, Journal of Machine Learning Research. 3Nov. 2002</p>
<p>Online difficulty filtering for reasoning oriented reinforcement learning. Sanghwan Bae, Jiwoo Hong, Min Young Lee, Hanbyul Kim, Jeongyeon Nam, Donghyun Kwak, arXiv:2504.033802025arXiv preprint</p>
<p>Training a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam Mccandlish, Chris Olah, Ben Mann, Jared Kaplan, arXiv:2204.058622022arXiv preprint</p>
<p>Trajectory balance with asynchrony: Decoupling exploration and learning for fast. Siddarth Brian R Bartoldson, James Venkatraman, Moksh Diffenderfer, Tal Jain, Seanie Ben-Nun, Minsu Lee, Johan Kim, Yoshua Obando-Ceron, Bhavya Bengio, Kailkhura, arXiv:2503.18929scalable llm post-training. 2025arXiv preprint</p>
<p>Flow network based generative models for non-iterative diverse candidate generation. Emmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, Yoshua Bengio, Advances in Neural Information Processing Systems. 202134</p>
<p>Curriculum learning. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, Jason Weston, 10.1145/1553374.1553380Proceedings of the 26th Annual International Conference on Machine Learning, ICML '09. the 26th Annual International Conference on Machine Learning, ICML '09New York, NY, USAAssociation for Computing Machinery2009</p>
<p>The zone of proximal development in vygotsky's analysis of learning and instruction. Vygotsky's educational theory in cultural context. Seth Chaiklin, 20031</p>
<p>François Chollet, arXiv:1911.01547On the measure of intelligence. 2019arXiv preprint</p>
<p>Paul Christiano, Jan Leike, Tom B Brown, Miljan Martic, Shane Legg, Dario Amodei, arXiv:1706.03741Deep reinforcement learning from human preferences. 2017arXiv preprint</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Deepseek-Ai , 2025</p>
<p>Emergent complexity and zero-shot transfer via unsupervised environment design. Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew Critch, Sergey Levine, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Curran Associates, Inc202033</p>
<p>Reverse curriculum generation for reinforcement learning. Carlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, Pieter Abbeel, Conference on robot learning. PMLR2017</p>
<p>Automatic goal generation for reinforcement learning agents. Carlos Florensa, David Held, Xinyang Geng, Pieter Abbeel, International conference on machine learning. PMLR2018</p>
<p>Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, Noah D Goodman, arXiv:2503.013072025arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, D Song, J Steinhardt, NeurIPS Datasets and Benchmarks. 2021</p>
<p>Matthew Ho, Vincent Zhu, Xiaoyin Chen, Moksh Jain, Nikolay Malkin, Edwin Zhang, arXiv:2410.13224Proof flow: Preliminary study on generative flow network language model tuning for formal reasoning. 2024arXiv preprint</p>
<p>J Edward, Moksh Hu, Eric Jain, Younesse Elmoznino, Guillaume Kaddar, Lajoie, Yoshua Bengio, and Nikolay Malkin. Amortizing intractable inference in large language models. International Conference on Learning Representations (ICLR). 2024</p>
<p>Not all samples are created equal: Deep learning with importance sampling. Angelos Katharopoulos, François Fleuret, International conference on machine learning. PMLR2018</p>
<p>Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, Nicolas Le Roux, arXiv:2410.01679Vineppo: Unlocking rl potential for llm reasoning through refined credit assignment. 2024arXiv preprint</p>
<p>Minsu Kim, Sanghyeok Choi, Taeyoung Yun, Emmanuel Bengio, Leo Feng, Jarrid Rector-Brooks, Sungsoo Ahn, Jinkyoo Park, Nikolay Malkin, and Yoshua Bengio. Adaptive teachers for amortized samplers. International Conference on Learning Representations (ICLR). 2025</p>
<p>P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980Adam: A method for stochastic optimization. 2014arXiv preprint</p>
<p>Buy 4 REINFORCE samples, get a baseline for free! In ICLR Workshop on Deep Reinforcement Learning for Structured Prediction. Wouter Kool, Herke Van Hoof, Max Welling, 2019</p>
<p>Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, M Lei, Kay Zhang, Disha Mckinney, Cosmin Shrivastava, George Paduraru, Tucker, arXiv:2409.12917Doina Precup, Feryal Behbahani, and Aleksandra Faust. Training language models to self-correct via reinforcement learning. 2024arXiv preprint</p>
<p>Self-paced learning for latent variable models. M Pawan Kumar, Benjamin Packer, Daphne Koller, Advances in Neural Information Processing Systems (NeurIPS). 2010</p>
<p>Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James, V Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A Smith, Yizhong Wang, arXiv:2411.15124Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing frontiers in open language model post-training. 2024arXiv preprint</p>
<p>Let's verify step by step. Vineet Hunter Lightman, Yuri Kosaraju, Harrison Burda, John Edwards ; Leike, Ilya Schulman, Karl Sutskever, Cobbe, The Twelfth International Conference on Learning Representations. Bowen Baker, Teddy LeeJan. 2023</p>
<p>Online batch selection for faster training of neural networks. Ilya Loshchilov, Frank Hutter, arXiv:1511.063432015arXiv preprint</p>
<p>Deepcoder: A fully open-source 14b coder at o3-mini level. Michael Luo, Sijun Tan, Roy Huang, Xiaoxiang Shi, Rachel Xin, Colin Cai, Ameen Patel, Alpay Ariyak, Qingyang Wu, Ce Zhang, Li Erran Li, Raluca , Ada Popa, Ion Stoica, DeepCoder-A-Fully-Open-Source-14B-Coder-at-O3-mini-Level-1cf81902c14680b3bee5eb349a512a51. 2025Notion Blog</p>
<p>Teacher-student curriculum learning. Tambet Matiisen, Avital Oliver, Taco Cohen, John Schulman, 10.1109/TNNLS.2020.2983146IEEE Transactions on Neural Networks and Learning Systems. 3192020</p>
<p>Simpo: Simple preference optimization with a reference-free reward. Yu Meng, Mengzhou Xia, Danqi Chen, Advances in Neural Information Processing Systems. 202437</p>
<p>Michael Noukhovitch, Shengyi Huang, Sophie Xhonneux, Arian Hosseini, Rishabh Agarwal, Aaron Courville, arXiv:2410.18252Asynchronous rlhf: Faster and more efficient off-policy rl for language models. 2024arXiv preprint</p>
<p>. Open-Thought, 2025</p>
<p>Learning to reason with llms. Openai, </p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 202235</p>
<p>. Jiayi Pan, Junjie Zhang, Xingyao Wang, Lifan Yuan, Hao Peng, Alane Suhr, Tinyzero, 2025</p>
<p>Evolving curricula with regret-based environment design. Jack Parker-Holder, Minqi Jiang, Michael Dennis, Mikayel Samvelyan, Jakob Foerster, Edward Grefenstette, Tim Rocktäschel, Proceedings of the 39th International Conference on Machine Learning (ICML). the 39th International Conference on Machine Learning (ICML)2022</p>
<p>Automatic curriculum learning for deep rl: A short survey. Rémy Portelas, Cédric Colas, Lilian Weng, Katja Hofmann, Pierre-Yves Oudeyer, 10.24963/ijcai.2020/671International Joint Conference on Artificial Intelligence. 2020</p>
<p>Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Wenyi Zhao, Yu Yang, Xinyue Yang, Jiadai Sun, Shuntian Yao, arXiv:2411.02337Training llm web agents via self-evolving online curriculum reinforcement learning. 2024arXiv preprint</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, Advances in Neural Information Processing Systems. 362023</p>
<p>. Tom Schaul, John Quan, Ioannis Antonoglou, David Silver, arXiv:1511.059522015Prioritized experience replay. arXiv preprint</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y K Li, Y Wu, Daya Guo, arXiv:2402.03300Deepseekmath: Pushing the limits of mathematical reasoning in open language models. 2024arXiv preprint</p>
<p>Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, Chuan Wu, arXiv:2409.19256Hybridflow: A flexible and efficient rlhf framework. 2024arXiv preprint</p>
<p>Efficient reinforcement finetuning via adaptive curriculum learning. Taiwei Shi, Yiyang Wu, Linxin Song, Tianyi Zhou, Jieyu Zhao, arXiv:2504.055202025arXiv preprint</p>
<p>Fastcurl: Curriculum reinforcement learning with progressive context extension for efficient training r1-like reasoning models. Mingyang Song, Mao Zheng, Zheng Li, Wenjie Yang, Xuan Luo, Yue Pan, Feng Zhang, arXiv:2503.172872025arXiv preprint</p>
<p>Learning to predict by the methods of temporal differences. Richard S Sutton, 10.1023/A:1022633531479Mach. Learn. 0885-612531August 1988</p>
<p>Reinforcement Learning: An Introduction. Richard S Sutton, Andrew G Barto, 2018Bradford Book, Cambridge, MA, USA</p>
<p>Kimi k1. 5: Scaling reinforcement learning with llms. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, arXiv:2501.125992025arXiv preprint</p>
<p>On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. William R Thompson, Biometrika. 253/41933</p>
<p>Proximal curriculum for reinforcement learning agents. Georgios Tzannetos, Bárbara Gomes Ribeiro, Parameswaran Kamalaruban, Adish Singla, arXiv:2304.128772023arXiv preprint</p>
<p>Solving math word problems with process-and outcome-based feedback. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, Irina Higgins, arXiv:2211.142752022arXiv preprint</p>
<p>Mind in society: Development of higher psychological processes. Lev Semenovich, Vygotsky , Michael Cole, 1978Harvard university press</p>
<p>Paired open-ended trailblazer (poet): Endlessly generating increasingly complex and diverse learning environments and their solutions. Rui Wang, Joel Lehman, Jeff Clune, Kenneth O Stanley, Proceedings of the Genetic and Evolutionary Computation Conference (GECCO '19). the Genetic and Evolutionary Computation Conference (GECCO '19)ACM2019</p>
<p>Dump: Automated distributionlevel curriculum learning for rl-based llm post-training. Zhenting Wang, Guofeng Cui, Wan Kun, Wentian Zhao, arXiv:2504.097102025arXiv preprint</p>
<p>Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, Sida I Wang, arXiv:2502.18449Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution. 2025arXiv preprint</p>
<p>Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, arXiv:2503.10460Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond. 2025arXiv preprint</p>
<p>Simple statistical gradient-following algorithms for connectionist reinforcement learning. Williams Ronald, Machine learning. 81992</p>
<p>Training large language models for reasoning through reverse curriculum reinforcement learning. Zhiheng Xi, Wenxiang Chen, Boyang Hong, Senjie Jin, Rui Zheng, Wei He, Yiwen Ding, Shichun Liu, Xin Guo, Junzhe Wang, arXiv:2402.058082024arXiv preprint</p>
<p>Yudong Xu, Wenhao Li, Pashootan Vaezipoor, Scott Sanner, Elias B Khalil, arXiv:2305.18354Llms and the abstraction and reasoning corpus: Successes, failures, and the importance of object-based representations. 2023arXiv preprint</p>
<p>. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, arXiv:2412.1511520245 technical report. arXiv preprint</p>
<p>Demystifying long chain-of-thought reasoning in llms. Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, Xiang Yue, arXiv:2502.033732025arXiv preprint</p>
<p>Dapo: An open-source llm reinforcement learning system at scale. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, arXiv:2503.144762025arXiv preprint</p>
<p>Star: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah Goodman, Advances in Neural Information Processing Systems. 202235</p>
<p>M Daniel, Nisan Ziegler, Jeffrey Stiennon, Tom B Wu, Alec Brown, Dario Radford, Paul Amodei, Geoffrey Christiano, Irving, arXiv:1909.08593Fine-tuning language models from human preferences. 2019arXiv preprint</p>
<p>The bird keeper is directly left of the Dunhill smoker. </p>
<p>Alice is the dog owner. </p>
<p>Arnold is in the second house. </p>
<p>Alice is the Prince smoker. </p>
<p>What is Name of the person who lives in House 1? Provide only the name of the person as your final answer and put in in \boxed{}. Arnold is the cat lover. for example: \boxed{Alice}. Assistant: Let me solve this step by step</p>            </div>
        </div>

    </div>
</body>
</html>
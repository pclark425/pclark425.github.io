<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-622 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-622</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-622</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-322d91190acd8ac8c64598f5126947b0485ba249</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/322d91190acd8ac8c64598f5126947b0485ba249" target="_blank">Quantified Reproducibility Assessment of NLP Results</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> The proposed QRA method produces degree-of-reproducibility scores that are comparable across multiple reproductions not only of the same, but also of different, original studies, which facilitates insights into causes of variation between reproductions.</p>
                <p><strong>Paper Abstract:</strong> This paper describes and tests a method for carrying out quantified reproducibility assessment (QRA) that is based on concepts and definitions from metrology. QRA produces a single score estimating the degree of reproducibility of a given system and evaluation measure, on the basis of the scores from, and differences between, different reproductions. We test QRA on 18 different system and evaluation measure combinations (involving diverse NLP tasks and types of evaluation), for each of which we have the original results and one to seven reproduction results. The proposed QRA method produces degree-of-reproducibility scores that are comparable across multiple reproductions not only of the same, but also of different, original studies. We find that the proposed method facilitates insights into causes of variation between reproductions, and as a result, allows conclusions to be drawn about what aspects of system and/or evaluation design need to be changed in order to improve reproducibility.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e622.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e622.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QRA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Quantified Reproducibility Assessment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A metrology-derived method to quantify the degree of reproducibility of evaluation scores across multiple reproductions by computing precision (using a de-biased coefficient of variation) over measured quantity values and reporting confidence statistics alongside explicit conditions of measurement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various (NTS, PASS, multilingual essay scorers)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing (NLP)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Assessing reproducibility of evaluation scores across original and reproduction studies for systems (e.g., neural text simplifiers, rule-based generators, classifiers)</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Variation in conditions of measurement (system code, compile/training environment, dependencies, evaluation method implementation, evaluation script/tokeniser versions, whether reproductions regenerate outputs or reuse original outputs, test set / cross-validation folds, random seeds, who performed measurement and evaluator cohorts for human evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Coefficient of variation with small-sample correction (CV*), sample mean, unbiased sample standard deviation (s*), 95% confidence intervals for standard deviation, percentage within n standard deviations (mentioned as possible).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>QRA reports CV* values per object/measurand: examples from the paper include CV*=1.562 (NTS_def BLEU, sample size 7), CV*=2.487 (NTS_def SARI, n=5), CV*=4.176 (NTS-w2v_def BLEU, n=6), CV*=3.572 (NTS-w2v_def SARI, n=4), PASS: Clarity CV*=13.193 (n=2), Fluency CV*=16.372 (n=2), Stance identifiability CV*=6.107 (n=2), Essay-scoring variants: CV* range e.g. mult-POS ≈3.81 (n=8) and mult-dom ≈17.15 (n=8).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Same as variability metrics: CV* (primary), mean and s*, and confidence intervals; repeatability defined when conditions identical, reproducibility when some condition values differ. The method explicitly requires reporting of condition values for interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Quantified reproducibility scores (CV*) allow cross-study comparisons; example takeaways: some measurands/systems show low CV* (good reproducibility) e.g. mult-POS wF1 CV*≈3.81, some high CV* (poor) e.g. mult-dom wF1 CV*≈17.15 and PASS Fluency CV*≈16.37. QRA finds that standardising conditions reduces CV* (see mitigation examples below).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Lack of standard definitions and reporting in NLP; many dimensions of variation (not just code/data/team); small sample sizes in reproduction studies requiring de-biased estimators; missing or nonstandardised evaluation scripts (BLEU implementations/tokenisers); variations arising from dependencies and run-time environments; human evaluator cohort and interface differences; different test folds or cross-validation seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Explicitly specify conditions of measurement (object, measurand, system code, compile/training, method specification, implementation, procedure, test set, performed by); use repeatability tests (identical conditions except operator) to get baseline; share outputs when possible to separate evaluation-script variability from system-run variability; use standardised evaluation implementations (e.g., SacreBLEU for BLEU); provide checklists and datasheets; report random seeds and cross-validation splits; use de-biased estimators and small-sample corrections (unbiased s* and CV* correction).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Observed reductions: when reproductions evaluated exactly the same system outputs, CV* for SARI could be 0 (perfect); for BLEU, using identical outputs but different BLEU scripts produced CV* as low as 0.838 (NTS_def, four scores) and 1.314 (NTS-w2v_def, three scores). Running systems in different environments increased BLEU CV* to 2.154 (NTS_def variants) and 6.598 (NTS-w2v_def variants) compared to the lower CV* values when outputs or scripts were held constant — demonstrating that standardising outputs and evaluation scripts substantially reduces measured variability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Varies by experiment: examples in paper include NTS_def BLEU n=7, NTS_def SARI n=5, NTS-w2v_def BLEU n=6, PASS human evaluations n=2 (original + one reproduction), essay-scoring variants n=8 (one original + seven reproductions).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>QRA operationalises metrology concepts to produce a single, comparable reproducibility score (CV*) and associated CIs; it identifies concrete sources of variability (evaluation scripts, dependencies, environment, human evaluators) and shows that standardising conditions (e.g., reusing same outputs, using standard scripts) materially reduces measured variability, enabling actionable recommendations to improve reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantified Reproducibility Assessment of NLP Results', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e622.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e622.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CV*</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Small-sample-corrected Coefficient of Variation (CV*)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scale-invariant precision metric used as the primary quantified reproducibility score in QRA: the coefficient of variation (standard deviation divided by mean) with a small-sample correction applied to reduce bias when sample sizes are small.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applied to scores from systems (NTS, PASS, essay scoring variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing (NLP) reproducibility assessment</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Quantify precision (degree of reproducibility) of measured evaluation scores across multiple reproductions</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Not a source itself, but used to quantify variability arising from measurement differences (see QRA sources).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>CV* computed from unbiased sample standard deviation s* and mean; confidence intervals for s* via t-distribution; small-sample correction CV* = (1 + 1/(4n)) * CV; shifting of scales for measures not starting at zero before computing CV*.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Paper reports numerous CV* values as the primary reproducibility scores: e.g., PASS Clarity CV*=13.193, PASS Fluency CV*=16.372, NTS_def BLEU CV*=1.562, NTS_def SARI CV*=2.487, NTS-w2v_def BLEU CV*=4.176, essay-scoring mult-POS CV*≈3.818, mult-dom CV*≈17.147.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>CV* is explicitly used as the reproducibility metric; supplementary measures reported include mean, unbiased stdev s*, and 95% CI for s*.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>CV* provided a scale-invariant, comparable assessment across diverse tasks and evaluation measures; enabled ranking of reproducibility across systems/measurands (lower CV* = better reproducibility).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>CV* can be sensitive when means are near zero (hence scores shifted to start at zero where needed); small sample sizes necessitate de-biased estimators and careful CI estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Use unbiased sample standard deviation s*, t-distribution CIs, small-sample correction to CV*; shift scores for measures with non-zero lower bounds before computing CV*.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Small-sample correction and unbiased estimators reduce bias in CV estimates for the small n used in reproduction studies (explicit formulae and reference to Sokal & Rohlf (1971) and Rao (1973) provided).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CV* (with small-sample correction and de-biased estimators) is an appropriate, scale-invariant metric for quantifying reproducibility across heterogeneous NLP evaluation measures and small-sample reproduction studies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantified Reproducibility Assessment of NLP Results', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e622.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e622.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NTS-BLEU/SARI case</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Text Simplification (NTS) reproducibility case (BLEU and SARI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical analysis showing how differences in evaluation scripts, tokenisers, regenerating outputs, and dependencies (openNMT, word2vec) produce measurable variability in BLEU and SARI scores across reproductions of Nisioi et al. (2017) systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring neural text simplification models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Nisioi et al.'s NTS (two variants: NTS_def and NTS-w2v_def)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing — text simplification / machine translation style evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Reproductions of original NTS outputs and re-evaluation under differing evaluation scripts, tokenisers and run environments to assess variability in BLEU and SARI scores.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Different BLEU script implementations and tokeniser versions, whether reproductions regenerated system outputs or reused original outputs, differences in compile/training environment (who compiled/trained), third-party dependencies (openNMT, word2vec), different run-time environments and evaluation implementations (SacreBLEU vs original BLEU script).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>CV* (coefficient of variation with correction), mean, unbiased stdev s*, 95% CI for stdev; absolute score differences (e.g., up to 1.4 BLEU points between runs using different scripts).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Examples: NTS_def BLEU overall CV*=1.562 (n=7); NTS_def SARI CV*=2.487 (n=5); NTS-w2v_def BLEU CV*=4.176 (n=6); NTS-w2v_def SARI CV*=3.572 (n=4). When same outputs reused but evaluation scripts differed: BLEU CV* as low as 0.838 (NTS_def, 4 scores) and 1.314 (NTS-w2v_def, 3 scores). When outputs regenerated and environments differed, BLEU CV* increased to 2.154 (NTS_def variants subset) and 6.598 (NTS-w2v_def variants subset). Observed BLEU score differences up to 1.4 points attributable to BLEU script differences.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>CV* and direct comparison of measured quantity values across reproductions; identification of which condition values differ (e.g., Implem. by, Comp./trained by, Procedure, Performed by) to interpret CV*.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Reproducibility was significantly degraded by differing evaluation scripts and run environments; holding outputs constant removed variability due to model runs (SARI CV* could be 0 when same outputs used), while differing evaluation scripts still introduced measurable variability (BLEU differences and nonzero CV*). Systems depending on more external tools (NTS-w2v_def using both openNMT and word2vec) showed higher CV* than simpler variants.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Nonstandardised BLEU implementations and tokenisers; dependency-induced variability across environments; difficulty in re-running original evaluation tokens due to library changes (NLTK tokeniser changes); partial reproductions where outputs are not regenerated confound sources of variation.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Re-use original outputs to separate evaluation variability from run-time variability; standardise evaluation scripts (use SacreBLEU or well-documented, versioned scripts); document and share full environment/compile/training instructions and dependencies; explicitly list conditions of measurement; perform repeatability tests under identical conditions to obtain baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Empirical evidence: same-outputs evaluations produced CV*≈0 for SARI and much lower CV* for BLEU (0.838 / 1.314) compared to cases where outputs were regenerated and environments varied (BLEU CV* up to 2.154 / 6.598), showing that standardising outputs and evaluation implementations substantially reduces variability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>NTS_def BLEU n=7 (various reproductions), NTS_def SARI n=5, NTS-w2v_def BLEU n=6, NTS-w2v_def SARI n=4; subsets with identical outputs smaller (e.g., 3-4).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>BLEU and SARI reproducibility is sensitive to whether reproductions regenerate outputs, to evaluation script/tokeniser choices, and to dependencies; standardising outputs and evaluation implementations materially reduces measured variability, and systems with more third-party dependencies show larger reproducibility problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantified Reproducibility Assessment of NLP Results', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e622.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e622.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PASS human-eval case</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PASS (football report generator) human evaluation reproducibility case</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reproduction of human evaluations (Clarity, Fluency, Stance Identifiability) showing variability across evaluator cohorts and evaluation procedure differences, quantified using CV*.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PASS: A Dutch data-to-text system for soccer, targeted towards specific audiences</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PASS (rule-based football report generator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Generation / NLP human evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Compare original human assessment (van der Lee et al. 2017) with reproduction (Mille et al. 2021) on 7-point rating scales and a binary identifiability task to assess reproducibility of human-evaluated measures.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Different evaluation interface and evaluator cohorts, who performed the procedure, and minor changes in Implemented by/Procedure even when system outputs were identical (the reproducing study reused original outputs). Human rater subjectivity and rating scale properties (7-point vs binary) are sources.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>CV* (computed on shifted scores for measures not starting at 0), mean, unbiased stdev s*, 95% CI for stdev; for identifiability the metric is percent correct then shifted for CV* computation.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>PASS results: Clarity scores 5.64 vs 6.30 (n=2) -> CV*=13.193; Fluency 5.36 vs 6.14 -> CV*=16.372; Stance identifiability 91% vs 97% -> CV*=6.107. These indicate human-assessed Fluency had highest variability, Stance identifiability lowest.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>CV* and component statistics; explicit reporting of which condition values differed (evaluator cohort, interface, performed by) to interpret reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Reproducibility varied by measurand: binary identifiability showed better reproducibility (lower CV*) than 7-point subjective scales, indicating that task design and scale choice affect reproducibility of human evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Human evaluator cohorts and interfaces differ across reproductions; subjectivity of rating scales; small sample of reproductions (often only one reproduction available) limits statistical power.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Document and report detailed measurement procedure conditions (who performed evaluation, procedure, interface), use human evaluation datasheets and checklists, and where possible perform repeatability tests (same conditions, different operators) to get baseline reproducibility and detect whether variability arises from method or cohorts.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Paper argues (and shows via CV*) that holding outputs constant but changing evaluators/procedure can still produce substantial CV* (PASS Fluency CV*=16.37), implying that better standardisation of procedure and clearer measurement design (e.g., simpler/binary tasks) can improve reproducibility, though no numeric before/after mitigation experiment reported beyond the two evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>2 (original study + one reproduction) for each PASS measurand reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Human evaluation reproducibility depends strongly on task design and evaluator agreement; binary tasks (identifiability) can be more reproducible than subjective multi-point rating scales, and explicitly specifying/standardising measurement procedure and evaluator selection can reduce variability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantified Reproducibility Assessment of NLP Results', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e622.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e622.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Essay-scoring case</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multilingual essay scoring reproducibility case (Vajjala & Rama variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>QRA applied to 11 multilingual essay scoring system variants across multiple reproduction studies showing that feature choices, model types, seeds and environment differences affect reproducibility as measured by CV*.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Experiments with universal CEFR classification</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multilingual essay scoring system variants (mult-base, mult-word, mult-POS, mult-dep, mult-dom, mult-emb; various +/- language-info variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Educational NLP / Automatic essay scoring</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Assess reproducibility of weighted F1 (wF1) scores across five reproduction papers for 11 classifier variants using QRA to compute CV* and interpret which design choices affect reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Different compile/run-time environments, different random seeding and cross-validation strategies (seed i), different test sets or CV folds (ii), minor differences in implementation by reproduction teams, possible evaluation-code issues (e.g., macro-F1 vs weighted-F1 misapplication), and whether models include language-identifying features, embeddings or domain-specific features.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>CV* (primary), mean, unbiased stdev s*, 95% CI for stdev, sample size n (usually 8 for these sets).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>wF1 CV* examples (n=8): mult-base CV*=14.633; mult-word^- CV*=10.609; mult-word^+ CV*=10.440; mult-POS^- CV*=3.818; mult-POS^+ CV*=3.808; mult-dep^- CV*=4.500; mult-dep^+ CV*=4.387; mult-dom^- CV*=17.147; mult-dom^+ CV*=18.248; mult-emb^- CV*=17.033; mult-emb^+ CV*=16.226. These show syntactic-feature models (mult-POS, mult-dep) had much better reproducibility (lower CV*) than domain-feature or embedding-based models.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>CV* and inspection of conditions of measurement across eight scores (original + seven reproductions) to attribute variability to seeds, environment, or feature pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Using/including language identification made little difference to CV*; models using syntactic features (random-forest variants) were consistently more reproducible (CV* ≈3.8–4.5) than logistic regressors using domain or embedding features (CV* ≈16–18). Baseline and some high CV* cases may be due to evaluation-code errors in some reproductions.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Reproductions used different seeds, environments, and occasionally different evaluation code (leading to evaluation metric errors); pipelines that rely on external preprocessing or embeddings are harder to reproduce; small but real differences in implementation and test fold selection propagate into measurable CV*.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Specify seeds and cross-validation folds, provide full pipeline/code and environment, list condition values in measurement datasheets, perform repeatability QRA under identical conditions to establish baseline expected variability, and investigate/standardise feature extraction pipelines (esp. for embeddings and domain features).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Empirical pattern: syntactic-feature pipelines (less reliance on unstable external artifacts) exhibited low CV* without further intervention, indicating that reducing dependency complexity is effective; explicit numeric experiments altering conditions were not performed beyond the cross-study comparisons, but the contrast between CV*≈3.8 and CV*≈17 suggests substantial gains by simplifying/standardising feature pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>8 (one original + seven reproduction studies) per system variant in the essay-scoring reproducibility analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Reproducibility is strongly linked to system design: models relying on syntactic features and simpler pipelines are far more reproducible across independent reproductions than those depending on domain-specific features or dense embeddings; QRA makes these differences explicit and comparable using CV*.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantified Reproducibility Assessment of NLP Results', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A systematic review of reproducibility research in natural language processing <em>(Rating: 2)</em></li>
                <li>The reprogen shared task on reproducibility of human evaluations in NLG: Overview and results <em>(Rating: 2)</em></li>
                <li>A call for clarity in reporting bleu scores <em>(Rating: 2)</em></li>
                <li>CombiNMT: An exploration into neural text simplification models <em>(Rating: 2)</em></li>
                <li>Another PASS: A reproduction study of the human evaluation of a football report generation system <em>(Rating: 2)</em></li>
                <li>REPROLANG2020: A shared task of a new, collaborative type to foster reproducibility <em>(Rating: 2)</em></li>
                <li>The machine learning reproducibility checklist v2.0 <em>(Rating: 2)</em></li>
                <li>The human evaluation datasheet 1.0: A template for recording details of human evaluation experiments in NLP <em>(Rating: 2)</em></li>
                <li>Exploring neural text simplification models <em>(Rating: 2)</em></li>
                <li>Experiments with universal CEFR classification <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-622",
    "paper_id": "paper-322d91190acd8ac8c64598f5126947b0485ba249",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "QRA",
            "name_full": "Quantified Reproducibility Assessment",
            "brief_description": "A metrology-derived method to quantify the degree of reproducibility of evaluation scores across multiple reproductions by computing precision (using a de-biased coefficient of variation) over measured quantity values and reporting confidence statistics alongside explicit conditions of measurement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "various (NTS, PASS, multilingual essay scorers)",
            "model_size": null,
            "scientific_domain": "Natural Language Processing (NLP)",
            "experimental_task": "Assessing reproducibility of evaluation scores across original and reproduction studies for systems (e.g., neural text simplifiers, rule-based generators, classifiers)",
            "variability_sources": "Variation in conditions of measurement (system code, compile/training environment, dependencies, evaluation method implementation, evaluation script/tokeniser versions, whether reproductions regenerate outputs or reuse original outputs, test set / cross-validation folds, random seeds, who performed measurement and evaluator cohorts for human evaluation).",
            "variability_measured": true,
            "variability_metrics": "Coefficient of variation with small-sample correction (CV*), sample mean, unbiased sample standard deviation (s*), 95% confidence intervals for standard deviation, percentage within n standard deviations (mentioned as possible).",
            "variability_results": "QRA reports CV* values per object/measurand: examples from the paper include CV*=1.562 (NTS_def BLEU, sample size 7), CV*=2.487 (NTS_def SARI, n=5), CV*=4.176 (NTS-w2v_def BLEU, n=6), CV*=3.572 (NTS-w2v_def SARI, n=4), PASS: Clarity CV*=13.193 (n=2), Fluency CV*=16.372 (n=2), Stance identifiability CV*=6.107 (n=2), Essay-scoring variants: CV* range e.g. mult-POS ≈3.81 (n=8) and mult-dom ≈17.15 (n=8).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Same as variability metrics: CV* (primary), mean and s*, and confidence intervals; repeatability defined when conditions identical, reproducibility when some condition values differ. The method explicitly requires reporting of condition values for interpretation.",
            "reproducibility_results": "Quantified reproducibility scores (CV*) allow cross-study comparisons; example takeaways: some measurands/systems show low CV* (good reproducibility) e.g. mult-POS wF1 CV*≈3.81, some high CV* (poor) e.g. mult-dom wF1 CV*≈17.15 and PASS Fluency CV*≈16.37. QRA finds that standardising conditions reduces CV* (see mitigation examples below).",
            "reproducibility_challenges": "Lack of standard definitions and reporting in NLP; many dimensions of variation (not just code/data/team); small sample sizes in reproduction studies requiring de-biased estimators; missing or nonstandardised evaluation scripts (BLEU implementations/tokenisers); variations arising from dependencies and run-time environments; human evaluator cohort and interface differences; different test folds or cross-validation seeds.",
            "mitigation_methods": "Explicitly specify conditions of measurement (object, measurand, system code, compile/training, method specification, implementation, procedure, test set, performed by); use repeatability tests (identical conditions except operator) to get baseline; share outputs when possible to separate evaluation-script variability from system-run variability; use standardised evaluation implementations (e.g., SacreBLEU for BLEU); provide checklists and datasheets; report random seeds and cross-validation splits; use de-biased estimators and small-sample corrections (unbiased s* and CV* correction).",
            "mitigation_effectiveness": "Observed reductions: when reproductions evaluated exactly the same system outputs, CV* for SARI could be 0 (perfect); for BLEU, using identical outputs but different BLEU scripts produced CV* as low as 0.838 (NTS_def, four scores) and 1.314 (NTS-w2v_def, three scores). Running systems in different environments increased BLEU CV* to 2.154 (NTS_def variants) and 6.598 (NTS-w2v_def variants) compared to the lower CV* values when outputs or scripts were held constant — demonstrating that standardising outputs and evaluation scripts substantially reduces measured variability.",
            "comparison_with_without_controls": true,
            "number_of_runs": "Varies by experiment: examples in paper include NTS_def BLEU n=7, NTS_def SARI n=5, NTS-w2v_def BLEU n=6, PASS human evaluations n=2 (original + one reproduction), essay-scoring variants n=8 (one original + seven reproductions).",
            "key_findings": "QRA operationalises metrology concepts to produce a single, comparable reproducibility score (CV*) and associated CIs; it identifies concrete sources of variability (evaluation scripts, dependencies, environment, human evaluators) and shows that standardising conditions (e.g., reusing same outputs, using standard scripts) materially reduces measured variability, enabling actionable recommendations to improve reproducibility.",
            "uuid": "e622.0",
            "source_info": {
                "paper_title": "Quantified Reproducibility Assessment of NLP Results",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "CV*",
            "name_full": "Small-sample-corrected Coefficient of Variation (CV*)",
            "brief_description": "A scale-invariant precision metric used as the primary quantified reproducibility score in QRA: the coefficient of variation (standard deviation divided by mean) with a small-sample correction applied to reduce bias when sample sizes are small.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "applied to scores from systems (NTS, PASS, essay scoring variants)",
            "model_size": null,
            "scientific_domain": "Natural Language Processing (NLP) reproducibility assessment",
            "experimental_task": "Quantify precision (degree of reproducibility) of measured evaluation scores across multiple reproductions",
            "variability_sources": "Not a source itself, but used to quantify variability arising from measurement differences (see QRA sources).",
            "variability_measured": true,
            "variability_metrics": "CV* computed from unbiased sample standard deviation s* and mean; confidence intervals for s* via t-distribution; small-sample correction CV* = (1 + 1/(4n)) * CV; shifting of scales for measures not starting at zero before computing CV*.",
            "variability_results": "Paper reports numerous CV* values as the primary reproducibility scores: e.g., PASS Clarity CV*=13.193, PASS Fluency CV*=16.372, NTS_def BLEU CV*=1.562, NTS_def SARI CV*=2.487, NTS-w2v_def BLEU CV*=4.176, essay-scoring mult-POS CV*≈3.818, mult-dom CV*≈17.147.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "CV* is explicitly used as the reproducibility metric; supplementary measures reported include mean, unbiased stdev s*, and 95% CI for s*.",
            "reproducibility_results": "CV* provided a scale-invariant, comparable assessment across diverse tasks and evaluation measures; enabled ranking of reproducibility across systems/measurands (lower CV* = better reproducibility).",
            "reproducibility_challenges": "CV* can be sensitive when means are near zero (hence scores shifted to start at zero where needed); small sample sizes necessitate de-biased estimators and careful CI estimation.",
            "mitigation_methods": "Use unbiased sample standard deviation s*, t-distribution CIs, small-sample correction to CV*; shift scores for measures with non-zero lower bounds before computing CV*.",
            "mitigation_effectiveness": "Small-sample correction and unbiased estimators reduce bias in CV estimates for the small n used in reproduction studies (explicit formulae and reference to Sokal & Rohlf (1971) and Rao (1973) provided).",
            "comparison_with_without_controls": null,
            "number_of_runs": null,
            "key_findings": "CV* (with small-sample correction and de-biased estimators) is an appropriate, scale-invariant metric for quantifying reproducibility across heterogeneous NLP evaluation measures and small-sample reproduction studies.",
            "uuid": "e622.1",
            "source_info": {
                "paper_title": "Quantified Reproducibility Assessment of NLP Results",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "NTS-BLEU/SARI case",
            "name_full": "Neural Text Simplification (NTS) reproducibility case (BLEU and SARI)",
            "brief_description": "Empirical analysis showing how differences in evaluation scripts, tokenisers, regenerating outputs, and dependencies (openNMT, word2vec) produce measurable variability in BLEU and SARI scores across reproductions of Nisioi et al. (2017) systems.",
            "citation_title": "Exploring neural text simplification models",
            "mention_or_use": "use",
            "model_name": "Nisioi et al.'s NTS (two variants: NTS_def and NTS-w2v_def)",
            "model_size": null,
            "scientific_domain": "Natural Language Processing — text simplification / machine translation style evaluation",
            "experimental_task": "Reproductions of original NTS outputs and re-evaluation under differing evaluation scripts, tokenisers and run environments to assess variability in BLEU and SARI scores.",
            "variability_sources": "Different BLEU script implementations and tokeniser versions, whether reproductions regenerated system outputs or reused original outputs, differences in compile/training environment (who compiled/trained), third-party dependencies (openNMT, word2vec), different run-time environments and evaluation implementations (SacreBLEU vs original BLEU script).",
            "variability_measured": true,
            "variability_metrics": "CV* (coefficient of variation with correction), mean, unbiased stdev s*, 95% CI for stdev; absolute score differences (e.g., up to 1.4 BLEU points between runs using different scripts).",
            "variability_results": "Examples: NTS_def BLEU overall CV*=1.562 (n=7); NTS_def SARI CV*=2.487 (n=5); NTS-w2v_def BLEU CV*=4.176 (n=6); NTS-w2v_def SARI CV*=3.572 (n=4). When same outputs reused but evaluation scripts differed: BLEU CV* as low as 0.838 (NTS_def, 4 scores) and 1.314 (NTS-w2v_def, 3 scores). When outputs regenerated and environments differed, BLEU CV* increased to 2.154 (NTS_def variants subset) and 6.598 (NTS-w2v_def variants subset). Observed BLEU score differences up to 1.4 points attributable to BLEU script differences.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "CV* and direct comparison of measured quantity values across reproductions; identification of which condition values differ (e.g., Implem. by, Comp./trained by, Procedure, Performed by) to interpret CV*.",
            "reproducibility_results": "Reproducibility was significantly degraded by differing evaluation scripts and run environments; holding outputs constant removed variability due to model runs (SARI CV* could be 0 when same outputs used), while differing evaluation scripts still introduced measurable variability (BLEU differences and nonzero CV*). Systems depending on more external tools (NTS-w2v_def using both openNMT and word2vec) showed higher CV* than simpler variants.",
            "reproducibility_challenges": "Nonstandardised BLEU implementations and tokenisers; dependency-induced variability across environments; difficulty in re-running original evaluation tokens due to library changes (NLTK tokeniser changes); partial reproductions where outputs are not regenerated confound sources of variation.",
            "mitigation_methods": "Re-use original outputs to separate evaluation variability from run-time variability; standardise evaluation scripts (use SacreBLEU or well-documented, versioned scripts); document and share full environment/compile/training instructions and dependencies; explicitly list conditions of measurement; perform repeatability tests under identical conditions to obtain baselines.",
            "mitigation_effectiveness": "Empirical evidence: same-outputs evaluations produced CV*≈0 for SARI and much lower CV* for BLEU (0.838 / 1.314) compared to cases where outputs were regenerated and environments varied (BLEU CV* up to 2.154 / 6.598), showing that standardising outputs and evaluation implementations substantially reduces variability.",
            "comparison_with_without_controls": true,
            "number_of_runs": "NTS_def BLEU n=7 (various reproductions), NTS_def SARI n=5, NTS-w2v_def BLEU n=6, NTS-w2v_def SARI n=4; subsets with identical outputs smaller (e.g., 3-4).",
            "key_findings": "BLEU and SARI reproducibility is sensitive to whether reproductions regenerate outputs, to evaluation script/tokeniser choices, and to dependencies; standardising outputs and evaluation implementations materially reduces measured variability, and systems with more third-party dependencies show larger reproducibility problems.",
            "uuid": "e622.2",
            "source_info": {
                "paper_title": "Quantified Reproducibility Assessment of NLP Results",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "PASS human-eval case",
            "name_full": "PASS (football report generator) human evaluation reproducibility case",
            "brief_description": "Reproduction of human evaluations (Clarity, Fluency, Stance Identifiability) showing variability across evaluator cohorts and evaluation procedure differences, quantified using CV*.",
            "citation_title": "PASS: A Dutch data-to-text system for soccer, targeted towards specific audiences",
            "mention_or_use": "use",
            "model_name": "PASS (rule-based football report generator)",
            "model_size": null,
            "scientific_domain": "Natural Language Generation / NLP human evaluation",
            "experimental_task": "Compare original human assessment (van der Lee et al. 2017) with reproduction (Mille et al. 2021) on 7-point rating scales and a binary identifiability task to assess reproducibility of human-evaluated measures.",
            "variability_sources": "Different evaluation interface and evaluator cohorts, who performed the procedure, and minor changes in Implemented by/Procedure even when system outputs were identical (the reproducing study reused original outputs). Human rater subjectivity and rating scale properties (7-point vs binary) are sources.",
            "variability_measured": true,
            "variability_metrics": "CV* (computed on shifted scores for measures not starting at 0), mean, unbiased stdev s*, 95% CI for stdev; for identifiability the metric is percent correct then shifted for CV* computation.",
            "variability_results": "PASS results: Clarity scores 5.64 vs 6.30 (n=2) -&gt; CV*=13.193; Fluency 5.36 vs 6.14 -&gt; CV*=16.372; Stance identifiability 91% vs 97% -&gt; CV*=6.107. These indicate human-assessed Fluency had highest variability, Stance identifiability lowest.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "CV* and component statistics; explicit reporting of which condition values differed (evaluator cohort, interface, performed by) to interpret reproducibility.",
            "reproducibility_results": "Reproducibility varied by measurand: binary identifiability showed better reproducibility (lower CV*) than 7-point subjective scales, indicating that task design and scale choice affect reproducibility of human evaluations.",
            "reproducibility_challenges": "Human evaluator cohorts and interfaces differ across reproductions; subjectivity of rating scales; small sample of reproductions (often only one reproduction available) limits statistical power.",
            "mitigation_methods": "Document and report detailed measurement procedure conditions (who performed evaluation, procedure, interface), use human evaluation datasheets and checklists, and where possible perform repeatability tests (same conditions, different operators) to get baseline reproducibility and detect whether variability arises from method or cohorts.",
            "mitigation_effectiveness": "Paper argues (and shows via CV*) that holding outputs constant but changing evaluators/procedure can still produce substantial CV* (PASS Fluency CV*=16.37), implying that better standardisation of procedure and clearer measurement design (e.g., simpler/binary tasks) can improve reproducibility, though no numeric before/after mitigation experiment reported beyond the two evaluations.",
            "comparison_with_without_controls": true,
            "number_of_runs": "2 (original study + one reproduction) for each PASS measurand reported.",
            "key_findings": "Human evaluation reproducibility depends strongly on task design and evaluator agreement; binary tasks (identifiability) can be more reproducible than subjective multi-point rating scales, and explicitly specifying/standardising measurement procedure and evaluator selection can reduce variability.",
            "uuid": "e622.3",
            "source_info": {
                "paper_title": "Quantified Reproducibility Assessment of NLP Results",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Essay-scoring case",
            "name_full": "Multilingual essay scoring reproducibility case (Vajjala & Rama variants)",
            "brief_description": "QRA applied to 11 multilingual essay scoring system variants across multiple reproduction studies showing that feature choices, model types, seeds and environment differences affect reproducibility as measured by CV*.",
            "citation_title": "Experiments with universal CEFR classification",
            "mention_or_use": "use",
            "model_name": "Multilingual essay scoring system variants (mult-base, mult-word, mult-POS, mult-dep, mult-dom, mult-emb; various +/- language-info variants)",
            "model_size": null,
            "scientific_domain": "Educational NLP / Automatic essay scoring",
            "experimental_task": "Assess reproducibility of weighted F1 (wF1) scores across five reproduction papers for 11 classifier variants using QRA to compute CV* and interpret which design choices affect reproducibility.",
            "variability_sources": "Different compile/run-time environments, different random seeding and cross-validation strategies (seed i), different test sets or CV folds (ii), minor differences in implementation by reproduction teams, possible evaluation-code issues (e.g., macro-F1 vs weighted-F1 misapplication), and whether models include language-identifying features, embeddings or domain-specific features.",
            "variability_measured": true,
            "variability_metrics": "CV* (primary), mean, unbiased stdev s*, 95% CI for stdev, sample size n (usually 8 for these sets).",
            "variability_results": "wF1 CV* examples (n=8): mult-base CV*=14.633; mult-word^- CV*=10.609; mult-word^+ CV*=10.440; mult-POS^- CV*=3.818; mult-POS^+ CV*=3.808; mult-dep^- CV*=4.500; mult-dep^+ CV*=4.387; mult-dom^- CV*=17.147; mult-dom^+ CV*=18.248; mult-emb^- CV*=17.033; mult-emb^+ CV*=16.226. These show syntactic-feature models (mult-POS, mult-dep) had much better reproducibility (lower CV*) than domain-feature or embedding-based models.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "CV* and inspection of conditions of measurement across eight scores (original + seven reproductions) to attribute variability to seeds, environment, or feature pipelines.",
            "reproducibility_results": "Using/including language identification made little difference to CV*; models using syntactic features (random-forest variants) were consistently more reproducible (CV* ≈3.8–4.5) than logistic regressors using domain or embedding features (CV* ≈16–18). Baseline and some high CV* cases may be due to evaluation-code errors in some reproductions.",
            "reproducibility_challenges": "Reproductions used different seeds, environments, and occasionally different evaluation code (leading to evaluation metric errors); pipelines that rely on external preprocessing or embeddings are harder to reproduce; small but real differences in implementation and test fold selection propagate into measurable CV*.",
            "mitigation_methods": "Specify seeds and cross-validation folds, provide full pipeline/code and environment, list condition values in measurement datasheets, perform repeatability QRA under identical conditions to establish baseline expected variability, and investigate/standardise feature extraction pipelines (esp. for embeddings and domain features).",
            "mitigation_effectiveness": "Empirical pattern: syntactic-feature pipelines (less reliance on unstable external artifacts) exhibited low CV* without further intervention, indicating that reducing dependency complexity is effective; explicit numeric experiments altering conditions were not performed beyond the cross-study comparisons, but the contrast between CV*≈3.8 and CV*≈17 suggests substantial gains by simplifying/standardising feature pipelines.",
            "comparison_with_without_controls": true,
            "number_of_runs": "8 (one original + seven reproduction studies) per system variant in the essay-scoring reproducibility analysis.",
            "key_findings": "Reproducibility is strongly linked to system design: models relying on syntactic features and simpler pipelines are far more reproducible across independent reproductions than those depending on domain-specific features or dense embeddings; QRA makes these differences explicit and comparable using CV*.",
            "uuid": "e622.4",
            "source_info": {
                "paper_title": "Quantified Reproducibility Assessment of NLP Results",
                "publication_date_yy_mm": "2022-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A systematic review of reproducibility research in natural language processing",
            "rating": 2
        },
        {
            "paper_title": "The reprogen shared task on reproducibility of human evaluations in NLG: Overview and results",
            "rating": 2
        },
        {
            "paper_title": "A call for clarity in reporting bleu scores",
            "rating": 2
        },
        {
            "paper_title": "CombiNMT: An exploration into neural text simplification models",
            "rating": 2
        },
        {
            "paper_title": "Another PASS: A reproduction study of the human evaluation of a football report generation system",
            "rating": 2
        },
        {
            "paper_title": "REPROLANG2020: A shared task of a new, collaborative type to foster reproducibility",
            "rating": 2
        },
        {
            "paper_title": "The machine learning reproducibility checklist v2.0",
            "rating": 2
        },
        {
            "paper_title": "The human evaluation datasheet 1.0: A template for recording details of human evaluation experiments in NLP",
            "rating": 2
        },
        {
            "paper_title": "Exploring neural text simplification models",
            "rating": 2
        },
        {
            "paper_title": "Experiments with universal CEFR classification",
            "rating": 2
        }
    ],
    "cost": 0.018632749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Quantified Reproducibility Assessment of NLP Results</h1>
<p>Anya Belz and Maja Popović ADAPT Research Centre Dublin City University, Ireland {anya.belz,maja.popovic}@adaptcentre.ie</p>
<p>Simon Mille<br>Universitat Pompeu Fabra<br>Barcelona, Spain<br>simon.mille@upf.edu</p>
<h4>Abstract</h4>
<p>This paper describes and tests a method for carrying out quantified reproducibility assessment (QRA) that is based on concepts and definitions from metrology. QRA produces a single score estimating the degree of reproducibility of a given system and evaluation measure, on the basis of the scores from, and differences between, different reproductions. We test QRA on 18 system and evaluation measure combinations (involving diverse NLP tasks and types of evaluation), for each of which we have the original results and one to seven reproduction results. The proposed QRA method produces degree-of-reproducibility scores that are comparable across multiple reproductions not only of the same, but of different original studies. We find that the proposed method facilitates insights into causes of variation between reproductions, and allows conclusions to be drawn about what changes to system and/or evaluation design might lead to improved reproducibility.</p>
<h2>1 Introduction</h2>
<p>Reproduction studies are becoming more common in Natural Language Processing (NLP), with the first shared tasks being organised, including REPROLANG (Branco et al., 2020) and ReproGen (Belz et al., 2021b). In NLP, reproduction studies generally address the following question: if we create and/or evaluate this system multiple times, will we obtain the same results?</p>
<p>To answer this question for a given specific system, typically (Wieling et al., 2018; Arhiliuc et al., 2020; Popović and Belz, 2021) an original study is selected and repeated more or less closely, before comparing the results obtained in the original study with those obtained in the repeat, and deciding whether the two sets of results are similar enough to support the same conclusions.</p>
<p>This framing, whether the same conclusions can be drawn, involves subjective judgments and different researchers can come to contradictory con-
clusions: e.g. the four papers (Arhiliuc et al., 2020; Bestgen, 2020; Caines and Buttery, 2020; Huber and Çöltekin, 2020) reproducing Vajjala and Rama (2018) in REPROLANG all report similarly large differences, but only Arhiliuc et al. conclude that reproduction was unsuccessful.</p>
<p>There is no standard way of going about a reproduction study in NLP, and different reproduction studies of the same original set of results can differ substantially in terms of their similarity in system and/or evaluation design (as is the case with the Vajjala and Rama (2018) reproductions, see Section 4 for details). Other things being equal, a more similar reproduction can be expected to produce more similar results, and such (dis)similarities should be factored into reproduction analysis and conclusions, but NLP lacks a method for doing so.</p>
<p>Being able to assess reproducibility of results objectively and comparably is important not only to establish that results are valid, but to provide evidence about which methods have better/worse reproducibility and what may need to be changed to improve reproducibility. To do this, assessment has to be done in a way that is also comparable across reproduction studies of different original studies, e.g. to develop common expectations of how similar original and reproduction results should be for different types of system, task and evaluation.</p>
<p>In this paper, we (i) describe a method for quantified reproducibility assessment (QRA) directly derived from standard concepts and definitions from metrology which addresses the above issues, and (ii) test it on diverse sets of NLP results. Following a review of related research (Section 2), we present the method (Section 3), tests and results (Section 4), discuss method and results (Section 5), and finish with some conclusions (Section 6).</p>
<h2>2 Related Research</h2>
<p>The situation memorably caricatured by Pedersen (2008) still happens all the time: you download</p>
<p>some code you read about in a paper and liked the sound of, you run it on the data provided, only to find that the results are not the same as reported in the paper, in fact they are likely to be worse (Belz et al., 2021a). When both data and code are provided, the number of potential causes of such differences is limited, and the NLP field has shared increasingly detailed information about system, dependencies and evaluation to chase down sources of differences. Sharing code and data together with detailed information about them is now expected as standard, and checklists and datasheets have been proposed to standardise information sharing (Pineau, 2020; Shimorina and Belz, 2021).</p>
<p>Reproducibility more generally is becoming more of a research focus. There have been several workshops and initiatives on reproducibility, including workshops at ICML 2017 and 2018, the reproducibility challenge at ICLR 2018 and 2019, and at NeurIPS 2019 and 2020, the REPROLANG (Branco et al., 2020) initiative at LREC 2020, and the ReproGen shared task on reproducibility in NLG (Belz et al., 2021b).</p>
<p>Despite this growing body of research, no consensus has emerged about standards, terminology and definitions. Particularly for the two most frequently used terms, reproducibility and replicability, multiple divergent definitions are in use, variously conditioned on same vs. different teams, methods, artifacts, code, and data. For example, for Rougier et al. (2017), reproducing a result means running the same code on the same data and obtaining the same result, while replicating the result is writing and running new code based on the information provided by the original publication. For Wieling et al. (2018), reproducibility is achieving the same results using the same data and methods.</p>
<p>According to the ACM's definitions (Association for Computing Machinery, 2020), results have been reproduced if obtained in a different study by a different team using artifacts supplied in part by the original authors, and replicated if obtained in a different study by a different team using artifacts not supplied by the original authors. The ACM originally had these definitions the other way around until asked by ISO to bring them in line with the scientific standard (ibid.).</p>
<p>Conversely, in Drummond's view 2009 obtaining the same result by re-running an experiment in the same way as the original is replicability, while reproducibility is obtaining it in a different way.</p>
<p>Whitaker (2017), followed by Schloss (2018), defines four concepts rather than two, basing definitions of reproducibility, replicability, robustness and generalisability on the different possible combinations of same vs. different data and code.</p>
<p>None of these definitions adopt the general scientific concepts and definitions pertaining to reproducibility, codified in the International Vocabulary of Metrology, VIM (JCGM, 2012). One issue is that they all reduce the in principle open-ended number of dimensions of variation between measurements accounted for by VIM to just two or three (code, data and/or team). Another, that unlike VIM, they don't produce comparable results.</p>
<p>NLP does not currently have a shared approach to deciding reproducibility, and results from reproductions as currently reported are not comparable across studies and can, as mentioned in the introduction, lead to contradictory conclusions about an original study's reproducibility. There appears to be no work at all in NLP that aims to estimate degree of reproducibility which would allow crossstudy comparisons and conclusions.</p>
<h2>3 Metrology-based Reproducibility Assessment</h2>
<p>Metrology is a meta-science: its subject is the standardisation of measurements across all of science to ensure comparability. Computer science has long borrowed terms, most notably reproducibility, from metrology, albeit not adopting the same definitions (as discussed in Section 2 above).</p>
<p>In this section, we describe quantified reproducibility assessment (QRA), an approach that is directly derived from the concepts and definitions of metrology, adopting the latter exactly as they are, and yields assessments of the degree of similarity between numerical results and between the studies that produced them. We start below with the concepts and definitions that QRA is based on, followed by an overview of the framework (Section 3.2) and steps in applying it in practice (Section 3.3).</p>
<h3>3.1 VIM Definitions of Repeatability and Reproducibility</h3>
<p>The International Vocabulary of Metrology (VIM) (JCGM, 2012) defines repeatability and reproducibility as follows (defined terms in bold, see VIM for subsidiary defined terms):
2.21 measurement repeatability (or repeatability,</p>
<p>for short) is measurement precision under a set of repeatability conditions of measurement.
2.20 a repeatability condition of measurement (repeatability condition) is a condition of measurement, out of a set of conditions that includes the same measurement procedure, same operators, same measuring system, same operating conditions and same location, and replicate measurements on the same or similar objects over a short period of time.
2.25 measurement reproducibility (reproducibility) is measurement precision under reproducibility conditions of measurement.
2.24 a reproducibility condition of measurement (reproducibility condition) is a condition of measurement, out of a set of conditions that includes different locations, operators, measuring systems, etc. A specification should give the conditions changed and unchanged, to the extent practical.</p>
<p>In other words, VIM considers repeatability and reproducibility to be properties of measurements (not objects, scores, results or conclusions), and defines them as measurement precision, i.e. both are quantified by calculating the precision of a set of measured quantity values. Both concepts are defined relative to a set of conditions of measurement: the conditions have to be known and specified for assessment of repeatability and reproducibility to be meaningful. In repeatability, conditions are the same, whereas in reproducibility, they differ.</p>
<p>In an NLP context, objects are systems, and measurements involve applying an evaluation method to a system usually via obtaining a sample of its outputs and applying the method to the sample (further details of how concepts map to NLP are provided in Section 3.3).</p>
<h3>3.2 Assessment framework</h3>
<p>The VIM definitions translate directly to the following definition of repeatability $R^{0}$ (where all conditions of measurement $C$ are the same across measurements):</p>
<p>$$
\begin{gathered}
R^{0}\left(M_{1}, M_{2}, \ldots M_{n}\right):=\operatorname{Precision}\left(v_{1}, v_{2}, \ldots v_{n}\right) \
\text { where } M_{i}:\left(m, O, t_{i}, C\right) \mapsto v_{i}
\end{gathered}
$$</p>
<p>and the $M_{i}$ are repeat measurements for measurand $m$ performed on object $O$ at different times $t_{i}$ under (the same) set of conditions $C$, producing measured quantity values $v_{i}$. Below, the coefficient
of variation is used as the precision measure, but other measures are possible. Conditions of measurement are attribute/value pairs each consisting of a name and a value (for examples, see following section). Reproducibility $R$ is defined in the same way as $R^{0}$ except that condition values (but not names) differ for one or more of the conditions of measurement $C_{i}$ :</p>
<p>$$
\begin{gathered}
R\left(M_{1}, M_{2}, \ldots M_{n}\right):=\operatorname{Precision}\left(v_{1}, v_{2}, \ldots v_{n}\right) \
\text { where } M_{i}:\left(m, O, t_{i}, C_{i}\right) \mapsto v_{i}
\end{gathered}
$$</p>
<p>Precision is typically reported in terms of some or all of the following: mean, standard deviation with $95 \%$ confidence intervals, coefficient of variation, and percentage of measured quantity values within $n$ standard deviations. We opt for the coefficient of variation (CV), ${ }^{1}$ because it is a general measure, not in the unit of the measurements (unlike mean and standard deviation), providing a quantification of precision (degree of reproducibility) that is comparable across studies (Ahmed, 1995, p. 57). This also holds for percentage within $n$ standard deviations but the latter is a less recognised measure, and likely to be the less intuitive for many.</p>
<p>In reproduction studies in NLP/ML, sample sizes tend to be very small (a sample size of 8 , one original study plus 7 reproductions, as in Table 6 is currently unique). We therefore need to use de-biased sample estimators: we use the unbiased sample standard deviation, denoted $s^{<em>}$, with confidence intervals calculated using a t-distribution, and standard error (of the unbiased sample standard deviation) approximated on the basis of the standard error of the unbiased sample variance $\operatorname{se}\left(s^{2}\right)$ as $\operatorname{se}_{s^{2}}\left(s^{</em>}\right) \approx \frac{1}{2 \sigma} \operatorname{se}\left(s^{2}\right)$ (Rao, 1973). Assuming measured quantity values are normally distributed, we calculate the standard error of the sample variance in the usual way: $\operatorname{se}\left(s^{2}\right)=\sqrt{\frac{2 \sigma^{4}}{n-1}}$. Finally, we also use a small sample correction (indicated by the star) for the coefficient of variation: $\mathrm{CV}^{*}=\left(1+\frac{1}{4 n}\right) \mathrm{CV}$ (Sokal and Rohlf, 1971). ${ }^{2}$</p>
<p>Before applying $\mathrm{CV}^{<em>}$ to values on scales that do not start at 0 (mostly in human evaluations) we shift values to start at 0 to ensure comparability. ${ }^{3}$ This means that to calculate the $\mathrm{CV}^{</em>}$ scores in the tables below, measurements are first shifted.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>3.3 Application of the framework</h3>
<p>Using the defined VIM terms and the notations from Section 3.2, we can refine the question from the start of this paper as follows: if we perform multiple measurements of object O and measurand m under reproducibility conditions of measurement $\mathrm{C}_{i}$, what is the precision of the measured quantity values we obtain? For NLP, this means calculating the precision of multiple evaluation scores for the same system and evaluation measure.</p>
<p>Focusing here on reproducibility assessment where we start from an existing set of results (rather than a set of experiments specifically designed to test reproducibility), the steps in performing QRA are as follows:</p>
<ol>
<li>For a set of $n$ measurements to be assessed, identify the shared object and measurand.</li>
<li>Identify all conditions of measurement $C_{i}$ for which information is available for all measurements, and specify values for each condition, including measurement method and procedure.</li>
<li>Gather the $n$ measured quantity values $v_{1}, v_{2}, \ldots v_{n}$.</li>
<li>Compute precision for $v_{1}, v_{2}, \ldots v_{n}$, giving reproducibility score $R$.</li>
<li>Report resulting $R$ score and associated confidence statistics, alongside the $C_{i}$.</li>
</ol>
<p>In NLP terms, the object is the ready-to-use system (binaries if available; otherwise code, dependencies, parameter values, how the system was compiled and trained) being evaluated (e.g. the NTSdefault system variant in Table 1), the measurand is the quantity intended to be measured (e.g. BLEUstyle modified n-gram precision), and measurement method and procedure capture how to evaluate the system (e.g. obtaining system outputs for a specified set of inputs, and applying preprocessing and a given BLEU implementation to the latter).</p>
<p>VIM holds that reproducibility assessment is only meaningful if the reproducibility conditions of measurement are specified for a given test. Conditions of measurement cover every aspect and detail of how a measurement was performed and how the measured quantity value was obtained. The key objective is to capture all respects in which the measurements to be assessed are known to be either the same or different. If QRA is performed for a set of existing results, it is often not possible to
discover every aspect and detail of how a measurement was performed, so a reduced set may have to be used (unlike in experiments designed to test reproducibility where such details can be gathered as part of the experimental design).</p>
<p>The reproducibility and evaluation checklists mentioned in Section 2 (Pineau, 2020; Shimorina and Belz, 2021) capture properties that are in effect conditions of measurement, and in combination with code, data and other resources serve well as a way of specifying conditions of measurement, if they have been completed by authors. However, at the present time, completed checklists are not normally available. The following is a simple set of conditions of measurement the information required for which is typically available for existing work (we include object and measurand for completeness although strictly they are not conditions, as they must be the same in each measurement in a given QRA test):</p>
<ol>
<li>Object: the system (variant) being evaluated. ${ }^{4}$ E.g. a given MT system.</li>
<li>Measurand: the quantity intended to be evaluated. ${ }^{5}$ E.g. BLEU-style n-gram precision or human-assessed Fluency.</li>
<li>Object conditions:
(a) System code: source code including any parameters. E.g. the complete code implementing an MT system.
(b) Compile/training information: steps from code plus parameters to fully compiled and trained system, including dependencies and environment. E.g. complete information about how the MT system code was compiled and the system trained.</li>
<li>Measurement method conditions: ${ }^{6}$
(a) Method specification: full description of method used for obtaining values quantifying the measurand. E.g. a formal definition of BLEU.
(b) Implementation: the method implemented in a form that can be applied to the object in order to obtain measured quantity values. E.g. a full implementation of BLEU.
<sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></li>
</ol>
<table>
<thead>
<tr>
<th style="text-align: center;">System (Object)</th>
<th style="text-align: center;">Evaluation measure <br> (Measurand)</th>
<th style="text-align: center;">N <br> scores</th>
<th style="text-align: center;">Papers reporting results</th>
<th style="text-align: center;">NLP task</th>
<th style="text-align: center;">Evaluation type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">PASS</td>
<td style="text-align: center;">Clarity <br> Fluency <br> Identifiability <br> of stance</td>
<td style="text-align: center;">$\begin{gathered} 2 \ 2 \ 2 \end{gathered}$</td>
<td style="text-align: center;">van der Lee et al. (2017), <br> Mille et al. (2021)</td>
<td style="text-align: center;">data-to-text</td>
<td style="text-align: center;">human, intrinsic</td>
</tr>
<tr>
<td style="text-align: center;">mult-base</td>
<td style="text-align: center;">wf1</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">mult-word ${ }^{-}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">mult-word ${ }^{+}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">mult-POS ${ }^{-}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">Vajjala and Rama (2018),</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">mult-POS ${ }^{+}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">Huber and Çöltekin (2020),</td>
<td style="text-align: center;">multilingual essay</td>
<td style="text-align: center;">metric: intrinsic,</td>
</tr>
<tr>
<td style="text-align: center;">mult-dep ${ }^{-}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">Arhiliuc et al. (2020),</td>
<td style="text-align: center;">scoring as text</td>
<td style="text-align: center;">evaluated against</td>
</tr>
<tr>
<td style="text-align: center;">mult-dep ${ }^{+}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">Bestgen (2020),</td>
<td style="text-align: center;">classification</td>
<td style="text-align: center;">single reference</td>
</tr>
<tr>
<td style="text-align: center;">mult-dom ${ }^{-}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">Caines and Buttery (2020)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">mult-dem ${ }^{+}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">mult-emb ${ }^{+}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">NTS_default</td>
<td style="text-align: center;">BLEU <br> SARI <br> BLEU <br> SARI</td>
<td style="text-align: center;">$\begin{gathered} 7 \ 5 \ 6 \ 4 \end{gathered}$</td>
<td style="text-align: center;">Nisioi et al. (2017), <br> Cooper \&amp; Shardlow (2020), additional reproduction study for this paper</td>
<td style="text-align: center;">text simplification</td>
<td style="text-align: center;">metric: intrinsic, eval. against input and/or multiple references</td>
</tr>
</tbody>
</table>
<p>Table 1: Summary overview of the 18 object/measurand combinations taht were QRA-tested for this paper.
5. Measurement procedure conditions: ${ }^{7}$
(a) Procedure: specification of how system outputs (or other system characteristics) are obtained and the measurement method is applied to them. E.g. running a BLEU tool on system outputs and reference outputs.
(b) Test set: the data used in obtaining and evaluating system outputs (or other system characteristics). E.g. a test set of source-language texts and reference translations.
(c) Performed by: who performed the measurement procedure and any additional information about how they did it. E.g. the team applying the BLEU tool, and the run-time environment they used.</p>
<p>The names of the conditions of measurement used in this paper are boldfaced above. The values for each condition characterise how measurements differ in respect of the condition. In reporting results from QRA tests in the following section, we use paper identifiers as shorthand for each distinct condition value (full details in each case being available from the referenced papers).</p>
<h2>4 QRA Tests</h2>
<p>Table 1 provides an overview of the 18 object/ measurand pairs (corresponding to 116 individual mea-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>surements) for which we performed QRA tests in this study. For each object/measurand pair, the columns show, from left to right, information about the system evaluated (object), the evaluation measure applied (measurand), the number of scores (measured quantity values) obtained, the papers in which systems and scores were first reported, and the NLP task and type of evaluation involved.</p>
<p>There are three sets of related systems: (i) the (single) PASS football report generator (van der Lee et al., 2017), (ii) Vajjala and Rama (2018)'s 11 multilingual essay scoring system variants, and (iii) two variants of Nisioi et al. (2017)'s neural text simplifier (NTS). PASS is evaluated with three evaluation measures (human-assessed Clarity, Fluency and Stance Identifiability), the essay scoring systems with one (weighted F1), and the NTS systems with two (BLEU and SARI). For PASS we have one reproduction study, for the essay scorers seven, and for the NTS systems, from three to six. The PASS reproduction was carried out as part of ReproGen (Belz et al., 2021b), the reproductions of the essay-scoring systems and of one of the NTS systems as part of REPROLANG (Branco et al., 2020), and we carried out an additional reproduction study of the NTS systems for this paper. ${ }^{8}$</p>
<p>The PASS text generation system is rule-based, the essay classifiers are 'theory-guided and datadriven' hybrids, and the text simplifiers are end-toend neural systems. This gives us a good breadth</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Object</th>
<th style="text-align: center;">Measurand</th>
<th style="text-align: center;">Measured quantity value</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Sample <br> size</th>
<th style="text-align: center;">mean</th>
<th style="text-align: center;">stdev</th>
<th style="text-align: center;">stdev 95\% CI</th>
<th style="text-align: center;">$\mathrm{CV}^{*} \downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">van der Lee et al. <br> (2017)</td>
<td style="text-align: center;">Mille et al. (2021)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PASS</td>
<td style="text-align: center;">Clarity</td>
<td style="text-align: center;">5.64</td>
<td style="text-align: center;">6.30</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">4.969</td>
<td style="text-align: center;">0.583</td>
<td style="text-align: center;">$[-2.75,3.92]$</td>
<td style="text-align: center;">13.193</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fluency</td>
<td style="text-align: center;">5.36</td>
<td style="text-align: center;">6.14</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">4.75</td>
<td style="text-align: center;">0.691</td>
<td style="text-align: center;">$[-3.26,4.65]$</td>
<td style="text-align: center;">16.372</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Stance id.</td>
<td style="text-align: center;">$91 \%$</td>
<td style="text-align: center;">$97 \%$</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">93.88</td>
<td style="text-align: center;">5.096</td>
<td style="text-align: center;">$[-24.05,34.24]$</td>
<td style="text-align: center;">6.107</td>
</tr>
</tbody>
</table>
<p>Table 2: Precision $\left(\mathrm{CV}^{*}\right)$ and component measures (mean, standard deviation, standard deviation, confidence intervals) for measured quantity values obtained in two measurements for each of the three human-assessed evaluation measures for the PASS system. Columns 6-9 calculated on shifted scores (see Section 3.2).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Object</th>
<th style="text-align: center;">Measurand</th>
<th style="text-align: center;">Object conditions</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Measurement method conditions</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Measurement procedure conditions</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Measured quantity value</th>
<th style="text-align: center;">$C V^{*}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Code by</td>
<td style="text-align: center;">Comp./trained by</td>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">Implem. by</td>
<td style="text-align: center;">Procedure</td>
<td style="text-align: center;">Test set</td>
<td style="text-align: center;">Performed by</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PASS</td>
<td style="text-align: center;">Clarity</td>
<td style="text-align: center;">vdL\&amp;al</td>
<td style="text-align: center;">vdL\&amp;al</td>
<td style="text-align: center;">vdL\&amp;al</td>
<td style="text-align: center;">vdL\&amp;al</td>
<td style="text-align: center;">vdL\&amp;al</td>
<td style="text-align: center;">vdL\&amp;al</td>
<td style="text-align: center;">vdL\&amp;al</td>
<td style="text-align: center;">5.64</td>
<td style="text-align: center;">13.193</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">vdL\&amp;al</td>
<td style="text-align: center;">vdL\&amp;al</td>
<td style="text-align: center;">vdL\&amp;al</td>
<td style="text-align: center;">M\&amp;al</td>
<td style="text-align: center;">M\&amp;al</td>
<td style="text-align: center;">vdL\&amp;al</td>
<td style="text-align: center;">M\&amp;al</td>
<td style="text-align: center;">6.30</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fluency</td>
<td style="text-align: center;">vdL\&amp;al</td>
<td style="text-align: center;">vdL\&amp;al</td>
<td style="text-align: center;">vdL\&amp;al</td>
<td style="text-align: center;">vdL\&amp;al</td>
<td style="text-align: center;">vdL\&amp;al</td>
<td style="text-align: center;">vdL\&amp;al</td>
<td style="text-align: center;">vdL\&amp;al</td>
<td style="text-align: center;">5.36</td>
<td style="text-align: center;">16.372</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">vdL\&amp;al</td>
<td style="text-align: center;">vdL\&amp;al</td>
<td style="text-align: center;">vdL\&amp;al</td>
<td style="text-align: center;">M\&amp;al</td>
<td style="text-align: center;">M\&amp;al</td>
<td style="text-align: center;">vdL\&amp;al</td>
<td style="text-align: center;">M\&amp;al</td>
<td style="text-align: center;">6.14</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Stance id.</td>
<td style="text-align: center;">vdL\&amp;al</td>
<td style="text-align: center;">vdL\&amp;al</td>
<td style="text-align: center;">vdL\&amp;al</td>
<td style="text-align: center;">vdL\&amp;al</td>
<td style="text-align: center;">vdL\&amp;al</td>
<td style="text-align: center;">vdL\&amp;al</td>
<td style="text-align: center;">vdL\&amp;al</td>
<td style="text-align: center;">91\%</td>
<td style="text-align: center;">6.107</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">vdL\&amp;al</td>
<td style="text-align: center;">vdL\&amp;al</td>
<td style="text-align: center;">vdL\&amp;al</td>
<td style="text-align: center;">M\&amp;al</td>
<td style="text-align: center;">M\&amp;al</td>
<td style="text-align: center;">vdL\&amp;al</td>
<td style="text-align: center;">M\&amp;al</td>
<td style="text-align: center;">96.75\%</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 3: Conditions of measurement for two measurements each for three evaluation measures (measurands) and the PASS system. vdL\&amp;al = van der Lee et al. (2017); M\&amp;al = Mille et al. (2021).
of NLP tasks, system types, and evaluation types and measures to test QRA on.</p>
<h3>4.1 QRA for NTS systems</h3>
<p>The neural text simplification systems reported by Nisioi et al. (2017) were evaluated with BLEU (n-gram similarity between outputs and multiple reference texts) and SARI (based on word added/retained/deleted in outputs compared to both inputs and reference texts, summing over addition and retention F-scores and deletion Precisions).</p>
<p>Table 4 shows BLEU and SARI scores for the two system variants from the original paper and the two reproduction studies, alongside the four corresponding $\mathrm{CV}^{*}$ values. In their reproduction, Cooper and Shardlow (2020) regenerated test outputs for NTS-w2v_def, but not for NTS_def, which explains the missing scores in Column 4. The different numbers of scores in different rows in Columns 6-9 are due to our own reproduction using Nisioi et al.'s SARI script, but two different BLEU scripts: (i) Nisioi et al.'s script albeit with the tokeniser replaced by our own because the former did not work due to changes in the NLTK library; and (ii) SacreBLEU (Xu et al., 2016).</p>
<p>Table 5 shows the conditions of measurement for each of the 22 individual measurements. The measured quantity values for those measurements where Comp./trained by=Nisioi et al. are identical for the SARI metric (scores highlighted by
green/lighter shading and italics), but differ by up to 1.4 points for BLEU (scores highlighted by blue/darker shading). Because Test set=Nisioi et al. in all cases, the differences in these BLEU scores can only be caused by differences in BLEU scripts and how they were run. The corresponding $\mathrm{CV}^{*}$ is as big as 0.838 for (just) the four NTS_def BLEU scores, and 1.314 for (just) the three NTS-w2v_def BLEU scores, reflecting known problems with nonstandardised BLEU scripts (Post, 2018).</p>
<p>If we conversely look just at those measurements (identifiable by boldfaced measured quantity values in Table 5) where the reproducing team regenerated outputs (with the same system code) and evaluation scripts were the same, SARI CV<em> is 3.11 for the NTS_def variants, and 4.05 for the NTS-w2v_def variants (compared in both cases to 0 (perfect) when the same outputs are used). BLEU CV</em> is 2.154 for the NTS_def variants (compared to 0.838 for same outputs but different evaluation scripts, as above), and 6.598 for the NTS-w2v_def variants (compared to 1.314 for same outputs but different evaluation scripts). These differences arise simply from running the system in different environments.</p>
<p>The overall higher (worse) $\mathrm{CV}^{*}$ values for NTSw2v_def variants (compared to NTS_def) are likely to be partly due to the NTS models using one third party tool (openNMT), and the NTS-w2v models using two (openNMT and word2vec), i.e. the latter are more susceptible to changes in dependencies.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Object</th>
<th style="text-align: center;">Measurand</th>
<th style="text-align: center;">Measured quantity value</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Sample <br> size</th>
<th style="text-align: center;">mean</th>
<th style="text-align: center;">stdev</th>
<th style="text-align: center;">stdev $95 \%$ CI</th>
<th style="text-align: center;">$\mathrm{CV}^{*} \downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">Cooper \&amp; Shardlow</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">this paper</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">outputs 1</td>
<td style="text-align: center;">outputs 1</td>
<td style="text-align: center;">outputs 2</td>
<td style="text-align: center;">outputs 1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">outputs 3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">s1 / b1</td>
<td style="text-align: center;">s1 / b2</td>
<td style="text-align: center;">s1 / b2</td>
<td style="text-align: center;">s1 / b3</td>
<td style="text-align: center;">s1 / b4</td>
<td style="text-align: center;">s1 / b3</td>
<td style="text-align: center;">s1 / b4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">NTS_def</td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">84.51</td>
<td style="text-align: center;">84.50</td>
<td style="text-align: center;">87.46</td>
<td style="text-align: center;">85.60</td>
<td style="text-align: center;">84.20</td>
<td style="text-align: center;">86.61</td>
<td style="text-align: center;">86.20</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">85.58</td>
<td style="text-align: center;">1.29 [0.45, 2.13]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SARI</td>
<td style="text-align: center;">30.65</td>
<td style="text-align: center;">30.65</td>
<td style="text-align: center;">29.13</td>
<td style="text-align: center;">30.65</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">29.96</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">30.21</td>
<td style="text-align: center;">0.72 [0.095, 1.34]</td>
<td style="text-align: center;">2.487</td>
</tr>
<tr>
<td style="text-align: center;">NTS-w2v_def</td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">87.50</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">80.75</td>
<td style="text-align: center;">89.36</td>
<td style="text-align: center;">88.10</td>
<td style="text-align: center;">89.64</td>
<td style="text-align: center;">88.80</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">87.36</td>
<td style="text-align: center;">3.502 [0.92, 6.08]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SARI</td>
<td style="text-align: center;">31.11</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">30.28</td>
<td style="text-align: center;">31.11</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">29.12</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">30.41</td>
<td style="text-align: center;">1.02 [-0.11, 2.15]</td>
<td style="text-align: center;">3.572</td>
</tr>
</tbody>
</table>
<p>Table 4: Precision $\left(\mathrm{CV}^{*}\right)$ and component measures (mean, standard deviation, standard deviation confidence intervals) for measured quantity values obtained in multiple measurements of the two NTS systems. Outputs $1=$ test set outputs as generated by Nisioi et al. (2017); outputs $2=$ test set outputs regenerated by Cooper and Shardlow (2020); outputs $3=$ test set outputs regenerated by the present authors. $\mathrm{s} 1=$ SARI script (always the same); $\mathrm{b} 1=$ Nisioi et al.'s BLEU script, run by Nisioi et al.; b2 = Nisioi et al.'s BLEU script, run by Cooper \&amp; Shardlow; b3 = Nisioi et al.'s BLEU script with different version of NLTK tokeniser (see in text), run by the present authors; b4 = SacreBLEU (Xu et al., 2016), run by the present authors.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Object</th>
<th style="text-align: center;">Measurand</th>
<th style="text-align: center;">Object conditions</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Measurement method conditions</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Measurement procedure conditions</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Measured quantity value</th>
<th style="text-align: center;">$C V^{*}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Code by</td>
<td style="text-align: center;">Comp./trained by</td>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">Implem. by</td>
<td style="text-align: center;">Procedure</td>
<td style="text-align: center;">Test set</td>
<td style="text-align: center;">Performed by</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">NTS_def</td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">bleu(0,1)</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">84.51</td>
<td style="text-align: center;">2.487</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">bleu(0,1)</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">Coop. \&amp; Shard.</td>
<td style="text-align: center;">84.50</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">bleu(0,1)</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">this paper</td>
<td style="text-align: center;">85.60</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">bleu(0,1)</td>
<td style="text-align: center;">SacreBLEU</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">this paper</td>
<td style="text-align: center;">84.20</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">Coop. \&amp; Shard.</td>
<td style="text-align: center;">bleu(0,1)</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">Coop. \&amp; Shard.</td>
<td style="text-align: center;">87.46</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">this paper</td>
<td style="text-align: center;">bleu(0,1)</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">this paper</td>
<td style="text-align: center;">86.61</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">this paper</td>
<td style="text-align: center;">bleu(0,1)</td>
<td style="text-align: center;">SacreBLEU</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">this paper</td>
<td style="text-align: center;">86.20</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SARI</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">$\operatorname{sari}(0,0,1)$</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">OITE</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">30.65</td>
<td style="text-align: center;">2.487</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">$\operatorname{sari}(0,0,1)$</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">Coop. \&amp; Shard.</td>
<td style="text-align: center;">30.65</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">$\operatorname{sari}(0,0,1)$</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">this paper</td>
<td style="text-align: center;">30.65</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">Coop. \&amp; Shard.</td>
<td style="text-align: center;">$\operatorname{sari}(0,0,1)$</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">Coop. \&amp; Shard.</td>
<td style="text-align: center;">29.13</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">this paper</td>
<td style="text-align: center;">$\operatorname{sari}(0,0,1)$</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">this paper</td>
<td style="text-align: center;">29.96</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">NTS-w2v_def</td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">bleu(0,1)</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">87.50</td>
<td style="text-align: center;">3.572</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">bleu(0,1)</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">this paper</td>
<td style="text-align: center;">89.36</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">bleu(0,1)</td>
<td style="text-align: center;">SacreBLEU</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">this paper</td>
<td style="text-align: center;">88.10</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">Coop. \&amp; Shard.</td>
<td style="text-align: center;">bleu(0,1)</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">Coop. \&amp; Shard.</td>
<td style="text-align: center;">80.75</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">this paper</td>
<td style="text-align: center;">$\operatorname{bleu}(0,1)$</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">this paper</td>
<td style="text-align: center;">69.64</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">this paper</td>
<td style="text-align: center;">$\operatorname{bleu}(0,1)$</td>
<td style="text-align: center;">SacreBLEU</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">this paper</td>
<td style="text-align: center;">88.80</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SARI</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">$\operatorname{sari}(0,0,1)$</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">31.11</td>
<td style="text-align: center;">3.572</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">$\operatorname{sari}(0,0,1)$</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">this paper</td>
<td style="text-align: center;">31.11</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">Coop. \&amp; Shard.</td>
<td style="text-align: center;">$\operatorname{sari}(0,0,1)$</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">Coop. \&amp; Shard.</td>
<td style="text-align: center;">30.28</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">this paper</td>
<td style="text-align: center;">$\operatorname{sari}(0,0,1)$</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Nisioi et al.</td>
<td style="text-align: center;">this paper</td>
<td style="text-align: center;">29.12</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 5: Conditions of measurement for each measurement carried out for the NTS systems. OTE $=$ outputs vs. targets evaluation, OITE $=$ outputs vs. inputs and targets evaluation. Shaded cells: evaluation of the same system outputs, i.e. the reproductions did not regenerate outputs. Bold: evaluation of (potentially) different system outputs, i.e. the reproductions did regenerate outputs.</p>
<h3>4.2 QRA for PASS system</h3>
<p>The PASS system, developed by van der Lee et al. (2017), generates football match reports from the perspective of each of the competing teams. The original study evaluated the system for Clarity, Fluency and Stance Identifiability in an evaluation with 20 evaluators and a test set of 10 output pairs. The evaluation was repeated with a slightly different evaluation interface and a different cohort of evaluators by Mille et al. (2021). Table 2 shows the results from the original and reproduction evaluations (columns 3 and 4), where the Clarity and Fluency results are the mean scores from 7-point agreement
scales, and Identifiability results are the percentage of times the evaluators correctly guessed the team whose supporters a report was written for. Columns 6-9 show the corresponding sample size (number of reproductions plus original study), mean, standard deviation (stdev), the confidence interval (CI) for the standard deviation, and $\mathrm{CV}^{*}$, all calculated on the shifted scores (see Section 3.2).</p>
<p>Table 3 shows the values (here, paper identifiers) for the nine conditions of measurement introduced in Section 3.3, for each of the six individual measurements (three evaluation measures times two studies). Note that both object conditions and the</p>
<p>test set condition are the same, because Mille et al. used the system outputs shared by van der Lee et al. The values for the Implemented by, Procedure and Performed by conditions reflect the differences in the two evaluations in design, evaluator cohorts, and the teams that performed them.</p>
<p>The scores vary to different degrees for the three measurands, with $\mathrm{CV}^{<em>}$ lowest (reproducibility best) for Stance Identifiability, and highest (worst) for Fluency. These $\mathrm{CV}^{</em>}$ results are likely to reflect that evaluators agreed more on Clarity than Fluency. Moreover, the binary stance identification assessment has better reproducibility than the other two criteria which are assessed on 7-point rating scales.</p>
<h3>4.3 QRA for essay scoring system variants</h3>
<p>The 11 multilingual essay scoring system variants reported by Vajjala and Rama (2018) were evaluated by weighted F1 (wF1) score. Table 6 shows wF1 scores for the 11 multilingual system variants from each of the five papers, alongside the 11 corresponding $\mathrm{CV}^{*}$ values. Table 7 in the appendix shows the corresponding conditions of measurement. The baseline classifier (mult-base) uses document length (number of words) as its only feature. For the other variants, +/- indicates that the multilingual classifier was / was not given information about which language the input was in; the multword variants use word n-grams only; mult-word uses POS (part of speech) tag n-grams only; multdep uses n-grams over dependency relation, dependent POS, and head POS triples; mult-dom uses domain-specific linguistic features including document length, lexical richness and errors; mult-emb uses word and character embeddings. The multbase and mult-dom models are logistic regressors, the others are random forests.</p>
<p>A very clear picture emerges: system variant pairs that differ only in whether they do or do not use language information have very similar CV scores. For example, mult-POS ${ }^{-}$(POS n-grams without language information) and mult-POS ${ }^{+}$ (POS n-grams with language information) both have a very good degree of wF1-reproducibility, their $\mathrm{CV}^{<em>}$ being 3.818 and 3.808 respectively; mult-word ${ }^{-}$(word n-grams without language information) and mult-word ${ }^{+}$(word n-grams with language information) have notably higher $\mathrm{CV}^{</em>}$, around 10. This tendency holds for all such pairs, indicating that using language information makes next to no difference to reproducibility. Moreover, the mult-
dom and mult-emb variants all have similar $\mathrm{CV}^{*} .{ }^{9}$
The indication is that the syntactic information is obtained/used in a way that is particularly reproducible, whereas the domain-specific information and the embeddings are obtained/used in a way that is particularly hard to reproduce. Overall, the random forest models using syntactic features have the best reproducibility; the logistic regressors using domain-specific features have the worst.</p>
<h2>5 Discussion</h2>
<p>Quantified reproducibility assessment (QRA) enables assessment of the degree of reproducibility of evaluation results for any given system and evaluation measure in a way that is scale-invariant ${ }^{10}$ and comparable across different QRAs, for reproductions involving either the same or different original studies. Moreover, formally capturing (dis)similarities between systems and evaluation designs enables reproducibility to be assessed relative to such (dis)similarities. In combination, a set of results from QRA tests for the same system and evaluation measure can provide pointers to which aspects of the system and evaluation might be associated with low reproducibility. E.g. for the wF1 evaluations of the essay scoring systems above, it is clear that variations in reproducibility are associated at least in part with the different features used by systems.</p>
<p>It might be expected that the reproducibility of human-assessed evaluations is generally worse than metric-assessed. Our study revealed a more mixed picture. As expected, the Fluency and Clarity evaluations of the PASS system were among those with highest $\mathrm{CV}^{<em>}$, and the BLEU and SARI evaluation of the NTS systems and wF1 evaluation of the mult-POS and mult-dep systems were among those with lowest $\mathrm{CV}^{</em>}$. However, human-assessed Stance Identifiability of PASS was among the most reproducible, and metric-assessed wF1 of mult-base, mult-dom and mult-emb were among the worst.</p>
<p>In this paper, our focus has been QRA testing of existing research results. However, ideally, QRA would be built into new method development from the outset, where at first reporting, a detailed stan-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Object</th>
<th style="text-align: center;">Meas- <br> urand</th>
<th style="text-align: center;">Measured quantity value</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Sample size</th>
<th style="text-align: center;">mean</th>
<th style="text-align: center;">stdev</th>
<th style="text-align: center;">stdev 95\% CI</th>
<th style="text-align: center;">$\mathrm{CV}^{*} \downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Vajjala \&amp; Rama <br> seed 1 <br> e1 / i1</td>
<td style="text-align: center;">Huber \&amp; <br> Coltekin <br> seed 2 <br> e2 / i2</td>
<td style="text-align: center;">$\begin{gathered} \text { Arhiliuc } \ \text { et al. } \ \text { seed } 7 \ \text { e3 / i1 } \end{gathered}$</td>
<td style="text-align: center;">Bestgen</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Caines \&amp; Buttery <br> seed 1 <br> e5 / i3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { Caines } \ &amp; \text { e4 } 7 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { seed } 7 \ &amp; \text { e5 / i3 } \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { seed } 1 \ &amp; \text { e6 / i1 } \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { seed } 2 \ &amp; \text { e7 / i4 } \end{aligned}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">multi-base</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">0.428</td>
<td style="text-align: center;">0.493</td>
<td style="text-align: center;">0.426</td>
<td style="text-align: center;">0.574</td>
<td style="text-align: center;">0.579</td>
<td style="text-align: center;">0.590</td>
<td style="text-align: center;">0.574</td>
<td style="text-align: center;">0.600</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.533</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">$[0.03,0.12]$</td>
</tr>
<tr>
<td style="text-align: center;">multi-word ${ }^{-}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">0.721</td>
<td style="text-align: center;">0.603</td>
<td style="text-align: center;">0.605</td>
<td style="text-align: center;">0.606</td>
<td style="text-align: center;">0.720</td>
<td style="text-align: center;">0.732</td>
<td style="text-align: center;">0.606</td>
<td style="text-align: center;">0.740</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.667</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">$[0.03,0.11]$</td>
</tr>
<tr>
<td style="text-align: center;">multi-word ${ }^{+}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">0.719</td>
<td style="text-align: center;">0.604</td>
<td style="text-align: center;">0.607</td>
<td style="text-align: center;">0.607</td>
<td style="text-align: center;">0.723</td>
<td style="text-align: center;">0.733</td>
<td style="text-align: center;">0.607</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.667</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">$[0.03,0.11]$</td>
</tr>
<tr>
<td style="text-align: center;">multi-POS ${ }^{-}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">0.726</td>
<td style="text-align: center;">0.681</td>
<td style="text-align: center;">0.680</td>
<td style="text-align: center;">0.680</td>
<td style="text-align: center;">0.722</td>
<td style="text-align: center;">0.728</td>
<td style="text-align: center;">0.680</td>
<td style="text-align: center;">0.732</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.704</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">$[0.01,0.04]$</td>
</tr>
<tr>
<td style="text-align: center;">multi-POS ${ }^{+}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">0.724</td>
<td style="text-align: center;">0.680</td>
<td style="text-align: center;">0.680</td>
<td style="text-align: center;">0.681</td>
<td style="text-align: center;">0.725</td>
<td style="text-align: center;">0.729</td>
<td style="text-align: center;">0.681</td>
<td style="text-align: center;">0.731</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.704</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">$[0.01,0.04]$</td>
</tr>
<tr>
<td style="text-align: center;">multi-dep ${ }^{-}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">0.703</td>
<td style="text-align: center;">0.660</td>
<td style="text-align: center;">0.650</td>
<td style="text-align: center;">0.651</td>
<td style="text-align: center;">0.699</td>
<td style="text-align: center;">0.711</td>
<td style="text-align: center;">0.651</td>
<td style="text-align: center;">0.710</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.679</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">$[0.01,0.05]$</td>
</tr>
<tr>
<td style="text-align: center;">multi-dep ${ }^{+}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">0.693</td>
<td style="text-align: center;">0.661</td>
<td style="text-align: center;">0.652</td>
<td style="text-align: center;">0.653</td>
<td style="text-align: center;">0.699</td>
<td style="text-align: center;">0.712</td>
<td style="text-align: center;">0.653</td>
<td style="text-align: center;">0.716</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">$[0.01,0.05]$</td>
</tr>
<tr>
<td style="text-align: center;">multi-dom ${ }^{-}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">0.449</td>
<td style="text-align: center;">0.600</td>
<td style="text-align: center;">0.433</td>
<td style="text-align: center;">0.597</td>
<td style="text-align: center;">0.635</td>
<td style="text-align: center;">0.646</td>
<td style="text-align: center;">0.597</td>
<td style="text-align: center;">0.698</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.582</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">$[0.04,0.15]$</td>
</tr>
<tr>
<td style="text-align: center;">multi-dom ${ }^{+}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">0.471</td>
<td style="text-align: center;">0.647</td>
<td style="text-align: center;">0.447</td>
<td style="text-align: center;">0.647</td>
<td style="text-align: center;">0.696</td>
<td style="text-align: center;">0.711</td>
<td style="text-align: center;">0.647</td>
<td style="text-align: center;">0.726</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.624</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">$[0.05,0.18]$</td>
</tr>
<tr>
<td style="text-align: center;">multi-emb ${ }^{-}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">0.693</td>
<td style="text-align: center;">0.658</td>
<td style="text-align: center;">0.683</td>
<td style="text-align: center;">0.668</td>
<td style="text-align: center;">0.692</td>
<td style="text-align: center;">0.689</td>
<td style="text-align: center;">0.659</td>
<td style="text-align: center;">0.391</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.642</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">$[0.04,0.17]$</td>
</tr>
<tr>
<td style="text-align: center;">multi-emb ${ }^{+}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">0.689</td>
<td style="text-align: center;">0.662</td>
<td style="text-align: center;">0.681</td>
<td style="text-align: center;">0.659</td>
<td style="text-align: center;">0.681</td>
<td style="text-align: center;">0.684</td>
<td style="text-align: center;">0.657</td>
<td style="text-align: center;">0.401</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.639</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">$[0.04,0.16]$</td>
</tr>
</tbody>
</table>
<p>Table 6: Precision $\left(\mathrm{CV}^{*}\right)$ and component measures (mean, standard deviation, standard deviation confidence intervals) for measured quantity values obtained in multiple measurements of the essay scoring systems. Seed $i=$ different approaches to random seeding and cross-validation; $\mathrm{e} i=$ different compile/run-time environments; $\mathrm{i} i=$ different test data sets and/or cross-validation folds.
dardised set of conditions of measurement is specified, and repeatability tests (where all conditions are identical except for the team conducting the tests, see Section 3.2) are performed to determine baseline reproducibility. Such repeatability QRA would provide quality assurance for new methods as well as important pointers for future reproductions regarding what degree of reproducibility to expect for given (types of) methods.</p>
<p>If this is not possible, post-hoc reproducibility QRA (where there are differences in conditions of measurement values) is performed instead. If this yields high (poor) $\mathrm{CV}^{<em>}$, one way to proceed is to minimise differences in conditions of measurement between the studies and observe the effect on $\mathrm{CV}^{</em>}$, changing aspects of system and evaluation design and adding further conditions of measurement if need be. For human evaluation in particular, persistently high $\mathrm{CV}^{*}$ would indicate a problem with the method itself.</p>
<h2>6 Conclusion</h2>
<p>We have described an approach to quantified reproducibility assessment (QRA) based on concepts and definitions from metrology, and tested it on 18 system and evaluation measure combinations involving diverse NLP tasks and types of evaluation.</p>
<p>QRA produces a single score that quantifies the degree of reproducibility of a given system and evaluation measure, on the basis of the scores from, and differences between, multiple reproductions of the same original study. We found that the approach facilitates insights into sources of variation
between reproductions, produces results that are comparable across different reproducibility assessments, and provides pointers about what needs to be changed in system and/or evaluation design to improve reproducibility.</p>
<p>A recent survey (Belz et al., 2021a) found that just $14 \%$ of the 513 original/reproduction score pairs analysed were exactly the same. Judging the remainder simply 'not reproduced' is of limited usefulness, as some are much closer to being the same than others. At the same time, assessments of whether the same conclusions can be drawn on the basis of different scores involve subjective judgments and are prone to disagreement among assessors. Quantifying the closeness of results as in QRA, and, over time, establishing expected levels of closeness, seems a better way forward.</p>
<h2>Acknowledgements</h2>
<p>We are grateful to the anonymous reviewers and area chairs for their exceptionally detailed and helpful feedback.</p>
<p>Popović's work on this s study was funded by the ADAPT SFI Centre for Digital Media Technology which is funded by Science Foundation Ireland through the SFI Research Centres Programme, and co-funded under the European Regional Development Fund (ERDF) through Grant 13/RC/2106. Mille's work was supported by the European Commission under the H2020 program contract numbers $786731,825079,870930$ and 952133.</p>
<h2>References</h2>
<p>SE Ahmed. 1995. A pooling methodology for coefficient of variation. Sankhyā: The Indian Journal of Statistics, Series B, pages 57-75.</p>
<p>Cristina Arhiliuc, Jelena Mitrović, and Michael Granitzer. 2020. Language proficiency scoring. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 5624-5630, Marseille, France. European Language Resources Association.</p>
<p>Association for Computing Machinery. 2020. Artifact review and badging Version 1.1. Accessed August 24, 2020 .</p>
<p>Anya Belz, Shubham Agarwal, Anastasia Shimorina, and Ehud Reiter. 2021a. A systematic review of reproducibility research in natural language processing. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 381-393.</p>
<p>Anya Belz, Anastasia Shimorina, Shubham Agarwal, and Ehud Reiter. 2021b. The reprogen shared task on reproducibility of human evaluations in NLG: Overview and results. In The 14th International Conference on Natural Language Generation.</p>
<p>Yves Bestgen. 2020. Reproducing monolingual, multilingual and cross-lingual CEFR predictions. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 5595-5602, Marseille, France. European Language Resources Association.</p>
<p>António Branco, Nicoletta Calzolari, Piek Vossen, Gertjan Van Noord, Dieter van Uytvanck, João Silva, Luís Gomes, André Moreira, and Willem Elbers. 2020. A shared task of a new, collaborative type to foster reproducibility: A first exercise in the area of language science and technology with REPROLANG2020. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 5539-5545, Marseille, France. European Language Resources Association.</p>
<p>Andrew Caines and Paula Buttery. 2020. REPROLANG 2020: Automatic proficiency scoring of Czech, English, German, Italian, and Spanish learner essays. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 5614-5623, Marseille, France. European Language Resources Association.</p>
<p>Michael Cooper and Matthew Shardlow. 2020. CombiNMT: An exploration into neural text simplification models. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 55885594, Marseille, France. European Language Resources Association.</p>
<p>Chris Drummond. 2009. Replicability is not reproducibility: nor is it good science. Presented at 4th Workshop on Evaluation Methods for Machine Learning held at ICML'09.</p>
<p>Eva Huber and Çağrı Çöltekin. 2020. Reproduction and replication: A case study with automatic essay scoring. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 5603-5613, Marseille, France. European Language Resources Association.</p>
<p>JCGM. 2012. International vocabulary of metrology: Basic and general concepts and associated terms (VIM). Joint Committee for Guides in Metrology, https://www.bipm.org/utils/common/ documents/jcgm/JCGM_200_2012.pdf.</p>
<p>Simon Mille, Thiago Castro Ferreira, Anya Belz, and Brian Davis. 2021. Another PASS: A reproduction study of the human evaluation of a football report generation system. In Proceedings of the 14th International Conference on Natural Language Generation (INLG 2021).</p>
<p>Sergiu Nisioi, Sanja Štajner, Simone Paolo Ponzetto, and Liviu P. Dinu. 2017. Exploring neural text simplification models. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 85-91, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Ted Pedersen. 2008. Empiricism is not a matter of faith. Computational Linguistics, 34(3):465-470.</p>
<p>Joelle Pineau. 2020. The machine learning reproducibility checklist v2.0.</p>
<p>Maja Popović and Anya Belz. 2021. A reproduction study of an annotation-based human evaluation of MT outputs. In Proceedings of the 14th International Conference on Natural Language Generation, pages 293-300, Aberdeen, Scotland, UK. Association for Computational Linguistics.</p>
<p>Matt Post. 2018. A call for clarity in reporting bleu scores. WMT 2018, page 186.</p>
<p>Calyampudi Radhakrishna Rao. 1973. Linear statistical inference and its applications. Wiley.</p>
<p>Nicolas P. Rougier, Konrad Hinsen, Frédéric Alexandre, Thomas Arildsen, Lorena A Barba, Fabien CY Benureau, C Titus Brown, Pierre De Buyl, Ozan Caglayan, Andrew P Davison, et al. 2017. Sustainable computational science: The ReScience initiative. PeerJ Computer Science, 3:e142.</p>
<p>Patrick D. Schloss. 2018. Identifying and overcoming threats to reproducibility, replicability, robustness, and generalizability in microbiome research. MBio, $9(3)$.</p>
<p>Anastasia Shimorina and Anya Belz. 2021. The human evaluation datasheet 1.0: A template for recording details of human evaluation experiments in NLP. arXiv preprint arXiv:3910940.
R.R. Sokal and F.J. Rohlf. 1971. Biometry: The Principles and Practice of Statistics in Biological Research. WH Freeman.</p>
<p>Sowmya Vajjala and Taraka Rama. 2018. Experiments with universal CEFR classification. In Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 147-153, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Chris van der Lee, Emiel Krahmer, and Sander Wubben. 2017. PASS: A Dutch data-to-text system for soccer, targeted towards specific audiences. In Proceedings of the 10th International Conference on Natural Language Generation, pages 95-104.</p>
<p>Kirstie Whitaker. 2017. The MT Reproducibility Checklist. https://www.cs.mcgill.ca/ jpineau/ReproducibilityChecklist. pdf.</p>
<p>Martijn Wieling, Josine Rawee, and Gertjan van Noord. 2018. Reproducibility in computational linguistics: Are we willing to share? Computational Linguistics, 44(4):641-649.</p>
<p>Wei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen, and Chris Callison-Burch. 2016. Optimizing statistical machine translation for text simplification. Transactions of the Association for Computational Linguistics, 4:401-415.</p>
<h1>A Conditions of Measurement for the Essay Scoring Systems</h1>
<p>Table 7 shows the conditions of measurement for each of the 88 individual measurements for the Essay Scoring Systems.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Object</th>
<th style="text-align: center;">Measurand</th>
<th style="text-align: center;">Object conditions</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Measurement method conditions</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Measurement procedure conditions</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Measured quantity value</th>
<th style="text-align: center;">CV*</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Code by</td>
<td style="text-align: center;">Comp./trained by</td>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">Implem. by</td>
<td style="text-align: center;">Procedure</td>
<td style="text-align: center;">Test set</td>
<td style="text-align: center;">Performed by</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">mult-base</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">Va. \&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">0.428</td>
<td style="text-align: center;">14.633</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Huber \&amp; Coltekin</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Huber \&amp; Coltekin</td>
<td style="text-align: center;">0.493</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Arhiluc et al.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Arhiluc et al.</td>
<td style="text-align: center;">0.426</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Bestgen</td>
<td style="text-align: center;">0.574</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Bestgen</td>
<td style="text-align: center;">0.579</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">$\approx$ Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Bestgen</td>
<td style="text-align: center;">0.590</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">0.574</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp;Ra.</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">0.600</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">mult-word ${ }^{-}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">0.721</td>
<td style="text-align: center;">10.609</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Huber \&amp; Coltekin</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Huber \&amp; Coltekin</td>
<td style="text-align: center;">0.603</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Arhiluc et al.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Arhiluc et al.</td>
<td style="text-align: center;">0.605</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Bestgen</td>
<td style="text-align: center;">0.606</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Bestgen</td>
<td style="text-align: center;">0.720</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">$\approx$ Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Bestgen</td>
<td style="text-align: center;">0.732</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">0.606</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp;Ra.</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">0.740</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">mult-word ${ }^{+}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">0.719</td>
<td style="text-align: center;">10.44</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Huber \&amp; Coltekin</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Huber \&amp; Coltekin</td>
<td style="text-align: center;">0.604</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Arhiluc et al.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Arhiluc et al.</td>
<td style="text-align: center;">0.607</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Bestgen</td>
<td style="text-align: center;">0.607</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Bestgen</td>
<td style="text-align: center;">0.723</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">$\approx$ Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Bestgen</td>
<td style="text-align: center;">0.733</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">0.607</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp;Ra.</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">mult-POS ${ }^{-}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">0.726</td>
<td style="text-align: center;">3.818</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Huber \&amp; Coltekin</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Huber \&amp; Coltekin</td>
<td style="text-align: center;">0.681</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Arhiluc et al.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Arhiluc et al.</td>
<td style="text-align: center;">0.680</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Bestgen</td>
<td style="text-align: center;">0.680</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Bestgen</td>
<td style="text-align: center;">0.722</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">$\approx$ Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Bestgen</td>
<td style="text-align: center;">0.728</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">0.680</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp;Ra.</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">0.732</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">mult-POS ${ }^{+}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">0.724</td>
<td style="text-align: center;">3.808</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Huber \&amp; Coltekin</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Huber \&amp; Coltekin</td>
<td style="text-align: center;">0.680</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Arhiluc et al.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Arhiluc et al.</td>
<td style="text-align: center;">0.680</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Bestgen</td>
<td style="text-align: center;">0.681</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Bestgen</td>
<td style="text-align: center;">0.725</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">$\approx$ Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Bestgen</td>
<td style="text-align: center;">0.729</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">0.681</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp;Ra.</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">0.731</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">mult-dep ${ }^{-}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">0.703</td>
<td style="text-align: center;">4.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Huber \&amp; Coltekin</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Huber \&amp; Coltekin</td>
<td style="text-align: center;">0.660</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Arhiluc et al.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Arhiluc et al.</td>
<td style="text-align: center;">0.650</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Bestgen</td>
<td style="text-align: center;">0.651</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Bestgen</td>
<td style="text-align: center;">0.699</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">$\approx$ Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Bestgen</td>
<td style="text-align: center;">0.711</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">0.651</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp;Ra.</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">0.710</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">mult-dep ${ }^{+}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">0.693</td>
<td style="text-align: center;">4.387</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Huber \&amp; Coltekin</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Huber \&amp; Coltekin</td>
<td style="text-align: center;">0.661</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Arhiluc et al.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Arhiluc et al.</td>
<td style="text-align: center;">0.652</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Bestgen</td>
<td style="text-align: center;">0.653</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Bestgen</td>
<td style="text-align: center;">0.699</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">$\approx$ Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Bestgen</td>
<td style="text-align: center;">0.712</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">0.653</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">wF1 (o,t)</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp;Ra.</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">0.716</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table continued on next page.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Object</th>
<th style="text-align: center;">Measurand</th>
<th style="text-align: center;">Object conditions</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Measurement method conditions</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Measurement procedure conditions</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Measured quantity value</th>
<th style="text-align: center;">CV*</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Code by</td>
<td style="text-align: center;">Comp./trained by</td>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">Implem. by</td>
<td style="text-align: center;">Procedure</td>
<td style="text-align: center;">Test set</td>
<td style="text-align: center;">Performed by</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">mult-dom ${ }^{-}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1(0,1)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">0.449</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Huber \&amp; Coltekin</td>
<td style="text-align: center;">wF1(0,1)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Huber \&amp; Coltekin</td>
<td style="text-align: center;">0.600</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">A/htiluc et al.</td>
<td style="text-align: center;">wF1(0,1)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">A/htiluc et al.</td>
<td style="text-align: center;">0.433</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1(0,1)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Bestgen</td>
<td style="text-align: center;">0.597</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1(0,1)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Bestgen</td>
<td style="text-align: center;">0.635</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1(0,1)</td>
<td style="text-align: center;">$\approx$ Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Bestgen</td>
<td style="text-align: center;">0.646</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1(0,1)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">0.597</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">wF1(0,1)</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp;Ra.</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">0.698</td>
</tr>
<tr>
<td style="text-align: center;">mult-dom ${ }^{+}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1(0,1)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">0.471</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Huber \&amp; Coltekin</td>
<td style="text-align: center;">wF1(0,1)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Huber \&amp; Coltekin</td>
<td style="text-align: center;">0.647</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">A/htiluc et al.</td>
<td style="text-align: center;">wF1(0,1)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">A/htiluc et al.</td>
<td style="text-align: center;">0.447</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1(0,1)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Bestgen</td>
<td style="text-align: center;">0.647</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1(0,1)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Bestgen</td>
<td style="text-align: center;">0.696</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1(0,1)</td>
<td style="text-align: center;">$\approx$ Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Bestgen</td>
<td style="text-align: center;">0.711</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1(0,1)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">0.647</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">wF1(0,1)</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp;Ra.</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">0.726</td>
</tr>
<tr>
<td style="text-align: center;">mult-emb ${ }^{-}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1(0,1)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">0.693</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Huber \&amp; Coltekin</td>
<td style="text-align: center;">wF1(0,1)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Huber \&amp; Coltekin</td>
<td style="text-align: center;">0.658</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">A/htiluc et al.</td>
<td style="text-align: center;">wF1(0,1)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">A/htiluc et al.</td>
<td style="text-align: center;">0.683</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1(0,1)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Bestgen</td>
<td style="text-align: center;">0.668</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1(0,1)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Bestgen</td>
<td style="text-align: center;">0.692</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1(0,1)</td>
<td style="text-align: center;">$\approx$ Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Bestgen</td>
<td style="text-align: center;">0.689</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1(0,1)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">0.659</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">wF1(0,1)</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp;Ra.</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">0.391</td>
</tr>
<tr>
<td style="text-align: center;">mult-emb ${ }^{+}$</td>
<td style="text-align: center;">wF1</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1(0,1)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">0.689</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Huber \&amp; Coltekin</td>
<td style="text-align: center;">wF1(0,1)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Huber \&amp; Coltekin</td>
<td style="text-align: center;">0.662</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">A/htiluc et al.</td>
<td style="text-align: center;">wF1(0,1)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">A/htiluc et al.</td>
<td style="text-align: center;">0.681</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1(0,1)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Bestgen</td>
<td style="text-align: center;">0.659</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1(0,1)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Bestgen</td>
<td style="text-align: center;">0.681</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1(0,1)</td>
<td style="text-align: center;">$\approx$ Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Bestgen</td>
<td style="text-align: center;">0.684</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">wF1(0,1)</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp; Ra.</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">0.657</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">wF1(0,1)</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">OTE</td>
<td style="text-align: center;">Va.\&amp;Ra.</td>
<td style="text-align: center;">Cai. \&amp; But.</td>
<td style="text-align: center;">0.401</td>
</tr>
</tbody>
</table>
<p>Table 7: Conditions of measurement for each measurement carried out for the multilingual essay scoring systems. OTE $=$ outputs vs.targets evaluation.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{9}$ The high $\mathrm{CV}^{<em>}$ for the baseline system may be due to an issue with the evaluation code (macro-F1 instead of weightedF1), as reported by Bestgen (Section 3.2, first paragraph), Caines and Buttery (Section 2.5, one before last paragraph) and Huber and Çöltekin (Section 3.2, second paragraph).
${ }^{10}$ If evaluation scores are multiplied by a common factor, $\mathrm{CV}^{</em>}$ does not change.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{8}$ Authors of original studies gave permission for their work to be reproduced (Branco et al., 2020; Belz et al., 2021b).&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
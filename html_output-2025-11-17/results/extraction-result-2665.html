<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2665 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2665</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2665</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-68.html">extraction-schema-68</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <p><strong>Paper ID:</strong> paper-257aee73d83a87921fd2d56b524de394dcf6a264</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/257aee73d83a87921fd2d56b524de394dcf6a264" target="_blank">A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level</a></p>
                <p><strong>Paper Venue:</strong> Proceedings of the National Academy of Sciences of the United States of America</p>
                <p><strong>Paper TL;DR:</strong> This work solves university-level mathematics courses and improves upon state-of-the-art, increasing automatic accuracy on randomly sampled questions on a benchmark by order of magnitude.</p>
                <p><strong>Paper Abstract:</strong> Significance We demonstrate that a neural network automatically solves, explains, and generates university-level problems from the largest Massachusetts Institute of Technology (MIT) mathematics courses at a human level. Our methods combine three innovations: 1) using recent neural networks pretrained on text and fine-tuned on code rather than pretrained on text; 2) few-shot learning synthesizing programs that correctly solve course problems automatically; and 3) a pipeline to solve questions, explain solutions, and generate new questions indistinguishable by students from course questions. Our work solves university-level mathematics courses and improves upon state-of-the-art, increasing automatic accuracy on randomly sampled questions on a benchmark by order of magnitude. Implications for higher education include roles of artificial intelligence (AI) in automated course evaluation and content generation.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2665.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2665.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Codex</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI Codex (code-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based language model pre-trained on text and fine-tuned on source code, used here to synthesize executable programs that solve, explain, and generate university-level mathematics problems via few-shot and zero-shot program synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating large language models trained on code.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>OpenAI Codex (code-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Transformer-based large language model pre-trained on text and then fine-tuned on large corpora of source code. In this paper Codex is prompted (docstring-style prompts) to synthesize Python programs (often using SymPy, NumPy, Matplotlib, SciPy) that implement step-by-step solutions; generated programs are executed in a Python runtime to produce final answers and visualizations. Prompts include explicit instruction tokens (e.g., "write a program", library imports) and few-shot examples (question+code pairs). Hyperparameters for deterministic runs were top_p=0 and temperature=0; generation for question-creation used top_p=0.1 and temperature=0.1.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based (program-synthesis / code-finetuned transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematics education / automated problem solving (applied to university-level math courses and MATH benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Generates executable programs that implement candidate solutions to problems (i.e., programmatic candidate hypotheses). Uses zero-shot generation from augmented question prompts, and few-shot generation by retrieving nearest solved question-code pairs (via embeddings + cosine similarity) and including them as in-context examples. Generation produces multi-modal outputs (numerical answers, symbolic expressions, plots).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility is assessed by executing the synthesized program and checking whether the program output matches the expected solution (automatic evaluation), and by human evaluation in cases (student survey for generated questions). The paper also uses similarity-based retrieval to provide relevant few-shot examples, which increases plausibility of generated code.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Automatic solve rate (accuracy) on sampled problems: primary metric is percentage of problems where the executed program's output equals the correct solution. Reported values: 71.1% (zero-shot, MIT courses), 81.1% (few-shot, MIT courses); for MATH benchmark: 72.2% (zero-shot), 81.1% (few-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Computational validation by executing the synthesized Python program and comparing printed output or return value to the ground-truth solution; manual prompt-editing for problematic cases (~19% of questions); human (student) surveys used to evaluate quality of generated questions. For probabilistic tasks, Codex produced simulation programs whose empirical outputs are used as validation.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Fixed generation hyperparameters (top_p=0, temperature=0) for deterministic reproducible runs; prompt-format standardization (Python docstrings in triple quotes). Data and code are publicly released (GitHub link provided).</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Mitigation via (1) forcing program outputs to be executable and then validating by execution, (2) constraining prompts ("write a program", specify language and libraries), and (3) deterministic decoding (top_p=0, temperature=0) to reduce random outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Detection via program execution and automatic comparison of outputs to ground-truth answers (i.e., if executed program output does not match correct solution, generation considered incorrect). Manual inspection and iterative few-shot prompting also used for problematic cases.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Random samples from seven university courses (MIT: 18.01, 18.02, 18.03, 18.05, 18.06, 6.042; Columbia COMS3251) and the MATH benchmark (topics: Prealgebra, Algebra, Counting & Probability, Intermediate Algebra, Number Theory, Precalculus).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Automatic solve (execution-correct) rates: MIT courses: 71.1% (zero-shot), 81.1% (few-shot). MATH benchmark topics: 72.2% (zero-shot), 81.1% (few-shot). Overall solved 213/265 questions automatically (81% on course questions). Generation speed: programs and new questions generated in seconds (generation of questions <1s).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Compared against GPT-3 (text-davinci-002): GPT-3 achieved 18.8% zero-shot and 30.8% few-shot+CoT on MIT course sample; Codex improves automatic solution accuracy from prior SOTA 8.8% (on the MATH benchmark) to 81.1%.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Cannot handle non-text modalities (images) or proof-style problems; fails on computationally intractable generalizations (e.g., large-prime factoring); ~19% of problems require manual tidying or interactive prompting for visualization; does not provide formal uncertainty quantification for answers beyond deterministic execution checks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2665.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2665.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3 (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large transformer language model pre-trained on text-only corpora, used in the paper as a baseline for solving mathematics questions via zero-shot, few-shot, and chain-of-thought prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>OpenAI GPT-3 (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Transformer-based LLM pre-trained on broad text corpora (no code fine-tuning). In experiments the authors used zero-shot, few-shot, and chain-of-thought prompting (e.g., appending "Let's think step by step.") to produce textual solutions to mathematical problems; generation hyperparameters matched to Codex experiments where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based (text-only pre-trained transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematics problem solving (benchmarking / NLP baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Generates textual step-by-step reasoning (chain-of-thought) or final answers via few-shot/zero-shot prompts; not used to generate executable programmatic hypotheses in these experiments (except as a comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Assess plausibility by comparing textual answers to ground-truth solutions (automatic/evaluation labeling) and via qualitative error analysis; chain-of-thought prompting used to increase reasoning plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Automatic solve rate (accuracy) on the sampled course questions: 18.8% (zero-shot), 30.8% (few-shot + CoT) for MIT courses; on MATH benchmark: 25.5% (zero-shot), 42.2% (few-shot + CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Comparison of generated textual answers to ground-truth answers; no program execution for validation (unlike Codex).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Detected implicitly by mismatch to ground-truth answers and qualitative evaluation (GPT-3 outputs often incorrect on math problems).</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Same course samples and MATH benchmark as Codex comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>See hypothesis_quality_metrics above (accuracy percentages).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Used as the main text-only baseline; Codex (code-fine-tuned) substantially outperforms GPT-3 on automatic solve rates (e.g., 81.1% vs 30.8% few-shot on MIT course sample).</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Pre-training on text only led to poor performance on advanced mathematics tasks; even with CoT prompting GPT-3 fails on many university-level math problems compared to code-finetuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2665.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2665.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting ("Let's think step by step")</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that encourages large language models to generate intermediate reasoning steps (thought chains) to improve performance on multi-step reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are zero-shot reasoners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Prompt engineering method: append instructions such as "Let's think step by step." or provide few-shot examples that include intermediate reasoning steps. In the paper CoT was applied with GPT-3 to attempt to improve mathematical reasoning and as a comparison to Codex's inherently stepwise program outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Prompting technique for LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP methods applied to mathematical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Encourages the LLM to produce multi-step textual reasoning traces (intermediate deductive steps) which serve as candidate explanatory hypotheses for the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility judged by whether the chain leads to a correct final answer; used as an evaluation lever for GPT-3 but not sufficient to reach Codex performance.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Measured indirectly by change in automatic solve rate: GPT-3 few-shot+CoT achieved higher solve rates than GPT-3 without CoT (e.g., 30.8% vs 18.8% on MIT courses), but still far below Codex.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Comparison of final answers produced after CoT to ground truth; no program execution validation.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>CoT implemented by adding the text "Let's think step by step." after few-shot examples and before the new question (per paper's methods).</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Applied on the same MIT course questions and MATH benchmark comparisons for GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Improved GPT-3 solve rates compared to no-CoT (see above), but still far lower than Codex.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>CoT improves GPT-3 performance relative to GPT-3 without CoT, but Codex (program synthesis) remains substantially better.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>For text-only LMs, CoT did not close the performance gap to code-fine-tuned models on advanced math tasks; CoT is not a substitute for executable program synthesis in this domain.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2665.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2665.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot program synthesis (retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot program synthesis using nearest-neighbor retrieval of question-code examples</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that retrieves semantically similar solved question-code pairs (via embeddings + cosine similarity) and uses them as in-context few-shot examples to improve Codex program generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Few-shot program synthesis with embedding-based retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pipeline: (1) embed all solved questions using OpenAI's text-similarity-babbage-001 into 2048-dimensional vectors; (2) compute cosine similarity to the unsolved question; (3) select the nearest solved question-code pair(s) as few-shot examples (up to 5 examples used); (4) provide these question+code examples plus the target question as the prompt to Codex to synthesize code. This retrieval-augmented few-shot prompting increased automatic solve rates from ~71% (zero-shot) to ~81% (few-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Retrieval-augmented LLM in-context learning</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematics problem solving / program synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Generates candidate solution programs by conditioning Codex on semantically similar solved examples (in-context learning); retrieved examples bias generation toward patterns that solved similar problems.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility implicitly improved by using examples known to execute correctly; final plausibility checked by executing generated program.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Increase in automatic execution-correct solve rate from 71% (zero-shot) to 81% (few-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Execution of synthesized program and matching output to ground truth; iterative addition of further few-shot examples if first generated code fails.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Standardized embedding engine (text-similarity-babbage-001) and cosine similarity for retrieval; up to five examples used; prompt formatting standardized (docstrings).</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Using in-context correct examples reduces chance of spurious code patterns; execution-based verification further filters hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Detected via failed program execution or mismatch to ground truth; iterative retrieval and prompting used to correct failures.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Applied to the sampled course questions and MATH benchmark described in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Raised automatic solve rate from 71% (zero-shot) to 81% (few-shot) on course questions; similar improvements on MATH benchmark topics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Outperformed zero-shot Codex and GPT-3 baselines; demonstrates retrieval-augmented few-shot prompting is an effective booster for program synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Depends on presence of semantically similar solved examples; may fail when no close examples exist; retrieval quality bounded by embedding quality and dataset coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2665.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2665.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-similarity-babbage-001</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI text-similarity-babbage-001 embedding engine</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An embedding model that maps text inputs into 2048-dimensional vectors used to measure semantic similarity between questions for retrieval of few-shot examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>text-similarity-babbage-001 (embedding engine)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Embedding model from OpenAI used to convert textual questions into 2048-dimensional vectors; cosine similarity between embeddings is used to find the nearest solved question-code examples within each course for few-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Embedding / retrieval model</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP retrieval for program synthesis / math problem solving</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Used only for retrieval of relevant examples to condition program-synthesis (Codex) generation; not directly generating hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Improves plausibility of generated programs by selecting semantically similar exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Indirect: retrieval quality validated by increase in automatic solve rates when used for few-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Embedding dimensionality (2048) and cosine similarity stated; retrieval within-course to limit scope.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Used on the internal set of solved question-code pairs from the sampled course questions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Enabling retrieval-based few-shot prompting increased Codex automatic solve rate from ~71% to ~81%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Improves over prompts without retrieval; baseline was zero-shot Codex.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Quality depends on coverage of solved examples and embedding fidelity; no explicit uncertainty scores provided for retrieval relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2665.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2665.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Execution-based validation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Program execution as computational validation of generated solutions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A validation method where candidate solutions are represented as executable programs; generated code is run and its output compared to ground-truth to determine correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Execution-based validation of synthesized programs</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Treats synthesized code as the canonical representation of a candidate solution/hypothesis. Validation is performed by executing the program in a Python environment and comparing printed outputs or return values to known correct solutions. For simulation-based tasks the empirical outputs of the program serve as estimates of probabilistic quantities.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Computational validation / execution-based verifier</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematics problem solving; could generalize to scientific computation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Hypotheses are programmatic solutions produced by Codex; each is testable by running the program.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility equates to producing correct outputs upon execution; program execution also reveals runtime errors or exceptions that indicate invalid hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Binary execution-correct metric: whether program output equals ground truth (used to compute automatic solve rate/accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Automatic execution in Python interpreter and direct comparison to ground-truth answers; manual correction or additional few-shot examples if execution output is incorrect.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Fixed runtime environment and deterministic generation hyperparameters; code and data shared in repository.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>By requiring executable, testable outputs, many unsupported text hallucinations are avoided; runtime errors signal invalid generations.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Non-matching outputs and runtime exceptions act as detectors of incorrect or hallucinated claims.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Applied to the course samples and MATH benchmark questions used in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Primary metric: automatic solve rate (execution-correct percentages reported for Codex).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Program-execution validation distinguishes Codex (executable outputs) from GPT-3 (textual outputs) and is a key factor in improved automatic accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Execution only verifies the implemented computation; it does not guarantee human-interpretability of correctness in proof-style problems, cannot validate problems requiring non-executable reasoning (proofs), and is limited by the correctness of test oracles/ground-truth.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2665.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2665.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Probabilistic programming / simulation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simulation-based probabilistic program generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using program synthesis to generate simulation code that empirically estimates probabilistic quantities for statistics/probability problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Simulation-based probabilistic program generation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>For certain probability/statistics questions the system appends context (e.g., "using simulation") and Codex generates Python code that runs Monte Carlo simulations to estimate probabilities or expectations; the empirical outputs of these simulations are used as the answer.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-generated simulation programs / probabilistic programming</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Probability and statistics / empirical estimation</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Generates simulation programs (Monte Carlo) as candidate estimators for quantities of interest.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility judged by simulation convergence and agreement with analytic solutions when available; empirical statistics produced by running the generated code provide numerical checks.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Implicit empirical convergence of simulation outputs; no explicit convergence diagnostics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Execution of simulation code and interpretation of empirical statistics as the answer; comparison to ground-truth if available.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Not specified beyond prompt context and code execution; seed control not described in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Implicitly provides empirical variability (e.g., sample variance) from simulation, but no formal uncertainty quantification protocol reported.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Applied ad hoc to probability/statistics problems within sampled course questions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>No formal reporting of simulation convergence diagnostics or uncertainty intervals; simulation-based answers may require many samples for high precision and the paper does not detail sample sizes or error bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2665.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2665.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generate & Rank</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generate & Rank (multi-task framework for math word problems)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior method that generates candidate solutions and ranks them, used in related work for math word problems and verification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generate & rank: A multi-task framework for math word problems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Generate & Rank (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned in related work as an approach that generates multiple candidate solutions and uses a ranking/verifier step to select the most plausible solution. Cited as part of prior literature on verification/co-training for math word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Generation + verification pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Math word problem solving / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Generates multiple candidate answers (or solution expressions) and ranks them using learned scoring/verifier models.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Ranking/verifier model estimates plausibility of each generated candidate.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Verifier/ranker used to select outputs; details in cited work (paper not used in experiments here).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Mentioned in context of prior approaches to math word problems (MAWPS, Math23k, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Described as effective on elementary math problems but not extending well to high-school or university-level problems per the paper's related-work discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2665.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2665.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Verifier training</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Training verifiers to solve math word problems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior approach that trains a verifier model to judge candidate equations/solutions; cited as related work addressing verification in math problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Training verifiers to solve math word problems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Verifier training (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Approaches that train a secondary model to verify or score generated solutions (e.g., to detect incorrect expressions or rank correct ones higher). Cited as prior literature on verification/co-training for math word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Verification / ranking neural model</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Math word problem solving / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Separates generation and verification: generator proposes candidates, verifier assesses correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Verifier outputs a score or label indicating plausibility/correctness of candidate solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Verifier model applied to candidate outputs; details in cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Referenced in relation to MAWPS, Math23k and other math-problem datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Paper notes these verifier-based methods achieve strong results on elementary problems but do not generalize to university-level courses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2665.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2665.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Expression-tree / GNN methods</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Expression-tree prediction and graph neural network (GNN) approaches</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior methods that predict arithmetic/expression trees (sometimes using graph neural networks) to solve math problems; cited as related work with limited generalization to university-level math in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Expression-tree / GNN solvers (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Approaches that parse math word problems into expression trees or structured programs using sequence-to-tree or graph-to-tree models, including GNNs that co-train representations to predict arithmetic expression trees. Not used in this paper's main experiments but discussed for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Neural-structured prediction (graph-to-tree / sequence-to-tree)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Math word problem solving / structured semantic parsing</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Generates expression trees representing formulas that compute numerical answers.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Scoring / supervised learning to predict correct expression trees; often evaluated by exact-match or numeric correctness on datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Evaluated by computing numeric outputs from predicted expression trees and comparing to ground-truth answers; cited works report high performance on restricted tasks/datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>MAWPS, Math23k, and course-specific datasets mentioned in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Cited prior results: some GNN-based methods achieved up to 95% accuracy on narrowly scoped university-level machine-learning course problems but overfit to specific numeric-answer tasks and do not generalize to broad courses.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Paper contrasts Codex's program-synthesis generality with expression-tree methods that are more brittle or specialized.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>These approaches typically target numeric-answer problems, can overfit, and often do not generalize across course topics or to multi-modal outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generate & rank: A multi-task framework for math word problems <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Evaluating large language models trained on code <em>(Rating: 2)</em></li>
                <li>Measuring mathematical problem solving with the MATH dataset <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2665",
    "paper_id": "paper-257aee73d83a87921fd2d56b524de394dcf6a264",
    "extraction_schema_id": "extraction-schema-68",
    "extracted_data": [
        {
            "name_short": "Codex",
            "name_full": "OpenAI Codex (code-davinci-002)",
            "brief_description": "A transformer-based language model pre-trained on text and fine-tuned on source code, used here to synthesize executable programs that solve, explain, and generate university-level mathematics problems via few-shot and zero-shot program synthesis.",
            "citation_title": "Evaluating large language models trained on code.",
            "mention_or_use": "use",
            "system_name": "OpenAI Codex (code-davinci-002)",
            "system_description": "Transformer-based large language model pre-trained on text and then fine-tuned on large corpora of source code. In this paper Codex is prompted (docstring-style prompts) to synthesize Python programs (often using SymPy, NumPy, Matplotlib, SciPy) that implement step-by-step solutions; generated programs are executed in a Python runtime to produce final answers and visualizations. Prompts include explicit instruction tokens (e.g., \"write a program\", library imports) and few-shot examples (question+code pairs). Hyperparameters for deterministic runs were top_p=0 and temperature=0; generation for question-creation used top_p=0.1 and temperature=0.1.",
            "system_type": "LLM-based (program-synthesis / code-finetuned transformer)",
            "scientific_domain": "Mathematics education / automated problem solving (applied to university-level math courses and MATH benchmark)",
            "hypothesis_generation_method": "Generates executable programs that implement candidate solutions to problems (i.e., programmatic candidate hypotheses). Uses zero-shot generation from augmented question prompts, and few-shot generation by retrieving nearest solved question-code pairs (via embeddings + cosine similarity) and including them as in-context examples. Generation produces multi-modal outputs (numerical answers, symbolic expressions, plots).",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Plausibility is assessed by executing the synthesized program and checking whether the program output matches the expected solution (automatic evaluation), and by human evaluation in cases (student survey for generated questions). The paper also uses similarity-based retrieval to provide relevant few-shot examples, which increases plausibility of generated code.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Automatic solve rate (accuracy) on sampled problems: primary metric is percentage of problems where the executed program's output equals the correct solution. Reported values: 71.1% (zero-shot, MIT courses), 81.1% (few-shot, MIT courses); for MATH benchmark: 72.2% (zero-shot), 81.1% (few-shot).",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Computational validation by executing the synthesized Python program and comparing printed output or return value to the ground-truth solution; manual prompt-editing for problematic cases (~19% of questions); human (student) surveys used to evaluate quality of generated questions. For probabilistic tasks, Codex produced simulation programs whose empirical outputs are used as validation.",
            "reproducibility_measures": "Fixed generation hyperparameters (top_p=0, temperature=0) for deterministic reproducible runs; prompt-format standardization (Python docstrings in triple quotes). Data and code are publicly released (GitHub link provided).",
            "hallucination_prevention_method": "Mitigation via (1) forcing program outputs to be executable and then validating by execution, (2) constraining prompts (\"write a program\", specify language and libraries), and (3) deterministic decoding (top_p=0, temperature=0) to reduce random outputs.",
            "hallucination_detection_method": "Detection via program execution and automatic comparison of outputs to ground-truth answers (i.e., if executed program output does not match correct solution, generation considered incorrect). Manual inspection and iterative few-shot prompting also used for problematic cases.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Random samples from seven university courses (MIT: 18.01, 18.02, 18.03, 18.05, 18.06, 6.042; Columbia COMS3251) and the MATH benchmark (topics: Prealgebra, Algebra, Counting & Probability, Intermediate Algebra, Number Theory, Precalculus).",
            "performance_metrics": "Automatic solve (execution-correct) rates: MIT courses: 71.1% (zero-shot), 81.1% (few-shot). MATH benchmark topics: 72.2% (zero-shot), 81.1% (few-shot). Overall solved 213/265 questions automatically (81% on course questions). Generation speed: programs and new questions generated in seconds (generation of questions &lt;1s).",
            "comparison_with_baseline": "Compared against GPT-3 (text-davinci-002): GPT-3 achieved 18.8% zero-shot and 30.8% few-shot+CoT on MIT course sample; Codex improves automatic solution accuracy from prior SOTA 8.8% (on the MATH benchmark) to 81.1%.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Cannot handle non-text modalities (images) or proof-style problems; fails on computationally intractable generalizations (e.g., large-prime factoring); ~19% of problems require manual tidying or interactive prompting for visualization; does not provide formal uncertainty quantification for answers beyond deterministic execution checks.",
            "uuid": "e2665.0",
            "source_info": {
                "paper_title": "A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "GPT-3",
            "name_full": "OpenAI GPT-3 (text-davinci-002)",
            "brief_description": "A large transformer language model pre-trained on text-only corpora, used in the paper as a baseline for solving mathematics questions via zero-shot, few-shot, and chain-of-thought prompting.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "use",
            "system_name": "OpenAI GPT-3 (text-davinci-002)",
            "system_description": "Transformer-based LLM pre-trained on broad text corpora (no code fine-tuning). In experiments the authors used zero-shot, few-shot, and chain-of-thought prompting (e.g., appending \"Let's think step by step.\") to produce textual solutions to mathematical problems; generation hyperparameters matched to Codex experiments where applicable.",
            "system_type": "LLM-based (text-only pre-trained transformer)",
            "scientific_domain": "Mathematics problem solving (benchmarking / NLP baseline)",
            "hypothesis_generation_method": "Generates textual step-by-step reasoning (chain-of-thought) or final answers via few-shot/zero-shot prompts; not used to generate executable programmatic hypotheses in these experiments (except as a comparison).",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Assess plausibility by comparing textual answers to ground-truth solutions (automatic/evaluation labeling) and via qualitative error analysis; chain-of-thought prompting used to increase reasoning plausibility.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Automatic solve rate (accuracy) on the sampled course questions: 18.8% (zero-shot), 30.8% (few-shot + CoT) for MIT courses; on MATH benchmark: 25.5% (zero-shot), 42.2% (few-shot + CoT).",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Comparison of generated textual answers to ground-truth answers; no program execution for validation (unlike Codex).",
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": "Detected implicitly by mismatch to ground-truth answers and qualitative evaluation (GPT-3 outputs often incorrect on math problems).",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Same course samples and MATH benchmark as Codex comparisons.",
            "performance_metrics": "See hypothesis_quality_metrics above (accuracy percentages).",
            "comparison_with_baseline": "Used as the main text-only baseline; Codex (code-fine-tuned) substantially outperforms GPT-3 on automatic solve rates (e.g., 81.1% vs 30.8% few-shot on MIT course sample).",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Pre-training on text only led to poor performance on advanced mathematics tasks; even with CoT prompting GPT-3 fails on many university-level math problems compared to code-finetuned models.",
            "uuid": "e2665.1",
            "source_info": {
                "paper_title": "A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT) prompting",
            "name_full": "Chain-of-Thought prompting (\"Let's think step by step\")",
            "brief_description": "A prompting technique that encourages large language models to generate intermediate reasoning steps (thought chains) to improve performance on multi-step reasoning tasks.",
            "citation_title": "Large language models are zero-shot reasoners",
            "mention_or_use": "use",
            "system_name": "Chain-of-Thought (CoT) prompting",
            "system_description": "Prompt engineering method: append instructions such as \"Let's think step by step.\" or provide few-shot examples that include intermediate reasoning steps. In the paper CoT was applied with GPT-3 to attempt to improve mathematical reasoning and as a comparison to Codex's inherently stepwise program outputs.",
            "system_type": "Prompting technique for LLMs",
            "scientific_domain": "NLP methods applied to mathematical reasoning",
            "hypothesis_generation_method": "Encourages the LLM to produce multi-step textual reasoning traces (intermediate deductive steps) which serve as candidate explanatory hypotheses for the final answer.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Plausibility judged by whether the chain leads to a correct final answer; used as an evaluation lever for GPT-3 but not sufficient to reach Codex performance.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Measured indirectly by change in automatic solve rate: GPT-3 few-shot+CoT achieved higher solve rates than GPT-3 without CoT (e.g., 30.8% vs 18.8% on MIT courses), but still far below Codex.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Comparison of final answers produced after CoT to ground truth; no program execution validation.",
            "reproducibility_measures": "CoT implemented by adding the text \"Let's think step by step.\" after few-shot examples and before the new question (per paper's methods).",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Applied on the same MIT course questions and MATH benchmark comparisons for GPT-3.",
            "performance_metrics": "Improved GPT-3 solve rates compared to no-CoT (see above), but still far lower than Codex.",
            "comparison_with_baseline": "CoT improves GPT-3 performance relative to GPT-3 without CoT, but Codex (program synthesis) remains substantially better.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "For text-only LMs, CoT did not close the performance gap to code-fine-tuned models on advanced math tasks; CoT is not a substitute for executable program synthesis in this domain.",
            "uuid": "e2665.2",
            "source_info": {
                "paper_title": "A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "Few-shot program synthesis (retrieval)",
            "name_full": "Few-shot program synthesis using nearest-neighbor retrieval of question-code examples",
            "brief_description": "A method that retrieves semantically similar solved question-code pairs (via embeddings + cosine similarity) and uses them as in-context few-shot examples to improve Codex program generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Few-shot program synthesis with embedding-based retrieval",
            "system_description": "Pipeline: (1) embed all solved questions using OpenAI's text-similarity-babbage-001 into 2048-dimensional vectors; (2) compute cosine similarity to the unsolved question; (3) select the nearest solved question-code pair(s) as few-shot examples (up to 5 examples used); (4) provide these question+code examples plus the target question as the prompt to Codex to synthesize code. This retrieval-augmented few-shot prompting increased automatic solve rates from ~71% (zero-shot) to ~81% (few-shot).",
            "system_type": "Retrieval-augmented LLM in-context learning",
            "scientific_domain": "Mathematics problem solving / program synthesis",
            "hypothesis_generation_method": "Generates candidate solution programs by conditioning Codex on semantically similar solved examples (in-context learning); retrieved examples bias generation toward patterns that solved similar problems.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Plausibility implicitly improved by using examples known to execute correctly; final plausibility checked by executing generated program.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Increase in automatic execution-correct solve rate from 71% (zero-shot) to 81% (few-shot).",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Execution of synthesized program and matching output to ground truth; iterative addition of further few-shot examples if first generated code fails.",
            "reproducibility_measures": "Standardized embedding engine (text-similarity-babbage-001) and cosine similarity for retrieval; up to five examples used; prompt formatting standardized (docstrings).",
            "hallucination_prevention_method": "Using in-context correct examples reduces chance of spurious code patterns; execution-based verification further filters hallucinations.",
            "hallucination_detection_method": "Detected via failed program execution or mismatch to ground truth; iterative retrieval and prompting used to correct failures.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Applied to the sampled course questions and MATH benchmark described in the paper.",
            "performance_metrics": "Raised automatic solve rate from 71% (zero-shot) to 81% (few-shot) on course questions; similar improvements on MATH benchmark topics.",
            "comparison_with_baseline": "Outperformed zero-shot Codex and GPT-3 baselines; demonstrates retrieval-augmented few-shot prompting is an effective booster for program synthesis.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Depends on presence of semantically similar solved examples; may fail when no close examples exist; retrieval quality bounded by embedding quality and dataset coverage.",
            "uuid": "e2665.3",
            "source_info": {
                "paper_title": "A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "text-similarity-babbage-001",
            "name_full": "OpenAI text-similarity-babbage-001 embedding engine",
            "brief_description": "An embedding model that maps text inputs into 2048-dimensional vectors used to measure semantic similarity between questions for retrieval of few-shot examples.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "text-similarity-babbage-001 (embedding engine)",
            "system_description": "Embedding model from OpenAI used to convert textual questions into 2048-dimensional vectors; cosine similarity between embeddings is used to find the nearest solved question-code examples within each course for few-shot prompting.",
            "system_type": "Embedding / retrieval model",
            "scientific_domain": "NLP retrieval for program synthesis / math problem solving",
            "hypothesis_generation_method": "Used only for retrieval of relevant examples to condition program-synthesis (Codex) generation; not directly generating hypotheses.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Improves plausibility of generated programs by selecting semantically similar exemplars.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Indirect: retrieval quality validated by increase in automatic solve rates when used for few-shot prompting.",
            "reproducibility_measures": "Embedding dimensionality (2048) and cosine similarity stated; retrieval within-course to limit scope.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Used on the internal set of solved question-code pairs from the sampled course questions.",
            "performance_metrics": "Enabling retrieval-based few-shot prompting increased Codex automatic solve rate from ~71% to ~81%.",
            "comparison_with_baseline": "Improves over prompts without retrieval; baseline was zero-shot Codex.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Quality depends on coverage of solved examples and embedding fidelity; no explicit uncertainty scores provided for retrieval relevance.",
            "uuid": "e2665.4",
            "source_info": {
                "paper_title": "A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "Execution-based validation",
            "name_full": "Program execution as computational validation of generated solutions",
            "brief_description": "A validation method where candidate solutions are represented as executable programs; generated code is run and its output compared to ground-truth to determine correctness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Execution-based validation of synthesized programs",
            "system_description": "Treats synthesized code as the canonical representation of a candidate solution/hypothesis. Validation is performed by executing the program in a Python environment and comparing printed outputs or return values to known correct solutions. For simulation-based tasks the empirical outputs of the program serve as estimates of probabilistic quantities.",
            "system_type": "Computational validation / execution-based verifier",
            "scientific_domain": "Mathematics problem solving; could generalize to scientific computation tasks",
            "hypothesis_generation_method": "Hypotheses are programmatic solutions produced by Codex; each is testable by running the program.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Plausibility equates to producing correct outputs upon execution; program execution also reveals runtime errors or exceptions that indicate invalid hypotheses.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Binary execution-correct metric: whether program output equals ground truth (used to compute automatic solve rate/accuracy).",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Automatic execution in Python interpreter and direct comparison to ground-truth answers; manual correction or additional few-shot examples if execution output is incorrect.",
            "reproducibility_measures": "Fixed runtime environment and deterministic generation hyperparameters; code and data shared in repository.",
            "hallucination_prevention_method": "By requiring executable, testable outputs, many unsupported text hallucinations are avoided; runtime errors signal invalid generations.",
            "hallucination_detection_method": "Non-matching outputs and runtime exceptions act as detectors of incorrect or hallucinated claims.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Applied to the course samples and MATH benchmark questions used in the paper.",
            "performance_metrics": "Primary metric: automatic solve rate (execution-correct percentages reported for Codex).",
            "comparison_with_baseline": "Program-execution validation distinguishes Codex (executable outputs) from GPT-3 (textual outputs) and is a key factor in improved automatic accuracy.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Execution only verifies the implemented computation; it does not guarantee human-interpretability of correctness in proof-style problems, cannot validate problems requiring non-executable reasoning (proofs), and is limited by the correctness of test oracles/ground-truth.",
            "uuid": "e2665.5",
            "source_info": {
                "paper_title": "A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "Probabilistic programming / simulation",
            "name_full": "Simulation-based probabilistic program generation",
            "brief_description": "Using program synthesis to generate simulation code that empirically estimates probabilistic quantities for statistics/probability problems.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Simulation-based probabilistic program generation",
            "system_description": "For certain probability/statistics questions the system appends context (e.g., \"using simulation\") and Codex generates Python code that runs Monte Carlo simulations to estimate probabilities or expectations; the empirical outputs of these simulations are used as the answer.",
            "system_type": "LLM-generated simulation programs / probabilistic programming",
            "scientific_domain": "Probability and statistics / empirical estimation",
            "hypothesis_generation_method": "Generates simulation programs (Monte Carlo) as candidate estimators for quantities of interest.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Plausibility judged by simulation convergence and agreement with analytic solutions when available; empirical statistics produced by running the generated code provide numerical checks.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Implicit empirical convergence of simulation outputs; no explicit convergence diagnostics reported.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Execution of simulation code and interpretation of empirical statistics as the answer; comparison to ground-truth if available.",
            "reproducibility_measures": "Not specified beyond prompt context and code execution; seed control not described in main text.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Implicitly provides empirical variability (e.g., sample variance) from simulation, but no formal uncertainty quantification protocol reported.",
            "benchmark_dataset": "Applied ad hoc to probability/statistics problems within sampled course questions.",
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "No formal reporting of simulation convergence diagnostics or uncertainty intervals; simulation-based answers may require many samples for high precision and the paper does not detail sample sizes or error bounds.",
            "uuid": "e2665.6",
            "source_info": {
                "paper_title": "A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "Generate & Rank",
            "name_full": "Generate & Rank (multi-task framework for math word problems)",
            "brief_description": "Referenced prior method that generates candidate solutions and ranks them, used in related work for math word problems and verification.",
            "citation_title": "Generate & rank: A multi-task framework for math word problems",
            "mention_or_use": "mention",
            "system_name": "Generate & Rank (related work)",
            "system_description": "Mentioned in related work as an approach that generates multiple candidate solutions and uses a ranking/verifier step to select the most plausible solution. Cited as part of prior literature on verification/co-training for math word problems.",
            "system_type": "Generation + verification pipeline",
            "scientific_domain": "Math word problem solving / NLP",
            "hypothesis_generation_method": "Generates multiple candidate answers (or solution expressions) and ranks them using learned scoring/verifier models.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Ranking/verifier model estimates plausibility of each generated candidate.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": "Verifier/ranker used to select outputs; details in cited work (paper not used in experiments here).",
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Mentioned in context of prior approaches to math word problems (MAWPS, Math23k, etc.).",
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "Described as effective on elementary math problems but not extending well to high-school or university-level problems per the paper's related-work discussion.",
            "uuid": "e2665.7",
            "source_info": {
                "paper_title": "A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "Verifier training",
            "name_full": "Training verifiers to solve math word problems",
            "brief_description": "Prior approach that trains a verifier model to judge candidate equations/solutions; cited as related work addressing verification in math problem solving.",
            "citation_title": "Training verifiers to solve math word problems",
            "mention_or_use": "mention",
            "system_name": "Verifier training (related work)",
            "system_description": "Approaches that train a secondary model to verify or score generated solutions (e.g., to detect incorrect expressions or rank correct ones higher). Cited as prior literature on verification/co-training for math word problems.",
            "system_type": "Verification / ranking neural model",
            "scientific_domain": "Math word problem solving / NLP",
            "hypothesis_generation_method": "Separates generation and verification: generator proposes candidates, verifier assesses correctness.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Verifier outputs a score or label indicating plausibility/correctness of candidate solutions.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": "Verifier model applied to candidate outputs; details in cited work.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Referenced in relation to MAWPS, Math23k and other math-problem datasets.",
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "Paper notes these verifier-based methods achieve strong results on elementary problems but do not generalize to university-level courses.",
            "uuid": "e2665.8",
            "source_info": {
                "paper_title": "A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "Expression-tree / GNN methods",
            "name_full": "Expression-tree prediction and graph neural network (GNN) approaches",
            "brief_description": "Prior methods that predict arithmetic/expression trees (sometimes using graph neural networks) to solve math problems; cited as related work with limited generalization to university-level math in this paper.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Expression-tree / GNN solvers (related work)",
            "system_description": "Approaches that parse math word problems into expression trees or structured programs using sequence-to-tree or graph-to-tree models, including GNNs that co-train representations to predict arithmetic expression trees. Not used in this paper's main experiments but discussed for comparison.",
            "system_type": "Neural-structured prediction (graph-to-tree / sequence-to-tree)",
            "scientific_domain": "Math word problem solving / structured semantic parsing",
            "hypothesis_generation_method": "Generates expression trees representing formulas that compute numerical answers.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Scoring / supervised learning to predict correct expression trees; often evaluated by exact-match or numeric correctness on datasets.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": "Evaluated by computing numeric outputs from predicted expression trees and comparing to ground-truth answers; cited works report high performance on restricted tasks/datasets.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "MAWPS, Math23k, and course-specific datasets mentioned in related work.",
            "performance_metrics": "Cited prior results: some GNN-based methods achieved up to 95% accuracy on narrowly scoped university-level machine-learning course problems but overfit to specific numeric-answer tasks and do not generalize to broad courses.",
            "comparison_with_baseline": "Paper contrasts Codex's program-synthesis generality with expression-tree methods that are more brittle or specialized.",
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "These approaches typically target numeric-answer problems, can overfit, and often do not generalize across course topics or to multi-modal outputs.",
            "uuid": "e2665.9",
            "source_info": {
                "paper_title": "A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level",
                "publication_date_yy_mm": "2021-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generate & rank: A multi-task framework for math word problems",
            "rating": 2
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2
        },
        {
            "paper_title": "Evaluating large language models trained on code",
            "rating": 2
        },
        {
            "paper_title": "Measuring mathematical problem solving with the MATH dataset",
            "rating": 1
        }
    ],
    "cost": 0.02224125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Neural Network Solves, Explains, and Generates University Math Problems by Program Synthesis and Few-Shot Learning at Human Level</h1>
<p>Iddo Drori ${ }^{1, a, b}$, Sarah Zhang ${ }^{\text {a }}$, Reece Shuttleworth ${ }^{\text {a }}$, Leonard Tang ${ }^{\circ}$, Albert Lu ${ }^{\text {a }}$, Elizabeth Ke ${ }^{\text {a }}$, Kevin Liu ${ }^{\text {a }}$, Linda Chen ${ }^{\text {a }}$, Sunny Tran ${ }^{\text {a }}$, Newman Cheng ${ }^{\text {b }}$, Roman Wang ${ }^{\text {b }}$, Nikhil Singh ${ }^{\text {a }}$, Taylor L. Patti ${ }^{\circ}$, Jayson Lynch ${ }^{\text {c }}$, Avi Shporer ${ }^{\text {a }}$, Nakul Verma ${ }^{\text {b }}$, Eugene Wu ${ }^{\text {b }}$, and Gilbert Strang ${ }^{\text {a }}$<br>${ }^{a}$ Massachusetts Institute of Technology; ${ }^{\text {b }}$ Columbia University; ${ }^{\circ}$ Harvard University; ${ }^{\text {c }}$ University of Waterloo</p>
<p>This manuscript was compiled on June 1, 2022</p>
<p>We demonstrate that a neural network pre-trained on text and finetuned on code solves mathematics course problems, explains solutions, and generates new questions at a human level. We automatically synthesize programs using few-shot learning and OpenAI's Codex transformer and execute them to solve course problems at $81 \%$ automatic accuracy. We curate a new dataset of questions from MIT's largest mathematics courses (Single Variable and Multivariable Calculus, Differential Equations, Introduction to Probability and Statistics, Linear Algebra, and Mathematics for Computer Science) and Columbia University's Computational Linear Algebra. We solve questions from a MATH dataset (on Prealgebra, Algebra, Counting and Probability, Intermediate Algebra, Number Theory, and Precalculus), the latest benchmark of advanced mathematics problems designed to assess mathematical reasoning. We randomly sample questions and generate solutions with multiple modalities, including numbers, equations, and plots. The latest GPT-3 language model pre-trained on text automatically solves only $18.8 \%$ of these university questions using zero-shot learning and $30.8 \%$ using few-shot learning and the most recent chain of thought prompting. In contrast, program synthesis with few-shot learning using Codex finetuned on code generates programs that automatically solve $81 \%$ of these questions. Our approach improves the previous state-of-theart automatic solution accuracy on the benchmark topics from $8.8 \%$ to $81.1 \%$. We perform a survey to evaluate the quality and difficulty of generated questions. This work is the first to automatically solve university-level mathematics course questions at a human level and the first work to explain and generate university-level mathematics course questions at scale, a milestone for higher education.</p>
<p>Neural networks | Mathematics courses | Answering, explaining, and generating questions</p>
<h2>Introduction</h2>
<p>Until this work; it was widely believed that neural networks could not solve advanced mathematics problems (1). However, the previous unsuccessful studies used only text-based pre-training. We now demonstrate that a neural network, OpenAI Codex, that is pre-trained on text and finetuned on code automatically answers $81 \%$ of university-level mathematics problems by program synthesis using few-shot learning.</p>
<p>Figure 1 illustrates several example problems: computing the volume generated by rotating the graph of a single variable function around an axis, computing the Lorenz attractor and its projection, and computing and demonstrating the geometry of a singular value decomposition (SVD). For the first time, we show that a single machine learning model can solve these
example problems and solve a wide variety of mathematics courses at scale.</p>
<p>Related Work. Transformers are deep learning architectures based only on attention mechanisms (2) that do not use recurrent neural networks or convolutional neural networks. Transformer-based language models have enjoyed tremendous success across various natural language processing (NLP) tasks, including zero-shot and few-shot language tasks (3). However, these models have largely failed to solve math problems (4-6). In particular, previous work using transformers, such as GPT3 (3), has failed to solve mathematics problems because the transformers were pre-trained on text alone. Using few-shot learning and chain of thought (CoT) prompting (7) improves the mathematical reasoning ability of GPT-3; however, without code, GPT-3 with few-shot learning and CoT still fails on university-level mathematics problems and the MATH benchmark.</p>
<p>Pre-training a transformer is computationally expensive and often involves vast amounts of unlabeled data. The most common optimization objectives for pre-training language</p>
<h2>Significance Statement</h2>
<p>We demonstrate that a neural network automatically solves, explains, and generates university-level problems from the largest MIT mathematics courses at a human level. Our methods combine three innovations: (1) using recent neural networks pretrained on text and fine-tuned on code rather than pre-trained on text, (2) few-shot learning synthesizing programs that correctly solve course problems automatically, and (3) a pipeline to solve questions, explain solutions, and generate new questions indistinguishable by students from course questions. Our work is the first to solve university-level mathematics courses and improves upon state-of-the-art increasing automatic accuracy on randomly sampled questions on a benchmark by order of magnitude. Implications for higher education include the new roles of AI in automated course evaluation and content generation.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. We apply a neural network, OpenAI Codex, to solve, explain, and generate mathematics problems. We randomly sample the input math problems from MIT and Columbia University courses and the MATH dataset (left). We use zero-shot and few-shot learning to automatically generate programs that solve $81 \%$ of the questions. We then use Codex to explain the generated programs. The generated programs can output diverse forms of answers, like printing a numerical answer or generating a plot (right). For example, in Calculus: the volume generated by rotating the finite 2-dimensional region bounded by two 2-dimensional graphs about the plotted axis (top right); in Differential Equations: the Lorenz strange attractor (middle right); In Linear Algebra: the geometry of the singular value decomposition (SVD) (middle right). An example of Codex's ability to produce line-by-line explanations of synthesized programs is demonstrated for a problem from Introduction to Probability and Statistics (bottom right).
models are (1) masked word prediction: predicting a random deleted word in a sentence or predicting the next word, or (2) classifying whether two sentences follow each other. This computationally expensive step is usually done once, followed by a relatively fast fine-tuning step. In fine-tuning, the pretrained model is tuned using a specific dataset or task.</p>
<p>This work demonstrates that OpenAI's Codex (8), a transformer that has been pre-trained on text and then fine-tuned on code, generates programs (i.e., conducts program synthesis) that solve math problems at scale and, with few-shot learning, automatically solves $81 \%$ of the math course problems.</p>
<p>Previous work has seen modest success on simpler or specialized mathematics problem benchmarks. Techniques based on co-training output to verify $(9,10)$ or predict expression trees (11-16), such as MAWPS and Math23k, are able to solve elementary school-level math problems with over $81 \%$ accuracy. However, these approaches do not extend to high-school, math Olympiad, or university-level courses. Co-training paired with graph neural networks (GNNs) to predict arithmetic expression trees is able to solve university-level problems in Machine Learning (17) with up to $95 \%$ accuracy. However, that work is limited to numeric answers and overfits a specific course, which does not generalize to other courses.</p>
<p>Major Contributions. Our main contribution, as shown in Figure 2, is demonstrating that a single neural network model, OpenAI Codex, automatically solves $81 \%$ of randomly selected university-level mathematics problems (from six MIT mathematics courses and one Columbia University course) by using program synthesis and few-shot learning. We also automatically explain the solutions and generate new questions, a process requiring only seconds per problem. The courses are listed in Table 1. We randomly sample 25 questions per course, and the problems are solved as-is or with minor contextual information that is automatically applied. The neural net-
work outputs an executable program that answers the problem when prompted with the question. Furthermore, our method explains the solutions and generates new problems nearly indistinguishable from human-written problems.</p>
<p>This methodology increases the solution accuracy on the MATH benchmark (5) from $8.8 \%$ accuracy using previous state-of-the-art methods to $81.1 \%$ accuracy using automatic few-shot learning. The MATH benchmark measures the mathematical problem-solving ability of neural network models with challenging problems sourced from high school math competitions, such as the AMC $10^{*}$, AMC 12, and AIME ${ }^{1}$.</p>
<p>The methods we propose are simple and broadly applicable. The first is using a transformer model pre-trained on text and fine-tuned on code so that it is adept at synthesizing programmatic solutions. The second is to use zero-shot learning of the questions as-is or automatically added contextual information about the problem or program. The third is to use few-shot learning based on question-code pairs of similar questions that have been solved, found by using the cosine similarity of the question embeddings.</p>
<h2>Methods</h2>
<p>Dataset. We randomly sample 25 questions from each of the seven courses: MIT's 18.01 Single Variable Calculus, 18.02 Multivariable Calculus, 18.03 Differential Equations, 18.05 Introduction to Probability and Statistics, 18.06 Linear Algebra, 6.042 Mathematics for Computer Science, and Columbia University's COMS3251 Computational Linear Algebra. For the MATH dataset, we randomly sample 15 questions from six topics in the dataset (Algebra, Counting \&amp; Probability, Intermediate Algebra, Number Theory, Prealgebra, and Precalculus). We validate that our results are not merely overfitting</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. We select a random sample of questions from each course or topic that do not contain input images or require proofs. A language model pre-trained on text (GPT-3 text-davinci-002) automatically solves only 18% (for courses) and 25.5% (for the MATH benchmark topics) of these questions. In contrast, using zero-shot learning with a network pre-trained on text and fine tuned on code (OpenAI Codex code-davinci-002), we synthesize programs that automatically solve 71% (for courses) and 72.2% (for the MATH benchmark topics) of the questions. Using the same network but using few-shot learning, we automatically solve 81% (for courses) and 81.1% (for the MATH benchmark topics) of the questions. We use the nearest embedded zero-shot questions and their synthesized code for few-shot learning. The remaining 19% of the course questions and 18.9% of MATH benchmark topic questions are manually prompted to solve the question.</p>
<p>Training data by solving questions from a new Computational Linear Algebra course COMS3251 which is unavailable online and was unseen by Codex when trained. We automatically obtain correct answers for 81% of the randomly sampled university math course questions and 81.1% of the MATH benchmark questions. Before this work, the previous state-of-the-art on this benchmark was 8.8% (4).</p>
<p><strong>Workflow.</strong> Our method takes a course problem as input and synthesizes a program that, when run, outputs the solution. Figure 4 compares the percent of automatically solved questions for each course using our zero-shot learning and few-shot learning approaches with the latest GPT-3 (text-davinci-002) and Codex (code-davinci-002) versions. The error bars on the totals are standard errors.</p>
<p>Figures 3 show examples of automatic workflows for solving course questions and generating explanations using Codex. The panels show the original question, the automatic augmentation with context, the resulting synthesized program, the executed output answer that is the solution, and the explanation of the solution program. Questions are given to Codex either as-is or by automatically adding minor context, as described below. The output answer may be of numerous modalities. In the examples featured in Figure 3, the output answers are an equation (18.01), a Boolean value (18.02), a plot (18.03), and a numerical value (18.05), and a vector (18.03 and 18.06).</p>
<h3>Automatic Contextualization</h3>
<p><strong>Programming Language Context.</strong> Best results are obtained when the Codex prompt specifies that a program should be written and specifies which programming language should be used. We add the text "write a program" before the question and focus on the Python programming language by placing the text within Pythonic triple quotes like a docstring.</p>
<p><strong>Library Context.</strong> Likewise, the best results are obtained when the Codex prompt specifies which programming package should be used. For instance, we may add the Python library SymPy as context (see Figure 3 top panel 18.01), specifying that the program synthesized to solve the problem should use this package.</p>
<p>Figure 5 shows the Python programming packages used by each course. Each colored stacked bar represents the number of questions in the class using that package. All courses use NumPy and Sympy. Matplotlib is used in classes with questions that require plotting. Around half of the courses use math, random, and SciPy. The usage patterns of these courses are incorporated automatically in our approach, as we only specify SymPy or plot-related imports; these other package imports are automatically synthesized.</p>
<h3>Automatic Zero-Shot and Few-Shot Learning</h3>
<p>Zero-shot learning synthesizes a program from the original question or the automatically augmented question without examples. This method automatically solves 71% of the questions. Next, we describe the few-shot learning process in detail: If the question is not solved, we do the following: we embed all the questions using OpenAI's text-similarity-babbage-001 embedding engine, which embeds the questions onto a 2,048-dimensional space. Then, we calculate the most similar solved questions to the unsolved question from within its course using the cosine similarity of the embeddings. We take the most similar question and its corresponding code and use these as few-shot examples for the new question. If the generated code does not output the correct answer, we add another solved questioncode pair, using the next similar solved question each time. We found that using up to five examples for few-shot learning works well in practice, increasing the total number of questions automat-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">ID</th>
<th style="text-align: center;">Course</th>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">18.01 <br> Single Variable Calculus</td>
<td style="text-align: center;">A bacteria population is 4000 at time $t=0$ and its rate of growth is $1000 * 2^{t}$ bacteria per hour after $t$ hours. What is the population after one hour?</td>
<td style="text-align: center;">$4000+\frac{1000}{\log (2)}$</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">18.02 <br> Multi-variable Calculus</td>
<td style="text-align: center;">Describe the graph of the function $f$ : $f(x, y)=10-\sqrt{x^{2}+y^{2}}$</td>
<td style="text-align: center;"><img alt="img-2.jpeg" src="img-2.jpeg" /></td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">18.03 <br> Differential Equations</td>
<td style="text-align: center;">Find general solutions of the differential equations. If an initial condition is given, find the corresponding particular solution. Throughout, primes denote derivatives with respect to $x . y^{\prime}+y=2, y(0)=0$</td>
<td style="text-align: center;">$y(x)=2\left(1-e^{-x}\right)$</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">18.05 <br> Introduction to Probability and Statistics</td>
<td style="text-align: center;">Calculate the probability of getting a three-of-a-kind poker hand.</td>
<td style="text-align: center;">0.021128</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">18.06 <br> Linear Algebra</td>
<td style="text-align: center;">Find a combination $x_{1} w_{1}+x_{2} w_{2}+x_{3} w_{3}$ that gives the zero vector with $x_{1}=1$. $w_{1}$ is the vector $(1 ; 2 ; 3) . w_{2}$ is the vector $(4 ; 5 ; 6) . w_{3}$ is the vector $(7 ; 8 ; 9)$.</td>
<td style="text-align: center;">$x_{1}=1, x_{2}=-2, x_{3}=1$</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">6.042 <br> Mathematics for <br> Computer Science</td>
<td style="text-align: center;">Find a number $x \in{0,1, \ldots, 112}$ such that $11 x \equiv 1(\bmod 113)$.</td>
<td style="text-align: center;">72</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">COMS3251 <br> Computational <br> Linear Algebra</td>
<td style="text-align: center;">Given a d-dimensional non-zero vector $v$, compute the rank of the matrix $v v^{\prime}$</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">MATH <br> Prealgebra</td>
<td style="text-align: center;">What is the greatest common factor of 84,112 and 210 ?</td>
<td style="text-align: center;">14</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">MATH <br> Algebra</td>
<td style="text-align: center;">Let $N, O$ be functions such that $N(x)=2 \sqrt{x}$, and $O(x)=x^{2}$. What is $N(O(N(O(N(O(3))))))$ ?</td>
<td style="text-align: center;">24</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">MATH <br> Number Theory</td>
<td style="text-align: center;">How many four-digit numbers whose digits add up to 9 are divisible by 11 ?</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">MATH <br> Counting and Probability</td>
<td style="text-align: center;">A standard six-sided fair die is rolled four times. The probability that the product of all four numbers rolled is a perfect square is $\frac{m}{n}$, where $m$ and $n$ are relatively prime positive integers. Find $m+n$.</td>
<td style="text-align: center;">187</td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">MATH <br> Intermediate Algebra</td>
<td style="text-align: center;">Given that $x^{2}+y^{2}=14 x+6 y+6$, find the largest possible value of $3 x+4 y$.</td>
<td style="text-align: center;">73</td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">MATH <br> Precalculus</td>
<td style="text-align: center;">If the six solutions of $x^{6}=-64$ are written in the form $a+b i$, where $a$ and $b$ are real, find the product of those solutions with $a&gt;0$.</td>
<td style="text-align: center;">4</td>
</tr>
</tbody>
</table>
<p>Table 1. Example questions and solutions from six MIT courses (18.01, 18.02, 18.03, 18.05, 18.06, 6.042), one Columbia University course (COMS3251), and six topics from the MATH dataset. The solutions can contain numerical answers, equations, plots, or other modalities.
ically solved from $71 \%$ using zero-shot learning to $81 \%$ using few-shot learning. Figure 3 (18.02) demonstrates few-shot learning.</p>
<p>Simulation. Figure 3 (18.05) shows an example from Probability and Statistics where the question is turned into a probabilistic programming task that generates simulations in order to compute an empirical statistic.</p>
<h2>Manual Prompt Modification.</h2>
<p>Question Tidying. While $81 \%$ of the question is automatically solved by zero-shot and few-shot learning, $19 \%$ of the questions may require manual editing to be solved by Codex. These questions may be vague or contain redundant information (e.g., reference movie characters or current events) and require tidying to extract the essence of the question. Question tidying primarily involves removing redundant information, breaking down long sentence structures into smaller components, and converting prompts into a programming format.</p>
<p>Interaction for Visualization. Another form of manual prompting occurs when an answer involves a plot and requires multiple
steps to generate a visually pleasing and clear plot. These special cases, which are among the remaining $19 \%$ of the questions, require interactively prompting Codex until reaching the desired visualizations.</p>
<p>Automatic Explanation. Explanations are generated automatically using the question, the code generated by Codex when prompted with the question, and a prompt consisting of three quotes followed by the text "Here is what the above code is doing: 1.". This prompt is given after both the question and the generated code since the code may be a lossy representation of the question. The result is a step-by-step explanation of the solution code given to Codex.</p>
<p>Question Generation and their Human Evaluation. We also use Codex to generate new questions for each course. This is done by creating a numbered list of human-written questions from each class. This list is cut off after a random number of questions, and the result is used to prompt Codex to generate the next question. This process is repeated to create many new questions for each course.</p>
<p>To evaluate the generated questions, we survey MIT stu-</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 3. Example pipelines automatically solve questions from MIT mathematics courses and explain the solutions. 18.01 Single Variable Calculus Zero-Shot example: Given a question and the automatically generated prefix "using SymPy," Codex is prompted and outputs a program. Running the program results in equations that are the correct answer. The program is then fed to Codex again with an automatic prompt, resulting in a generated code explanation. 18.02 Multivariable Calculus Few-Shot example: Given a question, the prefix "write a program using SymPy" is automatically generated. The question is embedded with the other zero-shot questions in the course. The nearest zero-shot question and its corresponding code are used as a few-shot example. The few-shot example pair and the input question are fed into Codex, which generates a program that solves the question. The question, program, and prompt for explanation are fed into Codex to generate the explanation. 18.03 Differential Equations Zero-Shot example: In this example, the answer is both a vector and a plot. 18.05 Introduction to Probability and Statistics Zero-Shot example: Given the question, a probabilistic program is generated by adding "using simulation" to the prompt. 18.06 Linear Algebra Zero-Shot example: The output answer is the correct vector.
dents who have taken these courses or their equivalents to compare the quality and difficulty of machine-generated questions with human-written questions for each of the courses. ${ }^{1}$</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>We randomly sampled five original, human-written questions and five generated questions for each of the six MIT courses. Students are asked to read these ten questions per course in the survey, mixed and presented randomly.
tion. For each question, you will be asked (a) whether you think the question is human-written or machine-generated, (b) whether the question is appropriate for the given course, and finally, (c) how you would rate the difficulty of the question. Please carefully read each question and answer to the best of your ability".</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 4. Comparison of the automatic solve rates on (A) MIT math courses and a Columbia University course and on (B) MATH benchmark dataset. The latest OpenAI GPT-3 (text-davinci-002), a transformer pre-trained on text, achieves on the MIT math courses (A) of $18.8 \%$ with zero-shot, and $30.8 \%$ with few-shot and CoT, and on the MATH benchmark (B) $25.5 \%$ with zero-shot, and $42.2 \%$ with few-shot and CoT. In contrast, program synthesis with using the latest OpenAI Codex (code-davinci-002), a transformer pre-trained on text and fine-tuned on code, achieve automatic solve rates on the MIT math courses (A) $71.1 \%$ with zero-shot learning and $81.1 \%$ with few-shot learning, and on the MATH benchmark (B) $72.2 \%$ with zero-shot learning and $81.1 \%$ with few-shot learning.</p>
<p>For each of the 60 questions, the students are asked three survey questions: (1) "Is the question human-written or machine-generated?", (2) "Is the question appropriate or not appropriate for the specific course?", and (3) "What is the question's difficulty level on a scale between 1 (easiest) and 5 (hardest)?" An example of this survey format is given in Figure 6. The students are asked to provide their ratings and not solve the questions. The survey is conducted online and anonymously.</p>
<h2>Results</h2>
<p>Questions Solved. We solve 265 questions, 213 of them automatically, as described in the Supplementary Information. These 265 questions include 25 randomly sampled questions from each of the seven courses (18.01/18.02/18.03/18.05/18.06/6.042/COMS3251) and 15 randomly sampled questions for each of the six topics in the MATH dataset (Prealgebra/Algebra/Number Theory/Count-
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 5. Imported Python programming libraries by course: NumPy is used by nearly all courses. Matplotlib is used in courses with questions that involve plotting. Sympy is used by most of the courses, and SciPy by half of the courses.
ing and Probability/Intermediate Algebra/Precalculus). The breakdown of automatic solve rate by zero-shot and few-shot learning using Codex as compared with GPT-3 and GPT-3 with CoT is shown in Figure 4. Programs involve step-by-step commands; therefore, CoT is inherent in programs.</p>
<p>Visualization of Embedded Questions. We embed the 175 mathematics course questions onto a 2,048-dimensional space using OpenAI's text-similarity-babbage-001 embedding engine, which captures semantic similarity between texts. We then use uniform manifold approximation and projection (UMAP) (18) to reduce the dimensionality of the 175 question embeddings to two. Figure 8, the plot of these two dimensions, shows that the embedded questions are clustered by course topics. We see clusters of questions representing linear algebra from MIT's 18.06 Linear Algebra and Columbia's COMS3251 Computational Linear Algebra on the top right. On the left side, we see a collection of the questions representing calculus from MIT's 18.01 Single Variable Calculus, 18.02 Multivariable Calculus, and 18.03 Differential Equations. On the bottom right, we see a cluster of the questions from MIT's 18.05 Introduction to Probability and Statistics and 6.042 Mathematics for Computer Science, covering probability and statistics.</p>
<p>Automatically Generating New Questions. We generate new questions for each course and topic by prompting Codex with numbered human-written questions to generate the next question automatically. Specifically, we create prompts of 25 randomly selected problems for which Codex generates correct answers, remove the questions after a randomly chosen question in the list, and have Codex complete the next new question. We present 130 new questions generated by Codex in the Supplementary Information to demonstrate this capability. These include ten new questions for each of the seven courses and each of the six MATH topics. Table 2 shows one generated question for each class and MATH topic. Generating a question takes less than a second. We can generate an arbi-</p>
<p>Course Name:
Introduction to Probability and Statistics
For each of the following questions, indicate whether you think the question is Human Written or Machine Generated, whether it is appropriate for the above course, as well as its level of difficulty between 1-5 where 1 is easiest, 2 is below average, 3 is average for the course, 4 is above average, and 5 in hardest.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Human vs. Machine <br> Generated</th>
<th style="text-align: center;">Appropriate for Course</th>
<th style="text-align: center;">Difficulty <br> (1=Easiest;5=Hardest)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Human <br> Written</td>
<td style="text-align: center;">Machine <br> Generated</td>
<td style="text-align: center;">Appropriate Not <br> Appropriate</td>
</tr>
<tr>
<td style="text-align: left;">Boxes of Raisin Bran cereal are 30cm tall. Due to settling, boxes have a <br> higher density of raisins at the bottom ( $h=6$ than at the top $(h=30$ ). <br> Suppose the density (in raisins per cm of height) is given by $\mathrm{f}(\mathrm{h})=40-$ <br> h. What is the probability that a random raisin is in the top third of the <br> box?</td>
<td style="text-align: center;">00</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">00000</td>
</tr>
</tbody>
</table>
<p>Fig. 6. Student survey example question: For each of 60 questions, students are asked if (1) the question is human-written or machine-generated, (2) the question is appropriate or inappropriate for the course, and (3) to rate the difficulty level of each question on a scale between 1 (easiest) and 5 (hardest).
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 7. Student survey results: Panel A compares the level of difficulty of human-written questions and questions generated by our approach for each course based on the student ratings. The plot shows the means of the difficulty ratings between 1 (easiest) and 5 (hardest) and their $95 \%$ confidence intervals. Panel B shows the percentage of human-written and machine-generated questions rated as appropriate and not appropriate for the course. Panel C shows the percentage of human-written questions rated as human-written or machine-generated (left) and the percentage of machine-generated questions rated as human-written or machine-generated (right).
trarily large number of questions, demonstrating that this is a practical and effective method for creating new course content.</p>
<p>Student Survey Results. Fifteen participants completed our survey, answering questions about all 60 questions, taking a median of 40 minutes. Figure 7 summarizes the results of the student survey comparing human-written and machinegenerated questions. Panel A compares the difficulty level of human-written questions and the machine-generated questions for each course based on the student ratings. The plot shows the means of the difficulty ratings between 1 (easiest) and 5 (hardest) and their $95 \%$ confidence intervals. Panel B shows the percentage of human-written and machine-generated questions rated by students as appropriate or not appropriate for the courses. Panel C shows the percentage of human-written questions rated as human-written or machine-generated (left) and the percentage of machine-generated questions rated as human-written or machine-generated (right).</p>
<p>Summarizing the student survey results:</p>
<ul>
<li>Survey participants rated our machine-generated and human-written questions to be similar in difficulty within confidence intervals.</li>
<li>Survey participants rated human-written questions slightly more appropriate for the courses than machinegenerated ones.</li>
<li>Survey participants rated human-written questions more likely to be human-written as shown on the left side of Panel C. Survey participants rated machine-generated questions equally likely to be machine-generated and human-written as shown on the right side of Panel C.</li>
</ul>
<p>Human Level. With our methodology, Codex reaches human performance levels in both the contexts of solving existing questions and generating new content. We achieve $81 \%$ automatic accuracy in solving mathematics course problems at MIT and</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Fig. 8. Visualization of embeddings of course questions: We embed the course questions into a 2,048-dimensional space using OpenAI's text-similarity-babbage-001 embedding engine, which captures semantic similarity between texts. We then use uniform manifold approximation and projection to reduce the dimensionality to two. This shows distinctive clusters based on topics. We see clusters of questions from MIT's 18.06 Linear Algebra and Columbia's COMS3251 Computational Linear Algebra on the top right. On the left side, we see a cluster of the questions from MIT's 18.01, 18.02, and 18.03. On the bottom right, we see a cluster of the questions from MIT's 18.05 Introduction to Probability and Statistics and 6.042 Mathematics for Computer Science, covering probability and statistics.</p>
<p>Columbia, comparable to typical student performance on these problem sets in our MIT and Columbia University courses. Furthermore, we automatically generate new questions that are indistinguishable to students from human-written course questions.</p>
<p>Implementation Details. We make our data and code publicly available. ${ }^{5}$ We use the latest version of OpenAI's GPT-3 text-davinci-002 and Codex codex-davinci-002 engines for all of our experiments. We fix all Codex's hyperparameters to be the same for all solution and explanation experiments to yield deterministic and reproducible results. Specifically, top P, which controls diversity, is set to 0 and sampling temperature, which controls randomness, is also set to 0 . The frequency and presence penalties are set to 0 , and we do not halt on any stop sequences. We allow diversity and randomness for all new question generation experiments by setting the top P and temperature to 0.1 . Each prompt is structured as a Python documentation comment surrounded by triple quotations and line breaks. We evaluate the solution by running the generated program using a Python interpreter. Evaluations are considered correct if the printed output or the value returned by the generated program is the correct solution.</p>
<p>Few-shot learning prompts are structured as follows: for each question-code examples being used, we insert the question in a docstring on the following available line, have a line break, and then insert the code on the following lines. After all the examples, we insert the target question at the end in the same way as described above and prompt Codex.</p>
<p>Chain of thought (CoT) prompts for GPT-3 are implemented by adding the text "Let's think step by step." (7) after</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>the few-shot questions and answers, and the new question.
Types of Problems the Model Cannot Solve. There are a few different types of problems the model is incapable of solving: (1) any problem for which the question is in the form of an image or other non-text modality; (2) questions with solutions that require proofs; and (3) problems that are computationally intractable, such as factoring very large primes. This last category is not expected in any math course assignment, as students themselves would also be unable to answer them. That being said, many questions that students can answer have generalizations that are computationally intractable.</p>
<h2>Conclusion</h2>
<p>We demonstrate that few-shot learning and program synthesis using OpenAI Codex is able to solve, explain, and generate university-level mathematics problems at a human level. In contrast, previous methods using transformers only pre-trained on text, such as GPT-3, fail on these tasks. We verify that our strong results are not overfitting the training data by solving a new course that is not available online. We also generate and analyze new problem sets. The success of this work confirms that programs serve as a good representation and computation environment for solving math problems. Since our approach requires no additional training, it is easily scalable. This work addresses significant pedagogical challenges, bringing substantial benefits to higher education like curriculum design and analysis tools and automatic content generation.</p>
<p>We show that neural network synthesis with modern programming languages is more dynamic and widely applicable than expression trees and likely solves a broader range of problems. Although any finite computation could be expressed as</p>
<table>
<thead>
<tr>
<th style="text-align: center;">ID</th>
<th style="text-align: center;">Course</th>
<th style="text-align: center;">Machine-generated question</th>
<th style="text-align: center;">Most similar human-written question</th>
<th style="text-align: center;">Similarity</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">18.01 <br> Single-Variable Calculus</td>
<td style="text-align: center;">Find the area of the region bounded by the curve and the $x$-axis. $y=x^{2} \sin (x), 0 \leq x \leq \pi$</td>
<td style="text-align: center;">Find the area of the region under the given curve from 1 to $2 . y=\left(x^{2}+1\right) /\left(3 x-x^{2}\right)$</td>
<td style="text-align: center;">0.61</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">18.02 <br> Multi-Variable Calculus</td>
<td style="text-align: center;">Find $a \times b . a=(9,-2,1), b=(-2,1,1)$</td>
<td style="text-align: center;">Find $a \times b . a=(5,-1,-2), b=(-3,2,4)$</td>
<td style="text-align: center;">0.87</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">18.03 <br> Differential Equations</td>
<td style="text-align: center;">Use the method of separable variables to solve the initial-value problem $\frac{d x}{d y}=5 e^{x} \cdot y(2)=$ 12 when $x=2$</td>
<td style="text-align: center;">Separate variables and use partial fractions to solve the initial value problems. Use either the exact solution or a computer-generated slope field to sketch the graphs of several solutions of the given differential equation, and highlight the indicated particular solution. $f^{\prime}(x)=3 f(x)(5-f(x)), f(0)=8$</td>
<td style="text-align: center;">0.21</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">18.05 <br> Introduction to Probability and Statistics</td>
<td style="text-align: center;">Let $X$ be a uniformly distributed random variable over the interval $[0,1)$. Find $\mathbb{E}\left[X^{2}\right]$</td>
<td style="text-align: center;">Let $X$ be the result of rolling a fair 4 -sided die. Let $Y$ be the result of rolling a fair 6 -sided die. You win $2 X$ dollars if $X&gt;Y$ and lose 1 dollar otherwise. After playing this game 60 times, what is your expected total gain?</td>
<td style="text-align: center;">0.29</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">18.06 <br> Linear Algebra</td>
<td style="text-align: center;">Write a Matlab code to determine if the given matrix $A=[1,1 ; 4,4]$ is positive semidefinite and if it is negative semidefinite.</td>
<td style="text-align: center;">Find $A^{\prime} A$ if the columns of $A$ are unit vectors, all mutually perpendicular.</td>
<td style="text-align: center;">0.21</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">6.042 <br> Mathematics for <br> Computer Science</td>
<td style="text-align: center;">A student is taking a test consisting of $n$ multiplechoice questions. Each question has five possible answers, and only one is correct. The student knows that the probability that any particular question is answered correctly is $\frac{1}{5}$. Let $X$ be the number of questions answered correctly by the student. What is $\mathbb{E}(X)$ ?</td>
<td style="text-align: center;">MIT students sometimes delay laundry for a few days. Assume all random values described below are mutually independent. A busy student must complete three problem sets before doing laundry. Each problem set requires 1 day with probability $\frac{2}{3}$ and 2 days with probability $\frac{1}{3}$. Let $B$ be the number of days a busy student delays laundry. What is $\mathbb{E}(B)$ ?</td>
<td style="text-align: center;">0.47</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">COMS3251 <br> Computational</td>
<td style="text-align: center;">Find a combination of the vectors $\left[\begin{array}{lll}1 &amp; 2 &amp; 3 \ 4 &amp; 5 &amp; 6 \ 7 &amp; 8 &amp; 9\end{array}\right]$ that gives the vector $\left[\begin{array}{lll}1 &amp; 2 &amp; 3 \ 4 &amp; 5 &amp; 6 \ 7 &amp; 8 &amp; 9\end{array}\right]$ that give the zero vector.</td>
<td style="text-align: center;">Find a combination of the vectors $\left[\begin{array}{lll}1 &amp; 2 &amp; 3 \ 4 &amp; 5 &amp; 6 \ 7 &amp; 8 &amp; 9\end{array}\right]$ that give the zero vector.</td>
<td style="text-align: center;">0.90</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">MATH <br> Pre-Algebra</td>
<td style="text-align: center;">How many four-digit positive integers are there with hundreds digit 2 ?</td>
<td style="text-align: center;">How many four-digit positive integers are there with thousands digit 2 ?</td>
<td style="text-align: center;">0.90</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">MATH <br> Algebra</td>
<td style="text-align: center;">Find the distance between the points $(0,0)$ and $(3,4)$.</td>
<td style="text-align: center;">Find the distance between the points $(0,4)$ and $(3,0)$.</td>
<td style="text-align: center;">0.99</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">MATH <br> Number Theory</td>
<td style="text-align: center;">Find the smallest positive integer $n$ such that $n^{2}$ is divisible by $2^{10}$ and $n^{3}$ is divisible by $3^{10}$.</td>
<td style="text-align: center;">How many four-digit numbers whose digits add up to 9 are divisible by 11 ?</td>
<td style="text-align: center;">0.25</td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">MATH <br> Counting and Probability</td>
<td style="text-align: center;">How many ways are there to divide a set of 10 objects into two sets of equal size?</td>
<td style="text-align: center;">Compute $\binom{8}{4}$.</td>
<td style="text-align: center;">0.12</td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">MATH <br> Intermediate Algebra</td>
<td style="text-align: center;">Let $x$ and $y$ be positive real numbers such that $x^{2}+$ $y^{2}=1$. Find the maximum value of $x y$.</td>
<td style="text-align: center;">Given that $x^{2}+y^{2}=14 x+6 y+6$, find the largest possible value of $3 x+4 y$.</td>
<td style="text-align: center;">0.59</td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">MATH <br> Precalculus</td>
<td style="text-align: center;">Let $A$ be the matrix $\left[\begin{array}{lll}1 &amp; 2 &amp; 3 \ 4 &amp; 5 &amp; 6 \ 7 &amp; 8 &amp; 9\end{array}\right]$ <br> minant of $A^{2}+A^{3}$.</td>
<td style="text-align: center;">If $\operatorname{det}(A)=2$ and $\operatorname{det}(B)=12$, then find $\operatorname{det}(A B)$.</td>
<td style="text-align: center;">0.41</td>
</tr>
</tbody>
</table>
<p>Table 2. Examples of new questions generated automatically by Codex for each course and the most similar question from its course.
a sufficiently large expression tree, one may see an arbitrarily large expansion in the size of the expression tree needed, as opposed to a Turing-complete language. This flexibility is bolstered by the massive corpus of existing programs, which eclipses the number of labeled expression trees available. Program outputs are also inherently more human-readable, as the ability to use abstraction, modularity, and high-level logic leads to more explicit illustrations of the path to a solution. Furthermore, program synthesis can convey logical deductions directly through explanatory comments and function and variable names. In particular, we see such descriptive text and derivations in a number of the Codex outputs. The unification of such formal and informal language is an inherent advantage of our methodology. We emphasize that the results may be
complex and multi-modal. For example, by using packages such as Matplotlib, we can produce graphs of equations. This advanced and unique ability is time-consuming for humans and offers a significant pedagogical benefit.</p>
<p>In summary, we automatically solve, explain, and generate university-level mathematics course questions in real-time at a human level. Students rated machine-generated questions as equally likely to have been human-written as machinegenerated. Students also rated machine-generated questions as similarly difficult to human-written questions and most appropriate for their respective courses. Finally, we have succeeded in scaling up this work to over thirty STEM courses across 13 departments in science and engineering schools at MIT and Ivy League universities, with excellent results.</p>
<ol>
<li>CQ Choi, 7 revealing ways AIs fail: Neural networks can be disastrously brittle, forgetful, and surprisingly bad at math. IEEE Spectr. 58, 42-47 (2021).</li>
<li>A Vaswani, et al., Attention is all-you need in Proceedings of Advances in Neural Information Processing Systems. Vol. 30, (2017).</li>
<li>TB Brown, et al., Language models are few-shot learners in Proceedings of Advances in Neural Information Processing Systems. Vol. 33, pp. 1877-1901 (2020).</li>
<li>D Hendrycks, et al., Measuring massive multitask language understanding in Proceedings of the International Conference on Learning Representations. (2021).</li>
<li>D Hendrycks, et al., Measuring mathematical problem solving with the MATH dataset in Proceedings of Advances in Neural Information Processing Systems: Datasets and Benchmarks. (2021).</li>
<li>JW Rae, et al., Scaling language models: Methods, analysis \&amp; insights from training Gopher. arXiv preprint arXiv:2112.11446 (2021).</li>
<li>T Kojima, S Shane Gu, M Reid, Y Matsuo, Y Iwasawa, Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916 (2022).</li>
<li>M Chen, , et al., Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021).</li>
<li>J Shen, et al., Generate \&amp; rank: A multi-task framework for math word problems in Proceedings of the Conference on Empirical Methods in Natural Language Processing. pp. 22692279 (2021).</li>
<li>K Cobbe, et al., Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 (2021).</li>
<li>Z Xie, S Sun, A goal-driven tree-structured neural model for math word problems in Proceedings of the International Joint Conference on Artificial Intelligence. pp. 5299-5305 (2019).</li>
<li>Q Wu, Q Zhang, J Fu, XJ Huang, A knowledge-aware sequence-to-tree network for math word problem solving in Proceedings of the Conference on Empirical Methods in Natural Language Processing. pp. 7137-7146 (2020).</li>
<li>J Qin, L Lin, X Liang, R Zhang, L Lin, Semantically-aligned universal tree-structured solver for math word problems in Proceedings of the Conference on Empirical Methods in Natural Language Processing. pp. 3780-3789 (2020).</li>
<li>J Zhang, et al., Graph-to-tree learning for solving math word problems in Proceedings of the Annual Meeting of the Association for Computational Linguistics. pp. 3928-3937 (2020).</li>
<li>S Li, et al., Graph-to-tree neural networks for learning structured input-output translation with applications to semantic parsing and math word problem in Proceedings of the Conference on Empirical Methods in Natural Language Processing. pp. 2841-2852 (2020).</li>
<li>Z Liang, J Zhang, J Shao, X Zhang, MWP-BERT: A strong baseline for math word problems. arXiv preprint arXiv:2107.13435 (2021).</li>
<li>S Tran, et al., Solving machine learning problems in Proceedings of the Asian Conference on Machine Learning. pp. 470-485 (2021).</li>
<li>L McInnes, J Healy, J Melville, Umap: Uniform manifold approximation and projection for dimension reduction. J. Open Source Softw. 3, 861 (2018).</li>
</ol>
<h1>Contents</h1>
<p>A Solutions for MIT 18.01: Single Variable Calculus ..... 11
B Solutions for MIT 18.02: Multivariable Calculus ..... 29
C Solutions for MIT 18.03: Differential Equations ..... 44
D Solutions for MIT 18.05: Introduction to Probability and Statistics ..... 62
E Solutions for MIT 18.06: Introduction to Linear Algebra ..... 79
F Solutions for MIT 6.042: Mathematics for Computer Science ..... 95
G Solutions for Columbia University COMS3251: Computational Linear Algebra ..... 114
H Solutions for MATH: Prealgebra ..... 127
I Solutions for MATH: Algebra ..... 135
J Solutions for MATH: Number Theory ..... 141
K Solutions for MATH: Counting and Probability ..... 149
L Solutions for MATH: Intermediate Algebra ..... 157
M Solutions for MATH: Precalculus ..... 166
N Generation of MIT 18.01: Single-Variable Calculus ..... 174
O Generation of MIT 18.02: Multi-Variable Calculus ..... 174
P Generation of MIT 18.03: Differential Equations ..... 174
Q Generation of MIT 18.05: Introduction to Probability and Statistics ..... 175
R Generation of MIT 18.06: Introduction to Linear Algebra ..... 176
S Generation of MIT 6.042: Mathematics for Computer Science ..... 176
T Generation of Columbia University COMS3251: Computational Linear Algebra ..... 177
U Generation of MATH: Algebra ..... 178
V Generation of MATH: Counting and Probability ..... 178
W Generation of MATH: Intermediate Algebra ..... 179
X Generation of MATH: Number Theory ..... 179
Y Generation of MATH: Prealgebra ..... 180
Z Generation of MATH: Precalculus ..... 180
A. Solutions for MIT 18.01: Single Variable Calculus
Prereq None
Units $\quad 5-0-7$
Syllabus Differentiation and integration of functions of one variable, with applications. Informal treatment of limits and continuity. Differentiation: definition, rules, application to graphing, rates, approximations, and extremum problems. Indefinite integration; separable first-order differential equations. Definite integral; fundamental theorem of calculus. Applications of integration to geometry and science. Elementary functions. Techniques of integration. Polar coordinates. L'Hopital's rule. Improper integrals. Infinite series: geometric, p-harmonic, simple comparison tests, power series for some elementary functions.</p>
<p>Table 3. MIT 18.01, Question 1: Original question, Codex input and code and code explanation, Program solution, Solution type, GPT-3 response and evaluation</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Topic</th>
<th style="text-align: center;">Functions and Models</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Original question</td>
<td style="text-align: center;">Sketch the graph of the function. $f(x)=x+</td>
</tr>
<tr>
<td style="text-align: center;">Codex input</td>
<td style="text-align: center;">Same as original question</td>
</tr>
<tr>
<td style="text-align: center;">Codex code</td>
<td style="text-align: center;"><code>import numpy as np import matplotlib.pyplot as plt x = np.linspace(-10, 10, 100) y = x + np.abs(x) plt.plot(x, y) plt.xlabel('x') plt.ylabel('y') plt.grid() plt.show()</code></td>
</tr>
<tr>
<td style="text-align: center;">Program solution</td>
<td style="text-align: center;"><img alt="img-8.jpeg" src="img-8.jpeg" /></td>
</tr>
<tr>
<td style="text-align: center;">Solution type</td>
<td style="text-align: center;">Automatic Zero-Shot</td>
</tr>
<tr>
<td style="text-align: center;">Codex code explanation</td>
<td style="text-align: center;">1. We import the numpy and matplotlib.pyplot libraries. <br> 2. We create an array of 100 evenly spaced numbers from -10 to 10 . <br> 3. We create an array of the same size as $x$, where each element is the absolute value of the corresponding element in $x$. <br> 4. We plot the graph of $y$ against $x$. <br> 5. We label the $x$-axis and $y$-axis. <br> 6. We display the grid. <br> 7. We display the graph.</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 response</td>
<td style="text-align: center;">The graph of the function is a line with a slope of 1 and a $y$-intercept of 0 .</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 evaluation</td>
<td style="text-align: center;">Incorrect</td>
</tr>
</tbody>
</table>
<p>Table 4. MIT 18.01, Question 2: Original question, Codex input and code and code explanation, Program solution, Solution type, GPT-3 response and evaluation</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Topic</th>
<th style="text-align: left;">Functions and Models</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Original question</td>
<td style="text-align: left;">Find $f \circ g \circ h . f(x)=3 x-2, g(x)=\sin (x), h(x)=x^{2}$</td>
</tr>
<tr>
<td style="text-align: left;">Codex input</td>
<td style="text-align: left;">Same as original question</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Codex code</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">import math <br> def $f(x)$ : <br> return $3 * x-2$ <br> def $g(x)$ : <br> return math. $\sin (x)$ <br> def $h(x)$ : <br> return $x * * 2$ <br> def $f g h(x)$ : <br> return $f(g(h(x)))$ <br> print(fgh(2))</td>
</tr>
<tr>
<td style="text-align: center;">Program solution</td>
<td style="text-align: center;">$3 \sin \left(x^{2}\right)-2$</td>
</tr>
<tr>
<td style="text-align: center;">Solution type</td>
<td style="text-align: center;">Automatic Zero-Shot</td>
</tr>
<tr>
<td style="text-align: center;">Codex code explanation</td>
<td style="text-align: center;">1. fgh(2) <br> 2. $h(2)$ <br> 3. $g(4)$ <br> 4. $f(\sin (4))$ <br> 5. $3 \sin 4-2$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 response</td>
<td style="text-align: center;">$\mathrm{f}=\mathrm{g}=\mathrm{h}=\mathrm{f}(\mathrm{g}(\mathrm{h}(\mathrm{x})))=\mathrm{f}\left(\mathrm{g}\left(\mathrm{x}^{<em> </em>} 2\right)\right)=\mathrm{f}\left(\sin \left(\mathrm{x}^{<em> </em>} 2\right)\right)=3^{<em>} \sin \left(\mathrm{x}^{</em> *} 2\right)-2$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 evaluation</td>
<td style="text-align: center;">Correct</td>
</tr>
</tbody>
</table>
<p>Table 5. MIT 18.01, Question 3: Original question, Codex input and code and code explanation, Program solution, Solution type, GPT-3 response and evaluation</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Topic</th>
<th style="text-align: left;">Functions and Models</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Original question</td>
<td style="text-align: left;">Solve each equation for $x . \ln \left(x^{2}-1\right)=3$</td>
</tr>
<tr>
<td style="text-align: left;">Codex input</td>
<td style="text-align: left;">Same as original question</td>
</tr>
<tr>
<td style="text-align: left;">Codex code</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">import sympy</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$\mathrm{x}=$ sympy. Symbol $\left({ }^{\prime} \mathrm{x}^{\prime}\right)$ <br> eq = sympy.log $(\mathrm{x} * * 2-1)-3$ <br> print(sympy.solve(eq, x))</td>
</tr>
<tr>
<td style="text-align: left;">Program solution</td>
<td style="text-align: left;">$-\sqrt{1+e^{3}} \cdot \sqrt{1+e^{3}}$</td>
</tr>
<tr>
<td style="text-align: left;">Solution type</td>
<td style="text-align: left;">Automatic Zero-Shot</td>
</tr>
<tr>
<td style="text-align: left;">Codex code explanation</td>
<td style="text-align: left;">1. We import the sympy module. <br> 2. We create a symbol $x$. <br> 3. We create an equation. <br> 4. We solve the equation for $x$.</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3 response</td>
<td style="text-align: left;">$x^{<em> </em>} 2-1=e^{<em> </em>} 3$ <br> $x^{<em> </em>} 2=e^{<em> </em>} 3+1$ <br> $x=\operatorname{sqrt}\left(e^{<em> </em>} 3+1\right)$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3 evaluation</td>
<td style="text-align: left;">Correct</td>
</tr>
</tbody>
</table>
<p>Table 6. MIT 18.01, Question 4: Original question, Codex input and code and code explanation, Program solution, Solution type, GPT-3 response and evaluation</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Topic</th>
<th style="text-align: left;">Limits and Derivatives</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Original question</td>
<td style="text-align: left;">Determine the infinite limit. $\lim _{x \rightarrow 5^{+}}(x+1) /(x-5)$</td>
</tr>
<tr>
<td style="text-align: left;">Codex input</td>
<td style="text-align: left;">Using sympy determine the infinite limit. lim_x_5+ $(x+1) /(x-5)$</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: left;">Codex code</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">from sympy import limit, Symbol, S</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$\mathrm{x}=$ Symbol('x')</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">limit $((x+1) /(x-5), x, 5)$</td>
</tr>
<tr>
<td style="text-align: left;">Program solution</td>
<td style="text-align: left;">$\infty$</td>
</tr>
<tr>
<td style="text-align: left;">Solution type</td>
<td style="text-align: left;">Automatic Zero-Shot</td>
</tr>
<tr>
<td style="text-align: left;">Codex code explanation</td>
<td style="text-align: left;">1. We import the limit function from the sympy module. <br> 2. We create a Symbol object called $x$. <br> 3. We call the limit function and pass in the expression $(x+1) /(x-5)$ and the variable $x$. <br> 4. We pass in the point 5 as the third argument.</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3 response</td>
<td style="text-align: left;">The limit does not exist because the function is not defined when $x=5$.</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3 evaluation</td>
<td style="text-align: left;">Incorrect</td>
</tr>
</tbody>
</table>
<p>Table 7. MIT 18.01, Question 5: Original question, Codex input and code and code explanation, Program solution, Solution type, GPT-3 response and evaluation</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Topic</th>
<th style="text-align: center;">Limits and Derivatives</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Original question</td>
<td style="text-align: center;">Find the limits as $x \rightarrow \infty$ and as $x \rightarrow-\infty$. Use this information, together with intercepts, to give a rough sketch of the graph as in Example 12. $y=x^{2}\left(x^{2}-1\right)^{2}(x+2)$</td>
</tr>
<tr>
<td style="text-align: center;">Codex input</td>
<td style="text-align: center;">Same as original question</td>
</tr>
<tr>
<td style="text-align: center;">Codex code</td>
<td style="text-align: center;"><code>import sympy as sym from sympy import Symbol from sympy import pprint sigma = Symbol('sigma') x = Symbol('x') mu = Symbol('mu') pprint(2*sym.pi*sigma) pprint(sym.sqrt(2*sym.pi*sigma)) part_1 = 1/(sym.sqrt(2*sym.pi*sigma)) part_2 = sym.exp(-1*((x-mu)**2)/(2*sigma)) my_gauss_function = part_1*part_2 pprint(my_gauss_function) sym.plot(my_gauss_function.subs({mu:10,sigma:30}),(x,0,100),title='gauss distribution') x_values = [] y_values = [] for value in range(-5,5): y = my_gauss_function.subs({mu:10,sigma:30,x:value}).evalf() y_values.append(y) x_values.append(value) print(value,y) import matplotlib.pyplot as plt plt.plot(x_values,y_values) plt.show()</code></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: left;">Program solution</th>
<th style="text-align: left;">$\infty ;-\infty ;$</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">1.25</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">1.00</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">0.75</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">0.50</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">0.25</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">0.00</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$-0.25$</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$-0.50$</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$-0.75$</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Solution type</td>
<td style="text-align: left;">Automatic Zero-Shot</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Codex code explanation</td>
<td style="text-align: left;">1. We create a list of $x$ values from -5 to 5. <br> 2. We create an empty list to store the $y$ values. <br> 3. We loop through the $x$ values. <br> 4. For each $x$ value, we calculate the $y$ value. <br> 5. We append the $y$ value to the list of $y$ values. <br> 6. We plot the $x$ values against the $y$ values.</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">GPT-3 response</td>
<td style="text-align: left;">As $x \rightarrow$ infinity, $y \rightarrow$ infinity. As $x \rightarrow-$ infinity, $y \rightarrow$ infinity. <br> The graph will look like a parabola that opens up.</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">GPT-3 evaluation</td>
<td style="text-align: left;">Incorrect</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Table 8. MIT 18.01, Question 6: Original question, Codex input and code and code explanation, Program solution, Solution type, GPT-3 response and evaluation</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Topic</th>
<th style="text-align: left;">Limits and Derivatives</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Original question</td>
<td style="text-align: left;">Find the derivative of the function using the definition of a derivative. $f(x)=\left(x^{2}-1\right) /(2 x-3)$</td>
</tr>
<tr>
<td style="text-align: left;">Codex input</td>
<td style="text-align: left;">Using Sympy find the derivative of the function using the definition of a derivative. $f(x)=\left(x^{<em> </em>} 2-1\right) /\left(2^{*} x-3\right)$</td>
</tr>
<tr>
<td style="text-align: left;">Codex code</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">import sympy as sp</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$x=\operatorname{sp} . \operatorname{Symbol}\left(' x^{\prime}\right)$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$f=(x * * 2-1) /(2 * x-3)$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">print(sp.diff(f,x))</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$x=\frac{2 x}{2 x-3}-\frac{2\left(x^{*}-1\right)}{2 x-3}$</td>
</tr>
<tr>
<td style="text-align: left;">Program solution</td>
<td style="text-align: left;">Automatic Zero-Shot</td>
</tr>
<tr>
<td style="text-align: left;">Solution type</td>
<td style="text-align: left;">1. We import sympy as sp <br> 2. We create a symbol $x$ <br> 3. We create a function $f$ <br> 4. We print the derivative of $f$ with respect to $x$</td>
</tr>
<tr>
<td style="text-align: left;">Codex code explanation</td>
<td style="text-align: left;">$2^{<em>}(2 * * 2-1) /\left(2^{</em>} x-3\right)^{* * 2}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Correct</td>
</tr>
</tbody>
</table>
<p>Table 9. MIT 18.01, Question 7: Original question, Codex input and code and code explanation, Program solution, Solution type, GPT-3 response and evaluation</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Topic</th>
<th style="text-align: left;">Differentiation Rules</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Original question</td>
<td style="text-align: left;">Find the limit. $\lim _{x \rightarrow 0} \operatorname{csc}(x) \sin (\sin (x))$</td>
</tr>
<tr>
<td style="text-align: left;">Codex input</td>
<td style="text-align: left;">Same as original question</td>
</tr>
<tr>
<td style="text-align: left;">Codex code</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">from sympy import limit, csc, sin, Symbol</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$x=\operatorname{Symbol}\left(' x^{\prime}\right)$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">print(limit(csc(x)*sin(sin(x)), x, 0))</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: left;">Program solution</th>
<th style="text-align: left;">1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Solution type</td>
<td style="text-align: left;">Automatic Zero-Shot</td>
</tr>
<tr>
<td style="text-align: left;">Codex code explanation</td>
<td style="text-align: left;">1. We are finding the limit of $\mathrm{csc}(\mathrm{x})^{*} \sin (\sin (\mathrm{x}))$ as x approaches 0. <br> 2. We are using the limit function from the sympy library. <br> 3. We are passing in the function $\operatorname{csc}(x)^{\star} \sin (\sin (x))$ as the first argument. <br> 4. We are passing in the variable $x$ as the second argument. <br> 5. We are passing in the number 0 as the third argument.</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3 response</td>
<td style="text-align: left;">The limit does not exist.</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3 evaluation</td>
<td style="text-align: left;">Incorrect</td>
</tr>
</tbody>
</table>
<p>Table 10. MIT 18.01, Question 8: Original question, Codex input and code and code explanation, Program solution, Solution type, GPT-3 response and evaluation</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Topic</th>
<th style="text-align: center;">Differentiation Rules</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Original question</td>
<td style="text-align: center;">The half-life of cesium-137 is 30 years. Suppose we have a 100-mg sample. How much of the sample remains after 100 years?</td>
</tr>
<tr>
<td style="text-align: center;">Codex input</td>
<td style="text-align: center;">Write a program. The half-life of cesium-137 is 30 years. Suppose we have a 100-mg sample. How much of the sample remains after 100 years?</td>
</tr>
<tr>
<td style="text-align: center;">Codex code</td>
<td style="text-align: center;">import math <br> def half_life(t, h): <br> return 100 * math.pow(0.5, t/h) <br> print(half_life(100, 30))</td>
</tr>
<tr>
<td style="text-align: center;">Program solution</td>
<td style="text-align: center;">9.92 mg</td>
</tr>
<tr>
<td style="text-align: center;">Solution type</td>
<td style="text-align: center;">Automatic Zero-Shot</td>
</tr>
<tr>
<td style="text-align: center;">Codex code explanation</td>
<td style="text-align: center;">1. We import the math module. <br> 2. We define a function called half_life that takes two arguments: $t$ and $h$. <br> 3. We return the value of $100 * 0.5^{*} /(t / h)$. <br> 4. We print the result of half_life(100, 30).</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 response</td>
<td style="text-align: center;">After 100 years, 50 mg of the sample would remain.</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 evaluation</td>
<td style="text-align: center;">Incorrect</td>
</tr>
</tbody>
</table>
<p>Table 11. MIT 18.01, Question 9: Original question, Codex input and code and code explanation, Program solution, Solution type, GPT-3 response and evaluation</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Topic</th>
<th style="text-align: center;">Differentiation Rules</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Original question</td>
<td style="text-align: center;">The radius of a sphere is increasing at a rate of $4 \mathrm{~mm} / \mathrm{s}$. How fast is the volume increasing when the diameter is 80 mm ?</td>
</tr>
<tr>
<td style="text-align: center;">Codex input</td>
<td style="text-align: center;">Same as original question</td>
</tr>
<tr>
<td style="text-align: center;">Codex code</td>
<td style="text-align: center;">import sympy as sp <br> $r=$ sp.symbols('r') <br> $d=4 * r$ <br> $v=4 / 3 <em>$ sp.pi</em>r**3 <br> $d v _d r=$ sp.diff $(v, r)$ <br> print(dv_dr.subs $(r, 80))$</td>
</tr>
<tr>
<td style="text-align: center;">Program solution</td>
<td style="text-align: center;">25600 r</td>
</tr>
<tr>
<td style="text-align: center;">Solution type</td>
<td style="text-align: center;">Manual</td>
</tr>
<tr>
<td style="text-align: center;">Codex code explanation</td>
<td style="text-align: center;">1. We define the radius as a symbol. <br> 2. We define the diameter as a function of the radius. <br> 3. We define the volume as a function of the radius. <br> 4. We take the derivative of the volume with respect to the radius. <br> 5. We substitute the radius with 80 and print the result.</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 response</td>
<td style="text-align: center;">The volume of a sphere is increasing at a rate of approximately $201.06 \mathrm{~mm} 3 / \mathrm{s}$.</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: left;">GPT-3 evaluation</th>
<th style="text-align: left;">Incorrect</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 12. MIT 18.01, Question 10: Original question, Codex input and code and code explanation, Program solution, Solution type, GPT-3 response and evaluation</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Topic</th>
<th style="text-align: center;">Applications of Differentiation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Original question</td>
<td style="text-align: center;">The water level, measured in feet above mean sea level, of Lake Lanier in Georgia, USA, during 2012 can be modeled by the function $L(t)=0.01441 t^{3}-0.4177 t^{2}+2.703 t+1060.1$ where $t$ is measured in months since January 1, 2012. Estimate when the water level was highest during 2012.</td>
</tr>
<tr>
<td style="text-align: center;">Codex input</td>
<td style="text-align: center;">Same as original question</td>
</tr>
<tr>
<td style="text-align: center;">Codex code</td>
<td style="text-align: center;">import math <br> def $L(t)$ : <br> return $0.01441 * t * * 3-0.4177 * t * * 2+2.703 * t+1060.1$ <br> def $d L(t)$ : <br> return $0.04323 * t * * 2-0.8354 * t+2.703$ <br> def $d d L(t)$ : <br> return $0.08646 * t-0.8354$ <br> def newton(f, df, x0, epsilon): <br> $\mathrm{x}=\mathrm{x} 0$ <br> while True: <br> $\mathrm{x} 1=\mathrm{x}-\mathrm{f}(\mathrm{x}) / \mathrm{df}(\mathrm{x})$ <br> if abs(x1 - x) &lt; epsilon: <br> return x1 <br> $\mathrm{x}=\mathrm{x} 1$ <br> print(newton(dL, ddL, 0, 0.00001))</td>
</tr>
<tr>
<td style="text-align: center;">Program solution</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Solution type</td>
<td style="text-align: center;">Automatic Zero-Shot</td>
</tr>
<tr>
<td style="text-align: center;">Codex code explanation</td>
<td style="text-align: center;">1. We define the function $L(t)$ <br> 2. We define the derivative of $L(t)$ <br> 3. We define the second derivative of $L(t)$ <br> 4. We define a function that uses Newton's method to find the root of a function $f(x)$ <br> 5. We print the result of using Newton's method to find the root of the derivative of $L(t)$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 response</td>
<td style="text-align: center;">The water level was maximum at $t=2$.</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 evaluation</td>
<td style="text-align: center;">Incorrect</td>
</tr>
</tbody>
</table>
<p>Table 13. MIT 18.01, Question 11: Original question, Codex input and code and code explanation, Program solution, Solution type, GPT-3 response and evaluation</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Topic</th>
<th style="text-align: left;">Applications of Differentiation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Original question</td>
<td style="text-align: left;">If a resistor of $R$ ohms is connected across a battery of $E$ volts with internal resistance $r$ ohms, then the <br> power (in watts) in the external resistor is $P=E^{2} R /(R+r)^{2}$. If $E$ and $r$ are fixed but $R$ varies, what is <br> the maximum value of the power?</td>
</tr>
<tr>
<td style="text-align: left;">Codex input</td>
<td style="text-align: left;">Same as original question</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Codex code</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">import sympy as sym <br> from sympy import Symbol <br> from sympy import pprint <br> $R=$ Symbol('R') <br> $E=$ Symbol('E') <br> $r=$ Symbol('r') <br> $P=E * * 2 * R /(R+r) * * 2$ <br> print(P) <br> $d P=P . d i f f(R)$ <br> print(dP) <br> R_sol = sym.solve(dP,R) <br> print(R_sol) <br> R_max $=$ R_sol[0].evalf() <br> print(R_max) <br> P_max $=$ P.subs({R:R_max}) <br> print(P_max)</td>
</tr>
<tr>
<td style="text-align: center;">Program solution</td>
<td style="text-align: center;">$\frac{E^{<em>}}{R^{</em>}}$</td>
</tr>
<tr>
<td style="text-align: center;">Solution type</td>
<td style="text-align: center;">Automatic Few-Shot (5)</td>
</tr>
<tr>
<td style="text-align: center;">Codex code explanation</td>
<td style="text-align: center;">1. Define the variables <br> 2. Define the function <br> 3. Take the derivative of the function <br> 4. Solve the derivative for $R$ <br> 5. Evaluate the solution <br> 6. Substitute the solution into the original function</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 response</td>
<td style="text-align: center;">The maximum value of the power is $P=E^{\prime} 2 R \cdot(R+r)^{\prime} 2$ when $R=r$.</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 evaluation</td>
<td style="text-align: center;">Incorrect</td>
</tr>
</tbody>
</table>
<p>Table 14. MIT 18.01, Question 12: Original question, Codex input and code and code explanation, Program solution, Solution type, GPT-3 response and evaluation</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Topic</th>
<th style="text-align: left;">Applications of Differentiation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Original question</td>
<td style="text-align: left;">Use Newton's method to find all the solutions of the equation correct to eight decimal places. Start by <br> drawing a graph to find initial approximations. $x^{5}-3 x^{4}+x^{3}-x^{2}-x+6=0$</td>
</tr>
<tr>
<td style="text-align: left;">Codex input</td>
<td style="text-align: left;">Use Newton's method to find all the solutions between -100 and 100 with integer initial values. $x^{<em> </em>} 5-$ <br> $3^{<em>} x^{</em> <em>} 4+x^{</em> <em>} 3-x^{</em> *} 2-x+6=0$. Use initial points between -100 and 100. Round to 8 decimal places.</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Codex code</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">import numpy as np <br> import matplotlib.pyplot as plt <br> def $f(x)$ : <br> return $x * * 5-3 * x * * 4+x * * 3-x * * 2-x+6$ <br> def df(x): <br> return $5 * x * * 4-12 * x * * 3+3 * x * * 2-2 * x-1$ <br> def newton(x, f, df, epsilon=1e-8, max_iter=1000): <br> for i in range(max_iter): <br> $\mathrm{x} _\mathrm{new}=\mathrm{x}-\mathrm{f}(\mathrm{x}) / \mathrm{df}(\mathrm{x})$ <br> if abs(x_new - x) &lt; epsilon: <br> return x_new <br> $\mathrm{x}=\mathrm{x} _\mathrm{new}$ <br> return x_new <br> def main(): <br> $\mathrm{x}=$ np.linspace $(-100,100,1000)$ <br> $\mathrm{y}=\mathrm{f}(\mathrm{x})$ <br> plt.plot(x, y) <br> plt.grid() <br> plt.show() <br> x_sols = [] <br> for i in range $(-100,100)$ : <br> x_sols.append(newton(i, f, df)) <br> print(x_sols) <br> print(np.round(x_sols, 8)) <br> if <strong>name</strong> == '<strong>main</strong>' <br> main()</td>
</tr>
<tr>
<td style="text-align: center;">Program solution</td>
<td style="text-align: center;">$-1.044503071109637,2.7055120932482875$</td>
</tr>
<tr>
<td style="text-align: center;">Solution type</td>
<td style="text-align: center;">Manual</td>
</tr>
<tr>
<td style="text-align: center;">Codex code explanation</td>
<td style="text-align: center;">1. Plotting the function $f(x)=x^{<em> </em>} 5-3^{<em>} x^{</em> <em>} 4+x^{</em> <em>} 3-x^{</em> *} 2-x+6$ <br> 2. Finding the derivative of $f(x)$ <br> 3. Using Newton's method to find the roots of $f(x)$ <br> 4. Plotting the roots of $f(x)$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 response</td>
<td style="text-align: center;">There are five solutions. <br> -1.46557123 <br> -0.53728871 <br> 0.36082474 <br> 1.12403607 <br> 2.09132432</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 evaluation</td>
<td style="text-align: center;">Incorrect</td>
</tr>
</tbody>
</table>
<p>Table 15. MIT 18.01, Question 13: Original question, Codex input and code and code explanation, Program solution, Solution type, GPT-3 response and evaluation</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Topic</th>
<th style="text-align: left;">Integrals</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Original question</td>
<td style="text-align: left;">Evaluate $\int_{1}^{1} \sqrt{1+x^{4}} d x$.</td>
</tr>
<tr>
<td style="text-align: left;">Codex input</td>
<td style="text-align: left;">Write a program to evaluate the integral from 1 to 1 of $\operatorname{sqrt}\left(1+x^{\prime} 4\right) d x$.</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Codex code</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">from sympy import Symbol, sqrt, Integral, pprint $x=$ Symbol('x') <br> $k=$ Symbol('k') <br> $p=\operatorname{sqrt}(1+x * * 4)$ <br> pprint(Integral(p, (x, 1, 1)).doit())</td>
</tr>
<tr>
<td style="text-align: center;">Program solution</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">Solution type</td>
<td style="text-align: center;">Automatic Zero-Shot</td>
</tr>
<tr>
<td style="text-align: center;">Codex code explanation</td>
<td style="text-align: center;">1. We create a Symbol object called $x$. <br> 2. We create a Symbol object called $k$. <br> 3. We create a symbolic expression called $p$. <br> 4. We create an Integral object called p_integral. <br> 5. We call the doit() method on the Integral object.</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 response</td>
<td style="text-align: center;">The integral from 1 to 1 of $\operatorname{sqrt}(1+x^{*} 4) \mathrm{dx}$ is equal to 2 .</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 evaluation</td>
<td style="text-align: center;">Incorrect</td>
</tr>
</tbody>
</table>
<p>Table 16. MIT 18.01, Question 14: Original question, Codex input and code and code explanation, Program solution, Solution type, GPT-3 response and evaluation</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Topic</th>
<th style="text-align: center;">Integrals</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Original question</td>
<td style="text-align: center;">Sketch the region enclosed by the given curves and calculate its area. $y=4-x^{2}, y=0$</td>
</tr>
<tr>
<td style="text-align: center;">Codex input</td>
<td style="text-align: center;">Using numpy, graph the lines $y=0$ and $y=4-x^{<em> </em>} 2$. Use fill_between(). <br> Then using Sympy, find the points of intersection between and $y=0$ and $y=4-x^{<em> </em>} 2$. <br> Integrate between lines $y=4-x^{<em> </em>} 2$ and $y=0$, over the points of intersection to calculate the area of the region. $x$ should go from -5 to 5 .</td>
</tr>
<tr>
<td style="text-align: center;">Codex code</td>
<td style="text-align: center;"><code>import numpy as np import matplotlib.pyplot as plt import sympy as sp x = np.linspace(-5, 5, 100) y1 = 0*x y2 = 4-x**2 plt.plot(x, y1, '-b', label='y=0') plt.plot(x, y2, '-r', label='y=4-x**2') plt.fill_between(x, y1, y2, color='grey', alpha=0.5) plt.legend(loc='best') plt.show() x, y = sp.symbols('x y') f = 4-x**2 g = 0*x sols = sp.solve([f, g], [x, y]) x1 = sols[0][0] y1 = sols[0][1] x2 = sols[1][0] y2 = sols[1][1] area = sp.integrate(f, (x, x1, x2)) + sp.integrate(g, (x, x1, x2)) print(area)</code></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: left;">Program solution</th>
<th style="text-align: left;">$\frac{33}{3}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$\frac{1}{4}$</td>
</tr>
<tr>
<td style="text-align: left;">Solution type</td>
<td style="text-align: left;">Manual</td>
</tr>
<tr>
<td style="text-align: left;">Codex code explanation</td>
<td style="text-align: left;">1. We are using numpy to graph the lines $y=0$ and $y=4-x^{<em> </em>} 2$. We are using fill_between() to fill in the area <br> between the two lines. <br> 2. We are using Sympy to find the points of intersection between the two lines. <br> 3. We are using Sympy to integrate between the two lines, over the points of <br> intersection, to calculate the area of the region.</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3 Response</td>
<td style="text-align: left;">The region is enclosed by the curves $y=4-x^{*} 2, y=0$, and $x=2$. Its area is 16.</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3 evaluation</td>
<td style="text-align: left;">Incorrect</td>
</tr>
</tbody>
</table>
<p>Table 17. MIT 18.01, Question 15: Original question, Codex input and code and code explanation, Program solution, Solution type, GPT-3 response and evaluation</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Topic</th>
<th style="text-align: center;">Integrals</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Original question</td>
<td style="text-align: center;">A bacteria population is 4000 at time $t=0$ and its rate of growth is $1000 \cdot 2^{t}$ bacteria per hour after $t$ hours. What is the population after one hour?</td>
</tr>
<tr>
<td style="text-align: center;">Codex input</td>
<td style="text-align: center;">A bacteria population is 4000 at time $t=0$ and its rate of growth is $y=1000 * 2^{*}$ x bacteria per hour after t hours. Write a function to calculate the population after one hour. Use sympy to integrate the symbol $x$ from 0 to 1 . Then add the initial population to the result.</td>
</tr>
<tr>
<td style="text-align: center;">Codex code</td>
<td style="text-align: center;">import sympy as sp <br> $x=$ sp. Symbol('x') <br> def bacteria_population(x): <br> return $1000 * 2 * * x$ <br> print(sp.integrate(bacteria_population(x), (x, 0, 1)) + 4000)</td>
</tr>
<tr>
<td style="text-align: center;">Program solution</td>
<td style="text-align: center;">$\frac{1000}{1000}+4000$</td>
</tr>
<tr>
<td style="text-align: center;">Solution type</td>
<td style="text-align: center;">Manual</td>
</tr>
<tr>
<td style="text-align: center;">Codex code explanation</td>
<td style="text-align: center;">1. We import sympy as sp. <br> 2. We define $x$ as a symbol. <br> 3. We define a function bacteria_population $(x)$ that returns the population of bacteria after $x$ hours. <br> 4. We print the result of integrating the function bacteria_population $(x)$ from 0 to 1 and adding the initial <br> population of 4000 .</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 response</td>
<td style="text-align: center;">5000</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 evaluation</td>
<td style="text-align: center;">Incorrect</td>
</tr>
</tbody>
</table>
<p>Table 18. MIT 18.01, Question 16: Original question, Codex input and code and code explanation, Program solution, Solution type, GPT-3 response and evaluation</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Topic</th>
<th style="text-align: left;">Applications of Integration</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Original question</td>
<td style="text-align: left;">Graph the region between the curves and use your calculator to compute the area correct to five decimal <br> places. $y=\tan ^{2}(x), y=\sqrt{x}$</td>
</tr>
<tr>
<td style="text-align: left;">Codex input</td>
<td style="text-align: left;">Graph the region between the curves $y=\tan ^{-} 2(x)$ and $y=\operatorname{sqrt}(x)$ to find the two points where they <br> intersect. Then, write a program to compute the area between the curves where they cross. Use sympy.</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ Data and code: https://github.com/droni/matNQ&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
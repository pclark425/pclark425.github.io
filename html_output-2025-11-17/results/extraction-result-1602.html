<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1602 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1602</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1602</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-30.html">extraction-schema-30</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-9f4ceac35abe4b92b43128925d55e42c7f7ab702</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9f4ceac35abe4b92b43128925d55e42c7f7ab702" target="_blank">Interactive Language Learning by Question Answering</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work proposes and evaluates a set of baseline models for the QAit task that includes deep reinforcement learning agents, and shows that the task presents a major challenge for machine reading systems, while humans solve it with relative ease.</p>
                <p><strong>Paper Abstract:</strong> Humans observe and interact with the world to acquire knowledge. However, most existing machine reading comprehension (MRC) tasks miss the interactive, information-seeking component of comprehension. Such tasks present models with static documents that contain all necessary information, usually concentrated in a single short substring. Thus, models can achieve strong performance through simple word- and phrase-based pattern matching. We address this problem by formulating a novel text-based question answering task: Question Answering with Interactive Text (QAit). In QAit, an agent must interact with a partially observable text-based environment to gather information required to answer questions. QAit poses questions about the existence, location, and attributes of objects found in the environment. The data is built using a text-based game generator that defines the underlying dynamics of interaction with the environment. We propose and evaluate a set of baseline models for the QAit task that includes deep reinforcement learning agents. Experiments show that the task presents a major challenge for machine reading systems, while humans solve it with relative ease.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1602.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1602.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QAit (QA-DQN baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Question Answering with Interactive Text (QAit) and QA-DQN baseline agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>QAit is a text-based interactive QA benchmark (built on TextWorld) that requires agents to perform multi-step information-gathering procedures in partially observable homes to answer location, existence, and attribute questions; the paper evaluates transformer-based QA-DQN agents (DQN / DDQN / Rainbow variants) with reward shaping and supervised QA modules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>QA-DQN (DQN / DDQN / Rainbow variants)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Transformer-based encoder (word+char embeddings, context-query attention, stacks of transformer blocks) producing observation representations; a command generator that predicts Q-values separately for action/modifier/object lexicons and composes triplet commands; a question answerer that uses aggregation transformers to predict one-word answers. Trained with deep Q-learning variants (vanilla DQN, Double DQN with prioritized replay, and Rainbow) and supervised training for the QA head; uses reward shaping (sufficient-information bonus and episodic discovery bonus).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld / QAit</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Controlled, template-driven text-based house environments (TextWorld) with multiple locations (rooms), containers, and everyday objects. Agents issue triplet text commands {action, modifier, object} (e.g., open wooden door, take red ghargh) and receive observation strings and command feedback. Environments are partially observable, with randomization over object existence, names, and map layouts; two difficulty regimes: Fixed Map (same layout) and Random Map (layout sampled per game).</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>procedural knowledge (information-gathering procedures and object-attribute verification)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Exploration/navigation across rooms, opening containers, taking objects, cooking items, using sharp objects to cut foods, performing eat/drink to test edibility/drinkability, checking whether an object exists, locating the container holding an object, and verifying binary attributes (edible, sharp, cookable, heat_source, openable, portable).</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Procedures are compositional and multi-step: tasks decompose into primitive actions (navigate → inspect → manipulate → observe outcome). Many attribute verifications require sequences of subtasks (e.g., navigate to object → take/open container → perform cook/cut/eat action → observe feedback). The environment supports hierarchical and compositional combinations of primitives (action, modifier, object triplets).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>The paper does not implement an explicit curriculum-training schedule. It states the dataset is designed to facilitate curriculum learning and provides two difficulty regimes (Fixed Map vs Random Map) and several training-set regimes ([1, 2, 10, 100, 500] finite-game settings and an 'unlimited' on-the-fly sampling regime) that researchers could use to construct curricula, but no progressive curriculum ordering is applied in the reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>none (no explicit ordering used). The dataset supports grouping by difficulty (Fixed Map vs Random Map) and varying training-set sizes; sampling is random from the controlled distribution when training.</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Map sizes: 2–12 locations (Random Map); entities ~3*N_r to 6*N_r; games provide up to 80 interaction steps; tasks range from single-step observations (object visible in current observation) to multi-step procedures requiring planning and execution (open containers, cook/cut/insert actions) to reveal attributes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td>Reported zero-shot test accuracy under the paper's standard (no explicit curriculum) regimes: when trained with the 'unlimited' on-the-fly game sampling, zero-shot test accuracies (approx.) are: DQN — location 21.6%, existence 66.2%, attribute 51.4%; DDQN — location 25.8%, existence 62.8%, attribute 48.0%; Rainbow — location 28.0%, existence 69.2%, attribute 51.4%. When the agent has 'sufficient information' (i.e., the required observation is present), the QA module attains ≈100% accuracy on location questions (Table 5). Humans achieve near-perfect accuracy (100% on most categories; attribute 75% in one setting). The paper also reports strong overfitting on small finite training sets (high training QA accuracy but low 'sufficient information' bonus and poor zero-shot generalization).</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>The paper finds limited transfer/generalization: agents trained on small finite sets memorize game-specific Q-A mappings and fail in the 'unlimited' setting; attribute questions generalize poorly (test performance near random ~50% in many unlimited experiments), while the QA module itself generalizes well when sufficient information is provided (near 100% accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>QAit emphasizes procedural information seeking rather than static text matching; baseline RL agents can memorize small sets of games but fail to generalize to large/unseen distributions, especially on attribute (procedural) questions. Reward shaping (sufficient-information and episodic discovery bonuses) helps training but does not solve generalization. Given sufficient observed evidence, the supervised QA head generalizes well, suggesting the main challenge is learning the information-gathering procedures. The paper positions the dataset and its difficulty regimes as suitable infrastructure for future curriculum-learning approaches (e.g., progressive exposure to harder maps/tasks, map familiarization, or subtask/dense rewards), but it does not itself implement or compare curriculum strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Xingdi Yuan, Marc-Alexandre Côté, Jie Fu, Zhouhan Lin, Christopher Pal, Yoshua Bengio, Adam Trischler. Interactive Language Learning by Question Answering (QAit).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Language Learning by Question Answering', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1602.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1602.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Yin & May 2019</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learn how to cook a new recipe in a new house: Using map familiarization, curriculum learning, and common sense to learn families of text-based adventure games</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited work that (by its title) proposes map familiarization, curriculum learning, and common-sense priors to learn families of text-based adventure games; the QAit paper also references Yin & May for injecting weak commonsense (via BERT) into agents for text-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learn how to cook a new recipe in a new house: Using map familiarization, curriculum learning, and common sense to learn families of text-based adventure games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>text-based adventure games (families of games)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Text-based interactive adventure environments (new recipes in new houses), likely generated families of similar games with shared structure to enable curriculum/map familiarization approaches (title-based inference from the reference).</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>procedural (household/cooking tasks and map familiarization)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>From title: learning to cook a recipe in a novel house (implies navigation, object manipulation, cooking procedures); paper also cited for using common-sense priors.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td>map familiarization + curriculum learning (as indicated by title)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>By title and QC's citation, the approach includes map familiarization and curriculum learning across families of related text-based games to improve learning of procedural tasks and common-sense use; the QAit paper itself only cites Yin & May and notes their use of BERT to inject weak common sense, and does not report details or results from that paper.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td>map familiarization / curriculum (presumed progressive exposure to related games or increasing difficulty) — exact ordering not specified in QAit's citation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as a promising prior that leverages map familiarization and curriculum strategies for families of text-based games; QAit suggests such approaches (and BERT-based commonsense priors) as promising directions but does not report Yin & May's experimental results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Language Learning by Question Answering', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learn how to cook a new recipe in a new house: Using map familiarization, curriculum learning, and common sense to learn families of text-based adventure games <em>(Rating: 2)</em></li>
                <li>BabyAI: First steps towards grounded language learning with a human in the loop <em>(Rating: 1)</em></li>
                <li>Procedural level generation improves generality of deep reinforcement learning <em>(Rating: 1)</em></li>
                <li>Playing text-adventure games with graph-based deep reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1602",
    "paper_id": "paper-9f4ceac35abe4b92b43128925d55e42c7f7ab702",
    "extraction_schema_id": "extraction-schema-30",
    "extracted_data": [
        {
            "name_short": "QAit (QA-DQN baselines)",
            "name_full": "Question Answering with Interactive Text (QAit) and QA-DQN baseline agents",
            "brief_description": "QAit is a text-based interactive QA benchmark (built on TextWorld) that requires agents to perform multi-step information-gathering procedures in partially observable homes to answer location, existence, and attribute questions; the paper evaluates transformer-based QA-DQN agents (DQN / DDQN / Rainbow variants) with reward shaping and supervised QA modules.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "QA-DQN (DQN / DDQN / Rainbow variants)",
            "agent_description": "Transformer-based encoder (word+char embeddings, context-query attention, stacks of transformer blocks) producing observation representations; a command generator that predicts Q-values separately for action/modifier/object lexicons and composes triplet commands; a question answerer that uses aggregation transformers to predict one-word answers. Trained with deep Q-learning variants (vanilla DQN, Double DQN with prioritized replay, and Rainbow) and supervised training for the QA head; uses reward shaping (sufficient-information bonus and episodic discovery bonus).",
            "agent_size": null,
            "environment_name": "TextWorld / QAit",
            "environment_description": "Controlled, template-driven text-based house environments (TextWorld) with multiple locations (rooms), containers, and everyday objects. Agents issue triplet text commands {action, modifier, object} (e.g., open wooden door, take red ghargh) and receive observation strings and command feedback. Environments are partially observable, with randomization over object existence, names, and map layouts; two difficulty regimes: Fixed Map (same layout) and Random Map (layout sampled per game).",
            "procedure_type": "procedural knowledge (information-gathering procedures and object-attribute verification)",
            "procedure_examples": "Exploration/navigation across rooms, opening containers, taking objects, cooking items, using sharp objects to cut foods, performing eat/drink to test edibility/drinkability, checking whether an object exists, locating the container holding an object, and verifying binary attributes (edible, sharp, cookable, heat_source, openable, portable).",
            "compositional_structure": "Procedures are compositional and multi-step: tasks decompose into primitive actions (navigate → inspect → manipulate → observe outcome). Many attribute verifications require sequences of subtasks (e.g., navigate to object → take/open container → perform cook/cut/eat action → observe feedback). The environment supports hierarchical and compositional combinations of primitives (action, modifier, object triplets).",
            "uses_curriculum": false,
            "curriculum_name": null,
            "curriculum_description": "The paper does not implement an explicit curriculum-training schedule. It states the dataset is designed to facilitate curriculum learning and provides two difficulty regimes (Fixed Map vs Random Map) and several training-set regimes ([1, 2, 10, 100, 500] finite-game settings and an 'unlimited' on-the-fly sampling regime) that researchers could use to construct curricula, but no progressive curriculum ordering is applied in the reported experiments.",
            "curriculum_ordering_principle": "none (no explicit ordering used). The dataset supports grouping by difficulty (Fixed Map vs Random Map) and varying training-set sizes; sampling is random from the controlled distribution when training.",
            "task_complexity_range": "Map sizes: 2–12 locations (Random Map); entities ~3*N_r to 6*N_r; games provide up to 80 interaction steps; tasks range from single-step observations (object visible in current observation) to multi-step procedures requiring planning and execution (open containers, cook/cut/insert actions) to reveal attributes.",
            "performance_with_curriculum": null,
            "performance_without_curriculum": "Reported zero-shot test accuracy under the paper's standard (no explicit curriculum) regimes: when trained with the 'unlimited' on-the-fly game sampling, zero-shot test accuracies (approx.) are: DQN — location 21.6%, existence 66.2%, attribute 51.4%; DDQN — location 25.8%, existence 62.8%, attribute 48.0%; Rainbow — location 28.0%, existence 69.2%, attribute 51.4%. When the agent has 'sufficient information' (i.e., the required observation is present), the QA module attains ≈100% accuracy on location questions (Table 5). Humans achieve near-perfect accuracy (100% on most categories; attribute 75% in one setting). The paper also reports strong overfitting on small finite training sets (high training QA accuracy but low 'sufficient information' bonus and poor zero-shot generalization).",
            "has_curriculum_comparison": false,
            "alternative_curriculum_performance": null,
            "transfer_generalization": "The paper finds limited transfer/generalization: agents trained on small finite sets memorize game-specific Q-A mappings and fail in the 'unlimited' setting; attribute questions generalize poorly (test performance near random ~50% in many unlimited experiments), while the QA module itself generalizes well when sufficient information is provided (near 100% accuracy).",
            "key_findings": "QAit emphasizes procedural information seeking rather than static text matching; baseline RL agents can memorize small sets of games but fail to generalize to large/unseen distributions, especially on attribute (procedural) questions. Reward shaping (sufficient-information and episodic discovery bonuses) helps training but does not solve generalization. Given sufficient observed evidence, the supervised QA head generalizes well, suggesting the main challenge is learning the information-gathering procedures. The paper positions the dataset and its difficulty regimes as suitable infrastructure for future curriculum-learning approaches (e.g., progressive exposure to harder maps/tasks, map familiarization, or subtask/dense rewards), but it does not itself implement or compare curriculum strategies.",
            "citation": "Xingdi Yuan, Marc-Alexandre Côté, Jie Fu, Zhouhan Lin, Christopher Pal, Yoshua Bengio, Adam Trischler. Interactive Language Learning by Question Answering (QAit).",
            "uuid": "e1602.0",
            "source_info": {
                "paper_title": "Interactive Language Learning by Question Answering",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "Yin & May 2019",
            "name_full": "Learn how to cook a new recipe in a new house: Using map familiarization, curriculum learning, and common sense to learn families of text-based adventure games",
            "brief_description": "Cited work that (by its title) proposes map familiarization, curriculum learning, and common-sense priors to learn families of text-based adventure games; the QAit paper also references Yin & May for injecting weak commonsense (via BERT) into agents for text-based games.",
            "citation_title": "Learn how to cook a new recipe in a new house: Using map familiarization, curriculum learning, and common sense to learn families of text-based adventure games",
            "mention_or_use": "mention",
            "agent_name": null,
            "agent_description": null,
            "agent_size": null,
            "environment_name": "text-based adventure games (families of games)",
            "environment_description": "Text-based interactive adventure environments (new recipes in new houses), likely generated families of similar games with shared structure to enable curriculum/map familiarization approaches (title-based inference from the reference).",
            "procedure_type": "procedural (household/cooking tasks and map familiarization)",
            "procedure_examples": "From title: learning to cook a recipe in a novel house (implies navigation, object manipulation, cooking procedures); paper also cited for using common-sense priors.",
            "compositional_structure": null,
            "uses_curriculum": true,
            "curriculum_name": "map familiarization + curriculum learning (as indicated by title)",
            "curriculum_description": "By title and QC's citation, the approach includes map familiarization and curriculum learning across families of related text-based games to improve learning of procedural tasks and common-sense use; the QAit paper itself only cites Yin & May and notes their use of BERT to inject weak common sense, and does not report details or results from that paper.",
            "curriculum_ordering_principle": "map familiarization / curriculum (presumed progressive exposure to related games or increasing difficulty) — exact ordering not specified in QAit's citation.",
            "task_complexity_range": null,
            "performance_with_curriculum": null,
            "performance_without_curriculum": null,
            "has_curriculum_comparison": null,
            "alternative_curriculum_performance": null,
            "transfer_generalization": null,
            "key_findings": "Mentioned as a promising prior that leverages map familiarization and curriculum strategies for families of text-based games; QAit suggests such approaches (and BERT-based commonsense priors) as promising directions but does not report Yin & May's experimental results.",
            "uuid": "e1602.1",
            "source_info": {
                "paper_title": "Interactive Language Learning by Question Answering",
                "publication_date_yy_mm": "2019-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learn how to cook a new recipe in a new house: Using map familiarization, curriculum learning, and common sense to learn families of text-based adventure games",
            "rating": 2
        },
        {
            "paper_title": "BabyAI: First steps towards grounded language learning with a human in the loop",
            "rating": 1
        },
        {
            "paper_title": "Procedural level generation improves generality of deep reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "rating": 1
        }
    ],
    "cost": 0.016447749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Interactive Language Learning by Question Answering</h1>
<p>Xingdi Yuan ${ }^{\odot <em>}$ Marc-Alexandre Côté ${ }^{\odot </em>}$ Jie Fu ${ }^{\text {A }}$ Zhouhan Lin ${ }^{\odot *}$<br>Christopher Pal ${ }^{\text {A }}$ Yoshua Bengio ${ }^{\circ}$ Adam Trischler ${ }^{\circ}$<br>${ }^{\circ}$ Microsoft Research, Montréal ${ }^{\text {A }}$ Polytechnique Montréal<br>${ }^{\circ}$ Université de Montréal ${ }^{\text {A }}$ Mila<br>eric.yuan@microsoft.com macote@microsoft.com</p>
<h4>Abstract</h4>
<p>Humans observe and interact with the world to acquire knowledge. However, most existing machine reading comprehension (MRC) tasks miss the interactive, information-seeking component of comprehension. Such tasks present models with static documents that contain all necessary information, usually concentrated in a single short substring. Thus, models can achieve strong performance through simple word- and phrase-based pattern matching. We address this problem by formulating a novel text-based question answering task: Question Answering with Interactive Text (QAit) ${ }^{1}$. In QAit, an agent must interact with a partially observable text-based environment to gather information required to answer questions. QAit poses questions about the existence, location, and attributes of objects found in the environment. The data is built using a text-based game generator that defines the underlying dynamics of interaction with the environment. We propose and evaluate a set of baseline models for the QAit task that includes deep reinforcement learning agents. Experiments show that the task presents a major challenge for machine reading systems, while humans solve it with relative ease.</p>
<h2>1 Introduction</h2>
<p>The research community has defined the task of machine reading comprehension (MRC) to teach machines to read and understand text. In most MRC tasks, given a knowledge source (usually a text document) and a question on its content, a model is required to answer the question either by pointing to words in the source or by generating a text string. Recent years have seen a flourishing of MRC works, including the release of numerous</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><code>Game You find yourself in a backyard. You make out a patio
table. You see a patio chair. It's stylish. In a corner you
can see a BBQ. There's an open screen door leading south.
    Quest Is ghargh edible?
    Agent }-&gt;\mathrm{ Qapla'
Game That's not a verb I recognize.
    Agent }-&gt;\mathrm{ eat ghargh
Game You can't see any such thing.
    Agent }-&gt;\mathrm{ go south
Game You've just entered a kitchen. You can see an open fridge
    here. It is empty! You see a closed oven. Look over there,
    it's a counter. On the counter you can see a diced potato,
    a red ghargh and a knife. A an open door is leading north.
    Agent }-&gt;\mathrm{ eat ghargh
Game You need to take the red ghargh first.
    Agent }-&gt;\mathrm{ take red ghargh
Game You take the red ghargh from the counter.
    Agent }-&gt;\mathrm{ eat ghargh
Game You eat the red ghargh. Not bad.
    Agent }-&gt;\mathrm{ wait
    Agent }-&gt;\mathrm{ answer: yes</code></p>
<p>Table 1: Example of a player interacting with an environment to gather knowledge and answer a question.
datasets (Rajpurkar et al., 2016; Trischler et al., 2016; Nguyen et al., 2016; Reddy et al., 2018; Yang et al., 2018) and a host of neural models that address them (Wang and Jiang, 2016; Wang et al., 2017; Chen et al., 2017).</p>
<p>Most existing MRC datasets, as a consequence of their construction, encourage models to do fairly shallow phrase and word matching between the question and the knowledge source. There are three reasons for this. First, questions tend to paraphrase or permute the text that expresses the answer, making pattern matching an effective strategy. On the SQuAD development set, for example, the average precision, recall, and $F_{1}$ score between questions and the sentences containing answer spans are $0.409,0.271,0.301$, respectively. Exploiting this overlap, Trischler et al.</p>
<p>(2016) show that a simple Information Retrieval method can achieve high sentence-level accuracy on SQuAD.</p>
<p>Second, the information that supports predicting the answer from the source is often fully observed: the source is static, sufficient, and presented in its entirety. This does not match the information-seeking procedure that arises in answering many natural questions (Kwiatkowski et al., 2019), nor can it model the way humans observe and interact with the world to acquire knowledge.</p>
<p>Third, most existing MRC studies focus on declarative knowledge - the knowledge of facts or events that can be stated explicitly (i.e., declared) in short text snippets. Given a static description of an entity, declarative knowledge can often be extracted straightforwardly through pattern matching. For example, given the EMNLP website text, the conference deadline can be extracted by matching against a date mention. This focus overlooks another essential category of knowledge - procedural knowledge. Procedural knowledge entails executable sequences of actions. These might comprise the procedure for tying ones shoes, cooking a meal, or gathering new declarative knowledge. The latter will be our focus in this work. As an example, a more general way to determine EMNLP's deadline is to open a browser, head to the website, and then match against the deadline mention; this involves executing several mouse and keyboard interactions.</p>
<p>In order to teach MRC systems procedures for question answering, we propose a novel task: Question Answering with Interactive Text (QAit). Given a question $q \in Q$, rather than presenting a model with a static document $d \in D$ to read, QAit requires the model to interact with a partially observable environment $e \in E$ over a sequence of turns. The model must collect and aggregate evidence as it interacts, then produce an answer $a$ to $q$ based on its experience.</p>
<p>In our case, the environment $e$ is a text-based game with no explicit objective. The game places an agent in a simple modern house populated by various everyday objects. The agent may explore and manipulate the environment by issuing text commands. An example is shown in Table 1. We build a corpus of related text-based games using a generator from Côté et al. (2018), which enables us to draw games from a controlled distribution.</p>
<p>This means there are random variations across the environment set $E$, in map layouts and in the existence, location, and names of objects, etc. Consequently, an agent cannot answer questions merely by memorizing games it has seen before. Because environments are partially observable (i.e., not all necessary information is available at a single turn), an agent must take a sequence of decisions - analogous to following a search and reasoning procedure - to gather the required information. The learning target in QAit is thus not the declarative knowledge $a$ itself, but the procedure for arriving at $a$ by collecting evidence.</p>
<p>The main contributions of this work are as follows:</p>
<ol>
<li>We introduce a novel MRC dataset, QAit, which focuses on procedural knowledge. In it, an agent interacts with an environment to discover the answer to a given question.</li>
<li>We introduce to the MRC domain the practice of generating training data on the fly. We sample training examples from a distribution; hence, an agent is highly unlikely to encounter the same training example more than once. This helps to prevent overfitting and rote memorization.</li>
<li>We evaluate a collection of baseline agents on QAit, including state-of-the-art deep reinforcement learning agents and humans, and discuss limitations of existing approaches.</li>
</ol>
<h2>2 The QAit Dataset</h2>
<h3>2.1 Overview</h3>
<p>We make the question answering problem interactive by building text-based games along with relevant question-answer pairs. We use TextWorld (Côté et al., 2018) to generate these games. Each interactive environment is composed of multiple locations with paths connecting them in a randomly drawn graph. Several interactable objects are scattered across the locations. A player sends text commands to interact with the world, while the game's interpreter only recognizes a small subset of all possible command strings (we call these the valid commands). The environment changes state in response to a valid command and returns a string of text feedback describing the change.</p>
<p>The underlying game dynamics arise from a set of objects (e.g., doors) that possess attributes (e.g.,</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Butter knife</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">1</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Oven</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">1</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Raw chicken</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">1</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Fried chicken</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">1</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">1</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Table 2: Supported attributes along with examples.
doors are openable), and a set of rules (e.g., opening a closed door makes the connected room accessible). The supported attributes are shown in Table 2, while the rules can be inferred from the list of supported commands (see Appendix C). Note that player interactions might affect an object's attributes. For instance, cooking a piece of raw chicken on the stove with a frying pan makes it edible, transforming it into fried chicken.</p>
<p>In each game, the existence of objects, the location of objects, and their names are randomly sampled. Depending on the task, a name can be a made-up word. However, game dynamics are constant across all games - e.g., there will never be a drinkable heat source.</p>
<p>Text in QAit is generated by the TextWorld engine according to English templates, so it does not express the full variation of natural language. However, taking inspiration from the bAbI tasks (Weston et al., 2015), we posit that controlled simplifications of natural language are useful for isolating more complex reasoning behaviors.</p>
<h3>2.2 Available Information</h3>
<p>At every game step, the environment returns an observation string describing the information visible to the agent, as well as the command feedback, which is text describing the response to the previously issued command.</p>
<p>Optional Information: Since we have access to the underlying state representation of a generated game, various optional information can be made available. For instance, it is possible to access the subset of commands that are valid at the current game step. Other available metainformation includes all objects that exist in the game, plus their locations, attributes, and states.</p>
<p>During training, one is free to use any optional information to guide the agent's learning, e.g., to shape the rewards. However, at test time, only the observation string and the command feedback are available.</p>
<h3>2.3 Question Types and Difficulty Levels</h3>
<p>Using the game information described above, we can generate questions with known ground truth answers for any given game.</p>
<h3>2.3.1 Question Types</h3>
<p>For this initial version of QAit we consider three straightforward question types.</p>
<p>Location: ("Where is the can of soda?") Given an object name, the agent must answer with the name of the container that most directly holds the object. This can be either a location, a holder within a location, or the player's inventory. For example, if the can of soda is in a fridge which is in the kitchen, the answer would be "fridge".</p>
<p>Existence: ("Is there a raw egg in the world?") Given the name of an object, the agent must learn to answer whether the object exists in the game environment $e$.</p>
<p>Attribute: ("Is ghargh edible?") Given an object name and an attribute, the agent must answer with the value of the given attribute for the given object. Note that all attributes in our dataset are binary-valued. To discourage an agent from simply memorizing attribute values given an object name (Anand et al., 2018) (e.g., apples are always edible so agents can answer without interaction), we replace object names with unique, randomly drawn made-up words for this question type.</p>
<h3>2.3.2 Difficulty Levels</h3>
<p>To better analyze the limitations of learning algorithms and to facilitate curriculum learning approaches, we define two difficulty levels based on the environment layout.</p>
<p>Fixed Map: The map (location names and layout) is fixed across games. Random objects are distributed across the map in each game. Statistics for this game configuration are shown in Table 3.</p>
<p>Random Map: Both map layouts and objects are randomly sampled in each game.</p>
<h3>2.4 Action Space</h3>
<p>We describe the action space of QAit by splitting it into two subsets: information-gathering actions and question-answering actions.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Fixed Map</th>
<th style="text-align: center;">Random Map</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"># Locations, $N_{r}$</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">$N_{r} \sim$ Uniform $[2,12]$</td>
</tr>
<tr>
<td style="text-align: center;"># Entities, $N_{e}$</td>
<td style="text-align: center;">$N_{e} \sim$ Uniform</td>
<td style="text-align: center;">$3 \cdot N_{r}, 6 \cdot N_{r}]$</td>
</tr>
<tr>
<td style="text-align: center;">Actions / Game</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">17</td>
</tr>
<tr>
<td style="text-align: center;">Modifiers / Game</td>
<td style="text-align: center;">18.5</td>
<td style="text-align: center;">17.7</td>
</tr>
<tr>
<td style="text-align: center;">Objects / Game</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">27.5</td>
</tr>
<tr>
<td style="text-align: center;"># Obs. Tokens</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">89.7</td>
</tr>
</tbody>
</table>
<p>Table 3: Statistics of the QAit dataset. Numbers are averaged over 10,000 randomly sampled games.</p>
<p>Information Gathering The player generates text commands word by word to navigate through and interact with the environment. On encountering an object, the player must interact with it to discover its attributes. To succeed, an agent must map the feedback received from the environment, in text, to a useful state representation. This is a form of reading comprehension.</p>
<p>To make the QAit task more tractable, all text commands are triplets of the form {action, modifier, object} (e.g., open wooden door). When there is no ambiguity, the environment understands commands without modifiers (e.g., eat apple will result in eating the "red apple" provided it is the only apple in the player's inventory). We list all supported commands in Appendix C.</p>
<p>Each game provides a set of three lexicons that divide the full vocabulary into actions, modifiers, and objects. Statistics are shown in Table 3. A model can generate a command at each game step by, e.g., sampling from a probability distribution induced over each lexicon. This reduces the size of the action space compared to a sequential, freeform setting where a model can pick any vocabulary word at any generation step.</p>
<p>An agent decides when to stop interacting with the environment to answer the question by generating a special wait command ${ }^{2}$. However, the number of interaction steps is limited: we use 80 steps in all experiments. When an agent has exhausted its available steps, the game terminates and the agent is forced to answer the question.</p>
<p>Question Answering Currently, all QAit answers are one word. For existence and attribute questions, the answer is either yes or no; for loca-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>tion questions, the answer can be any word in an observation string.</p>
<h3>2.5 Evaluation Settings and Metrics</h3>
<p>We evaluate an agent's performance on QAit by its accuracy in answering questions. We propose three distinct settings for the evaluation.</p>
<p>Solving Training Games: We use QA accuracy during training, averaged over a window of training time, to evaluate an agent's training performance. We provide 5 training sets for this purpose with $[1,2,10,100,500]$ games, respectively. Each game in these sets is associated with multiple questions.</p>
<p>Unlimited Games: We implement a setup where games are randomly generated on the fly during training, rather than selected from a finite set as above. The distribution we draw from is controlled by a few parameters: number of locations, number of objects, type of map, and a random seed. From the fixed map game distribution described in Table 3, more than $10^{40}$ different games can be drawn. This means that a game is unlikely to be seen more than once during training. We expect that only a model with strong generalization capabilities will perform well in this setting.</p>
<p>Zero-shot Evaluation: For each game setting and question type, we provide 500 held out games that are never seen during training, each with one question. These are used to benchmark generalization in models in a reproducible manner, no matter the training setting. This set is analogous to the test set used in traditional supervised learning tasks, and can be used in conjunction with any training setting.</p>
<h2>3 Baseline Models</h2>
<h3>3.1 Random Baseline</h3>
<p>Our simplest baseline does not interact with the environment to answer questions; it samples an answer word uniformly from the QA action space (yes and no for attribute and existence questions; all possible object names in the game for location questions).</p>
<h3>3.2 Human Baseline</h3>
<p>We conducted a study with 21 participants to explore how humans perform on QAit in terms of</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overall architecture of our baseline agent.</p>
<p>QA accuracy. Participants played games they had not seen previously from a set generated by sampling 4 game-question pairs for each question type and difficulty level. The human results presented below always represent an average over 3 participants.</p>
<h3>3.3 QA-DQN</h3>
<p>We propose a neural baseline agent, QADQN, which takes inspiration from the work of Narasimhan et al. (2015) and Yu et al. (2018). The agent consists of three main components: an encoder, a command generator, and a question answerer. More precisely, at game step $t$, the encoder takes observation $o_{t}$ and question $q$ as input to generate hidden representations. ${ }^{3}$ In the information gathering phase, the command generator generates Q-values for all action, modifier, and object words, with rankings of these Q-values used to generate text commands $c_{t}$. At any game step, the agent may decide to terminate information gathering and answer the question (or it is forced to do so if it has used up all of its moves). The question answerer uses the hidden representations at the final information-gathering step to generate a probability distribution over possible answers.</p>
<p>An overview of this architecture is shown in Figure 1 and full details are given in Appendix A.</p>
<h3>3.3.1 Reward Shaping</h3>
<p>We design the following two rewards to help QADQN learn more efficiently; both used for training the command generator. Note that these rewards are part of the design of QA-DQN, but are not used to evaluate its performance. Question answering accuracy is the only evaluation metric for QAit tasks.</p>
<p>Sufficient Information Bonus: To tackle QAit tasks, an intelligent agent should know when to</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>stop interacting - it should stop as soon as it has gathered enough information to answer the question correctly. For guiding the agent to learn this behavior, we give an additional reward when the agent stops with sufficient information. Specifically, assuming the agent decides to stop at game step $k$ :</p>
<ul>
<li>Location: reward is 1 if the entity mentioned in the question is a sub-string of $o_{k}$, otherwise it is 0 . This means whenever an agent observes the entity, it has sufficient information to infer the entity's location.</li>
<li>Existence: when the correct answer is yes, a reward of 1 is assigned only if the entity is a sub-string of $o_{k}$. When the correct answer is no, a reward between 0 and 1 is given. The reward value corresponds to the exploration coverage of the environment, i.e., how many locations the agent has visited, and how many containers have been opened.</li>
<li>Attribute: we heuristically define a set of conditions to verify each attribute, and reward the agent based on its fulfilment of these conditions. For instance, determining if an object $\mathbf{x}$ is sharp corresponds to checking the outcome of a cut command (slice, chop, or dice) while holding the object $\mathbf{x}$ and a cuttable food item. If the outcome is successful then the object $\mathbf{x}$ is sharp otherwise it is not. Alternatively, if trying to take the object $\mathbf{x}$ results in a failure, then we can deduces it is not sharp as all sharp objects are portable. The list of conditions for each attribute used in our experiments is shown in Appendix D.</li>
</ul>
<p>Episodic Discovery Bonus: Following Yuan et al. (2018), we use an episodic counting reward to encourage the agent to discover unseen game states. The agent is assigned a positive reward whenever it encounters a new state (in text-based games, states are simply represented as strings):</p>
<p>$$
r\left(o_{t}\right)= \begin{cases}1.0 &amp; \text { if } n\left(o_{t}\right)=1 \ 0.0 &amp; \text { otherwise }\end{cases}
$$</p>
<p>where $n(\cdot)$ is reset to zero after each episode.</p>
<h3>3.3.2 Training Strategy</h3>
<p>We apply different training strategies for the command generator and the question answerer.</p>
<p>Command Generation: Text-based games are sequential decision-making problems that can</p>
<p>be described naturally by partially observable Markov decision processes (POMDPs) (Kaelbling et al., 1998). We use the Q-Learning (Watkins and Dayan, 1992) paradigm to train our agent. Specifically, following Mnih et al. (2015), our Q-value function is approximated with a deep neural network. Beyond vanilla DQN, we also apply several extensions, such as Rainbow (Hessel et al., 2017), to our training process. Details are provided in Section 4.</p>
<p>Question Answering: During training, we push all question answering transitions (observation strings when interaction stops, question strings, ground-truth answers) into a replay buffer. After every 20 game steps, we randomly sample a mini-batch of such transitions from the replay buffer and train the question answerer with supervised learning (e.g., using negative log-likelihood (NLL) loss).</p>
<h2>4 Experimental Results</h2>
<p>In this section, we report experimental results by difficulty levels. All random baseline performance values are averaged over 100 different runs. In the following subsections, we use "DQN", "DDQN" and "Rainbow" to indicate QA-DQN trained with vanilla DQN, Double DQN with prioritized experience replay, and Rainbow, respectively. Training curves shown in the following figures represent a sliding-window average with a window size of 500. Moreover, each curve is the average of 3 random seeds. For evaluation, we selected the model with the random seed yielding the highest training accuracy to compute its accuracy on the test games. Due to space limitations, we only report some key results here. See Appendix E for the full experimental results.</p>
<h3>4.1 Fixed Map</h3>
<p>Figure 2 shows the training curves for the neural baseline agents when trained using 10 games, 500 games and the "unlimited" games settings. Table 4 reports their zero-shot test performance.</p>
<p>From Figure 2, we observe that when training data size is small (e.g., 10 games), our baseline agent trained with all the three RL methods successfully master the training games. Vanilla DQN and DDQN are particularly strong at memorizing the training games. When training on more games (e.g., 500 games and unlimited games), in which case memorization is more difficult, Rain-
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Training accuracy over episodes on fixed map setup. Upper row: 10 games; middle row: 500 games; lower row: unlimited games.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Model</td>
<td>Loc.</td>
<td>Ext.</td>
<td>Att.</td>
<td>Loc.</td>
<td>Ext.</td>
<td>Att.</td>
</tr>
<tr>
<td>Human</td>
<td>1.000</td>
<td>1.000</td>
<td>1.000</td>
<td>1.000</td>
<td>1.000</td>
<td>0.750</td>
</tr>
<tr>
<td>Random</td>
<td>0.027</td>
<td>0.497</td>
<td>0.496</td>
<td>0.034</td>
<td>0.500</td>
<td>0.499</td>
</tr>
<tr>
<td>10 games</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>DQN</td>
<td>0.180</td>
<td>0.568</td>
<td>0.518</td>
<td>0.156</td>
<td>0.566</td>
<td>0.518</td>
</tr>
<tr>
<td>DDQN</td>
<td>0.188</td>
<td>0.566</td>
<td>0.516</td>
<td>0.142</td>
<td>0.606</td>
<td>0.500</td>
</tr>
<tr>
<td>Rainbow</td>
<td>0.156</td>
<td>0.590</td>
<td>0.520</td>
<td>0.144</td>
<td>0.586</td>
<td>0.530</td>
</tr>
<tr>
<td>500 games</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>DQN</td>
<td>0.224</td>
<td>0.674</td>
<td>0.534</td>
<td>0.204</td>
<td>0.678</td>
<td>0.530</td>
</tr>
<tr>
<td>DDQN</td>
<td>0.218</td>
<td>0.626</td>
<td>0.508</td>
<td>0.222</td>
<td>0.656</td>
<td>0.486</td>
</tr>
<tr>
<td>Rainbow</td>
<td>0.190</td>
<td>0.656</td>
<td>0.496</td>
<td>0.172</td>
<td>0.678</td>
<td>0.494</td>
</tr>
<tr>
<td>unlimited games</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>DQN</td>
<td>0.216</td>
<td>0.662</td>
<td>0.514</td>
<td>0.188</td>
<td>0.668</td>
<td>0.506</td>
</tr>
<tr>
<td>DDQN</td>
<td>0.258</td>
<td>0.628</td>
<td>0.480</td>
<td>0.206</td>
<td>0.694</td>
<td>0.482</td>
</tr>
<tr>
<td>Rainbow</td>
<td>0.280</td>
<td>0.692</td>
<td>0.514</td>
<td>0.258</td>
<td>0.686</td>
<td>0.470</td>
</tr>
</tbody>
</table>
<p>Table 4: Agent performance on zero-shot test games when trained on 10 games, 500 games and "unlimited" games settings. Note Att. and Ext. are binary questions with expected accuracy of 0.5 . bow agents start to show its superiority - it has similar accuracy as the other two methods, and even outperforms them in existence question type.</p>
<p>From Table 4 we see similar observation, when trained on 10 games and 500 games, DQN and DDQN performs better on test games but on the unlimited games setting, rainbow agent performs as good as them, and sometimes even better. We can also observe that our agents fail to generalize on attribute questions. In unlimited games setting as shown in Figure 2, all three agents produce an accuracy of 0.5 ; in zero-shot test as shown in Table 4, no agent performs significantly better than random. This suggests the agents memorize game-question-answer triples when data size is small, and fail to do so in unlimited games setting. This</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Training accuracy on the random map setup. Upper row: 10 games; middle row: 500 games; lower row: unlimited games.
can also be observed in Appendix E, where in attribute question experiments, the training accuracy is high, and sufficient information bonus is low (even close to 0 ).</p>
<h3>4.2 Random Map</h3>
<p>Figure 3 shows the training curves for the neural baseline agents when trained using 10 games, 500 games and "unlimited" games settings. The trends of our agents' performance on random map games are consistent with on fixed map games. However, because there exist easier games (as listed in Table 3, number of rooms is sampled between 2 and 12), agents show better training performance in such setting than fixed map setting in general.</p>
<p>Interestingly, we observe one of the DQN agent starts to learn in the unlimited games, attribute question setting. This may be because in games with smaller map size and less objects, there is a higher chance to accomplish some sub-tasks (e.g., it is easier to find an object when there are less rooms), and the agent learn such skills and apply them to similar tasks. Unfortunately, as shown in Table 4 that agent does not perform significantly better than random on test set. We expect with more training episodes, the agent can have a better generalization performance.</p>
<h3>4.3 Question Answering Given Sufficient Information</h3>
<p>The challenge in QAit is learning the interactive procedure for arriving at a state with the information needed to answer the question. We conduct the following experiments on location questions to investigate this challenge.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Fixed Map</th>
<th style="text-align: center;">Random Map</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">16.5</td>
</tr>
<tr>
<td style="text-align: center;">10 games</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DQN</td>
<td style="text-align: center;">95.7</td>
<td style="text-align: center;">97.5</td>
</tr>
<tr>
<td style="text-align: center;">DDQN</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">92.2</td>
</tr>
<tr>
<td style="text-align: center;">Rainbow</td>
<td style="text-align: center;">91.8</td>
<td style="text-align: center;">84.7</td>
</tr>
<tr>
<td style="text-align: center;">500 games</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DQN</td>
<td style="text-align: center;">91.8</td>
<td style="text-align: center;">94.4</td>
</tr>
<tr>
<td style="text-align: center;">DDQN</td>
<td style="text-align: center;">95.6</td>
<td style="text-align: center;">90.2</td>
</tr>
<tr>
<td style="text-align: center;">Rainbow</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">96.6</td>
</tr>
<tr>
<td style="text-align: center;">unlimited games</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DQN</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">DDQN</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">Rainbow</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
</tbody>
</table>
<p>Table 5: Test performance given sufficient information.</p>
<p>Based on the results in Table 4, we compute an agent's test accuracy only if it has obtained sufficient information - i.e., when the sufficient information bonus is 1 . Results shown in Table 5 support our assumption that the QA module can learn (and generalize) effectively to answer given sufficient information. Similarly, experiments show that when objects being asked about are in the current observation, the random baseline's performance goes up significantly as well. We report our baseline agents' question answering accuracy and sufficient information bonuses on all experiment settings in Appendix E.</p>
<h3>4.4 Full Information Setup</h3>
<p>To reframe the QAit games as a standard MRC task, we also designed an experimental setting that eliminates the need to gather information interactively. From a heuristic trajectory through the game environment that is guaranteed to observe sufficient information for $q$, we concatenate all observations into a static "document" $d$ to build a ${d, q, a}$ triplet. A model then uses this fully observed document as input to answer the question. We split this data into training, validation, and test sets and follow the evaluation protocol for standard supervised MRC tasks. We take an off-the-shelf MRC model, Match-LSTM (Wang and Jiang, 2016), trained with negative log-likelihood loss as a baseline.</p>
<p>Unsurprisingly, Match-LSTM does fairly well on all 3 question types ( $86.4,89.9$ and 93.2 test accuracy on location, existence, and attribute questions, respectively). This implies that without the need to interact with the environment for information gathering, the task is simple enough that a word-matching model can answer questions with</p>
<p>high accuracy.</p>
<h2>5 Related Work</h2>
<h3>5.1 MRC Datasets</h3>
<p>Many large-scale machine reading comprehension and question answering datasets have been proposed recently. The datasets of Rajpurkar et al. (2016); Trischler et al. (2016) contain crowdsourced questions based on single documents from Wikipedia and CNN news, respectively. Nguyen et al. (2016); Joshi et al. (2017); Dunn et al. (2017); Clark et al. (2018); Kwiatkowski et al. (2019) present question-answering corpora harvested from information retrieval systems, often containing multiple supporting documents for each question. This means a model must sift through a larger quantity of information and possibly reconcile competing viewpoints. Berant et al. (2013); Welbl et al. (2017); Talmor and Berant (2018) propose to leverage knowledge bases to generate question-answer pairs. Yang et al. (2018) focuses on questions that require multi-hop reasoning to answer, by building questions compositionally. Reddy et al. (2018); Choi et al. (2018) explore conversational question answering, in which a full understanding of the question depends on the conversation's history.</p>
<p>Most of these datasets focus on declarative knowledge and are static, with all information fully observable to a model. We contend that this setup, unlike QAit, encourages word matching. Supporting this contention, several studies highlight empirically that existing MRC tasks require little comprehension or reasoning. In Rychalska et al. (2018), it was shown that a question's main verb exerts almost no influence on the answer prediction: in over $90 \%$ of examined cases, swapping verbs for their antonyms does not change a system's decision. Jia and Liang (2017) show the accuracy of neural models drops from an average of $75 \% F_{1}$ score to $36 \% F_{1}$ when they manually insert adversarial sentences into SQuAD.</p>
<h3>5.2 Interactive Environments</h3>
<p>Several embodied or visual question answering datasets have been presented recently to address some of the problems of interest in our work, such as those of Brodeur et al. (2017); Das et al. (2017); Gordon et al. (2017). In contrast with these, our purely text-based environment circumvents challenges inherent to modelling interactions between
separate data modalities. Furthermore, most visual question answering environments only support navigating and moving the camera as interactions. In text-based environments, however, it is relatively cheap to build worlds with complex interactions. This is because text enables us to model interactions abstractly without the need for, e.g., a costly physics engine.</p>
<p>Closely related to QAit is BabyAI (ChevalierBoisvert et al., 2018). BabyAI is a gridworld environment that also features constrained language for generating simple home-based scenarios (i.e., instructions). However, observations and actions in BabyAI are not text-based. World of Bits (Shi et al., 2017) is a platform for training agents to interact with the internet to accomplish tasks like flight booking. Agents generally do not need to gather information in World of Bits, and the focus is on accomplishing tasks rather than answering questions.</p>
<h3>5.3 Information Seeking</h3>
<p>Information seeking behavior is an important capacity of intelligent systems that has been discussed for many years. Kuhlthau (2004) propose a holistic view of information search as a six-stage process. Schmidhuber (2010) discusses the connection between information seeking and formal notions of fun, creativity, and intrinsic motivation. Das et al. (2018) propose a model that continuously determines all entities' locations during reading and dynamically updates the associated representations in a knowledge graph. Bachman et al. (2016) propose a collection of tasks and neural methods for learning to gathering information efficiently in an environment.</p>
<p>To our knowledge, we are the first to consider interactive information-seeking tasks for question answering in worlds with complex dynamics. The QAit task was designed such that simple word matching methods do not apply, while more human-like information seeking models are encouraged.</p>
<h2>6 Discussion and Future Work</h2>
<p>Monitoring Information Seeking: In QAit, the only evaluation metric is question answering accuracy. However, the sufficient information bonus described in Section 3.3.1 is helpful for monitoring agents' ability to gather relevant information. We report its value for all experiments in</p>
<p>Appendix E. We observe that while the baseline agents can reach a training accuracy of $100 \%$ for answering attribute questions when trained on a few games, the sufficient information bonus is close to 0 . This is a clear indication that the agent overfits to the question-answer mapping of the games rather than learning how to gather useful information. This aligns with our observation that the agent does not perform better than random on the unlimited games setting, because it fails to gather the needed information.</p>
<p>Challenges in QAit: QAit focuses on learning procedural knowledge from interactive environments, so it is natural to use deep RL methods to tackle it. Experiments suggest the dataset presents a major challenge for existing systems, including Rainbow, which set the state of the art on Atari games. As a simplified and controllable text-based environment, QAit can drive research in both the RL and language communities, especially where they intersect. Until recently, the RL community focused mainly on solving single environments (i.e., training and testing on the same game). Now, we see a shift towards solving multiple games and testing for generalization (Cobbe et al., 2018; Justesen et al., 2018). We believe QAit serves this purpose.</p>
<p>Templated Language: As QAit is based on TextWorld, it has the obvious limitation of using templated English. However, TextWorld provides approximately 500 human-written templates for describing rooms and objects, so some textual diversity exists, and since game narratives are generated compositionally, this diversity increases along with the complexity of a game. We believe simplified and controlled text environments offer a bridge to full natural language, on which we can isolate the learning of useful behaviors like information seeking and command generation. Nevertheless, it would be interesting to further diversify the language in QAit, for instance by having human writers paraphrase questions.</p>
<p>Future Work: Based on our present efforts to tackle QAit, we propose the following directions for future work.</p>
<p>A structured memory (e.g., a dynamic knowledge graph as proposed in Das et al. (2018); Ammanabrolu and Riedl (2019a)) could be helpful for explicitly memorizing the places and objects that an agent has observed. This is especially useful
when an agent must revisit a location or object or should avoid doing so.</p>
<p>Likewise, a variety of external knowledge could be leveraged by agents. For instance, incorporating a pretrained language model could improve command generation by imparting knowledge of word and object affordances. In recent work, Hausknecht et al. (2019) show that pretrained modules together with handcrafted subpolicies help in solving text-based games, while Yin and May (2019) use BERT (Devlin et al., 2018) to inject 'weak common sense' into agents for text-based games. Ammanabrolu and Riedl (2019b) show that knowledge graphs and their associated neural encodings can be used as a medium for domain transfer across text-based games.</p>
<p>In finite game settings we observed significant overfitting, especially for attribute questions - as shown in Appendix E, our agent achieves high QA accuracy but low sufficient information bonus on the single-game setting. Sometimes attributes require long procedures to verify, and thus, we believe that denser rewards would help with this problem. One possible solution is to provide intermediate rewards whenever the agent achieves a sub-task.</p>
<h2>Acknowledgments</h2>
<p>The authors thank Romain Laroche, Rémi Tachet des Combes, Matthew Hausknecht, Philip Bachman, and Layla El Asri for insightful ideas and discussions. We thank Tavian Barnes, Wendy Tay, and Emery Fine for their work on the TextWorld framework. We also thank the anonymous EMNLP reviewers for their helpful feedback and suggestions.</p>
<h2>References</h2>
<p>Ammanabrolu, P. and Riedl, M. (2019a). Playing textadventure games with graph-based deep reinforcement learning. In NAACL, pages 3557-3565, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Ammanabrolu, P. and Riedl, M. (2019b). Transfer in deep reinforcement learning using knowledge graphs. CoRR, abs/1908.06556.</p>
<p>Anand, A., Belilovsky, E., Kastner, K., Larochelle, H., and Courville, A. C. (2018). Blindfold baselines for embodied QA. CoRR, abs/1811.05013.</p>
<p>Bachman, P., Sordoni, A., and Trischler, A. (2016). Towards information-seeking agents. CoRR, abs/1612.02605.</p>
<p>Berant, J., Chou, A., Frostig, R., and Liang, P. (2013). Semantic parsing on freebase from question-answer pairs. In EMNLP, pages 1533-1544. ACL.</p>
<p>Brodeur, S., Perez, E., Anand, A., Golemo, F., Celotti, L., Strub, F., Rouat, J., Larochelle, H., and Courville, A. C. (2017). Home: a household multimodal environment. CoRR, abs/1711.11017.</p>
<p>Chen, D., Fisch, A., Weston, J., and Bordes, A. (2017). Reading wikipedia to answer open-domain questions. CoRR, abs/1704.00051.</p>
<p>Chevalier-Boisvert, M., Bahdanau, D., Lahlou, S., Willems, L., Saharia, C., Nguyen, T. H., and Bengio, Y. (2018). Babyai: First steps towards grounded language learning with a human in the loop. CoRR, abs/1810.08272.</p>
<p>Choi, E., He, H., Iyyer, M., Yatskar, M., Yih, W., Choi, Y., Liang, P., and Zettlemoyer, L. (2018). Quac : Question answering in context. CoRR, abs/1808.07036.</p>
<p>Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. (2018). Think you have solved question answering? try arc, the AI2 reasoning challenge. CoRR, abs/1803.05457.</p>
<p>Cobbe, K., Klimov, O., Hesse, C., Kim, T., and Schulman, J. (2018). Quantifying generalization in reinforcement learning. CoRR, abs/1812.02341.</p>
<p>Côté, M.-A., Kádár, A., Yuan, X., Kybartas, B., Barnes, T., Fine, E., Moore, J., Hausknecht, M., Asri, L. E., Adada, M., Tay, W., and Trischler, A. (2018). Textworld: A learning environment for text-based games. CoRR, abs/1806.11532.</p>
<p>Das, A., Datta, S., Gkioxari, G., Lee, S., Parikh, D., and Batra, D. (2017). Embodied question answering. CoRR, abs/1711.11543.</p>
<p>Das, R., Munkhdalai, T., Yuan, X., Trischler, A., and McCallum, A. (2018). Building dynamic knowledge graphs from text using machine reading comprehension. CoRR, abs/1810.05682.</p>
<p>Devlin, J., Chang, M., Lee, K., and Toutanova, K. (2018). BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805.</p>
<p>Dunn, M., Sagun, L., Higgins, M., Güney, V. U., Cirik, V., and Cho, K. (2017). Searchqa: A new q\&amp;a dataset augmented with context from a search engine. CoRR, abs/1704.05179.</p>
<p>Fortunato, M., Azar, M. G., Piot, B., Menick, J., Osband, I., Graves, A., Mnih, V., Munos, R., Hassabis, D., Pietquin, O., Blundell, C., and Legg, S. (2017). Noisy networks for exploration. CoRR, abs/1706.10295.</p>
<p>Gordon, D., Kembhavi, A., Rastegari, M., Redmon, J., Fox, D., and Farhadi, A. (2017). IQA: visual question answering in interactive environments. CoRR, abs/1712.03316.</p>
<p>Hausknecht, M., Loynd, R., Yang, G., Swaminathan, A., and Williams, J. D. (2019). Nail: A general interactive fiction agent.</p>
<p>Hessel, M., Modayil, J., van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M. G., and Silver, D. (2017). Rainbow: Combining improvements in deep reinforcement learning. CoRR, abs/1710.02298.</p>
<p>Jia, R. and Liang, P. (2017). Adversarial examples for evaluating reading comprehension systems. arXiv preprint arXiv:1707.07328.</p>
<p>Joshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L. (2017). Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. CoRR, abs/1705.03551.</p>
<p>Justesen, N., Torrado, R. R., Bontrager, P., Khalifa, A., Togelius, J., and Risi, S. (2018). Procedural level generation improves generality of deep reinforcement learning. CoRR, abs/1806.10729.</p>
<p>Kaelbling, L. P., Littman, M. L., and Cassandra, A. R. (1998). Planning and acting in partially observable stochastic domains. Artificial intelligence, 101(12):99-134.</p>
<p>Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.</p>
<p>Kuhlthau, C. (2004). Seeking Meaning: A Process Approach to Library and Information Services. Information management, policy, and services. Libraries Unlimited.</p>
<p>Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Kelcey, M., Devlin, J., Lee, K., Toutanova, K. N., Jones, L., Chang, M.-W., Dai, A., Uszkoreit, J., Le, Q., and Petrov, S. (2019). Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics.</p>
<p>Mikolov, T., Grave, E., Bojanowski, P., Puhrsch, C., and Joulin, A. (2018). Advances in pre-training distributed word representations. In Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018).</p>
<p>Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540):529-533.</p>
<p>Narasimhan, K., Kulkarni, T., and Barzilay, R. (2015). Language understanding for text-based games using deep reinforcement learning. arXiv preprint arXiv:1506.08941.</p>
<p>Nguyen, T., Rosenberg, M., Song, X., Gao, J., Tiwary, S., Majumder, R., and Deng, L. (2016). MS MARCO: A human generated machine reading comprehension dataset. CoRR, abs/1611.09268.</p>
<p>Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer, A. (2017). Automatic differentiation in pytorch. In NIPS-W.</p>
<p>Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. (2016). Squad: 100, 000+ questions for machine comprehension of text. CoRR, abs/1606.05250.</p>
<p>Reddy, S., Chen, D., and Manning, C. D. (2018). Coqa: A conversational question answering challenge. CoRR, abs/1808.07042.</p>
<p>Rychalska, B., Basaj, D., Biecek, P., and Wroblewska, A. (2018). Does it care what you asked? understanding importance of verbs in deep learning qa system.</p>
<p>Schmidhuber, J. (2010). Formal theory of creativity, fun, and intrinsic motivation (1990\&amp;amp;#x2013;2010). IEEE Trans. on Auton. Ment. Dev., 2(3):230-247.</p>
<p>Shi, T., Karpathy, A., Fan, L., Hernandez, J., and Liang, P. (2017). World of bits: An open-domain platform for web-based agents. In Precup, D. and Teh, Y. W., editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 3135-3144, International Convention Centre, Sydney, Australia. PMLR.</p>
<p>Srivastava, R. K., Greff, K., and Schmidhuber, J. (2015). Highway networks. CoRR, abs/1505.00387.</p>
<p>Talmor, A. and Berant, J. (2018). The web as a knowledge-base for answering complex questions. CoRR, abs/1803.06643.</p>
<p>Trischler, A., Wang, T., Yuan, X., Harris, J., Sordoni, A., Bachman, P., and Suleman, K. (2016). Newsqa: A machine comprehension dataset. CoRR, abs/1611.09830.</p>
<p>Wang, S. and Jiang, J. (2016). Machine comprehension using match-lstm and answer pointer. CoRR, abs/1608.07905.</p>
<p>Wang, W., Yang, N., Wei, F., Chang, B., and Zhou, M. (2017). Gated self-matching networks for reading comprehension and question answering. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 189-198. Association for Computational Linguistics.</p>
<p>Watkins, C. J. C. H. and Dayan, P. (1992). Q-learning. Machine Learning, 8(3):279-292.</p>
<p>Welbl, J., Stenetorp, P., and Riedel, S. (2017). Constructing datasets for multi-hop reading comprehension across documents. CoRR, abs/1710.06481.</p>
<p>Weston, J., Bordes, A., Chopra, S., Rush, A. M., van Merrinboer, B., Joulin, A., and Mikolov, T. (2015). Towards ai-complete question answering: A set of prerequisite toy tasks. CoRR.</p>
<p>Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W., Salakhutdinov, R., and Manning, C. D. (2018). Hotpotqa: A dataset for diverse, explainable multi-hop question answering. CoRR, abs/1809.09600.</p>
<p>Yin, X. and May, J. (2019). Learn how to cook a new recipe in a new house: Using map familiarization, curriculum learning, and common sense to learn families of text-based adventure games. CoRR, abs/1908.04777.</p>
<p>Yu, A. W., Dohan, D., Luong, M.-T., Zhao, R., Chen, K., Norouzi, M., and Le, Q. V. (2018). Qanet: Combining local convolution with global self-attention for reading comprehension. arXiv preprint arXiv:1804.09541.</p>
<p>Yuan, X., Côté, M., Sordoni, A., Laroche, R., des Combes, R. T., Hausknecht, M. J., and Trischler, A. (2018). Counting to explore and generalize in textbased games. CoRR, abs/1806.11525.</p>
<h2>A Details of QA-DQN</h2>
<h2>Notations</h2>
<p>In this section, we use game step $t$ to denote one round of interaction between an agent with the QAit environment. We use $o_{t}$ to denote text observation at game step $t$, and $q$ to denote question text. We use $L$ to refer to a linear transformation. Brackets $[\cdot ; \cdot]$ denote vector concatenation.</p>
<h2>A. 1 Encoder</h2>
<p>We use a transformer-based text encoder, which consists of an embedding layer, two stacks of transformer blocks (denoted as encoder transformer blocks and aggregation transformer blocks), and an attention layer.</p>
<p>In the embedding layer, we aggregate both word- and character-level information to produce a vector for each token in text. Specifically, word embeddings are initialized by the 300-dimensional fastText (Mikolov et al., 2018) word vectors trained on Common Crawl (600B tokens), they are fixed during training. Character level embedding vectors are initialized with 32-dimensional random vectors. A convolutional layer with 64 kernels of size 5 is then used to aggregate the sequence of characters. We use a max pooling layer on the character dimension, then a multi-layer perceptron (MLP) of output size 64 is used to aggregate the concatenation of word- and character-level representations. Highway network (Srivastava et al., 2015) is applied on top of this MLP. The resulting vectors are used as input to the encoding transformer blocks.</p>
<p>Each encoding transformer block consists of a stack of convolutional layers, a self-attention layer, and an MLP. In which, each convolutional layer has 64 filters, each kernel's size is 7 , there are 2 such convolutional layers that share weights. In the self-attention layer, we use a block hidden size of 64 , as well as a single head attention mechanism. Layernorm and dropout are applied after each component inside the block. We add positional encoding into each block's input. We use one layer of such an encoding block.</p>
<p>At a game step $t$, the encoder processes text observation $o_{t}$ and question $q$, context aware encoding $h_{o_{t}} \in \mathbb{R}^{L^{o_{t}} \times H_{1}}$ and $h_{q} \in \mathbb{R}^{L^{q} \times H_{1}}$ are generated, where $L^{o_{t}}$ and $L^{q}$ denote number of tokens in $o_{t}$ and $q$ respectively, $H_{1}$ is 64 . Following (Yu et al., 2018), we use an context-query attention layer to aggregate the two representations $h_{o_{t}}$
and $h_{q}$.
Specifically, the attention layer first uses two MLPs to convert both $h_{o_{t}}$ and $h_{q}$ into the same space, the resulting tensors are denoted as $h_{o_{t}}^{\prime} \in$ $\mathbb{R}^{L^{o_{t}} \times H_{2}}$ and $h_{q}^{\prime} \in \mathbb{R}^{L^{q} \times H_{2}}$, in which $H_{2}$ is 64 .</p>
<p>Then, a tri-linear similarity function is used to compute the similarities between each pair of $h_{o_{t}}^{\prime}$ and $h_{q}^{\prime}$ items:</p>
<p>$$
S=W\left[h_{o_{t}}^{\prime} ; h_{q}^{\prime} ; h_{o_{t}}^{\prime} \odot h_{q}^{\prime}\right]
$$</p>
<p>where $\odot$ indicates element-wise multiplication, $W$ is trainable parameters of size 64.</p>
<p>Softmax of the resulting similarity matrix $S$ along both dimensions are computed, this produces $S^{A}$ and $S^{B}$. Information in the two representations are then aggregated by:</p>
<p>$$
\begin{aligned}
h_{o q} &amp; =\left[h_{o_{t}}^{\prime} ; P ; h_{o_{t}}^{\prime} \odot P ; h_{o_{t}}^{\prime} \odot Q\right] \
P &amp; =S_{q} h_{q}^{r \top} \
Q &amp; =S_{q} S_{o_{t}}^{\top} h_{o_{t}}^{r \top}
\end{aligned}
$$</p>
<p>where $h_{o q}$ is aggregated observation representation.</p>
<p>On top of the attention layer, a stack of aggregation transformer blocks is used to further map the observation representations to action representations and answer representations. The structure of aggregation transformer blocks are the same as the encoder transformer blocks, except the kernel size of convolutional layer is 5 , and the number of blocks is 3 .</p>
<p>Let $M_{t} \in \mathbb{R}^{L^{o_{t}} \times H_{3}}$ denote the output of the stack of aggregation transformer blocks, where $H_{3}$ is 64 .</p>
<h2>A. 2 Command Generator</h2>
<p>The command generator takes the hidden representations $M_{t}$ as input, it estimates Q-values for all action, modifier, and object words, respectively. It consists of a shared Multi-layer Perceptron (MLP) and three MLPs for each of the components:</p>
<p>$$
\begin{aligned}
R_{t}=\operatorname{ReLU}( &amp; \left(L_{\text {shared }}\left(\operatorname{mean}\left(M_{t}\right)\right)\right. \
Q_{t, \text { action }} &amp; =L_{\text {action }}\left(R_{t}\right) \
Q_{t, \text { modifier }} &amp; =L_{\text {modifier }}\left(R_{t}\right) \
Q_{t, \text { object }} &amp; =L_{\text {object }}\left(R_{t}\right)
\end{aligned}
$$</p>
<p>In which, the output size of $L_{\text {shared }}$ is 64 ; the dimensionalities of the other 3 MLPs are depending on the number of the amount of action, modifier</p>
<p>and object words available, respectively. The overall Q-value is the sum of the three components:</p>
<p>$$
Q_{t}=Q_{t, \text { action }}+Q_{t, \text { modifier }}+Q_{t, \text { object }}
$$</p>
<h2>A. 3 Question Answerer</h2>
<p>Similar to (Yu et al., 2018), we append an extra stacks of aggregation transformer blocks on top of the aggregation transformer blocks to compute answer positions:</p>
<p>$$
\begin{aligned}
U &amp; =\operatorname{ReLU}\left(L_{0}\left[M_{t} ; M_{t}^{\prime}\right]\right) \
\beta &amp; =\operatorname{softmax}\left(L_{1}(U)\right)
\end{aligned}
$$</p>
<p>In which $M_{t}^{\prime} \in \mathbb{R}^{L^{\prime \prime} \times H_{3}}$ is output of the extra transformer stack, $L_{0}, L_{1}$ are trainable parameters with output size 64 and 1, respectively.</p>
<p>For location questions, the agent outputs $\beta$ as the probability distribution of each word in observation $o_{t}$ being the answer of the question.</p>
<p>For binary classification questions, we apply an MLP, which takes weighted sum of matching representations as input, to compute a probability distribution $p(y)$ over both possible answers:</p>
<p>$$
\begin{aligned}
D &amp; =\sum_{i}\left(\beta^{i} \cdot M_{t}^{\prime}\right) \
p(y) &amp; =\operatorname{softmax}\left(L_{4}\left(\tanh \left(L_{3}(D)\right)\right)\right.
\end{aligned}
$$</p>
<p>Output size of $L_{3}$ and $L_{4}$ are 64 and 2, respectively.</p>
<h2>A. 4 Deep Q-Learning</h2>
<p>In a text-based game, an agent takes an action $a^{4}$ in state $s$ by consulting a state-action value function $Q(s, a)$, this value function is as a measure of the action's expected long-term reward. Q-Learning helps the agent to learn an optimal $Q(s, a)$ value function. The agent starts from a random Qfunction, it gradually updates its Q-values by interacting with environment, and obtaining rewards. Following Mnih et al. (2015), the Q-value function is approximated with a deep neural network.</p>
<p>We make use of a replay buffer. During playing the game, we cache all transitions into the replay buffer without updating the parameters. We periodically sample a random batch of transitions from the replay buffer. In each transition, we update the parameters $\theta$ to reduce the discrepancy between the predicted value of current state $Q\left(s_{t}, a_{t}\right)$ and</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>the expected Q-value given the reward $r_{t}$ and the value of next state $\max <em t_1="t+1">{a} Q\left(s</em>, a\right)$.</p>
<p>We minimize the temporal difference (TD) error, $\delta$ :</p>
<p>$$
\delta=Q\left(s_{t}, a_{t}\right)-\left(r_{t}+\gamma \max <em t_1="t+1">{a} Q\left(s</em>, a\right)\right)
$$</p>
<p>in which, $\gamma$ indicates the discount factor. Following the common practice, we use the Huber loss to minimize the TD error. For a randomly sampled batch with batch size $B$, we minimize:</p>
<p>$$
\begin{aligned}
\mathcal{L} &amp; =\frac{1}{|B|} \sum \mathcal{L}(\delta) \
\text { where } \mathcal{L}(\delta) &amp; =\left{\begin{array}{ll}
\frac{1}{2} \delta^{2} &amp; \text { for }|\delta| \leq 1 \
|\delta|-\frac{1}{2} &amp; \text { otherwise }
\end{array}\right.
\end{aligned}
$$</p>
<p>As described in Section 3.3.1, we design the sufficient information bonus to teach an agent to stop as soon as it has gathered enough information to answer the question. Therefore we assign this reward at the game step where the agent generates wait command (or it is forced to stop).</p>
<p>It is worth mentioning that for attribute type questions (considerably the most difficult question type in QAit, where the training signal is very sparse), we provide extra rewards to help QADQN to learn.</p>
<p>Specifically, we take a reward similar to as used in location questions: 1.0 if the agent has observed the object mentioned in the question. we also use a reward similar to as used in existence questions: the agent is rewarded by the coverage of its exploration. The two extra rewards are finally added onto the sufficient information bonus for attribute question, both with coefficient of 0.1 .</p>
<h2>B Implementation Details</h2>
<p>During training with vanilla DQN, we use a replay memory of size 500,000 . We use $\epsilon$-greedy, where the value of $\epsilon$ anneals from 1.0 to 0.1 within 100,000 episodes. We start updating parameters after 1,000 episodes of playing. We update our network after every 20 game steps. During updating, we use a mini-batch of size 64 . We use Adam (Kingma and Ba, 2014) as the step rule for optimization, The learning rate is set to 0.00025 .</p>
<p>When our agent is trained with Rainbow algorithm, we follow Hessel et al. (2017) on most of the hyper-parameter settings. The four MLPs $L_{\text {shared }}, L_{\text {action }}, L_{\text {modifier }}$ and $L_{\text {object }}$ as described</p>
<p>in Eqn. 3 are Noisy Nets layers (Fortunato et al., 2017) when the agent is trained in Rainbow setting. Detailed hyper-parameter setting of our Rainbow agent are shown in Table 6.</p>
<table>
<thead>
<tr>
<th style="text-align: right;">Parameter</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: right;">Exploration $\epsilon$</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: right;">Noisy Nets $\sigma_{0}$</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: right;">Target Network Period</td>
<td style="text-align: center;">1000 episodes</td>
</tr>
<tr>
<td style="text-align: right;">Multi-step returns $n$</td>
<td style="text-align: center;">$n \sim$ Uniform[1,3]</td>
</tr>
<tr>
<td style="text-align: right;">Distributional atoms</td>
<td style="text-align: center;">51</td>
</tr>
<tr>
<td style="text-align: right;">Distributional min/max values</td>
<td style="text-align: center;">$[-10,10]$</td>
</tr>
</tbody>
</table>
<p>Table 6: Hyper-parameter setup for rainbow agent.
The model is implemented using PyTorch (Paszke et al., 2017).</p>
<h1>C Supported Text Commands</h1>
<p>All supported text commands are listed in Table 7.</p>
<h2>D Heuristic Conditions for Attribute Questions</h2>
<p>Here, we derived some heuristic conditions to determine when an agent has gathered enough information to answer a given attribute question. Those conditions are used as part of the reward shaping for our proposed agent (Section 3.3.1). In Table 8, for each attribute we list all the commands for which their outcome (pass or fail) gives enough information to answer the question correctly. Also, in order for a command's outcome to be informative, each command needs to be executed while some state conditions hold. For example, to determine if an object is indeed a heat source, the agent needs to try to cook something that is cookable and uncooked while standing next to the given object.</p>
<h2>E Full results</h2>
<p>We provide full results of our agents on fixed map games in Table 9, and provide full results of our agents on random map games in Table 10. To help investigating the generalizability of the sufficient information bonus we used in our proposed agent, we also report the rewards during both training and test phases. Note during test phase, we do not update parameters with the rewards.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Command</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">look <br> inventory <br> go 〈dir〉 <br> examine <br> open <br> close <br> eat <br> drink <br> drop <br> take <br> put <br> insert <br> cook <br> slice <br> chop <br> dice <br> wait</td>
<td style="text-align: left;">describe the current location <br> display the player's inventory <br> move the player to north, east, south, or west <br> examine something more closely <br> open a door or a container <br> close a door or a container <br> eat edible object <br> drink drinkable object <br> drop an object on the floor <br> take an object from the floor, a container, or a supporter <br> put an object onto a supporter (supporter must be present at the location) <br> insert an object into a container (container must be present at the location) <br> cook an object (heat source must be present at the location) <br> slice cuttable object (a sharp object must be in the player's inventory) <br> chop cuttable object (a sharp object must be in the player's inventory) <br> dice cuttable object (a sharp object must be in the player's inventory) <br> stop interaction</td>
</tr>
</tbody>
</table>
<p>Table 7: Supported command list.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Attribute</th>
<th style="text-align: center;">Command</th>
<th style="text-align: center;">State</th>
<th style="text-align: center;">Pass</th>
<th style="text-align: center;">Fail</th>
<th style="text-align: center;">Explanation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">sharp</td>
<td style="text-align: center;">cut cuttable</td>
<td style="text-align: center;">holding (cuttable) <br> \&amp; uncut (cuttable) <br> \&amp; holding (object)</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Trying to cut something cuttable that hasn't been cut yet while holding the object.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">take object</td>
<td style="text-align: center;">reachable(object)</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Sharp objects should be portable.</td>
</tr>
<tr>
<td style="text-align: center;">cuttable</td>
<td style="text-align: center;">cut object</td>
<td style="text-align: center;">holding (object) <br> \&amp; holding (sharp)</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Trying to cut the object while holding something sharp.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">take object</td>
<td style="text-align: center;">reachable (object)</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Cuttable object should be portable.</td>
</tr>
<tr>
<td style="text-align: center;">edible</td>
<td style="text-align: center;">eat object</td>
<td style="text-align: center;">holding (object)</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Trying to eat the object.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">take object</td>
<td style="text-align: center;">reachable (object)</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Edible objects should be portable.</td>
</tr>
<tr>
<td style="text-align: center;">drinkable</td>
<td style="text-align: center;">drink object</td>
<td style="text-align: center;">holding (object)</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Trying to drink the object.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">take object</td>
<td style="text-align: center;">reachable (object)</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Drinkable objects should be portable.</td>
</tr>
<tr>
<td style="text-align: center;">holder</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">on (portable, object)</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">Observing object(s) on a supporter.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">in (portable, object)</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">Observing object(s) inside a container.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">take object</td>
<td style="text-align: center;">reachable (object)</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">Holder objects should not be portable.</td>
</tr>
<tr>
<td style="text-align: center;">portable</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">holding (object)</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">Holding the object means it is portable.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">take object</td>
<td style="text-align: center;">reachable (object)</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Portable objects can be taken.</td>
</tr>
<tr>
<td style="text-align: center;">heat_source</td>
<td style="text-align: center;">cook cookable</td>
<td style="text-align: center;">holding (cookable) <br> \&amp; uncooked (cookable) <br> \&amp; reachable (object)</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Trying to cook something cookable that hasn't been cooked yet while being next to the object.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">take object</td>
<td style="text-align: center;">reachable (object)</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">Heat source objects should not be portable.</td>
</tr>
<tr>
<td style="text-align: center;">cookable</td>
<td style="text-align: center;">cook object</td>
<td style="text-align: center;">holding (object) <br> \&amp; reachable (heat_source)</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Trying to cook the object while being next to a heat source.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">take object</td>
<td style="text-align: center;">reachable(object)</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Cookable objects should be portable.</td>
</tr>
<tr>
<td style="text-align: center;">openable</td>
<td style="text-align: center;">open object</td>
<td style="text-align: center;">reachable (object) <br> \&amp; closed (object)</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Trying to open the closed object.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">close object</td>
<td style="text-align: center;">reachable (object) <br> \&amp; open (object)</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Trying to close the open object.</td>
</tr>
</tbody>
</table>
<p>Table 8: Heuristic conditions for determining whether the agent has enough information to answer a given attribute question. We use "object" to refer to the object mentioned in the question. Words in italics represents placeholder that can be replaced by any object from the environment that has the appropriate attribute (e.g. carrot could be used as a cuttable). The columns Pass and Fail represent how much reward the agent will receive given the corresponding command's outcome (resp. success or failure). NB: cut can mean any of the following commands: slice, dice, or chop</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Location</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Existence</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Attribute</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">Test</td>
</tr>
<tr>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">1.000</td>
</tr>
<tr>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">0.027</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">0.497</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">0.496</td>
</tr>
<tr>
<td style="text-align: center;">1 game</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DQN</td>
<td style="text-align: center;">$0.972(0.972)$</td>
<td style="text-align: center;">$0.122(0.160)$</td>
<td style="text-align: center;">$1.000(0.881)$</td>
<td style="text-align: center;">$0.628(0.124)$</td>
<td style="text-align: center;">$1.000(0.049)$</td>
<td style="text-align: center;">$0.500(0.035)$</td>
</tr>
<tr>
<td style="text-align: center;">DDQN</td>
<td style="text-align: center;">$0.960(0.960)$</td>
<td style="text-align: center;">$0.156(0.178)$</td>
<td style="text-align: center;">$1.000(0.647)$</td>
<td style="text-align: center;">$0.624(0.148)$</td>
<td style="text-align: center;">$1.000(0.023)$</td>
<td style="text-align: center;">$0.498(0.033)$</td>
</tr>
<tr>
<td style="text-align: center;">Rainbow</td>
<td style="text-align: center;">$0.562(0.562)$</td>
<td style="text-align: center;">$0.164(0.178)$</td>
<td style="text-align: center;">$1.000(0.187)$</td>
<td style="text-align: center;">$0.616(0.083)$</td>
<td style="text-align: center;">$1.000(0.049)$</td>
<td style="text-align: center;">$0.516(0.039)$</td>
</tr>
<tr>
<td style="text-align: center;">2 games</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DQN</td>
<td style="text-align: center;">$0.698(0.698)$</td>
<td style="text-align: center;">$0.168(0.182)$</td>
<td style="text-align: center;">$0.948(0.700)$</td>
<td style="text-align: center;">$0.574(0.136)$</td>
<td style="text-align: center;">$1.000(0.011)$</td>
<td style="text-align: center;">$0.510(0.028)$</td>
</tr>
<tr>
<td style="text-align: center;">DDQN</td>
<td style="text-align: center;">$0.702(0.702)$</td>
<td style="text-align: center;">$0.172(0.178)$</td>
<td style="text-align: center;">$0.882(0.571)$</td>
<td style="text-align: center;">$0.550(0.109)$</td>
<td style="text-align: center;">$1.000(0.098)$</td>
<td style="text-align: center;">$0.508(0.036)$</td>
</tr>
<tr>
<td style="text-align: center;">Rainbow</td>
<td style="text-align: center;">$0.734(0.734)$</td>
<td style="text-align: center;">$0.160(0.168)$</td>
<td style="text-align: center;">$0.878(0.287)$</td>
<td style="text-align: center;">$0.616(0.085)$</td>
<td style="text-align: center;">$1.000(0.030)$</td>
<td style="text-align: center;">$0.524(0.022)$</td>
</tr>
<tr>
<td style="text-align: center;">10 games</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DQN</td>
<td style="text-align: center;">$0.654(0.654)$</td>
<td style="text-align: center;">$0.180(0.188)$</td>
<td style="text-align: center;">$0.822(0.390)$</td>
<td style="text-align: center;">$0.568(0.156)$</td>
<td style="text-align: center;">$1.000(0.055)$</td>
<td style="text-align: center;">$0.518(0.030)$</td>
</tr>
<tr>
<td style="text-align: center;">DDQN</td>
<td style="text-align: center;">$0.608(0.608)$</td>
<td style="text-align: center;">$0.188(0.208)$</td>
<td style="text-align: center;">$0.842(0.479)$</td>
<td style="text-align: center;">$0.566(0.128)$</td>
<td style="text-align: center;">$1.000(0.064)$</td>
<td style="text-align: center;">$0.516(0.036)$</td>
</tr>
<tr>
<td style="text-align: center;">Rainbow</td>
<td style="text-align: center;">$0.616(0.616)$</td>
<td style="text-align: center;">$0.156(0.170)$</td>
<td style="text-align: center;">$0.768(0.266)$</td>
<td style="text-align: center;">$0.590(0.131)$</td>
<td style="text-align: center;">$0.998(0.059)$</td>
<td style="text-align: center;">$0.520(0.023)$</td>
</tr>
<tr>
<td style="text-align: center;">100 games</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DQN</td>
<td style="text-align: center;">$0.498(0.498)$</td>
<td style="text-align: center;">$0.194(0.206)$</td>
<td style="text-align: center;">$0.756(0.139)$</td>
<td style="text-align: center;">$0.614(0.160)$</td>
<td style="text-align: center;">$0.838(0.019)$</td>
<td style="text-align: center;">$0.498(0.014)$</td>
</tr>
<tr>
<td style="text-align: center;">DDQN</td>
<td style="text-align: center;">$0.456(0.458)$</td>
<td style="text-align: center;">$0.168(0.196)$</td>
<td style="text-align: center;">$0.768(0.134)$</td>
<td style="text-align: center;">$0.650(0.216)$</td>
<td style="text-align: center;">$0.878(0.020)$</td>
<td style="text-align: center;">$0.528(0.017)$</td>
</tr>
<tr>
<td style="text-align: center;">Rainbow</td>
<td style="text-align: center;">$0.340(0.340)$</td>
<td style="text-align: center;">$0.156(0.160)$</td>
<td style="text-align: center;">$0.762(0.129)$</td>
<td style="text-align: center;">$0.602(0.207)$</td>
<td style="text-align: center;">$0.924(0.044)$</td>
<td style="text-align: center;">$0.524(0.022)$</td>
</tr>
<tr>
<td style="text-align: center;">500 games</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DQN</td>
<td style="text-align: center;">$0.430(0.430)$</td>
<td style="text-align: center;">$0.224(0.244)$</td>
<td style="text-align: center;">$0.742(0.136)$</td>
<td style="text-align: center;">$0.674(0.279)$</td>
<td style="text-align: center;">$0.700(0.015)$</td>
<td style="text-align: center;">0.534(0.014)</td>
</tr>
<tr>
<td style="text-align: center;">DDQN</td>
<td style="text-align: center;">$0.406(0.406)$</td>
<td style="text-align: center;">$0.218(0.228)$</td>
<td style="text-align: center;">$0.734(0.173)$</td>
<td style="text-align: center;">$0.626(0.213)$</td>
<td style="text-align: center;">$0.714(0.021)$</td>
<td style="text-align: center;">$0.508(0.026)$</td>
</tr>
<tr>
<td style="text-align: center;">Rainbow</td>
<td style="text-align: center;">$0.358(0.358)$</td>
<td style="text-align: center;">$0.190(0.196)$</td>
<td style="text-align: center;">$0.768(0.187)$</td>
<td style="text-align: center;">$0.656(0.207)$</td>
<td style="text-align: center;">$0.736(0.032)$</td>
<td style="text-align: center;">$0.496(0.029)$</td>
</tr>
<tr>
<td style="text-align: center;">unlimited games</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DQN</td>
<td style="text-align: center;">$0.300(0.300)$</td>
<td style="text-align: center;">$0.216(0.216)$</td>
<td style="text-align: center;">$0.752(0.119)$</td>
<td style="text-align: center;">$0.662(0.246)$</td>
<td style="text-align: center;">$0.562(0.034)$</td>
<td style="text-align: center;">$0.514(0.016)$</td>
</tr>
<tr>
<td style="text-align: center;">DDQN</td>
<td style="text-align: center;">$0.318(0.318)$</td>
<td style="text-align: center;">$0.258(0.258)$</td>
<td style="text-align: center;">$0.744(0.168)$</td>
<td style="text-align: center;">$0.628(0.134)$</td>
<td style="text-align: center;">$0.572(0.027)$</td>
<td style="text-align: center;">$0.480(0.024)$</td>
</tr>
<tr>
<td style="text-align: center;">Rainbow</td>
<td style="text-align: center;">$0.316(0.330)$</td>
<td style="text-align: center;">0.280(0.280)</td>
<td style="text-align: center;">$0.734(0.157)$</td>
<td style="text-align: center;">0.692(0.157)</td>
<td style="text-align: center;">$0.566(0.017)$</td>
<td style="text-align: center;">$0.514(0.014)$</td>
</tr>
</tbody>
</table>
<p>Table 9: Agent performance on fixed map games. Accuracies in percentage are shown in black. We also investigate the sufficient information bonus used in our agent proposed in Section 3.3.1, which are shown in blue.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Location</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Existence</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Attribute</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">Test</td>
</tr>
<tr>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.750</td>
</tr>
<tr>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.034</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.500</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.499</td>
</tr>
<tr>
<td style="text-align: center;">2 games</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DQN</td>
<td style="text-align: center;">$0.990(0.990)$</td>
<td style="text-align: center;">$0.148(0.162)$</td>
<td style="text-align: center;">$1.000(0.779)$</td>
<td style="text-align: center;">$0.638(0.157)$</td>
<td style="text-align: center;">$1.000(0.039)$</td>
<td style="text-align: center;">$0.534(0.033)$</td>
</tr>
<tr>
<td style="text-align: center;">DDQN</td>
<td style="text-align: center;">$0.978(0.978)$</td>
<td style="text-align: center;">$0.146(0.152)$</td>
<td style="text-align: center;">$1.000(0.727)$</td>
<td style="text-align: center;">$0.602(0.158)$</td>
<td style="text-align: center;">$1.000(0.043)$</td>
<td style="text-align: center;">$\mathbf{0 . 5 4 4 ( 0 . 0 3 2 )}$</td>
</tr>
<tr>
<td style="text-align: center;">Rainbow</td>
<td style="text-align: center;">$0.916(0.916)$</td>
<td style="text-align: center;">$0.178(0.178)$</td>
<td style="text-align: center;">$0.972(0.314)$</td>
<td style="text-align: center;">$0.602(0.136)$</td>
<td style="text-align: center;">$1.000(0.025)$</td>
<td style="text-align: center;">$0.512(0.021)$</td>
</tr>
<tr>
<td style="text-align: center;">10 games</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DQN</td>
<td style="text-align: center;">$0.818(0.818)$</td>
<td style="text-align: center;">$0.156(0.160)$</td>
<td style="text-align: center;">$0.898(0.607)$</td>
<td style="text-align: center;">$0.566(0.142)$</td>
<td style="text-align: center;">$1.000(0.056)$</td>
<td style="text-align: center;">$0.518(0.036)$</td>
</tr>
<tr>
<td style="text-align: center;">DDQN</td>
<td style="text-align: center;">$0.794(0.794)$</td>
<td style="text-align: center;">$0.142(0.154)$</td>
<td style="text-align: center;">$0.868(0.575)$</td>
<td style="text-align: center;">$0.606(0.153)$</td>
<td style="text-align: center;">$1.000(0.037)$</td>
<td style="text-align: center;">$0.500(0.033)$</td>
</tr>
<tr>
<td style="text-align: center;">Rainbow</td>
<td style="text-align: center;">$0.670(0.670)$</td>
<td style="text-align: center;">$0.144(0.170)$</td>
<td style="text-align: center;">$0.828(0.468)$</td>
<td style="text-align: center;">$0.586(0.128)$</td>
<td style="text-align: center;">$1.000(0.071)$</td>
<td style="text-align: center;">$0.530(0.018)$</td>
</tr>
<tr>
<td style="text-align: center;">100 games</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DQN</td>
<td style="text-align: center;">$0.550(0.550)$</td>
<td style="text-align: center;">$0.184(0.204)$</td>
<td style="text-align: center;">$0.758(0.230)$</td>
<td style="text-align: center;">$0.668(0.181)$</td>
<td style="text-align: center;">$0.878(0.021)$</td>
<td style="text-align: center;">$0.524(0.017)$</td>
</tr>
<tr>
<td style="text-align: center;">DDQN</td>
<td style="text-align: center;">$0.524(0.524)$</td>
<td style="text-align: center;">$0.188(0.204)$</td>
<td style="text-align: center;">$0.754(0.365)$</td>
<td style="text-align: center;">$0.662(0.205)$</td>
<td style="text-align: center;">$0.890(0.025)$</td>
<td style="text-align: center;">$\mathbf{0 . 5 4 4 ( 0 . 0 1 9 )}$</td>
</tr>
<tr>
<td style="text-align: center;">Rainbow</td>
<td style="text-align: center;">$0.442(0.442)$</td>
<td style="text-align: center;">$0.174(0.184)$</td>
<td style="text-align: center;">$0.754(0.285)$</td>
<td style="text-align: center;">$0.654(0.190)$</td>
<td style="text-align: center;">$0.878(0.044)$</td>
<td style="text-align: center;">$0.504(0.032)$</td>
</tr>
<tr>
<td style="text-align: center;">500 games</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DQN</td>
<td style="text-align: center;">$0.430(0.430)$</td>
<td style="text-align: center;">$0.204(0.216)$</td>
<td style="text-align: center;">$0.752(0.162)$</td>
<td style="text-align: center;">$0.678(0.214)$</td>
<td style="text-align: center;">$0.678(0.019)$</td>
<td style="text-align: center;">$0.530(0.017)$</td>
</tr>
<tr>
<td style="text-align: center;">DDQN</td>
<td style="text-align: center;">$0.458(0.458)$</td>
<td style="text-align: center;">$0.222(0.246)$</td>
<td style="text-align: center;">$0.754(0.158)$</td>
<td style="text-align: center;">$0.656(0.188)$</td>
<td style="text-align: center;">$0.716(0.024)$</td>
<td style="text-align: center;">$0.486(0.023)$</td>
</tr>
<tr>
<td style="text-align: center;">Rainbow</td>
<td style="text-align: center;">$0.370(0.370)$</td>
<td style="text-align: center;">$0.172(0.178)$</td>
<td style="text-align: center;">$0.748(0.275)$</td>
<td style="text-align: center;">$0.678(0.191)$</td>
<td style="text-align: center;">$0.636(0.020)$</td>
<td style="text-align: center;">$0.494(0.017)$</td>
</tr>
<tr>
<td style="text-align: center;">unlimited games</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DQN</td>
<td style="text-align: center;">$0.316(0.316)$</td>
<td style="text-align: center;">$0.188(0.188)$</td>
<td style="text-align: center;">$0.728(0.213)$</td>
<td style="text-align: center;">$0.668(0.218)$</td>
<td style="text-align: center;">$0.812(0.055)$</td>
<td style="text-align: center;">$0.506(0.018)$</td>
</tr>
<tr>
<td style="text-align: center;">DDQN</td>
<td style="text-align: center;">$0.326(0.326)$</td>
<td style="text-align: center;">$0.206(0.206)$</td>
<td style="text-align: center;">$0.740(0.246)$</td>
<td style="text-align: center;">$\mathbf{0 . 6 9 4 ( 0 . 1 9 6 )}$</td>
<td style="text-align: center;">$0.580(0.023)$</td>
<td style="text-align: center;">$0.482(0.017)$</td>
</tr>
<tr>
<td style="text-align: center;">Rainbow</td>
<td style="text-align: center;">$0.340(0.340)$</td>
<td style="text-align: center;">$\mathbf{0 . 2 5 8 ( 0 . 2 5 8 )}$</td>
<td style="text-align: center;">$0.728(0.210)$</td>
<td style="text-align: center;">$0.686(0.193)$</td>
<td style="text-align: center;">$0.564(0.018)$</td>
<td style="text-align: center;">$0.470(0.017)$</td>
</tr>
</tbody>
</table>
<p>Table 10: Agent performance on random map games. Accuracies in percentage are shown in black. We also investigate the sufficient information bonus used in our agent proposed in Section 3.3.1, which are shown in blue.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Training accuracy over episodes on location questions. Upper row: fixed map, 1/2/10/100/500/unlimited games; Lower row: random map, 2/10/100/500/unlimited games.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Training accuracy over episodes on existence questions. Upper row: fixed map, 1/2/10/100/500/unlimited games; Lower row: random map, 2/10/100/500/unlimited games.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Training accuracy over episodes on attribute questions. Upper row: fixed map, 1/2/10/100/500/unlimited games; Lower row: random map, 2/10/100/500/unlimited games.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ In our case, $a$ is a triplet contains {action, modifier, object} as described in Section 2.4.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
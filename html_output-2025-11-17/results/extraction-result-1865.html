<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1865 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1865</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1865</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-35.html">extraction-schema-35</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use computational predictions, proxy metrics, or surrogate objectives followed by experimental or ground-truth validation, including quantitative measures of agreement, disagreement, false positive rates, and factors affecting prediction accuracy.</div>
                <p><strong>Paper ID:</strong> paper-278016502</p>
                <p><strong>Paper Title:</strong> A review of machine learning methods for imbalanced data challenges in chemistry</p>
                <p><strong>Paper Abstract:</strong> Imbalanced data, where certain classes are significantly underrepresented in a dataset, is a widespread machine learning (ML) challenge across various fields of chemistry, yet it remains inadequately addressed. This data imbalance can lead to biased ML or deep learning (DL) models, which fail to accurately predict the underrepresented classes, thus limiting the robustness and applicability of these models. With the rapid advancement of ML and DL algorithms, several promising solutions to this issue have emerged, prompting the need for a comprehensive review of current methodologies. In this review, we examine the prominent ML approaches used to tackle the imbalanced data challenge in different areas of chemistry, including resampling techniques, data augmentation techniques, algorithmic approaches, and feature engineering strategies. Each of these methods is evaluated in the context of its application across various aspects of chemistry, such as drug discovery, materials science, cheminformatics, and catalysis. We also explore future directions for overcoming the imbalanced data challenge and emphasize data augmentation via physical models, large language models (LLMs), and advanced mathematics. The benefit of balanced data in new material design and production and the persistent challenges are discussed. Overall, this review aims to elucidate the prevalent ML techniques applied to mitigate the impacts of imbalanced data within the field of chemistry and offer insights into future directions for research and application.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1865.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1865.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use computational predictions, proxy metrics, or surrogate objectives followed by experimental or ground-truth validation, including quantitative measures of agreement, disagreement, false positive rates, and factors affecting prediction accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMOTE (catalyst/arsenene)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Synthetic Minority Over-sampling Technique applied to heteroatom-doped arsenene catalyst screening</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>SMOTE was used to balance a dataset of heteroatom-doped arsenenes split by a computational proxy (|ΔG_H| threshold 0.2 eV) to improve ML screening for hydrogen evolution catalysts; dataset counts and threshold are reported but no experimental validation metrics are provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An ensemble learning classiﬁer to discover arsenene catalysts with implanted heteroatoms for hydrogen evolution reaction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>SMOTE-based ML screening (with |ΔG_H| threshold)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>catalyst design / materials science / computational electrocatalysis</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Gibbs free energy change for hydrogen adsorption (|ΔG_H|) used as a proxy for hydrogen-evolution activity; dataset partition used a threshold of |ΔG_H| = 0.2 eV to define two classes (active/inactive) for ML classification.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_type</strong></td>
                            <td>physics-based simulation (DFT-derived descriptor) used as proxy for catalytic activity</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_extrapolation_distance</strong></td>
                            <td>Screening over a set of 126 heteroatom-doped arsenenes (computational dataset); extrapolation beyond these compositions not quantified in review.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>Not explicitly discussed; described as candidate screening (incremental lead-identification) rather than demonstration of breakthrough experimental catalysts in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>bias_correction_methods</strong></td>
                            <td>SMOTE used to rebalance class distribution to reduce ML training bias; no multifidelity bias-correction described in review.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_or_maturity_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Choice of ΔG_H threshold and limited sample counts (88 vs 38) influence class definitions and ML training; review notes sensitivity to sample imbalance and noisy labels in such tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>126 total heteroatom-doped arsenenes; split by |ΔG_H| threshold into 88 and 38 samples before oversampling</td>
                        </tr>
                        <tr>
                            <td><strong>cost_or_resource_discussion</strong></td>
                            <td>Review notes SMOTE and DFT descriptors have different cost profiles (DFT is expensive); no explicit cost numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>exceptional_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_discussion</strong></td>
                            <td>Review highlights that using a single DFT-derived threshold as a proxy may misrepresent experimental activity and that oversampling may introduce noise; no quantitative validation given.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1865.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1865.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use computational predictions, proxy metrics, or surrogate objectives followed by experimental or ground-truth validation, including quantitative measures of agreement, disagreement, false positive rates, and factors affecting prediction accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMOTE (polymer/molecular-dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SMOTE interpolation applied to molecular-dynamics-derived polymer mechanical property data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>SMOTE was applied to molecular-dynamics training data to interpolate sample boundaries and mitigate minority-class scarcity when predicting tensile stress of natural rubber; review reports the approach but does not report experimental-vs-prediction agreement metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A machine learning framework to predict the tensile stress of natural rubber: Based on molecular dynamics simulation data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>SMOTE-augmented ML model trained on MD simulation data</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>materials science / polymer mechanics</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Tensile stress values from molecular dynamics simulations used as target labels/proxy for mechanical performance; SMOTE generates synthetic minority-region samples to balance training data.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_type</strong></td>
                            <td>physics-based simulation (molecular dynamics) feeding a data-driven ML model</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_extrapolation_distance</strong></td>
                            <td>Training and augmentation performed within the MD-sampled conformational/material space; review does not quantify out-of-distribution prediction performance.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>Positioned as improving predictive performance on scarce-sample regimes (incremental methodological improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>bias_correction_methods</strong></td>
                            <td>SMOTE to rebalance training distribution; no multifidelity correction reported.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_or_maturity_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>MD-derived labels may not capture experimental complexities (e.g., processing-induced microstructure), which the review flags as a limitation of pure simulation-based augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>Original MD data from 23 rubber materials expanded to 483 via nearest-neighbor interpolation before SMOTE in polymer materials example (as described in review figure); exact MD/simulation counts for tensile stress study not provided in-text.</td>
                        </tr>
                        <tr>
                            <td><strong>cost_or_resource_discussion</strong></td>
                            <td>Review notes MD and SMOTE are computationally intensive relative to simple oversampling; no numerical cost figures provided.</td>
                        </tr>
                        <tr>
                            <td><strong>exceptional_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_discussion</strong></td>
                            <td>Review cautions that simulation-derived proxies may omit experimental effects and that SMOTE can introduce noise and poor-quality synthetic samples if minority-class internal structure is complex.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1865.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1865.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use computational predictions, proxy metrics, or surrogate objectives followed by experimental or ground-truth validation, including quantitative measures of agreement, disagreement, false positive rates, and factors affecting prediction accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GAN (antiviral peptides)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Adversarial Network for antiviral peptide data augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GANs were trained on a minority class of antiviral peptides to generate synthetic AVP-like sequences to rebalance an imbalanced dataset (2934 AVPs vs 17,184 non-AVPs); the review reports dataset sizes and augmentation but does not provide experimental wet-lab validation rates or FP/FN statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Developing an Antiviral Peptides Predictor with Generative Adversarial Network Data Augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>GAN-based data augmentation for peptide activity prediction</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>drug discovery / peptide bioactivity prediction</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>In silico classification of peptide sequences as antiviral based on sequence-derived features; GAN generates synthetic minority-class sequences to train classifiers (proxy = predicted antiviral activity).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_type</strong></td>
                            <td>data-driven ML (generative model producing synthetic training examples)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_extrapolation_distance</strong></td>
                            <td>Generation focused on producing AVP-like sequences within training distribution; review does not report OOD or novel scaffold performance.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>Applied to improve classifier performance on underrepresented peptide class (incremental dataset-augmentation strategy).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>bias_correction_methods</strong></td>
                            <td>Augmentation via GAN to reduce class imbalance; review warns of GAN training instability and mode collapse risk which can bias synthetic coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_or_maturity_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Sequence diversity and biological assay noise can limit how well GAN-generated sequences map to true antiviral activity; review notes unstable GAN training can impair minority-class representation.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>2934 antiviral peptides (minority) and 17,184 non-antiviral peptides (majority) before augmentation as reported in review example.</td>
                        </tr>
                        <tr>
                            <td><strong>cost_or_resource_discussion</strong></td>
                            <td>GAN training is computationally intensive; review notes training instability and resource needs but gives no quantitative cost figures.</td>
                        </tr>
                        <tr>
                            <td><strong>exceptional_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_discussion</strong></td>
                            <td>Review highlights GAN mode collapse and unstable training as limitations; no experimental validation rates or FP/FN numbers are provided in the review.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1865.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1865.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use computational predictions, proxy metrics, or surrogate objectives followed by experimental or ground-truth validation, including quantitative measures of agreement, disagreement, false positive rates, and factors affecting prediction accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NearMiss-2 (malonylation sites)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NearMiss-2 undersampling applied to protein malonylation site prediction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>NearMiss-2 was used to reduce majority-class samples while preserving ones close to minority-class entries, improving a deep-learning model (Malsite-Deep) for malonylation-site prediction; review provides dataset counts (4,242 positive, 71,809 negative) but no explicit experimental validation performance metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Malsite-deep: prediction of protein malonylation sites through deep learning and multi-information fusion based on NearMiss-2 strategy</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>NearMiss-2 undersampling + deep learning (Malsite-Deep)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>protein post-translational modification prediction / proteomics</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>In silico prediction of malonylation sites (binary site classifier) trained on sequence-derived features; undersampling selects majority negatives nearest to positives in feature space.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_type</strong></td>
                            <td>data-driven ML (classification on sequence features)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Curated experimentally-annotated malonylation sites (used as labels in training/validation) — review implies experimental annotation as ground truth but reports no validation statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_extrapolation_distance</strong></td>
                            <td>Model trained on annotated sites (in-distribution); review does not report out-of-distribution or cross-species generalization metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>Described as improving prediction accuracy in class-imbalanced proteomic datasets (incremental improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>bias_correction_methods</strong></td>
                            <td>NearMiss-2 undersampling to reduce majority dominance; no multifidelity correction discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_or_maturity_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>High class imbalance ratios and sequence homology can affect performance; review notes proximity-based undersampling may not capture complex non-linear relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>4,242 minority (malonylation sites) and 71,809 majority (non-sites) as stated in review example.</td>
                        </tr>
                        <tr>
                            <td><strong>cost_or_resource_discussion</strong></td>
                            <td>NearMiss is computationally efficient for high-dimensional data but may risk information loss; no numeric cost data.</td>
                        </tr>
                        <tr>
                            <td><strong>exceptional_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_discussion</strong></td>
                            <td>Review warns NearMiss can discard valuable majority-sample information and may struggle with complex non-linear feature relationships.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1865.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1865.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use computational predictions, proxy metrics, or surrogate objectives followed by experimental or ground-truth validation, including quantitative measures of agreement, disagreement, false positive rates, and factors affecting prediction accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cost-sensitive XGBoost (INFLAMeR)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cost-sensitive XGBoost classifier used in INFLAMeR for lncRNA detection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>INFLAMeR employed a cost-sensitive XGBoost to handle extreme class imbalance (example ratio 1:55) when identifying functional lncRNAs from genomic data; review describes method and imbalance ratio but does not provide explicit experimental validation numeric gaps in this article.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Integration of transcription regulation and functional genomic data reveals lncRNA SNHG6's role in hematopoietic differentiation and leukemia</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Cost-sensitive XGBoost</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>genomics / transcriptomics / functional lncRNA prediction</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>In silico classifier predicting functional lncRNAs from genomic and regulatory features; cost-sensitive weighting used to penalize minority-class misclassification more heavily.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_type</strong></td>
                            <td>data-driven ML (cost-weighted classifier)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Experimentally annotated functional lncRNAs from genomics/transcriptomics studies used as labels; review does not report post-hoc experimental validation success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_extrapolation_distance</strong></td>
                            <td>Addresses rare functional lncRNAs (extreme imbalance); review does not quantify OOD performance.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>Presented as a method to improve detection of rare functional lncRNAs (incremental pragmatic approach).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>bias_correction_methods</strong></td>
                            <td>Cost-sensitive weighting within XGBoost to rebalance effective training influence rather than resampling; no multifidelity recalibration discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_or_maturity_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Very high class imbalance (1:55) and label noise in genomic annotations; review highlights the importance of cost setting and risk of overfitting minority class if mis-specified.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>Imbalanced genomic example reported as minority:majority = 1:55 in the review figure.</td>
                        </tr>
                        <tr>
                            <td><strong>cost_or_resource_discussion</strong></td>
                            <td>XGBoost is computationally efficient for high-dimensional genomics compared with heavy sampling; no quantitative cost comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>exceptional_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_discussion</strong></td>
                            <td>Review cautions improper cost setting can cause overfitting to minority class and reduce generalization; no numeric validation stats provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1865.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1865.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use computational predictions, proxy metrics, or surrogate objectives followed by experimental or ground-truth validation, including quantitative measures of agreement, disagreement, false positive rates, and factors affecting prediction accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Physical-model augmentation (DFT/MD/docking)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Physical-model-based data augmentation using DFT, molecular dynamics, and molecular docking to create virtual training data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The review discusses using physics-based simulations (DFT, MD, docking) as proxies to generate augmentation data for ML (e.g., binding free energies, conformations, reaction energetics), noting potential to fill underrepresented regions but also limitations and lack of standardized validation reported in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>DFT/MD/docking-based data augmentation pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>computational chemistry / drug discovery / catalysis / materials</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Simulation-derived quantities such as DFT-predicted binding free energies, adsorption energies, electronic structure descriptors, MD-sampled conformations and docking scores used as surrogate labels or features for ML training and augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_type</strong></td>
                            <td>physics-based simulation</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Experimental measurements (e.g., binding assays, catalytic activity, spectroscopy, mechanical testing) are proposed as ground truth in discussion, but the review does not present paired quantitative comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_extrapolation_distance</strong></td>
                            <td>Used to generate rare/underrepresented conformations or reaction scenarios; review suggests MD/DFT can cover space not present in experiments but gives no quantitative OOD metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>Characterized as promising future direction to improve augmentation for hard-to-observe minority classes (potentially transformational if integrated well).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_uncertainty</strong></td>
                            <td>Review notes need for integrating physical laws into ML for better interpretability and calls for careful calibration; no specific calibration metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_correction_methods</strong></td>
                            <td>Proposal to integrate physics-based augmentation with ML and multifidelity approaches; review calls for further development but describes no implemented bias-correction with numerical results.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_or_maturity_effects</strong></td>
                            <td>Review states physical-model augmentation is an emerging trend and still nascent; no time-series performance numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Long-timescale dynamics, rare conformations, and solvent/temperature effects are identified as factors that limit fidelity of simulations to experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_comparison</strong></td>
                            <td>Review contrasts DFT, MD, and docking qualitatively (different strengths: energetics vs conformations vs binding modes) but does not present quantitative cross-proxy error comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cost_or_resource_discussion</strong></td>
                            <td>Review acknowledges DFT and MD are computationally expensive relative to simpler proxies; no numeric cost or walltime figures provided.</td>
                        </tr>
                        <tr>
                            <td><strong>exceptional_cases</strong></td>
                            <td>Review references cases where DFT-derived descriptors are useful (e.g., adsorption energies correlate with catalytic trends) but does not quantify agreement in this article.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_discussion</strong></td>
                            <td>Review emphasizes that simulated proxies may omit experimental complexities, parameter sensitivity, and that surrogate-to-experiment gaps remain a key challenge requiring multifidelity and validation frameworks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Heterogeneous catalyst design by generative adversarial network and first-principles based microkinetics <em>(Rating: 2)</em></li>
                <li>Machine Learning Boosted Entropy-Engineered Synthesis of stable Nanometric Solid Solution CuCo Alloys for Efficient Nitrate Reduction to Ammonia <em>(Rating: 2)</em></li>
                <li>Designing catalysts with deep generative models and computational data. A case study for Suzuki cross coupling reactions. <em>(Rating: 2)</em></li>
                <li>A machine learning framework to predict the tensile stress of natural rubber: Based on molecular dynamics simulation data <em>(Rating: 2)</em></li>
                <li>Developing an Antiviral Peptides Predictor with Generative Adversarial Network Data Augmentation <em>(Rating: 1)</em></li>
                <li>Malsite-deep: prediction of protein malonylation sites through deep learning and multi-information fusion based on NearMiss-2 strategy <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1865",
    "paper_id": "paper-278016502",
    "extraction_schema_id": "extraction-schema-35",
    "extracted_data": [
        {
            "name_short": "SMOTE (catalyst/arsenene)",
            "name_full": "Synthetic Minority Over-sampling Technique applied to heteroatom-doped arsenene catalyst screening",
            "brief_description": "SMOTE was used to balance a dataset of heteroatom-doped arsenenes split by a computational proxy (|ΔG_H| threshold 0.2 eV) to improve ML screening for hydrogen evolution catalysts; dataset counts and threshold are reported but no experimental validation metrics are provided in this review.",
            "citation_title": "An ensemble learning classiﬁer to discover arsenene catalysts with implanted heteroatoms for hydrogen evolution reaction",
            "mention_or_use": "use",
            "system_or_method_name": "SMOTE-based ML screening (with |ΔG_H| threshold)",
            "domain": "catalyst design / materials science / computational electrocatalysis",
            "proxy_metric_description": "Gibbs free energy change for hydrogen adsorption (|ΔG_H|) used as a proxy for hydrogen-evolution activity; dataset partition used a threshold of |ΔG_H| = 0.2 eV to define two classes (active/inactive) for ML classification.",
            "proxy_type": "physics-based simulation (DFT-derived descriptor) used as proxy for catalytic activity",
            "ground_truth_description": null,
            "quantitative_gap_measure": null,
            "proxy_performance": null,
            "ground_truth_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_or_extrapolation_distance": "Screening over a set of 126 heteroatom-doped arsenenes (computational dataset); extrapolation beyond these compositions not quantified in review.",
            "gap_varies_with_novelty": null,
            "gap_variation_details": null,
            "incremental_vs_transformational": "Not explicitly discussed; described as candidate screening (incremental lead-identification) rather than demonstration of breakthrough experimental catalysts in this review.",
            "calibration_or_uncertainty": null,
            "bias_correction_methods": "SMOTE used to rebalance class distribution to reduce ML training bias; no multifidelity bias-correction described in review.",
            "temporal_or_maturity_effects": null,
            "domain_specific_factors": "Choice of ΔG_H threshold and limited sample counts (88 vs 38) influence class definitions and ML training; review notes sensitivity to sample imbalance and noisy labels in such tasks.",
            "multiple_proxy_comparison": null,
            "sample_size": "126 total heteroatom-doped arsenenes; split by |ΔG_H| threshold into 88 and 38 samples before oversampling",
            "cost_or_resource_discussion": "Review notes SMOTE and DFT descriptors have different cost profiles (DFT is expensive); no explicit cost numbers provided.",
            "exceptional_cases": null,
            "limitations_discussion": "Review highlights that using a single DFT-derived threshold as a proxy may misrepresent experimental activity and that oversampling may introduce noise; no quantitative validation given.",
            "uuid": "e1865.0"
        },
        {
            "name_short": "SMOTE (polymer/molecular-dynamics)",
            "name_full": "SMOTE interpolation applied to molecular-dynamics-derived polymer mechanical property data",
            "brief_description": "SMOTE was applied to molecular-dynamics training data to interpolate sample boundaries and mitigate minority-class scarcity when predicting tensile stress of natural rubber; review reports the approach but does not report experimental-vs-prediction agreement metrics.",
            "citation_title": "A machine learning framework to predict the tensile stress of natural rubber: Based on molecular dynamics simulation data",
            "mention_or_use": "use",
            "system_or_method_name": "SMOTE-augmented ML model trained on MD simulation data",
            "domain": "materials science / polymer mechanics",
            "proxy_metric_description": "Tensile stress values from molecular dynamics simulations used as target labels/proxy for mechanical performance; SMOTE generates synthetic minority-region samples to balance training data.",
            "proxy_type": "physics-based simulation (molecular dynamics) feeding a data-driven ML model",
            "ground_truth_description": null,
            "quantitative_gap_measure": null,
            "proxy_performance": null,
            "ground_truth_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_or_extrapolation_distance": "Training and augmentation performed within the MD-sampled conformational/material space; review does not quantify out-of-distribution prediction performance.",
            "gap_varies_with_novelty": null,
            "gap_variation_details": null,
            "incremental_vs_transformational": "Positioned as improving predictive performance on scarce-sample regimes (incremental methodological improvement).",
            "calibration_or_uncertainty": null,
            "bias_correction_methods": "SMOTE to rebalance training distribution; no multifidelity correction reported.",
            "temporal_or_maturity_effects": null,
            "domain_specific_factors": "MD-derived labels may not capture experimental complexities (e.g., processing-induced microstructure), which the review flags as a limitation of pure simulation-based augmentation.",
            "multiple_proxy_comparison": null,
            "sample_size": "Original MD data from 23 rubber materials expanded to 483 via nearest-neighbor interpolation before SMOTE in polymer materials example (as described in review figure); exact MD/simulation counts for tensile stress study not provided in-text.",
            "cost_or_resource_discussion": "Review notes MD and SMOTE are computationally intensive relative to simple oversampling; no numerical cost figures provided.",
            "exceptional_cases": null,
            "limitations_discussion": "Review cautions that simulation-derived proxies may omit experimental effects and that SMOTE can introduce noise and poor-quality synthetic samples if minority-class internal structure is complex.",
            "uuid": "e1865.1"
        },
        {
            "name_short": "GAN (antiviral peptides)",
            "name_full": "Generative Adversarial Network for antiviral peptide data augmentation",
            "brief_description": "GANs were trained on a minority class of antiviral peptides to generate synthetic AVP-like sequences to rebalance an imbalanced dataset (2934 AVPs vs 17,184 non-AVPs); the review reports dataset sizes and augmentation but does not provide experimental wet-lab validation rates or FP/FN statistics.",
            "citation_title": "Developing an Antiviral Peptides Predictor with Generative Adversarial Network Data Augmentation",
            "mention_or_use": "use",
            "system_or_method_name": "GAN-based data augmentation for peptide activity prediction",
            "domain": "drug discovery / peptide bioactivity prediction",
            "proxy_metric_description": "In silico classification of peptide sequences as antiviral based on sequence-derived features; GAN generates synthetic minority-class sequences to train classifiers (proxy = predicted antiviral activity).",
            "proxy_type": "data-driven ML (generative model producing synthetic training examples)",
            "ground_truth_description": null,
            "quantitative_gap_measure": null,
            "proxy_performance": null,
            "ground_truth_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_or_extrapolation_distance": "Generation focused on producing AVP-like sequences within training distribution; review does not report OOD or novel scaffold performance.",
            "gap_varies_with_novelty": null,
            "gap_variation_details": null,
            "incremental_vs_transformational": "Applied to improve classifier performance on underrepresented peptide class (incremental dataset-augmentation strategy).",
            "calibration_or_uncertainty": null,
            "bias_correction_methods": "Augmentation via GAN to reduce class imbalance; review warns of GAN training instability and mode collapse risk which can bias synthetic coverage.",
            "temporal_or_maturity_effects": null,
            "domain_specific_factors": "Sequence diversity and biological assay noise can limit how well GAN-generated sequences map to true antiviral activity; review notes unstable GAN training can impair minority-class representation.",
            "multiple_proxy_comparison": null,
            "sample_size": "2934 antiviral peptides (minority) and 17,184 non-antiviral peptides (majority) before augmentation as reported in review example.",
            "cost_or_resource_discussion": "GAN training is computationally intensive; review notes training instability and resource needs but gives no quantitative cost figures.",
            "exceptional_cases": null,
            "limitations_discussion": "Review highlights GAN mode collapse and unstable training as limitations; no experimental validation rates or FP/FN numbers are provided in the review.",
            "uuid": "e1865.2"
        },
        {
            "name_short": "NearMiss-2 (malonylation sites)",
            "name_full": "NearMiss-2 undersampling applied to protein malonylation site prediction",
            "brief_description": "NearMiss-2 was used to reduce majority-class samples while preserving ones close to minority-class entries, improving a deep-learning model (Malsite-Deep) for malonylation-site prediction; review provides dataset counts (4,242 positive, 71,809 negative) but no explicit experimental validation performance metrics.",
            "citation_title": "Malsite-deep: prediction of protein malonylation sites through deep learning and multi-information fusion based on NearMiss-2 strategy",
            "mention_or_use": "use",
            "system_or_method_name": "NearMiss-2 undersampling + deep learning (Malsite-Deep)",
            "domain": "protein post-translational modification prediction / proteomics",
            "proxy_metric_description": "In silico prediction of malonylation sites (binary site classifier) trained on sequence-derived features; undersampling selects majority negatives nearest to positives in feature space.",
            "proxy_type": "data-driven ML (classification on sequence features)",
            "ground_truth_description": "Curated experimentally-annotated malonylation sites (used as labels in training/validation) — review implies experimental annotation as ground truth but reports no validation statistics.",
            "quantitative_gap_measure": null,
            "proxy_performance": null,
            "ground_truth_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_or_extrapolation_distance": "Model trained on annotated sites (in-distribution); review does not report out-of-distribution or cross-species generalization metrics.",
            "gap_varies_with_novelty": null,
            "gap_variation_details": null,
            "incremental_vs_transformational": "Described as improving prediction accuracy in class-imbalanced proteomic datasets (incremental improvement).",
            "calibration_or_uncertainty": null,
            "bias_correction_methods": "NearMiss-2 undersampling to reduce majority dominance; no multifidelity correction discussed.",
            "temporal_or_maturity_effects": null,
            "domain_specific_factors": "High class imbalance ratios and sequence homology can affect performance; review notes proximity-based undersampling may not capture complex non-linear relationships.",
            "multiple_proxy_comparison": null,
            "sample_size": "4,242 minority (malonylation sites) and 71,809 majority (non-sites) as stated in review example.",
            "cost_or_resource_discussion": "NearMiss is computationally efficient for high-dimensional data but may risk information loss; no numeric cost data.",
            "exceptional_cases": null,
            "limitations_discussion": "Review warns NearMiss can discard valuable majority-sample information and may struggle with complex non-linear feature relationships.",
            "uuid": "e1865.3"
        },
        {
            "name_short": "Cost-sensitive XGBoost (INFLAMeR)",
            "name_full": "Cost-sensitive XGBoost classifier used in INFLAMeR for lncRNA detection",
            "brief_description": "INFLAMeR employed a cost-sensitive XGBoost to handle extreme class imbalance (example ratio 1:55) when identifying functional lncRNAs from genomic data; review describes method and imbalance ratio but does not provide explicit experimental validation numeric gaps in this article.",
            "citation_title": "Integration of transcription regulation and functional genomic data reveals lncRNA SNHG6's role in hematopoietic differentiation and leukemia",
            "mention_or_use": "use",
            "system_or_method_name": "Cost-sensitive XGBoost",
            "domain": "genomics / transcriptomics / functional lncRNA prediction",
            "proxy_metric_description": "In silico classifier predicting functional lncRNAs from genomic and regulatory features; cost-sensitive weighting used to penalize minority-class misclassification more heavily.",
            "proxy_type": "data-driven ML (cost-weighted classifier)",
            "ground_truth_description": "Experimentally annotated functional lncRNAs from genomics/transcriptomics studies used as labels; review does not report post-hoc experimental validation success rates.",
            "quantitative_gap_measure": null,
            "proxy_performance": null,
            "ground_truth_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_or_extrapolation_distance": "Addresses rare functional lncRNAs (extreme imbalance); review does not quantify OOD performance.",
            "gap_varies_with_novelty": null,
            "gap_variation_details": null,
            "incremental_vs_transformational": "Presented as a method to improve detection of rare functional lncRNAs (incremental pragmatic approach).",
            "calibration_or_uncertainty": null,
            "bias_correction_methods": "Cost-sensitive weighting within XGBoost to rebalance effective training influence rather than resampling; no multifidelity recalibration discussed.",
            "temporal_or_maturity_effects": null,
            "domain_specific_factors": "Very high class imbalance (1:55) and label noise in genomic annotations; review highlights the importance of cost setting and risk of overfitting minority class if mis-specified.",
            "multiple_proxy_comparison": null,
            "sample_size": "Imbalanced genomic example reported as minority:majority = 1:55 in the review figure.",
            "cost_or_resource_discussion": "XGBoost is computationally efficient for high-dimensional genomics compared with heavy sampling; no quantitative cost comparisons.",
            "exceptional_cases": null,
            "limitations_discussion": "Review cautions improper cost setting can cause overfitting to minority class and reduce generalization; no numeric validation stats provided.",
            "uuid": "e1865.4"
        },
        {
            "name_short": "Physical-model augmentation (DFT/MD/docking)",
            "name_full": "Physical-model-based data augmentation using DFT, molecular dynamics, and molecular docking to create virtual training data",
            "brief_description": "The review discusses using physics-based simulations (DFT, MD, docking) as proxies to generate augmentation data for ML (e.g., binding free energies, conformations, reaction energetics), noting potential to fill underrepresented regions but also limitations and lack of standardized validation reported in the review.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_or_method_name": "DFT/MD/docking-based data augmentation pipelines",
            "domain": "computational chemistry / drug discovery / catalysis / materials",
            "proxy_metric_description": "Simulation-derived quantities such as DFT-predicted binding free energies, adsorption energies, electronic structure descriptors, MD-sampled conformations and docking scores used as surrogate labels or features for ML training and augmentation.",
            "proxy_type": "physics-based simulation",
            "ground_truth_description": "Experimental measurements (e.g., binding assays, catalytic activity, spectroscopy, mechanical testing) are proposed as ground truth in discussion, but the review does not present paired quantitative comparisons.",
            "quantitative_gap_measure": null,
            "proxy_performance": null,
            "ground_truth_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_or_extrapolation_distance": "Used to generate rare/underrepresented conformations or reaction scenarios; review suggests MD/DFT can cover space not present in experiments but gives no quantitative OOD metrics.",
            "gap_varies_with_novelty": null,
            "gap_variation_details": null,
            "incremental_vs_transformational": "Characterized as promising future direction to improve augmentation for hard-to-observe minority classes (potentially transformational if integrated well).",
            "calibration_or_uncertainty": "Review notes need for integrating physical laws into ML for better interpretability and calls for careful calibration; no specific calibration metrics provided.",
            "bias_correction_methods": "Proposal to integrate physics-based augmentation with ML and multifidelity approaches; review calls for further development but describes no implemented bias-correction with numerical results.",
            "temporal_or_maturity_effects": "Review states physical-model augmentation is an emerging trend and still nascent; no time-series performance numbers.",
            "domain_specific_factors": "Long-timescale dynamics, rare conformations, and solvent/temperature effects are identified as factors that limit fidelity of simulations to experiments.",
            "multiple_proxy_comparison": "Review contrasts DFT, MD, and docking qualitatively (different strengths: energetics vs conformations vs binding modes) but does not present quantitative cross-proxy error comparisons.",
            "sample_size": null,
            "cost_or_resource_discussion": "Review acknowledges DFT and MD are computationally expensive relative to simpler proxies; no numeric cost or walltime figures provided.",
            "exceptional_cases": "Review references cases where DFT-derived descriptors are useful (e.g., adsorption energies correlate with catalytic trends) but does not quantify agreement in this article.",
            "limitations_discussion": "Review emphasizes that simulated proxies may omit experimental complexities, parameter sensitivity, and that surrogate-to-experiment gaps remain a key challenge requiring multifidelity and validation frameworks.",
            "uuid": "e1865.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Heterogeneous catalyst design by generative adversarial network and first-principles based microkinetics",
            "rating": 2
        },
        {
            "paper_title": "Machine Learning Boosted Entropy-Engineered Synthesis of stable Nanometric Solid Solution CuCo Alloys for Efficient Nitrate Reduction to Ammonia",
            "rating": 2
        },
        {
            "paper_title": "Designing catalysts with deep generative models and computational data. A case study for Suzuki cross coupling reactions.",
            "rating": 2
        },
        {
            "paper_title": "A machine learning framework to predict the tensile stress of natural rubber: Based on molecular dynamics simulation data",
            "rating": 2
        },
        {
            "paper_title": "Developing an Antiviral Peptides Predictor with Generative Adversarial Network Data Augmentation",
            "rating": 1
        },
        {
            "paper_title": "Malsite-deep: prediction of protein malonylation sites through deep learning and multi-information fusion based on NearMiss-2 strategy",
            "rating": 1
        }
    ],
    "cost": 0.018441,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A review of machine learning methods for imbalanced data challenges in chemistry</p>
<p>DrJian Jiang jjiang@wtu.edu.cn 
Science</p>
<p>Science</p>
<p>Science</p>
<p>Chunhuan Zhang 
Science</p>
<p>Science</p>
<p>Lu Ke 
Science</p>
<p>Nicole Hayes 
Science</p>
<p>Yueyin Zhu 0000-0001-8132-5998
Science</p>
<p>DrHuahai Qiu 
Science</p>
<p>Science</p>
<p>Bengong Zhang 
Science</p>
<p>Tianshou Zhou 
Science</p>
<p>Guo-Wei Wei weig@msu.edu 
Science</p>
<p>A review of machine learning methods for imbalanced data challenges in chemistry
C5E4CA57383822F555D7A3540B60FC9010.1039/d5sc00270bReceived 13th January 2025 Accepted 6th April 2025
Imbalanced data, where certain classes are significantly underrepresented in a dataset, is a widespread machine learning (ML) challenge across various fields of chemistry, yet it remains inadequately addressed.This data imbalance can lead to biased ML or deep learning (DL) models, which fail to accurately predict the underrepresented classes, thus limiting the robustness and applicability of these models.With the rapid advancement of ML and DL algorithms, several promising solutions to this issue have emerged, prompting the need for a comprehensive review of current methodologies.In this review, we examine the prominent ML approaches used to tackle the imbalanced data challenge in different areas of chemistry, including resampling techniques, data augmentation techniques, algorithmic approaches, and feature engineering strategies.Each of these methods is evaluated in the context of its application across various aspects of chemistry, such as drug discovery, materials science, cheminformatics, and catalysis.We also explore future directions for overcoming the imbalanced data challenge and emphasize data augmentation via physical models, large language models (LLMs), and advanced mathematics.The benefit of balanced data in new material design and production and the persistent challenges are discussed.Overall, this review aims to elucidate the prevalent ML techniques applied to mitigate the impacts of imbalanced data within the field of chemistry and offer insights into future directions for research and application.</p>
<p>Introduction</p>
<p>The awarding of the 2024 Nobel Prize in Chemistry to David Baker for computational protein design and to Demis Hassabis and John M. Jumper for protein structure prediction underscores the growing inuence of articial intelligence (AI) in scientic discovery.As AI and machine learning (ML) become integral to advancing chemical research, 1 one of the most pressing challenges is the issue of imbalanced data.In many chemical datasets, the disproportionate distribution of classes poses signicant obstacles to the development of reliable and accurate models, particularly when applied to complex chemical phenomena.</p>
<p>Imbalanced data, a common phenomenon in data science, refers to signicant disparities in the number of samples from different categories in classication tasks.The emergence of imbalanced data in chemistry is primarily attributed to the complexity and diversity of molecular data due to several factors.Naturally occurring biases in molecular distributions, where certain structures are more abundant than others, lead to a skew in data availability.Additionally, "selection bias" in sample collection processes can further exacerbate the imbalance.For instance, datasets may over-represent specic types of molecules or reactions due to experimental priorities or technical limitations.In drug discovery, 2 active drug molecules are oen signicantly outnumbered by inactive ones due to the constraints of cost, safety, and time.Similarly, in molecular property prediction, 3 models designed to assess toxicity oen predict toxic outcomes more frequently, as toxic substances comprise a signicant portion of the data.The study of proteinprotein interactions also suffers from this imbalance, with experimentally validated interactions being much rarer than non-interactions. 4he presence of imbalanced data has direct implications for the performance of ML models.Most algorithms, such as random forests (RF) and support vector machines (SVM), 5 assume a uniform distribution of data across categories. 6When</p>
<p>Yueying Zhu</p>
<p>Yueying Zhu obtained her PhD degree in Physics from Le Mans University and Central China Normal University under the mentorship of Profs.Qiuping Alexandre Wang, Xu Cai, and Wei Li.Her PhD study focused on the uncertainty and sensitivity analysis of nonlinear dynamical systems, and the modeling and simulations of spreading dynamics on complex network.Now, she is an associate professor in Prof. Jie Liu's group at Wuhan Textile University.Her current research concerns the application of uncertainty and sensitivity analysis to spreading dynamics, especially epidemic and opinion spreading on a complex network.</p>
<p>Huahai Qiu</p>
<p>Huahai Qiu received his PhD degree in Applied Mathematics from Sun Yat-sen University in 2012 and completed his postdoctoral studies at The Shanghai Institutes for Biological Sciences in 2018 under the guidance of Prof. Luonan Chen.Now he is the universityappointed professor of Wuhan Textile University, Wuhan, China.His current research focuses on computational systems biology.</p>
<p>Lu Ke</p>
<p>Lu Ke obtained her B.S. degree in Applied Mathematics in 2022 from Wuhan Textile University.She is currently a M.S. candidate at Wuhan Textile University under Dr Bengong Zhang and Dr Jian Jiang.</p>
<p>Nicole Hayes</p>
<p>Nicole Hayes is a PhD candidate in applied mathematics at Michigan State University.There, she focuses on novel machine learning methods for molecular property prediction, with broad applications in drug discovery.Her work has included the integration of deep learning and spectral graph methods for the prediction of scarcely labeled and imbalanced molecular data, as well as the application of topological tools for protein exibility analysis.</p>
<p>trained on imbalanced datasets, models tend to focus on classes with more abundant data, oen neglecting the minority classes.This bias results in models that are less sensitive to underrepresented features, which can critically undermine the accuracy of predictions in real-world applications.Consequently, overcoming the limitations imposed by imbalanced data is essential for the advancement of ML in chemical research.</p>
<p>Addressing the issue of imbalanced data in chemistry has become a major area of interest for researchers.Various strategies have been proposed, including resampling techniques like oversampling and undersampling, data augmentation, and ensemble algorithms.Feature engineering and selection methods have also been explored to mitigate the negative effects of data imbalance.However, despite the increasing body of work in this area, the existing reviews provide a general overview of these methods without specically addressing their applications within chemistry.This review aims to ll this gap by offering a comprehensive overview of the imbalanced data challenge and solutions in chemical research, with a particular focus on recent advancements and their practical implications.Through this examination, we seek to provide researchers with a deeper understanding of the challenge posed by imbalanced data and to stimulate further progress in developing effective solutions.</p>
<p>The rest of this article is organized as follows.Sections 2.1-2.4 provide a detailed description of current technologies and algorithms for handling imbalanced data and demonstrate their applications in distinct elds of chemistry.Section 2.5 lists some indicators for evaluating model performance.In Section 3, we discuss new trends and challenges in the study of imbalanced data in chemistry and highlight future perspectives.</p>
<p>2 Current approaches and techniques  1a.This approach enhances the model's ability to learn the characteristics of the minority class, improving its predictive performance and reducing bias due to class imbalance.It is commonly applied in various elds of chemistry, such as genomics and transcriptomics, 9,10 as well as drug design, 11,12 quantum computing 13 and materials design. 7ne of the most prominent oversampling methods is the Synthetic Minority Over-sampling Technique (SMOTE), rst introduced by Chawla et al. in 2002. 14 polymer materials. 7The illustration of this process of balancing data is shown in Fig. 1b.Similarly, as part of an ML method trained on molecular dynamics data to predict the tensile stress of natural rubber, SMOTE is used to interpolate at a few sample boundaries to solve the problem of sample imbalance. 15In catalyst design, the authors used SMOTE to solve the problem of uneven data distribution in the original dataset, improving the predictive performance of ML models and promoting candidate screening of hydrogen evolution reaction catalysts. 8The illustration of SMOTE for balancing data for this catalyst design example is displayed in Fig. 1c.</p>
<p>However, SMOTE has limitations, such as introducing noisy data, struggling with complex decision boundaries, failing to account for internal distribution differences within the minority class, and requiring high computational costs.To address these issues, advanced oversampling techniques have been developed, including Borderline-SMOTE, 16 SVM-SMOTE, 17 RF-SMOTE, 18 Safe-level-SMOTE, 19 SMOTE-NC, 20 and ADASYN. 21hese methods rene SMOTE's approach by better handling class overlap, decision boundary complexity, and minority class distribution, expanding its applicability to more complex datasets.In drug discovery, the uneven distribution of active and inactive compounds affects the prediction accuracy of ML models.Therefore, in a search for new histone deacetylase 8 (HDAC8) inhibitors, Nurani et al. used SMOTE to construct a balanced dataset. 22They further selected an RF model, which demonstrated the best predictive performance on their training set compared to other tested ML methods, and the resulting RF-SMOTE prediction model was indicated to be helpful in identifying new HDAC8 inhibitors.In protein engineering, sample imbalance is a major challenge for predicting protein-protein interaction sites.As traditional SMOTE methods focus equally on every minority class sample, while Borderline-SMOTE methods are more sensitive to boundary samples, Jiang et al. used a CNN model with Borderline-SMOTE to predict proteinprotein interaction sites, which is helpful for protein design and mutation analysis. 23Furthermore, a combination of the most distant undersampling and Safe-level-SMOTE oversampling techniques has been used to address data imbalance issues, demonstrating its excellent performance in balancing the number of lysine formylation sites and non-formylation sites and enabling the prediction of lysine formylation sites when paired with ML. 24 2.1.2Undersampling techniques.Undersampling is a data preprocessing technique that reduces the number of majority class samples to address class imbalance, enabling the model to focus more on minority class patterns.An illustration of undersampling is shown in Fig. 2a.By rebalancing the dataset, undersampling improves the model's predictive performance on minority classes.Commonly used undersampling methods include Random Under-Sampling (RUS), 28 NearMiss, 27 and Tomek Links. 29US randomly removes a portion of majority class samples to balance the dataset.The sampling rate is typically determined by the ratio of majority to minority class samples.Aer removing the excess majority samples, the resulting dataset is better balanced, allowing the model to learn from both classes in an unbiased manner.RUS has been successfully applied in various chemistry domains, including the prediction of anti-parasitic peptides, 28 drug-target interaction (DTI) prediction, 30 and compound-protein Fig. 1 (a) A schematic diagram of an oversampling method, demonstrating the approach of the oversampling technique to balance the dataset.(b) This example demonstrates the application of Borderline-SMOTE method in properties prediction of polymer materials. 7Firstly, experimental data of 23 rubber materials were collected, and the nearest neighbor interpolation (NNI) algorithm was used to expand the dataset, resulting in a total of 483 datasets.Then, the K-means algorithm was used to cluster these datasets into two categories.Finally, based on the clustering results, Borderline-SMOTE was used to interpolate along the boundaries of the minority samples, generating two clusters with sample sizes of 314 and 396, respectively.(c) This illustration showcases the utilization of the SMOTE technique in the domain of catalyst development. 8126 heteroatoms doped arsenenes were collected as the original dataset, and the absolute value of Gibbs free energy changes (jDG H j) of 0.2 eV was selected as the threshold to divide the original data into two categories (88 with jDG H j &gt; 0.2 eV and 38 with jDG H j &lt; 0.2 eV).Then, SMOTE was applied to solve the problem of data imbalance and obtain two types of evenly distributed data.interaction prediction. 31In drug discovery, 25 due to the greater number of non-interacting (negative samples) drug-target pairs than interacting (positive samples) drug-target pairs, this imbalanced dataset reduces prediction accuracy.Therefore, a new RUSbased method was used to process the data, as shown in Fig. 2b.While RUS is simple to implement and can reduce training time by decreasing the dataset size, it has potential drawbacks.Removing too many majority class samples can lead to the loss of important information, which may negatively affect model performance, particularly in drug discovery and genomics.In areas such as protein engineering and quantum chemistry, where intricate patterns and subtle variations in data are crucial, careful consideration is required to avoid discarding valuable information.</p>
<p>The NearMiss algorithm reduces the number of majority class samples while preserving key distribution characteristics, improving classier performance, particularly in binary classi-cation tasks.Its core principle is to select majority class samples that are closest to the minority class in the feature space for undersampling.</p>
<p>NearMiss is widely used due to its efficiency in handling high-dimensional data.Its robustness against noisy data and outliers, combined with scalability and ease of integration with other algorithms, makes it suitable for various applications in chemistry.For example, in 2022, Wang et al. applied the NearMiss-2 method to address imbalanced data in protein acetylation site prediction, signicantly improving the Malsite-Deep model's accuracy in protein engineering. 27Their workow is shown in Fig. 2d.Similarly, in molecular dynamics simulations, NearMiss is used to address data imbalance, which facilitates the identication of different conformational states of protein receptors.This example demonstrates the application of a new method based on RUS technology in the realm of drug discovery. 25The majority samples in the drug target dataset are clustered using K-means clustering method and divided into different clusters.After that, the RUS method is used to randomly select a cluster from these clusters, repeat multiple times, and combine the selected cluster with minority samples in the original dataset to form a new balanced set.(c) This instance illustrates the use of the Tomek-Links approach for addressing imbalance in data within the realm of materials design. 26Initially, SMOTE is used to generate minority samples, making the dataset roughly balanced.Then, Tomek Links is used to identify and remove the majority samples in Tomek-Links (samples near the classification boundary) to clean the data, thereby refining the roughly balanced dataset into a finer one.(d) This example uses the NearMiss-2 method to address data imbalance within the domain of protein-ligand binding. 27Firstly, a training dataset of peptide sequences is constructed, containing 4242 minority samples with malonylation sites and 71 809 majority samples without malonylation sites.Next, the NearMiss-2 method is used to calculate the distance between each majority sample and each minority sample, and then the k farthest minority samples are selected to calculate the average distance to these k minority samples.Finally, the majority sample with the smallest average distance is retained to achieve data balance.</p>
<p>Despite its advantages, NearMiss may lead to the loss of valuable information due to undersampling, particularly in elds like drug discovery, catalyst design, and genomics.Additionally, due to its reliance on proximity in the feature space, NearMiss can struggle with capturing complex, nonlinear relationships, limiting its effectiveness in highly imbalanced or intricate datasets in protein-ligand binding or quantum chemistry.</p>
<p>The Tomek Links method reduces the number of majority class samples by identifying and removing those that are close to minority class samples in the feature space.This approach improves the model's ability to focus on the minority class by reducing class overlap.It works by identifying pairs of majority and minority class samples that are nearest neighbors, called Tomek Links, and removing the majority class samples from these pairs.This enhances the distinction between classes for model training.</p>
<p>Tomek Links has been applied in various chemical domains, including identifying glutarylation sites 29 and pharmacophoric fragments of DYRK1A inhibitors, 33 boosting the efficiency of experimental parameter optimization of nanometric solid solution alloys design 26 (an illustration of the specic process is shown in Fig. 2c), and predicting compound-protein interactions. 34This method is particularly effective for noise reduction while preserving the overall data structure, improving model performance in elds like genomics, materials, and drug discovery.However, its reliance on identifying noise points based on proximity can risk removing valuable data.Additionally, its efficiency declines with larger datasets, which limits its applicability in certain large-scale contexts.</p>
<p>2.1.3Hybrid techniques 2.1.3.1 SMOTE-Tomek links.The SMOTE-Tomek Links technique combines oversampling and undersampling to enhance dataset balance and classication performance. 35MOTE synthesizes new minority class samples, while Tomek Links removes overlapping boundary samples, rening the dataset and reducing class overlap.This approach effectively mitigates data imbalance and overtting, leading to clearer class boundaries and improved classier accuracy and generalization.Widely applied in elds such as protein engineering, 29 genomics, and transcriptomics, 36 SMOTE-Tomek Links has demonstrated its ability to improve classication models, particularly for high-dimensional gene expression data, by facilitating the identication of key biomarkers.However, it can be computationally expensive, especially for large datasets, and excessive oversampling may still risk overtting.Therefore, careful parameter tuning is essential to maximize the method's effectiveness, particularly in drug discovery and catalyst design.</p>
<p>2.1.3.2SMOTE-edited nearest neighbor (SMOTE-ENN).SMOTE-ENN (edited nearest neighbor) is a hybrid resampling method by combining SMOTE's oversampling with the ENN technique to remove noisy majority class samples. 37This approach rebalances class distributions, enhancing the representativeness of minority classes while improving the model's robustness by reducing overtting.SMOTE-ENN has been successfully applied in diverse chemical elds, such as proteinligand binding 38 and DTI prediction. 39Specically, considering the large number of non-interaction class samples and the low proportion of interaction class samples in the DTI dataset, there is a signicant class imbalance problem.Therefore, SMOTE-ENN technology was adopted to solve this problem, helping to improve the accuracy of drug-target interaction prediction. 40lthough highly effective in improving model performance on imbalanced datasets, SMOTE-ENN is computationally intensive and sensitive to parameter selection.Careful parameter tuning is needed to mitigate risks such as generating poor-quality samples, particularly in noisy or unevenly distributed datasets.</p>
<p>2.1.4Cluster-based techniques 2.1.4.1 Density-based spatial clustering of applications with noise-SMOTE (DBSCAN-SMOTE).DBSCAN-SMOTE (DBSM) is a hybrid method that combines the density-based clustering algorithm DBSCAN with SMOTE, 41 as shown in Fig. 3a.In DBSM, DBSCAN identies core, boundary, and noise points within clusters by using parameters like neighborhood radius and minimum sample size.SMOTE is then applied to the core points of these clusters, increasing the representation of minority class samples.This approach effectively reduces data imbalance and optimizes sample distribution, enhancing both the performance and generalization of classication models.DBSM is well-suited for imbalanced datasets with noise or irregular cluster shapes.It has broad applicability in chemistry.In predicting cervical cancer, 43 Gowri and colleagues employed DBSCAN to tackle the data imbalance in cervical cancer datasets.The selection of DBSCAN was due to its capability to detect anomalous samples by examining the density of data points, obviating the requirement for predened parameters.In the context of drug screening, Koh et al. used the DBSCAN method to process imbalanced data due to the fact that it can identify and classify different antagonists based on the density distribution of compound structures. 44BSCAN-SMOTE handles noisy data and outliers well, making it effective in elds like genomics and protein-ligand binding, but its performance depends heavily on parameter selection and can be computationally expensive for large datasets.</p>
<p>2.1.4.2 K-means SMOTE.K-means SMOTE is a hybrid technique created by integrating K-means clustering with SMOTE. 45t rst partitions the data into clusters using K-means, then focuses on those with a higher proportion of minority class samples for targeted oversampling.Minority samples are generated between selected clusters to improve distribution, with sample density guiding the oversampling process.This approach enhances both the quantity and representativeness of minority class samples, improving model performance.</p>
<p>In the biomedical context, due to the diversity of disease subtypes or drug responses leading to uneven class distribution in the data, the K-means SMOTE method, by combining clustering and oversampling techniques, can effectively balance the imbalanced dataset while preserving its intrinsic structure, thereby enhancing the drug prediction model's ability to identify minority class samples. 46In the eld of protein engineering, Nath et al. employed the K-means SMOTE method to manage imbalanced data, which is attributable to its capability to efficiently rene the class distribution in the dataset, catering to the challenge of low sequence similarity among bioluminescent proteins. 42An illustration of K-means SMOTE method is shown in Fig. 3b.</p>
<p>K-means SMOTE effectively improves model performance by generating realistic minority class samples, but its two-step process, consisting of clustering and oversampling, costs many computational resources and requires careful parameter optimization.</p>
<p>Data augmentation</p>
<p>2.2.1 Noise addition.Gaussian noise addition is a widely used data augmentation technique in ML.By introducing controlled randomness based on the Gaussian distribution, it simulates real-world noise and variability in data, which forces models to focus on the essential and generalizable features of minority class samples, rather than overtting to dominant patterns in the majority class.This helps counteract the issue of imbalance by making the model less sensitive to supercial trends in the data and better equipped to handle unpredictable environments.</p>
<p>This approach has proven effective in handling imbalanced data in various chemical applications.For example, in protein-ligand binding prediction, Lu et al. applied Gaussian noise addition to training data, since Gaussian noise can improve the model's adaptability to large-scale protein conformational changes and the ability of the model to identify hidden binding sites. 47Similarly, in drug discovery, Chakraborty et al. added Gaussian noise to the latent representation of autoencoders as the noise addition increased molecular diversity and complexity while maintaining the rationality of molecular structure. 48aussian noise addition improves model generalization on imbalanced data by simulating random disturbances, but its effectiveness depends on the careful tuning of noise levels to avoid distorting minority class features or compromising interpretability.</p>
<p>2  49 are a class of DL models consisting of two neural networks: a generator and a discriminator.The generator's goal is to create realistic synthetic data, while the discriminator tries to distinguish between real and fake data.In the context of imbalanced data, GANs can be used to generate synthetic samples for the minority class.By training the generator to produce new, realistic samples of the underrepresented class, GANs help to increase the quantity and diversity of minority class data.This approach allows models to learn better representations of the minority class, reducing the bias towards the majority class.GANs have been successfully applied in many elds to handle class imbalance, such as drug design, 50-53 materials design, 54 protein engineering, 55,56 catalyst design, 57 and others.Due to the limitations of classical GANs in training stability and exploring certain regions of chemical space, Li et al. proposed a novel quantum GAN in 2021, which had a hybrid generator (QGAN-HG) for discovering small drug molecules. 58In predicting antiviral peptides, Lin et al. used GAN to address the issue of imbalanced antiviral peptide datasets, 59 due to its ability to produce new samples that closely matched the distribution of real data.An illustration of balancing data process is given in Fig. 4a.</p>
<p>GANs effectively balance imbalanced data by generating diverse, high-quality minority class samples, but their unstable training and risk of mode collapse may limit their ability to fully capture minority class features.</p>
<p>Variational autoencoders (VAEs).</p>
<p>A VAE is a type of generative model that learns to map input data to a continuous latent space, from which it can generate new data samples. 61,62It consists of two main components: an encoder that compresses the input into a probabilistic latent representation, and a decoder that reconstructs the data from the latent space.When applied to imbalanced data, VAEs can generate synthetic data by producing new samples for the minority class, as shown in Fig. 4b.The encoder-decoder structure of VAEs allows them to learn meaningful latent representations of the minority class, and by sampling from the latent space, new realistic data points can be generated.This helps mitigate the bias towards the majority class by enriching the diversity and quantity of minority class samples, making VAEs effective for handling class imbalance in elds such as drug discovery, 63,64 protein engineering, 65 molecular dynamics, 66 and materials design.Recently, Schilter et al. used the VAE method in catalyst design to handle imbalanced data, based on the ability of VAE to autonomously learn meaningful structural features from the data.Compared to other existing advanced methods, VAE performed better in handling imbalanced datasets, as it not only maintained data distribution but also generated effective and innovative datasets. 68In protein-ligand binding, Ngo et al. used the VAE method, as it can learn information about the entire protein structure and utilize its powerful generative ability to generate ligands with high binding affinity and synthetic feasibility. 69AEs effectively augment minority classes by generating new samples from a continuous latent space; however, their reliance on xed distributions and tendency to produce blurry samples can constrain their ability to capture complex data features.</p>
<p>2.2.3 Feature augmentation.Feature augmentation is a technique used in ML to create new features or modify existing ones by applying transformations, combinations, or domain-specic manipulations to the data.1][72] Common methods of feature augmentation include polynomial features, 73 feature interactions, 74 mathematical features, 70,75 and domain-specic transformations such as logarithmic scaling 76 or statistical combinations. 77n imbalanced datasets, the minority class oen lacks sufficient diversity, making it harder for the model to learn its patterns.By augmenting the features, new dimensions of variation can be introduced to the minority class, providing more informative and diverse data points.This allows the model to better distinguish the minority class from the majority class, reducing bias and improving classication accuracy.Feature augmentation works well when combined with other imbalance-handling techniques such as oversampling, enhancing the model's ability to generalize across both minority and majority classes and providing a more balanced representation of the data.</p>
<p>This method has been applied in many chemical elds, such as in DTI prediction 78 and DDI prediction. 79In protein function prediction, Wan et al. proposed the FFPred-GAN method in 2020 and used feature augmentation to handle imbalanced data, 80 which can effectively simulate the complex features of proteins in organisms without changing the distribution of the original data, while generating high-quality synthetic protein feature samples.Hayes et al. proposed the BTDT-MBO algorithm in 2024, which transformed molecular structures into informative feature vectors and employed a feature augmentation strategy, signicantly improving the recognition capability of minority classes within molecular datasets. 75Additionally, in protein-ligand binding, Akbar et al. used feature augmentation techniques to handle imbalanced data owing to their capability to integrate information from multiple feature vectors and improve the model's recognition ability when parsing complex biological data. 81hile feature augmentation can improve model performance on imbalanced data, it risks adding irrelevant features, overtting, or noise, and does not directly address the core imbalance between majority and minority classes.</p>
<p>Fig. 4 (a) This example demonstrates the application of generative adversarial network (GAN) in identifying antiviral peptide activity. 59Firstly, an imbalanced dataset was constructed, consisting of 2934 antiviral peptides (AVPs) and 17 184 non-antiviral peptides.The AVPs were used as input data to train the GAN model and then many AVP-like data were generated.Finally, the generated data were added to the original AVP data to achieve balance between the majority and minority samples.(b) The illustration of the variational autoencoder (VAE) algorithm for balancing data. 60It is divided into two parts: encoder and decoder.The former compresses the input into probabilistic latent representations, while the latter reconstructs data from latent space, the part between the encoder and the decoder.as shown in Fig. 5a.It iteratively updates weights to make each subsequent learner focus more on misclassied minority class samples, thus balancing the attention to both minority and majority classes.Since the proposal of the boosting algorithm, many extensions have been proposed, such as Adaptive Boosting, 85 Extreme Gradient Boosting (XGBoost), 86 Gradient Boosting Decision Tree, 87 etc.
Chemical Science Review 2.
Boosting has many applications in chemical elds, such as drug discovery, 30 catalyst design, 88 protein engineering, 89 proteinligand binding, 90 biomaterials design, 91 etc. Xue et al. selected the Gradient Boosting method for the design of biomaterials, 91 due to its capability to effectively address data imbalance by incrementally building and rening models.This approach, in contrast to others, enabled a more precise identication of the intricate features inuencing biomaterial properties.In genomics and transcriptomics, Liu et al. chose the XGBoost method to handle imbalanced data, 92 thanks to its excellent generalization ability and higher prediction accuracy in handling high-dimensional problems, as well as its effectiveness in dealing with imbalanced data and categories.In drug discovery, Sikander et al. used the XGBoost method for accurate prediction of druggable proteins. 83This method demonstrated an excellent ability to handle high-dimensional data and strong resistance to over-tting, and its working principle diagram is shown in Fig. 5c.</p>
<p>Boosting enhances minority class classication by increasing sample weights, but its growing complexity with  83 Firstly, an imbalanced dataset was constructed including proteins that can interact with drugs and proteins that cannot interact with drugs.The model then randomly selects samples with the same weight and chance from the dataset to train the first classifier model.Then, each classifier is tested on all samples in the dataset, and the weights of misclassified samples are updated iteratively to generate the final classification model from several individual weak classifiers.(d) This example demonstrates the application of bagging methods in the field of protein-ligand binding. 84Firstly, the majority samples and minority samples are separated from the original training set.Then, a certain number of samples are randomly selected from the majority samples and merged with the minority samples to form a new subset, which is repeated multiple times.Using the two-dimensional convolutional neural network (2D-CNN) framework to learn on each subset, an ensemble model is finally formed according to the mean ensemble strategy.</p>
<p>Review</p>
<p>Chemical Science iterations requires careful tuning, and it oen needs to be combined with techniques like data sampling or feature selection to improve efficiency.</p>
<p>Bagging.</p>
<p>Bagging is an ensemble method that creates multiple training subsets through random sampling with replacement, training independent models on each subset, 93 as shown in Fig. 5b.The nal output is obtained by aggregating predictions via voting or averaging.In imbalanced datasets, bagging can reduce bias by increasing the presence of minority class samples in some subsets, improving recognition of minority classes and reducing the inuence of the majority class.While it stabilizes models like decision trees and mitigates overtting, bagging alone does not well solve sample imbalance and oen requires additional techniques like oversampling or undersampling for better minority class performance.</p>
<p>Bagging has a wide range of chemical applications, such as drug discovery, 94,95 genomics and transcriptomics, 96 catalyst design, 97 etc.In the study of drug toxicity detection, 98 Gupta used an ensemble model based on bagging because the bagging method can effectively reduce the misjudgment of minority class samples by the model.Compared with a single classier, it was more suitable for complex data classication in the eld of biochemistry.Gong et al. employed the bagging method for addressing imbalanced data in druggable protein prediction due to its efficacy in mitigating model bias that arose from such imbalance.This method, in contrast to a single SVM classier, offered a superior ability to integrate the signicance of various features. 99In terms of protein-ligand binding, Hu et al. developed a method termed PredDBR to predict protein-DNA binding residues, 84 as depicted in Fig. 5d, and employed the bagging method to address imbalanced data, since bagging was more adept at handling complex features in bioinformatics compared to a single model.</p>
<p>Cost-sensitive learning.</p>
<p>Cost-sensitive learning (CSL) is an ML algorithm that evaluates the cost of different mis-classied samples by applying different cost metrics, aiming to minimize the overall cost. 100It enables the model to pay more attention to high-cost minority sample errors by reweighting majority and minority samples, thereby reducing the probability of these errors and improving the performance on clas-sication tasks in practical situations, as shown in Fig. 6a.</p>
<p>CSL has been widely applied in multiple elds of chemistry.For example, due to the highly imbalanced data in the DTI dataset, Aleb adopted a CSL method to improve model performance, which can assign higher weights to minority class samples in biological contexts, thereby more effectively identifying and predicting drug compound and protein interactions in drug design. 102In genomics and transcriptomics, Hazan et al. developed an advanced ML model called INFLAMeR for the identication of novel functional long non-coding RNAs (lncRNAs).This model employed a cost-sensitive XGBoost classier to tackle the imbalance of training data, as depicted in Fig. 6b, with the rationale that the CSL allowed for the allocation of higher weights to minority categories, thereby enhancing the model's capability to detect key lncRNAs. 101L is efficient for large-scale molecular data and can addresses class imbalance by balancing performance across categories, improving predictive accuracy, especially in drug screening.However, improper cost settings may lead to over-tting minority classes and reduce generalization, and uncertainty in cost function design may impact model performance.</p>
<p>Feature engineering and selection strategies</p>
<p>Feature engineering is a key technology in data preprocessing, which involves extracting, processing, or creating new features from raw data to optimize the performance of ML models. 77It can not only help identify key features of minority classes, but also enhance the sensitivity of the model to minority class samples by designing specic features or transformations, improving the model's performance on imbalanced data.</p>
<p>Feature selection, as an important component of feature engineering, can remove redundant features and enhance the recognition accuracy of minority categories by ltering out the most relevant and important feature subsets in the dataset.Feature selection-including lter, wrapper, and embedded technologies as well as random feature selection-has various applications in elds such as drug discovery, 103 protein engineering, [104][105][106] genomics and transcriptomics, 107 etc.  101 The imbalanced genomic data (minority : majority = 1 : 55) is input into the cost-sensitive XGBoost framework for processing, using the CSL method to assign weights to the samples.Then, the XGBoost classifier is used for processing to obtain a balanced dataset for subsequent analysis or modeling processes.</p>
<p>Chemical Science Review</p>
<p>2.4.1 Filter technology.Filter technology offers an efficient means for selecting subsets of pivotal features by quantifying the predictive power of each feature through the analysis of its statistical attributes or predened criteria.The workow for lter technology is shown in Fig. 7a.Filter technology can enhance a model's ability to recognize minority classes when dealing with imbalanced data, and its independence from models and high-dimensional data processing capabilities have enabled its application in multiple elds of chemistry. 109or instance, in 2023, Le et al. used the lter technology method for feature selection in protein engineering, both because the method can perform feature selection independently of the model and because it was more suitable for qualitatively evaluating feature importance in biomaterials. 110n the realm of catalyst design, Benavides-Hernández et al. opted for lter technology on the grounds that it successfully delineated the features with the most signicant inuence on catalyst efficacy while obviating the need for an augmented experimental workload. 111ilter technology has some limitations.It may not adequately capture the signicance of certain features within a particular model or the intricate interactions among features.</p>
<p>This technology may require further adjustments to ensure that the model does not lean towards the majority of class.</p>
<p>2.4.2Wrapper technology.The wrapper method can accurately identify key features that have signicant predictive effects on minority categories, as it evaluates the effectiveness of features by repeatedly testing the interaction between feature subsets and the target model.</p>
<p>Case in point: in drug discovery, Mesrabadi et al. introduced a model for predicting DTI in 2023, using wrapper technology to remove irrelevant features.Compared to other methods, it was more suitable for predicting complex drug-target relationships in bioinformatics, ensuring that the selected features had a direct positive impact on model performance, 108 as shown in Fig. 7b.In catalyst design, Shi et al. used wrapper technology to select the most critical subset for predicting adsorption energy from a large number of candidate features, 112 as this technology ensured that the selected features contributed the most to the performance of the prediction model.</p>
<p>The wrapper method has high computational costs for large imbalanced datasets due to repeated model training, especially with few minority class samples, and is oen combined with ltering or embedding techniques to optimize feature selection.Embedded methods can automatically identify and select feature subsets that are crucial for minority class prediction during training, thereby reducing dependence on majority class features and enhancing sensitivity to minority classes.Its illustration is given in Fig. 7c.</p>
<p>The utility of embedded feature selection techniques spans various domains.For instance, in catalyst design, Ma et al. developed an ML-driven model to identify catalysts, 113 which used embedded recursive feature elimination (RFE) to remove redundant features.A key benet of this method is that it can dynamically select features during model training, enabling greater synchronization.Similarly, in drug discovery, Zhao et al. introduced a method based on convolutional neural networks (CNN) to embed relationship path features.Compared to other methods, embedded technology can better extract drug disease relationship path features. 114lthough the embedded method can help the model better focus on minority categories, it may not fully reect the importance of features or the complex interactions between features, which may require additional strategies to supplement when dealing with imbalanced data.</p>
<p>2.4.4Random feature selection.Random feature selection (RFS) reduces feature count without sacricing accuracy and helps identify key features for minority classes in imbalanced datasets, improving prediction and reducing computational complexity while enhancing generalization.Its workow is shown in Fig. 7d.</p>
<p>Recently, in DTI prediction, RFS was used in the training process of the model. 115In protein recognition research, Qiang et al. constructed a recognition model based on the RF model, 116 where each tree was constructed based on a randomly selected subset of features.The use of random feature selection was to enhance feature representation, aiming to extract information from different perspectives of numerous feature descriptors and eliminate redundant and irrelevant features through the optimization of the feature space.</p>
<p>While RFS saves time and reduces overtting, its randomness may cause accuracy uctuations, especially in correlated chemical data, requiring optimization or combination with other methods for stable minority class predictions.</p>
<p>Evaluation metrics suitable for imbalanced datasets</p>
<p>When dealing with imbalanced data, traditional metrics in ML models like accuracy and precision are not suitable, as they can be skewed by the majority class, giving a false sense of high performance even when the minority class is poorly predicted.In contrast, balanced accuracy, 117 the F1 score, 118 Area Under the Receiver Operating Characteristic Curve (AUC-ROC), 119 and Matthews correlation coefficient (MCC) 120 are more suitable.</p>
<p>The F1 Score focuses on the minority class by addressing false positives and false negatives, making it useful when false negatives are costly.The mathematical formula for the F1 score is given as follows:
F 1 score ¼ 2$TP 2$TP þ FP þ FN ¼ 2$ precision$recall precision þ recall ;(1)
where actual positives that are correctly predicted as positives are called true positives (TP).Actual positives that are wrongly predicted as negatives are called false negatives (FN).Actual negatives that are correctly predicted as negatives are called true negatives (TN).Actual negatives that are wrongly predicted as positives are called false positives (FP).</p>
<p>The AUC-ROC curve evaluates a classier's ability to distinguish between classes by measuring true positive and false positive rates across classication thresholds.The denitions of true positive and false positive rates are given by:
True positive rateðTPRÞ ¼ TP TP þ FN ;(2)
False positive rateðFPRÞ
¼ FP FP þ TN :(3)
MCC offers a balanced evaluation, accounting for true/false positives and negatives and providing a comprehensive performance view even with imbalanced classes.The mathematical formula is given in the following form:
MCC ¼ TP$TN À FP$FN ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ðTP þ FPÞ$ðTP þ FNÞ$ðTN þ FPÞ$ðTN þ FNÞ p :(4)
3 Perspectives for future directions and challenges</p>
<p>In reviewing techniques for addressing imbalanced data, we have analyzed their strengths, weaknesses, and applications in chemistry, as summarized in Table 1.In this chapter, we briey give the rules of thumb for selecting suitable methods to address data imbalance issues, explore future research directions, offer unique perspectives, and discuss new approaches to better tackle the imbalanced data challenge in chemistry.</p>
<p>Rules of thumb for selecting suitable methods</p>
<p>Selecting suitable methods for addressing data imbalance issues requires the consideration of several key factors, including the severity of imbalance, dataset size, computational resources, and data complexity.Firstly, for mild or moderate data imbalance cases (e.g., minority-to-majority ratio is equal to or less than 1 : 10), simpler techniques such as oversampling or undersampling can be considered.For datasets with relatively small sizes and low dimensinality, oversampling like SMOTE can effectively enhance minority class representation without signicantly increasing complexity.However, oversampling may inadvertently introduce noise or exacerbate overtting, especially in cases where minority class samples are noisy or contain outliers.To mitigate this, combining oversampling with noise-ltering techniques or employing advanced variants such as Borderline-SMOTE is recommended.</p>
<p>Conversely, if the dataset is large, signicantly imbalanced, or noisy, undersampling techniques oen become the method of choice due to their computational efficiency.They streamline</p>
<p>Chemical Science Review</p>
<p>Table 1 An overview of major machine learning approaches and their strengths and weaknesses for data imbalance challenges Methods In scenarios with severe imbalance (e.g., minority-to-majority ratio of 1 : 50 or higher), traditional sampling methods alone may be insufficient.Leveraging more sophisticated approaches such as deep generative models like GANs becomes advantageous.Nevertheless, such methods require substantial computational resources and careful tuning, making them more suitable for projects where high accuracy and predictive performance justify the resource investment.</p>
<p>Strengths</p>
<p>For datasets characterized by signicant noise, feature redundancy, or high dimensionality, feature selection or augmentation methods should be employed.Cost-sensitive learning methods are particularly suitable for contexts where different misclassication costs are explicitly dened or can be quantied precisely.Adjusting misclassication costs allows models to prioritize correctly classifying minority class instances, directly addressing imbalance at the algorithmic level without additional sampling.</p>
<p>Finally, ensemble-based methods, such as Random Forest, AdaBoost, or gradient boosting like XGBoost, are consistently robust and effective.They inherently manage data imbalance by aggregating multiple weak classiers, providing strong predictive accuracy and reduced variance.Combining ensemble methods with advanced sampling or feature-selection approaches can further enhance predictive performance, particularly in complex real-world applications.</p>
<p>Emerging trends in imbalanced data research in chemistry</p>
<p>In chemistry, recent research on the imbalanced data challenge reects the convergence of interdisciplinary approaches and the development of novel methodologies.</p>
<p>Multimodal data fusion.</p>
<p>Recently, multimodal data fusion has gained attention for integrating data from different sources to provide a more comprehensive understanding.By combining molecular data, such as gene expression, proteinprotein interaction, 121 and DDI data, 122 researchers can better explore structural features and interaction mechanisms.This approach enhances model learning capacity and performance, improving the detection of minority samples.</p>
<p>3.2.2Federated learning.Additionally, federated learning offers a potential avenue in handling imbalanced datasets by enabling collaborative model training without sharing raw data.This is particularly useful in drug discovery, where data from different laboratories, such as protein sequences 123 and drug molecules, 124 can be combined despite differences in size and diversity.Federated learning not only helps predict drug properties like activity and toxicity but also mitigates data imbalance, enhancing model accuracy and accelerating drug development.</p>
<p>3.2.3Self-supervised learning.Moreover, self-supervised learning has emerged as a burgeoning research direction.</p>
<p>Unlike semi-supervised learning, it uncovers patterns directly from data without external labels.This allows it to process large volumes of unlabeled data, improving model performance on imbalanced datasets.In protein engineering, it could predict molecular properties 125 and stability changes from mutations, 122 helping to guide the synthesis of novel compounds and accelerate their development.</p>
<p>Physical-model-based data augmentation</p>
<p>One future research trend on data augmentation strategies is focusing on integrating physical models to generate virtual data with meaningful physical properties, thereby enriching datasets and improving model performance.</p>
<p>In this research eld, molecular dynamics (MD) simulation plays a crucial role by simulating molecular motion and interactions, revealing conformations that experimental data may miss.This is especially relevant in areas like protein-ligand interactions 126 and reaction mechanisms, where rare but important congurations are underrepresented.By generating such scenarios, MD could help build richer datasets, addressing data imbalance and improving models' ability to predict molecular properties and reactions.</p>
<p>Density functional theory (DFT) and molecular docking are pivotal in data augmentation strategies.DFT, a quantum mechanical approach, can generate critical data on chemical reactivity, catalytic activity, photophysical properties, nuclear magnetic resonance spectra, 127 binding free energy, 128 and molecular electronic structures, 129 enhancing ML models' ability to predict molecular properties.Its broad applicability across molecular systems can help balance datasets and improve generalization.Through predicting interactions between molecules and receptors, 130 molecular docking could provide binding patterns and affinity data.Both techniques can enrich datasets, especially for imbalanced data, improving the accuracy of models in predicting molecular interactions and properties.</p>
<p>The integration of physical models with ML is emerging as a key trend.By incorporating physical laws into ML frameworks, models can enhance prediction accuracy and interpretability.Advances in computational power enable physics-based data augmentation, creating more diverse training datasets and fostering more generalizable chemical models.For example, thermodynamic and statistical mechanics simulations can generate data on equilibrium constants, free energy changes, and reaction rates, enriching datasets, addressing imbalances, and boosting model performance.</p>
<p>Large-language-model-based data augmentation</p>
<p>As chemistry faces the challenge of imbalanced datasets, the rise of deep learning (DL), especially Large Language Models (LLMs), such as ChatGPT and Gemini, offers promising solutions.LLMs excel in data augmentation, as demonstrated by Sarker et al., who showed that models like ChatGPT improve accuracy in drug identication and classication. 131Furthermore, models like Chemformer, introduced by Ross Irwin in 2022, 132 can handle a range of chemical tasks, including sequence-to-sequence and discriminative tasks, showcasing the potential of LLMs in chemical research.</p>
<p>LLMs could offer innovative solutions for addressing imbalanced chemical data by learning from diverse representations, such as textual descriptions and chemical structures.For example, generating molecular structures from textual representations demonstrates how LLMs can enrich datasets and mitigate the data imbalance challenge, advancing chemical data analysis. 133</p>
<p>Mathematics-enabled data augmentation</p>
<p>A trending topic in data science is the integration of AI with advanced mathematics, such as differential geometry, algebraic topology, combinatorics, geometric topology, etc. 70,71 Recently, mathematical AI has become an emerging paradigm in molecular data sciences, including drug design competitions, 134,135 the discovery of viral evolution mechanisms, 136 the forecasting of emerging dominant variants, 137,138 protein engineering, 139 protein mutation prediction, 140 toxicity prediction, 141 drug addiction analysis, 142 polymer property, 143 etc.The multiscale topology-enabled transformer by Chen et al. gives rise to the best prediction of protein-ligand binding affinities. 144This mathematical approach captures stereochemistry, 145 which is missing in typical sequence-based molecular language models.As such, mathematics-enabled generative models will be a new approach for data augmentation.</p>
<p>Additionally, the graph-based Merriman-Bence-Osher method has been utilized to handle imbalanced data. 75It leverages its diffusion process to propagate label information across the graph, enabling better representation of minority classes in semi-supervised learning tasks.</p>
<p>New materials design and manufacture</p>
<p>While the experimental synthesis of the minority class offers the ultimate solution to imbalanced data, balanced datasets play a key role in AI-assisted active design and manufacture of new materials.Enhanced computational power and rened algorithms are accelerating breakthroughs in new materials development by ensuring more equitable data distribution.</p>
<p>Balancing datasets signicantly improves the accuracy of ML models in predicting new materials.Recent studies using techniques like feature engineering 146 have led to more reliable models that excel in predicting material properties and optimizing synthesis pathways, accelerating material research and development.The use of balanced datasets is driving the integration of high-throughput experimentation and computational simulations in material innovation. 147By reducing bias and improving data efficiency, these datasets guide experimental design and, when combined with simulations, enable researchers to explore a wider range of parameters, accelerating the discovery and development of new materials.</p>
<p>Balanced datasets are crucial for fostering material diversity.By creating and utilizing these datasets, researchers can more effectively explore existing material databases, uncovering unique materials and driving innovative material design.</p>
<p>Persistent challenges and areas for further investigation</p>
<p>In chemical research, the management of imbalanced data continues to encounter signicant challenges.Although various solutions have been proposed in recent years, several fundamental issues remain unresolved and demand urgent attention.First, the combination of small data 148 and imbalanced data poses one of the most signicant challenges in molecular science.Data scarcity and imbalance are especially pronounced across numerous chemical applications.Due to the inherent difficulty in producing large volumes of balanced and high-quality data during experiments, particularly when studying new materials or rare compounds, or when performing toxicity evaluations, nding efficient methods to collect, share, and integrate data has become a pressing concern.</p>
<p>Additionally, the absence of standardized processes for data handling and dissemination between different research projects and laboratories further complicates the resolution of imbalanced data issues.The current mechanisms for data sharing require substantial improvement.Although a growing number of academic institutions and journals advocate for open data practices, operational challenges persist.For example, inconsistent data repository formats and incomplete or insufficiently detailed data documentation hinder effective reuse.Thus, establishing a standardized platform, spearheaded by relevant organizations or funding bodies, to regulate data submission and validation processes is crucial for enhancing data transparency and quality.</p>
<p>While several approaches have been applied to mitigate the imbalanced data challenge, 2,149 the resilience and generalizability of these methods still require signicant renement.Many existing techniques struggle when applied to small sample sizes, 148 making it difficult to address the complex molecular structures and reaction pathways inherent to chemical research.Consequently, future studies should prioritize the development of more precise and eld-specic data augmentation and modeling techniques.Notably, enhancement methods based on physical models may hold considerable promise in rectifying data imbalance, although these approaches are still in the nascent stages of exploration.</p>
<p>To drive progress in this area, forthcoming research should aim to create robust frameworks that facilitate the widespread implementation of imbalanced data technologies.Moreover, researchers should emphasize the thorough validation of new algorithms, particularly through testing across various chemical application scenarios, to ensure their practical utility and consistency.These efforts will provide a strong foundation and theoretical backing for addressing the imbalanced data problem in the chemical sciences.</p>
<ol>
<li>1
1
Resampling techniques 2.1.1Oversampling techniques.Oversampling is a widely used technique for addressing data imbalance, particularly when the minority class has signicantly fewer samples than the majority class.By duplicating or generating new samples for the minority class while maintaining the original data distribution, oversampling helps balance class proportions.An example schematic diagram for oversampling is shown in Fig.</li>
</ol>
<p>SMOTE generates new minority class samples by synthesizing them from the existing data, which helps preserve the original feature distribution and mitigates overtting.Its ability to enhance model generalization has led to widespread adoption across various chemistry domains.For instance, in materials design, SMOTE has been used to resolve class imbalance when integrated with Extreme Gradient Boosting (XGBoost) and nearest neighbor interpolation, improving the prediction of mechanical properties of Guo-Wei Wei Guo-Wei Wei received his PhD degree from the University of British Columbia and is currently an MSU Research Foundation Professor at Michigan State University.His research focuses on the mathematical foundations of bioscience and articial intelligence (AI).Dr Wei pioneered mathematical AI paradigms, such as topological deep learning (TDL), that integrate profound mathematical structures with AI to tackle biological challenges.His mathematical AI has led to victories in D3R Grand Challenges, a worldwide annual competition series in computer-aided drug design.Using TDL, genotyping, and computational biophysics, the Wei team unveiled the mechanisms of SARS-CoV-2 evolution and successfully predicted emerging dominant SARS-CoV-2 variants.University of Tokyo in 2013 under the guidance of Prof. Kazuyuki Aihara and Luonan Chen.His postdoctoral studies focused on computational systems biology.Now he is the professor of Wuhan Textile University, Wuhan, China.His current research concerns scRNAseq data analysis and machine learning.Tianshou Zhou Tianshou Zhou received his PhD degree in Academy of Mathematics and System Science, CAS in 2001 and completed his postdoctoral Research at The Tsinghua University in 2003 under the guidance of Prof. Yun Tang.His postdoctoral studies focused on dynamics of comlex systems.Now he is a professor of Sun Yatsen University, Guangzhou, China.His current research interest is in computational systems biology.</p>
<p>32
32</p>
<p>Fig. 2
2
Fig.2(a) A schematic diagram of undersampling method, demonstrating the approach of undersampling technique to balance the dataset.(b) This example demonstrates the application of a new method based on RUS technology in the realm of drug discovery.25The majority samples in the drug target dataset are clustered using K-means clustering method and divided into different clusters.After that, the RUS method is used to randomly select a cluster from these clusters, repeat multiple times, and combine the selected cluster with minority samples in the original dataset to form a new balanced set.(c) This instance illustrates the use of the Tomek-Links approach for addressing imbalance in data within the realm of materials design.26Initially, SMOTE is used to generate minority samples, making the dataset roughly balanced.Then, Tomek Links is used to identify and remove the majority samples in Tomek-Links (samples near the classification boundary) to clean the data, thereby refining the roughly balanced dataset into a finer one.(d) This example uses the NearMiss-2 method to address data imbalance within the domain of protein-ligand binding.27Firstly, a training dataset of peptide sequences is constructed, containing 4242 minority samples with malonylation sites and 71 809 majority samples without malonylation sites.Next, the NearMiss-2 method is used to calculate the distance between each majority sample and each minority sample, and then the k farthest minority samples are selected to calculate the average distance to these k minority samples.Finally, the majority sample with the smallest average distance is retained to achieve data balance.</p>
<p>67
67</p>
<p>Fig. 3
3
Fig.3 (a)The schematic diagram of DBSM algorithm flow.41The process of DBSM includes two parts: undersampling and oversampling.For the undersampling part, apply DBSCAN to create clusters from all training sets.Then a portion of the majority samples is deleted from each cluster.The output of the undersampling technique is only majority samples.For the oversampling part, SMOTE is used to add synthetic sampless of minority samples to the training set.Therefore, the final output of the DBSM algorithm is a new training set consisting only of the majority samples from the undersampling part and the minority samples from the oversampling part.(b) This example demonstrates the application of the K-means SMOTE method in predicting bioluminescent proteins to address imbalanced data.42Firstly, K-means is used to cluster the majority and minority samples separately to solve the problem of intra-class imbalance.Secondly, SMOTE is used for oversampling a small number of samples (luminescent proteins) to increase the number of minority samples and form a new balanced dataset with the majority samples.</p>
<p>Fig. 5
5
Fig. 5 (a) A schematic diagram of the boosting algorithm.This method constructs a powerful classifier by connecting multiple weak classifiers.It uses an iterative process to make each subsequent classifier focus more on the misclassified minority class samples in the previous classifier's classification results, thus balancing the attention to minority and majority classes.(b) A schematic diagram of the bagging algorithm.It creates multiple subsets through random sampling and substitution, and it improves the recognition of minority classes by increasing the presence of minority samples in the subsets.(c) This example demonstrates the application of boosting in drug discovery.83Firstly, an imbalanced dataset was constructed including proteins that can interact with drugs and proteins that cannot interact with drugs.The model then randomly selects samples with the same weight and chance from the dataset to train the first classifier model.Then, each classifier is tested on all samples in the dataset, and the weights of misclassified samples are updated iteratively to generate the final classification model from several individual weak classifiers.(d) This example demonstrates the application of bagging methods in the field of protein-ligand binding.84Firstly, the majority samples and minority samples are separated from the original training set.Then, a certain number of samples are randomly selected from the majority samples and merged with the minority samples to form a new subset, which is repeated multiple times.Using the two-dimensional convolutional neural network (2D-CNN) framework to learn on each subset, an ensemble model is finally formed according to the mean ensemble strategy.</p>
<p>©</p>
<p>2025 The Author(s).Published by the Royal Society of Chemistry Chem.Sci., 2025, 16, 7637-7658 | 7645</p>
<p>Fig. 6
6
Fig. 6 (a) A schematic diagram of the cost-sensitive learning (CSL) method.It assigns different weights to differently misclassified samples, focusing the model more on high-cost minority sample errors, thereby reducing the likelihood of misclassification.(b) This example demonstrates the application of the cost-sensitive XGBoost method in genomics and transcriptomics.101The imbalanced genomic data (minority : majority = 1 : 55) is input into the cost-sensitive XGBoost framework for processing, using the CSL method to assign weights to the samples.Then, the XGBoost classifier is used for processing to obtain a balanced dataset for subsequent analysis or modeling processes.</p>
<p>Fig. 7 2 . 4 . 3
7243
Fig. 7 (a) The filter method sorts the six input samples (each with four features, different colors represent different features) directly based on different performance evaluation indicators and selects the feature with the highest score.(b) This example demonstrates the application of the wrapper feature selection method in the field of drug discovery. 108Firstly, through evaluating the extracted features, different weights are assigned for features.Then, a subset is selected from the feature set, and the wrapper method is used to choose the features that are most beneficial for model performance.(c) The schematic diagram of the embedded method, which combines feature selection with model training to ultimately obtain an optimal feature subset.(d) The workflow diagram of the random feature selection method, which randomly selects a subset of features from the entire feature set as the final feature subset.</p>
<p>.2.2 Deep generative models 2.2.2.1 Generative adversarial networks (GANs).GANs, rst proposed by Goodfellow et al. in 2014,</p>
<p>3 Algorithmic approaches 2.3.1 Ensemble methods 2.3.1.1Boosting.The boosting algorithm constructs a powerful model by concatenating multiple simple weak learners,</p>
<p>Table 1 (
1
Contd. )
RefWeaknesses
© 2025 The Author(s). Published by the Royal Society of Chemistry
AcknowledgementsThis work was supported in part by NIH grants R01AI164266, and R35GM148196, National Science Foundation grants DMS2052983, and IIS-1900473, and MSU Foundation.The work of Huahai Qiu and Bengong Zhang was supported by the National Natural Science Foundation of China under Grant No. 12271416 and No. 12371500, respectively.Data availabilityNo primary research results, soware or code have been included and no new data were generated or analysed as part of this review.Jiang received his B.S and M.S. degrees in Theoretical Physics at Central China Normal University in 2005 and 2007, respectively.He received his PhD degree in Theoretical Physics from University of Le Mans in France in 2011.He is currently a professor in the School of Mathematical and Physical at Wuhan Textile University in China.His research interest includes AI based topological data analysis on molecular science, drug design and discovery, data mining, and modelling and analysis of complex networks.Author contributionsCZ contributed to writing the Section 2 and the preparation of gures; LK contributed to writing the Section 3; JJ contributedChemical Science Reviewto writing the remainder and the preparation of gures and table; NH, YZ, HQ, BZ, TZ, and GW revised the manuscript.Conflicts of interestThere are no conicts to declare.Chemical Science Review
Transformer technology in molecular science. J Jiang, L Ke, L Chen, B Dou, Y Zhu, J Liu, Wiley Interdiscip. Rev.: Comput. Mol. Sci. 144e17252024</p>
<p>Deep learning-based imbalanced data classication for drug discovery. S Korkmaz, J. Chem. Inf. Model. 6092020</p>
<p>A novel approach to generate robust classication models to predict developmental toxicity from imbalanced datasets. S B Gunturi, N Ramamurthi, SAR QSAR Environ. Res. 92014</p>
<p>Developing computational model to predict proteinprotein interaction sites based on the XGBoost algorithm. A Deng, H Zhang, W Wang, J Zhang, D Fan, P Chen, Int. J. Mol. Sci. 21722742020</p>
<p>Support vector machine. D A Pisner, D M Schnyer, Machine learning. Elsevier2020</p>
<p>A review on imbalanced data classication techniques. S J Basha, S R Madala, K Vivek, E S Kumar, T Ammannamma, 2022 International Conference on Advanced Computing Technologies and Applications (ICACTA). IEEE2022</p>
<p>A Novel Small Sample Analysis Method for Properties Prediction of Polymer Materials. D Li, J Liu, J Liu, Nni-Smote-Xgboost , Macromol. Theory Simul. 30521000102021</p>
<p>An ensemble learning classier to discover arsenene catalysts with implanted heteroatoms for hydrogen evolution reaction. A Chen, J Cai, Z Wang, Y Han, S Ye, J Li, J. Energy Chem. 782023</p>
<p>Identication of orphan genes in unbalanced datasets based on ensemble learning. Q Gao, X Jin, E Xia, X Wu, L Gu, H Yan, Front. Genet. 8202020</p>
<p>Automated annotation of rare-cell types from single-cell RNA-sequencing data through synthetic oversampling. S Bej, A M Galow, R David, M Wolen, O Wolkenhauer, BMC Bioinf. 222021</p>
<p>Using SMOTE to deal with class-imbalance problem in bioactivity data to predict mTOR inhibitors. C Kumari, M Abulaish, N Subbarao, SN Comput. Sci. 12020</p>
<p>Ahamed Hassain Malim, Comparative Studies on Resampling Techniques in Machine Learning and Deep Learning Models for Drug-Target Interaction Prediction. A K Azlim Khan, N H , Molecules. 28416632023</p>
<p>A Quantum Approach to Synthetic Minority Oversampling Technique (SMOTE), Quantum Machine Intelligence. N Mohanty, B K Behera, C Ferrie, 10.1007/s42484-025-00248-62025</p>
<p>SMOTE: synthetic minority oversampling technique. N V Chawla, K W Bowyer, L O Hall, W P Kegelmeyer, J. Artif. Intell. Res. 162002</p>
<p>A machine learning framework to predict the tensile stress of natural rubber: Based on molecular dynamics simulation data. Y Huang, Q Chen, Z Zhang, K Gao, A Hu, Y Dong, Polymers. 14918972022</p>
<p>Borderline-SMOTE: a new over-sampling method in imbalanced data sets learning. H Han, W Y Wang, B H Mao, International conference on intelligent computing. Springer2005</p>
<p>SMOTE variants for imbalanced binary classication: heart disease prediction. X Zheng, 2020Los AngelesUniversity of California</p>
<p>Machine Learning Classication Model for Screening of Infrared Nonlinear Optical Crystals. Z Fan, Z Sun, A Wang, Y Yin, G Jin, C Xin, J. Electron. Mater. 62023</p>
<p>Safe-level-smote: Safe-level-synthetic minority oversampling technique for handling the class imbalanced problem. C Bunkhumpornpat, K Sinapiromsaran, C Lursinsap, Advances in knowledge discovery and data mining: 13th Pacic-Asia conference. Bangkok, ThailandSpringer2009. April 27-30, 2009 proceedings 13. 2009</p>
<p>Estimation of obesity levels through the proposed predictive approach based on physical activity and nutritional habits. H G Gozukara Bag, F H Yagin, Y Gormez, P P González, C Colak, M Gülü, Diagnostics. 131829492023</p>
<p>ADASYN: Adaptive synthetic sampling approach for imbalanced learning. H He, Y Bai, E A Garcia, S Li, 2008 IEEE international joint conference on neural networks (IEEE world congress on computational intelligence). Ieee2008</p>
<p>Identication of a Histone Deacetylase 8 Inhibitor through Drug Screenings Based on Machine Learning. A Nurani, Y Yamashita, Y Taki, Y Takada, Y Itoh, T Suzuki, Chem. Pharm. Bull. 7222024</p>
<p>Protein-protein interaction sites prediction using batch normalization based CNNs and oversampling method borderline-SMOTE. C Jiang, W Lv, J Li, IEEE/ACM Trans. Comput. Biol. Bioinf. 2032023</p>
<p>Formator: predicting lysine formylation sites based on the most distant undersampling and safe-level synthetic minority oversampling. C Jia, M Zhang, C Fan, F Li, J Song, IEEE/ACM Trans. Comput. Biol. Bioinf. 1852019</p>
<p>Prediction of drug-target interaction based on protein features using undersampling and feature selection techniques with boosting. S H Mahmud, W Chen, H Meng, H Jahan, Y Liu, S M Hasan, Anal. Biochem. 1135072020</p>
<p>Machine Learning Boosted Entropy-Engineered Synthesis of stable Nanometric Solid Solution CuCo Alloys for Efficient Nitrate Reduction to Ammonia, arXiv. Y Hu, H Lan, B Hu, J Gong, D Wang, W D Zhang, 10.48550/arXiv.2408.001422024preprint</p>
<p>Malsite-deep: prediction of protein malonylation sites through deep learning and multi-information fusion based on NearMiss-2 strategy. M Wang, L Song, Y Zhang, H Gao, L Yan, B Yu, Knowl.-Based Syst. 1081912022</p>
<p>PredAPP: predicting anti-parasitic peptides with undersampling and ensemble approaches. W Zhang, E Xia, R Dai, W Tang, Y Bin, J Xia, Interdiscip. Sci.:Comput. Life Sci. 2022</p>
<p>A novel method for Identication of Glutarylation sites combining Borderline-SMOTE with Tomek links technique in imbalanced data. Q Ning, X Zhao, Z Ma, IEEE/ACM Trans. Comput. Biol. Bioinf. 1952021</p>
<p>PreDTIs: prediction of drug-target interactions based on multiple feature information using gradient boosting framework with data balancing and feature selection techniques. S H Mahmud, W Chen, Y Liu, M A Awal, K Ahmed, M H Rahman, Briengs Bioinf. 225462021</p>
<p>Exploration and augmentation of pharmacological space via adversarial auto-encoder model for facilitating kinase-centric drug development. X Bai, Y Yin, J. Cheminf. 132021</p>
<p>Recognition of Conformational States of a G Protein-Coupled Receptor from Molecular Dynamic Simulations Using Sampling Techniques. M A Gutiérrez-Mondragón, C König, A Vellido, International Work-Conference on Bioinformatics and Biomedical Engineering. Springer2023</p>
<p>M Bi, Z Guan, T Fan, N Zhang, J Wang, G Sun, Identication of Pharmacophoric Fragments of DYRK1A Inhibitors Using Machine Learning Classication Models. 2022271753</p>
<p>Performance comparison of data sampling techniques to handle imbalanced class on prediction of compoundprotein interaction. A R Purnajaya, W A Kusuma, M K D Hardhienata, Biogenesis: Jurnal Ilmiah Biologi. 812020</p>
<p>A study of the behavior of several methods for balancing machine learning training data. G E Batista, R C Prati, M C Monard, ACM SIGKDD Explorations Newsletter. 62004</p>
<p>Cancer Diagnosis by Gene-Environment Interactions via Combination of SMOTE-Tomek and Overlapped Group Screening Approaches with Application to Imbalanced TCGA Clinical and Genomic Data. J H Wang, C Y Liu, Y R Min, Z H Wu, P L Hou, Mathematics. 121422092024</p>
<p>A Comprehensive Investigation of the Performances of Different Machine Learning Classiers with SMOTE-ENN Oversampling Technique and Hyperparameter Optimization for Imbalanced Heart Failure Dataset. M Muntasir Nishat, F Faisal, I Jahan Ratul, A Al-Monsur, A M Ar-Ra, S M Nasrullah, Sci. Program. 136494062022</p>
<p>A hybrid resampling algorithms SMOTE and ENN based deep learning models for identication of Marburg virus inhibitors. M Kumari, N Subbarao, Future Med. Chem. 102022</p>
<p>DBGRU-SE: predicting drug-drug interactions based on double BiGRU and squeeze-and-excitation attention mechanism. M Zhang, H Gao, X Liao, B Ning, H Gu, B Yu, Briengs Bioinf. 2441842023</p>
<p>An ensemble-based approach using structural feature extraction method with class imbalance handling technique for drug-target interaction prediction. A Puri, M K Gupta, K Sachdev, Multimed. Tools Appl. 262022</p>
<p>DBSM: The combination of DBSCAN and SMOTE for imbalanced data classication. Y Sanguanmak, A Hanskunatai, 2016 13th International joint conference on computer science and soware engineering (JCSSE. IEEE2016</p>
<p>Unsupervised learning assisted robust prediction of bioluminescent proteins. A Nath, K Subbiah, Comput. Biol. Med. 682016</p>
<p>Cervical cancer prediction using outlier deduction and over sampling methods. G Kannan, ScienceOpen Preprints. 2022</p>
<p>Multi-step structureactivity relationship screening efficiently predicts diverse PPARg antagonists. D H Koh, W S Song, Ey Kim, Chemosphere. 2022, 286, 131540</p>
<p>Improving imbalanced learning through a heuristic oversampling method based on k-means and SMOTE. G Douzas, F Bacao, F Last, Inf. Sci. 4652018</p>
<p>X Song, Y Li, H Wu, IC-BIS 2022Research on random forest drug classication prediction model based on KMeans-SMOTE, in International Conference on Biomedical and Intelligent Systems. 12458SPIE, 2022</p>
<p>DynamicBind: Predicting ligand-specic protein-ligand complex structure with a deep equivariant generative model. W Lu, J Zhang, W Huang, Z Zhang, X Jia, Z Wang, Nat. Commun. 15110712024</p>
<p>Utilizing deep learning to explore chemical space for drug lead optimization. R Chakraborty, Y Hasija, Expert Syst. Appl. 1205922023</p>
<p>Generative adversarial nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, Advances in Neural Information Processing Systems. 201427</p>
<p>Designing optimized Chemical Science Review drug candidates with Generative Adversarial Network. M Abbasi, B P Santos, T C Pereira, R Soa, N R Monteiro, C J Simões, J. Cheminf. 141402022</p>
<p>Deep convolutional generative adversarial network (dcGAN) models for screening and design of small molecules targeting cannabinoid receptors. Y Bian, J Wang, J J Jun, X Q Xie, Mol. Pharmaceutics. 16112019</p>
<p>A de novo molecular generation method using latent vector based generative adversarial network. O Prykhodko, S V Johansson, P C Kotsias, J Arús-Pous, E J Bjerrum, O Engkvist, J. Cheminf. 112019</p>
<p>Mol-CycleGAN: a generative model for molecular optimization. Ł Maziarka, A Pocha, J Kaczmarczyk, K Rataj, T Danel, M Warchoł, J. Cheminf. 1212020</p>
<p>Learning representations of inorganic materials from generative adversarial networks, Symmetry. T Hu, H Song, T Jiang, S Li, 2020121889</p>
<p>De novo peptide and protein design using generative adversarial networks: an update. E Lin, C H Lin, H Y Lane, J. Chem. Inf. Model. 6242022</p>
<p>De novo protein design for novel folds using guided conditional wasserstein generative adversarial networks. M Karimi, S Zhu, Y Cao, Y Shen, J. Chem. Inf. Model. 122020</p>
<p>Heterogeneous catalyst design by generative adversarial network and rst-principles based microkinetics. A Ishikawa, Sci. Rep. 121116572022</p>
<p>Quantum generative models for small molecule drug discovery. J Li, R O Topaloglu, S Ghosh, IEEE Trans. Quantum Eng. 22021</p>
<p>Developing an Antiviral Peptides Predictor with Generative Adversarial Network Data Augmentation. T T Lin, Y Y Sun, W C Cheng, I H Lu, S H Chen, C Y Lin, 10.1101/2021.11.29.470292bioRxiv. 2021preprint</p>
<p>Data augmentation and machine learning techniques for control strategy development in bio-polymerization process. S Wei, Z Chen, S K Arumugasamy, I M L Chew, Environ. Sci. Ecotechnology. 111001722022</p>
<p>Recent advances in variational autoencoders with representation learning for biomedical informatics: A survey. R Wei, A Mahmood, IEEE Access. 92020</p>
<p>Variations in variational autoencoders-a comparative evaluation. R Wei, C Garcia, A El-Sayed, V Peterson, A Mahmood, IEEE Access. 82020</p>
<p>Co-VAE: Drug-target binding affinity prediction by co-regularized variational autoencoders. T Li, X M Zhao, L Li, IEEE Trans. Pattern Anal. Mach. Intell. 122021</p>
<p>Variational autoencoder for anti-cancer drug response prediction, arXiv. H Dong, J Xie, Z Jing, D Ren, 10.48550/arXiv.2008.097632020preprint</p>
<p>Generating functional protein variants with variational autoencoders. A Hawkins-Hooker, F Depardieu, S Baur, G Couairon, A Chen, D Bikard, PLoS Comput. Biol. 172e10087362021</p>
<p>Explore protein conformational space with variational autoencoder. H Tian, X Jiang, F Trozzi, S Xiao, E C Larson, P Tao, Front. Mol. Biosci. 2021, 8, 781635</p>
<p>Encoding and exploring latent design space of optimal material structures via a VAE-LSTM model. A J Lew, M J Buehler, Forces Mech. 2021, 5, 100054</p>
<p>Designing catalysts with deep generative models and computational data. A case study for Suzuki cross coupling reactions. O Schilter, A Vaucher, P Schwaller, T Laino, Digital Discovery. 232023</p>
<p>Multimodal protein representation learning and target-aware variational auto-encoders for protein-binding ligand generation. N K Ngo, T S Hy, Mach. Learn.: Sci. Technol. 52250212024</p>
<p>A review of mathematical representations of biomolecular data. D D Nguyen, Z Cang, G W Wei, Phys. Chem. Chem. Phys. 82020</p>
<p>Representation of molecular structures with persistent homology for machine learning applications in chemistry. J Townsend, C P Micucci, J H Hymel, V Maroulas, K D Vogiatzis, Nat. Commun. 11132302020</p>
<p>Algebraic graph-assisted bidirectional transformers for molecular property prediction. D Chen, K Gao, D D Nguyen, X Chen, Y Jiang, G W Wei, Nat. Commun. 12135212021</p>
<p>Cirrhosis disease classication by using polynomial feature and XGBoosting. A Goyal, A Zafar, M Kumar, S Bharadwaj, B Tejas, J Malik, 2022 13th International Conference on Computing Communication and Networking Technologies (ICCCNT). IEEE2022</p>
<p>Feature pyramid transformer. D Zhang, H Zhang, J Tang, M Wang, X Hua, Q Sun, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringerAugust 23-28, 2020. 2020Proceedings, Part XXVIII 16</p>
<p>Graph-Based Bidirectional Transformer Decision Threshold Adjustment Algorithm for Class-Imbalanced Molecular Data. N Hayes, E Merkurjev, G W Wei, J. Comput. Biophys. Chem. 102024</p>
<p>Adaptive factorization network: Learning adaptive-order feature interactions. W Cheng, Y Shen, L Huang, Proceedings of the AAAI Conference on Articial Intelligence. the AAAI Conference on Articial Intelligence202034</p>
<p>. T Verdonck, B Baesens, M Óskarsdóttir, S Broucke, Special issue on feature engineering editorial. 72024Mach. Learn.</p>
<p>GA-ENs: A novel drug-target interactions prediction method by incorporating prior Knowledge Graph into dual Wasserstein Generative Adversarial Network with gradient penalty. G Li, W Sun, J Xu, L Hu, W Zhang, P Zhang, Appl. So Comput. 1101512023</p>
<p>MDF-SA-DDI: predicting drug-drug interaction events based on multi-source drug fusion, multi-source feature fusion and transformer self-attention mechanism. S Lin, Y Wang, L Zhang, Y Chu, Y Liu, Y Fang, Royal Society of Chemistry Chem. Sci. 23176552022. 2025Briengs Bioinf.</p>
<p>. Review Chemical Science. </p>
<p>Protein function prediction is improved by creating synthetic feature samples with generative adversarial networks. C Wan, D T Jones, Nat. Mach. Intell. 292020</p>
<p>PAtbP-EnC: Identifying anti-tubercular peptides using multi-feature representation and genetic algorithm-based deep ensemble model. S Akbar, A Raza, T Al Shloul, A Ahmad, A Saeed, Y Y Ghadi, IEEE Access. 112023</p>
<p>Boosting methods for multi-class imbalanced data classication: an experimental review. J Tanha, Y Abdi, N Samadi, N Razzaghi, M Asadpour, J. Big Data. 72020</p>
<p>XGB-DrugPred: computational prediction of druggable proteins using eXtreme gradient boosting and optimized features set. R Sikander, A Ghulam, F Ali, Sci. Rep. 12155052022</p>
<p>Protein-DNA binding residue prediction via bagging strategy and sequence-based cube-format feature. J Hu, Y S Bai, L L Zheng, N X Jia, D J Yu, G J Zhang, IEEE/ACM Trans. Comput. Biol. Bioinf. 1962021</p>
<p>A brief introduction to boosting. R E Schapire, Ijcai, Citeseer. 199999</p>
<p>Xgboost: A scalable tree boosting system. T Chen, C Guestrin, Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining. the 22nd acm sigkdd international conference on knowledge discovery and data mining2016</p>
<p>Greedy function approximation: a gradient boosting machine. J H Friedman, Ann. Stat. 2001</p>
<p>Machine learning for electrocatalyst and photocatalyst design and discovery. H Mai, T C Le, D Chen, D A Winkler, R A Caruso, Chem. Rev. 122162022</p>
<p>Deep geometric representations for modeling effects of mutations on protein-protein binding affinity. X Liu, Y Luo, P Li, S Song, J Peng, PLoS Comput. Biol. 8e10092842021</p>
<p>Chemboost: A chemical language based approach for protein-ligand binding affinity prediction. R Özçelik, H Öztürk, A Özgür, E Ozkirimli, Mol. Inf. 5402021. 2000212</p>
<p>Biomaterials by design: Harnessing data for future development. K Xue, F Wang, A Suwardi, M Y Han, P Teo, P Wang, Mater. Today Bio. 121001652021</p>
<p>Epigenome-wide DNA methylation and transcriptome proling of localized and locally advanced prostate cancer: Uncovering new molecular markers. Q Liu, M Reed, H Zhu, Y Cheng, J Almeida, G Fruhbeck, Genomics. 11451104742022</p>
<p>Bagging-based machine learning algorithms for landslide susceptibility modeling. T Zhang, Q Fu, H Wang, F Liu, H Wang, L Han, Nat. Hazards. 11022022</p>
<p>An Evolution of Hybrid Bagging Technique in Chemoinformatic and Drug Discovery. S Aluvala, J N Kalshetty, S O Husain, J Kaur, H K Thind, 2024 Second International Conference on Data Science and Information System (ICDSIS). IEEE2024</p>
<p>Developing a Semi-Supervised Approach Using a PU-Learning-Based Data Augmentation Strategy for Multitarget Drug Discovery. Y Hao, B Li, D Huang, S Wu, T Wang, L Fu, Int. J. Mol. Sci. 1582392024</p>
<p>Forecasting Staphylococcus aureus infections using genome-wide association studies, machine learning, and transcriptomic approaches. M Sassi, J Bronsard, G Pascreau, M Emily, P Y Donnio, M Revest, Msystems. 74e003782022</p>
<p>Machine-learning adsorption on binary alloy surfaces for catalyst screening. Tr Wang, Jc Li, W Shu, S Hu, R Ouyang, W Li, Chin. J. Chem. Phys. 62020</p>
<p>Toxicity detection of small drug molecules of the mitochondrial membrane potential signalling pathway using bagging-based ensemble learning. V K Gupta, Int. J. Data Min. Bioinform. 271-32022</p>
<p>DrugHybrid_BS: Using hybrid feature combined with bagging-SVM to predict potentially druggable proteins. Y Gong, B Liao, P Wang, Q Zou, Front. Pharmacol. 7718082021</p>
<p>A review of methods for imbalanced multi-label classication. A N Tarekegn, M Giacobini, K Michalak, Pattern Recognit. 1079652021</p>
<p>Integration of transcription regulation and functional genomic data reveals lncRNA SNHG6's role in hematopoietic differentiation and leukemia. J M Hazan, R Amador, T Ali-Nasser, T Lahav, S R Shotan, M Steinberg, J. Biomed. Sci. 1272024</p>
<p>Cost-Sensitive Deep Learning Models for Drug-Target Interaction Prediction. N Aleb, Int. J. Adv. So Comput. Appl. 132252021</p>
<p>Machine learning and feature selection for drug response prediction in precision oncology applications. M Ali, T Aittokallio, Biophys. Rev. 1112019</p>
<p>Combining feature engineering and feature selection to improve the prediction of methionine oxidation sites in proteins. F J Veredas, D Urda, J L Subirats, F R Cantón, J C Aledo, Neural Comput. Appl. 3222020</p>
<p>Using big data analytics to "back engineer" protein conformational selection mechanisms. S Gupta, J Baudry, V Menon, Molecules. 27825092022</p>
<p>Ensemble learning-based feature selection for phage protein prediction. S Liu, C Cui, H Chen, T Liu, Front. Microbiol. 9326612022</p>
<p>Consistent gene signature of schizophrenia identied by a novel feature selection strategy from comprehensive sets of transcriptomic data. Q Yang, B Li, J Tang, X Cui, Y Wang, X Li, Briengs Bioinf. 2132020</p>
<p>. Chemical Science Review. </p>
<p>Drug-target interaction prediction based on protein features, using wrapper feature selection. H Mesrabadi, K Faez, J Pirgazi, Sci. Rep. 13135942023</p>
<p>A comprehensive review of dimensionality reduction techniques for feature selection and feature extraction. R Zebari, A Abdulazeez, D Zeebaree, D Zebari, J Saeed, J. Appl. Sci. Technol. Trends. 112020</p>
<p>Sequence-based prediction model of protein crystallization propensity using machine learning and two-level feature selection. N Q K Le, W Li, Y Cao, Briengs Bioinf. 53192023</p>
<p>From Characterization to Discovery: Articial Intelligence, Machine Learning and High-Throughput Experiments for Heterogeneous Catalyst Design. J Benavides-Hernández, F Dumeignil, ACS Catal. 152024</p>
<p>Machine Learning Prediction of CO Adsorption Energies and Properties of Layered Alloys Using an Improved Feature Selection Algorithm. T T Shi, G Y Liu, Z X Chen, J. Phys. Chem. C. 127202023</p>
<p>A machine-learning-based composition design of ternary Cu-based Rochow-Müller catalyst with high M2 selectivity. T Ma, J Wang, L Ban, H He, Z Lu, J Zhu, Appl. Catal., A. 1195922024</p>
<p>Relation path feature embedding based convolutional neural network method for drug discovery. D Zhao, J Wang, S Sang, H Lin, J Wen, C Yang, BMC Med. Inf. Decis. Making. 192019</p>
<p>DTI-CDF: a cascade deep forest model towards the prediction of drug-target interactions based on hybrid features. Y Chu, A C Kaushik, X Wang, W Wang, Y Zhang, X Shan, Briengs Bioinf. 2212021</p>
<p>CPPred-FL: a sequence-based predictor for large-scale identication of cell-penetrating peptides by feature representation learning. X Qiang, C Zhou, X Ye, P Du, R Su, L Wei, Briengs Bioinf. 2112020</p>
<p>Index of balanced accuracy: A performance measure for skewed class distributions. V García, R A Mollineda, J S Sánchez, Iberian conference on pattern recognition and image analysis. Springer2009</p>
<p>The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classication evaluation. D Chicco, G Jurman, BMC Genomics. 212020</p>
<p>A review of evaluation metrics in machine learning algorithms. G Naidu, T Zuva, E M Sibanda, Computer Science On-line Conference. Springer2023</p>
<p>The Matthews correlation coefficient (MCC) should replace the ROC AUC as the standard metric for assessing binary classication. D Chicco, G Jurman, BioData Min. 16142023</p>
<p>A Talukder, Multimodal Data Fusion and Machine Learning for Deciphering Protein-Protein Interactions. 2021Texas A&amp;M University</p>
<p>A multimodal data fusion-based deep learning approach for drug-drug interaction prediction. A Huang, X Xie, X Wang, S Peng, International Symposium on Bioinformatics Research and Applications. Springer2022</p>
<p>Empowering pandemic response with federated learning for protein sequence data analysis. P Chourasia, Z Tayebi, S Ali, M Patterson, 2023 International Joint Conference on Neural Networks (IJCNN). IEEE2023</p>
<p>FL-QSAR: a federated learning-based QSAR prototype for collaborative drug discovery. S Chen, D Xue, G Chuai, Q Yang, Q Liu, Bioinformatics. 362020</p>
<p>Self-supervised learning with chemistry-aware fragmentation for effective molecular property prediction. A Xie, Z Zhang, J Guan, S Zhou, Briengs Bioinf. 52962023</p>
<p>Molecular dynamics simulation of protein and protein-ligand complexes, Comput.-Aided Drug Des. R Shukla, T Tripathi, 2020</p>
<p>Density functional theory researches for atomic structure, properties prediction, and rational design of selective catalytic reduction catalysts: Current progresses and future perspectives. B Guan, H Jiang, Y Wei, Z Liu, X Wu, H Lin, Mol. Catal. 1117042021</p>
<p>Protein-ligand free energies of binding from full-protein DFT calculations: convergence and choice of exchangecorrelation functional. L Gundelach, T Fox, C S Tautermann, C K Skylaris, Phys. Chem. Chem. Phys. 152021</p>
<p>Mechanism exploration and catalyst design for hydrogen evolution reaction accelerated by density functional theory simulations. J Liu, Z Wang, L Kou, Y Gu, ACS Sustain. Chem. Eng. 1122023</p>
<p>Using molecular docking and molecular dynamics to investigate protein-ligand interactions. C J Morris, D D Corte, Mod. Phys. Lett. B. 350821300022021</p>
<p>Medical data augmentation via chatgpt: A case study on medication identication and medication event classication, arXiv. S Sarker, L Qian, X Dong, 10.48550/arXiv.2306.072972023preprint</p>
<p>Chemformer: a pre-trained transformer for computational chemistry. R Irwin, S Dimitriadis, J He, E J Bjerrum, Mach. Learn.: Sci. Technol. 2022115022</p>
<p>Leveraging molecular structure and bioactivity with chemical language models for de novo drug design. M Moret, I Pachon Angona, L Cotos, S Yan, K Atz, C Brunner, Nat. Commun. 1411142023</p>
<p>MathDL: mathematical deep learning for D3R Grand Challenge. D D Nguyen, K Gao, M Wang, G W Wei, J. Comput.-Aided Mol. Des. 42020</p>
<p>Mathematical deep learning for pose and binding affinity prediction and ranking in D3R Grand Challenges. D D Nguyen, Z Cang, K Wu, M Wang, Y Cao, G W Wei, J. Comput.-Aided Mol. Des. 332019</p>
<p>Mutations strengthened SARS-CoV-2 infectivity. J Chen, R Wang, M Wang, G W Wei, J. Mol. Biol. 192020</p>
<p>Omicron BA. 2 (B. 1.1. 529.2): high potential for becoming the next dominant variant. J Chen, G W Wei, J. Phys. Chem. Lett. 172022</p>
<p>Persistent Laplacian projected Omicron BA. 4 and BA. 5 to become new dominating variants. J Chen, Y Qiu, R Wang, G W Wei, Comput. Biol. Med. 1062622022</p>
<p>Persistent spectral theory-guided protein engineering. Y Qiu, G W Wei, Nat. Comput. Sci. 20232</p>
<p>A topology-based network tree for the prediction of protein-protein binding affinity changes following mutation. M Wang, Z Cang, G W Wei, Nat. Mach. Intell. 222020</p>
<p>TopP-S: Persistent homology-based multi-task deep neural networks for simultaneous predictions of partition coefficient and aqueous solubility. K Wu, Z Zhao, R Wang, G W Wei, J. Comput. Chem. 202018</p>
<p>Tidal: topology-inferred drug addiction learning. Z Zhu, B Dou, Y Cao, J Jiang, Y Zhu, D Chen, J. Chem. Inf. Model. 6352023</p>
<p>Multi-Cover Persistence (MCP)-based machine learning for polymer property prediction. Y Zhang, C Shen, K Xia, Briengs Bioinf. 6e4652024</p>
<p>Multiscale topology-enabled structure-to-sequence transformer for protein-ligand interaction predictions. D Chen, J Liu, G W Wei, Nat. Mach. Intell. 672024</p>
<p>Knot data analysis using multiscale Gauss link integral. L Shen, H Feng, F Li, F Lei, J Wu, G W Wei, Proc. Natl. Acad. Sci. U. S. A. 42e24084311212024</p>
<p>Machine learningdriven new material discovery. J Cai, X Chu, K Xu, H Li, J Wei, Nanoscale Adv. 282020</p>
<p>High-throughput computational screening of nanoporous materials in targeted applications. E Ren, P Guilbaud, F X Coudert, Digital Discovery. 142022</p>
<p>Machine learning methods for small data challenges in molecular science. B Dou, Z Zhu, E Merkurjev, L Ke, L Chen, J Jiang, Chem. Rev. 132023</p>
<p>LSTM-PHV: prediction of human-virus protein-protein interactions by LSTM with word2vec. S Tsukiyama, M M Hasan, S Fujii, H Kurata, Briengs Bioinf. 62282021</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-689 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-689</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-689</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-9a970e7ec3158655738d4f8494d27ab9e00337cc</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9a970e7ec3158655738d4f8494d27ab9e00337cc" target="_blank">QAFactEval: Improved QA-Based Factual Consistency Evaluation for Summarization</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work proposes an optimized metric, which they call QAFactEval, that leads to a 14% average improvement over previous QA-based metrics on the SummaC factual consistency benchmark, and also outperforms the best-performing entailment-based metric.</p>
                <p><strong>Paper Abstract:</strong> Factual consistency is an essential quality of text summarization models in practical settings. Existing work in evaluating this dimension can be broadly categorized into two lines of research, entailment-based and question answering (QA)-based metrics, and different experimental setups often lead to contrasting conclusions as to which paradigm performs the best. In this work, we conduct an extensive comparison of entailment and QA-based metrics, demonstrating that carefully choosing the components of a QA-based metric, especially question generation and answerability classification, is critical to performance. Building on those insights, we propose an optimized metric, which we call QAFactEval, that leads to a 14% average improvement over previous QA-based metrics on the SummaC factual consistency benchmark, and also outperforms the best-performing entailment-based metric. Moreover, we find that QA-based and entailment-based metrics can offer complementary signals and be combined into a single metric for a further performance boost.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e689.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e689.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>hyperparam_variation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hyperparameter and experimental-setup variation across papers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Differences in hyperparameter choices, input granularity, and baseline model selection across papers and implementations that lead to contrasting conclusions about metric performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Summarization metric benchmarking / SummaC experiments</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A suite of factual-consistency metric evaluations across multiple datasets (SummaC benchmark), comparing entailment-based and QA-based metrics under different hyperparameter and granularity choices.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section / literature claims</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>metric implementation scripts from original GitHub repositories and SacreROUGE library (PyTorch / Transformers inference code)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>hyperparameter mismatch / inconsistent experimental setup</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Prior papers and reported results differ in baseline models, input granularity, and hyperparameter choices; these differences are not always made explicit or standardized, producing contradictory claims about which metric family (entailment vs QA) performs best. The paper documents that some strong entailment baselines had hyperparameters derived from Laban et al. (2021) while QAFactEval tuned hyperparameters on SummaC validation, which affects comparability.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>experimental setup and hyperparameters (input granularity, model checkpoints, thresholding)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>comparative meta-analysis and controlled experiments: authors re-ran and/or used original implementations and varied hyperparameters (ablation studies) to observe performance changes; they reference differences reported across prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Performance on SummaC benchmark (balanced accuracy aggregated across six datasets) and dataset-specific test scores; use of threshold selection on validation sets; statistical testing via bootstrap resampling with Bonferroni correction to assess significance.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Substantial: different hyperparameter and granularity choices change which metric appears best. The paper reports that tuning QA components and hyperparameters yields QAFactEval at 77.8 benchmark vs prior QA-metric QuestEval at 68.2 (≈14% relative improvement). It also notes individual metric score variability across datasets (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Described as common across the literature on factual-consistency metrics; the paper cites multiple prior works with differing choices but does not provide an aggregate prevalence percentage.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Inconsistent or underspecified reporting of experimental details in natural-language descriptions (papers), and differing default choices in public code repositories and evaluation scripts.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Standardize evaluation by reusing common benchmarks (SummaC), apply uniform thresholding procedures drawn from validation sets, explicitly tune hyperparameters per benchmark, and release code/outputs for reproducibility (authors release QAFactEval code and follow SacreROUGE conventions).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective: tuning and standardizing led to a stable metric combination (QAFactEval) that outperforms prior QA-based metrics on SummaC (QAFactEval 77.8 vs QuestEval 68.2); statistical testing shows significant improvement on the overall benchmark at the 0.01 level after Bonferroni correction.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>natural language processing (summarization evaluation), machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'QAFactEval: Improved QA-Based Factual Consistency Evaluation for Summarization', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e689.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e689.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>qg_description_vs_output</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Question generation model description versus actual generated-question properties</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mismatch between the conceptual description that better (e.g., more fluent or abstractive) QG models produce better evaluation questions and the empirical observation that longer, more extractive QG output (BART-QA2D) performed better downstream.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>QA-based factual consistency pipeline (QG + QA + overlap scoring)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pipeline that extracts answers from a summary, generates questions conditioned on those answers, answers the questions from the source document, and computes an answer-overlap-based factuality score.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper description of expected QG behavior / prior work claims</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>question generation model implementations (BART-large QA2D, T5-base SQuAD, MixQG models) used via model checkpoints and generation code</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>algorithmic-behavior mismatch / description vs empirical output</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Although prior work and intuitions might suggest that more fluent or abstractive questions (shorter, human-like) are better, the authors find that BART-large (QA2D) produced longer (~17 tokens), more extractive questions (≈20% novel unigrams) that were easier for the QA model to answer and led to better metric performance than more abstractive QG outputs (e.g., T5-base SQuAD with ~47% novel unigrams). Thus the natural-language expectation (fluency equals better evaluation) misaligns with code-output consequences.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>question generation component in the QA pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Ablation studies across QG model variants and qualitative inspection of generated questions (Table 2 and Table 4 examples).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Downstream balanced accuracy on SummaC validation/test and ablation comparisons (e.g., QAFactEval with BART-large (QA2D) vs T5-base (SQuAD): reported validation scores 77.5 vs 67.0 in Table 2). Qualitative counts of question length and novel unigram rates were reported.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Large: choice of QG model changes performance by many points. Example: swapping to T5-base (SQuAD) dropped validation performance to 67.0 from the best 77.5 (approx. 10.5 absolute points). Authors note that more extractive, longer questions improved downstream factual consistency detection.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed within the authors' ablation experiments across multiple QG models; no cross-paper prevalence statistic given.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Simplifying assumptions in natural-language claims about quality (e.g., fluency or human-like questions are best) and lack of attention to how QG surface properties affect downstream QA model performance in code/implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Empirically evaluate multiple QG models and metrics, prefer QG models that produce extractive yet answerable questions for the downstream QA model (BART-large QA2D in this work), and include qualitative checks (length, extractiveness) when selecting QG implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Demonstrably effective in this paper: using BART-large (QA2D) as QG improved final metric performance relative to other QG variants (best validation score 77.5).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>natural language processing (question generation and evaluation), summarization</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'QAFactEval: Improved QA-Based Factual Consistency Evaluation for Summarization', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e689.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e689.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>answerability_handling_mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch in handling question answerability between descriptions and implemented scoring</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Differences between conceptual treatments of unanswerable/generated questions and concrete implementation choices (IsAnsweredInput, IsAnsweredSumm filtering, and Answerability Penalty) substantially affect evaluation scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>QA-based metric pipeline (answerability filtering and scoring policy)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mechanisms that decide whether a generated question is answerable from the input (source) and/or the summary, and how to score questions judged unanswerable (e.g., set overlap score to 0 or use QA model's top answer).</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>paper description of intended scoring policy / metric specification</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>filtering and scoring code that applies Electra-large IsAnswered classifiers and Answerability Penalty logic in evaluation scripts</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / different default scoring policy</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Natural-language descriptions often omit detailed policies for unanswerable questions; the authors implement explicit filters: (1) IsAnsweredInput (Electra-large) labels whether a question is answerable on the source; (2) IsAnsweredSumm Filter removes questions the QA model deems unanswerable on the summary; and (3) the Answerability Penalty sets overlap scores to 0 for questions unanswerable on the source. Alternative implementations (e.g., scoring the QA model's most probable answer even when judged unanswerable) produce notably different results.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>question filtering and answer-overlap evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Empirical ablation experiments that toggle IsAnsweredSumm Filter and Answerability Penalty and compare resulting metric performance (Table 2 ablations).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Validation balanced-accuracy changes on SummaC when removing filters/penalties: with both filters performance best (77.5), no IsAnsweredSumm Filter -> 73.8, no Answerability Penalty -> 72.1, neither filter -> 67.4.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Substantial quantitative impact: removing both filters reduced validation score from 77.5 to 67.4 (approx. 10.1 absolute points). Not applying just the IsAnsweredSumm Filter decreased score by 3.7 points; not applying just the Answerability Penalty decreased by 5.4 points.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed in the authors' QA-metric experiments; the paper cites prior work emphasizing answerability but finds that implementations that ignore explicit answerability handling perform worse.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Ambiguous or underspecified descriptions of how to handle unanswerable questions in natural-language method sections and variance in code-level choices for scoring such cases.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Explicitly incorporate answerability classifiers and apply an Answerability Penalty (set overlap to 0 when question unanswerable on source) and apply summary-answerability filtering to remove noisy questions; choose QA models trained on unanswerable detection for consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective: inclusion of both IsAnsweredSumm filtering and Answerability Penalty yields the best reported configuration (validation 77.5). Removing these components produced multipoint drops (see measurement_method).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>natural language processing (QA-based evaluation), evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'QAFactEval: Improved QA-Based Factual Consistency Evaluation for Summarization', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e689.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e689.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>answer_selection_misalignment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Misalignment between described answer-selection strategy and practical coverage/effect on metrics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Differences between simple natural-language recommendations to use named entities or noun phrases and the empirical behavior where different answer-selection methods (NP Chunks, Max NP, NER, ALL) lead to large performance differences due to coverage and precision trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Answer selection component of QA-based factuality pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Procedure to extract candidate 'answers' (information units) from the summary to be used as anchors for question generation (techniques: NER, NP Chunks, Max NP, ALL).</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>paper method specification and prior-work recommendations</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>answer-extraction code using dependency parsing, NP chunking, and named-entity recognition libraries called during preprocessing</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / coverage mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Though many descriptions suggest using named entities as salient answers, the code-level outcome shows NER extracts too few answers (≈3 per summary) resulting in poor metric coverage and lower performance; NP Chunks provide higher coverage (>10 answers per summary) and yield substantially better metric results. Thus an intuitive or commonly-stated choice (NER) misaligns with what the implementation needs for good evaluation coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data preprocessing / answer extraction</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Empirical ablation comparing answer-extraction strategies and reporting downstream metric performance (Table 2) and reporting average counts of extracted answers.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Validation balanced-accuracy for each answer-selection method: NP Chunks (best) -> baseline 77.5; Max NP -> 75.7; ALL -> 75.7; NER -> 66.4. Also reported average number of extracted answers (NER ~3, others >10).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Large: choosing NER over NP Chunks reduced validation balanced accuracy from 77.5 to 66.4 (≈11.1 absolute points). Reduced coverage also affects the sensitivity of QA-based metrics to factual errors.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed within this paper's experiments across datasets; the authors cite prior protocols (Deutsch et al., 2020) recommending NP Chunks and validate this empirically.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Simplified or generic natural-language guidance (e.g., 'use named entities') that omits practical consequences for coverage and precision in pipeline implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Adopt NP Chunks for answer selection (empirically validated), or combine methods (ALL) if needed; report number of extracted answers and validate coverage on target datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective: NP Chunks yielded the best validation performance (77.5). Using NER alone performed much worse (66.4).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>natural language processing (preprocessing for QA-based evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'QAFactEval: Improved QA-Based Factual Consistency Evaluation for Summarization', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Summac: Re-visiting nli-based models for inconsistency detection in summarization <em>(Rating: 2)</em></li>
                <li>Towards question-answering as an automatic metric for evaluating the content quality of a summary <em>(Rating: 2)</em></li>
                <li>FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization <em>(Rating: 2)</em></li>
                <li>Questeval: Summarization asks for fact-based evaluation <em>(Rating: 2)</em></li>
                <li>On faithfulness and factuality in abstractive summarization <em>(Rating: 1)</em></li>
                <li>FFCI: A framework for interpretable automatic evaluation of summarization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-689",
    "paper_id": "paper-9a970e7ec3158655738d4f8494d27ab9e00337cc",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "hyperparam_variation",
            "name_full": "Hyperparameter and experimental-setup variation across papers",
            "brief_description": "Differences in hyperparameter choices, input granularity, and baseline model selection across papers and implementations that lead to contrasting conclusions about metric performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Summarization metric benchmarking / SummaC experiments",
            "system_description": "A suite of factual-consistency metric evaluations across multiple datasets (SummaC benchmark), comparing entailment-based and QA-based metrics under different hyperparameter and granularity choices.",
            "nl_description_type": "research paper methods section / literature claims",
            "code_implementation_type": "metric implementation scripts from original GitHub repositories and SacreROUGE library (PyTorch / Transformers inference code)",
            "gap_type": "hyperparameter mismatch / inconsistent experimental setup",
            "gap_description": "Prior papers and reported results differ in baseline models, input granularity, and hyperparameter choices; these differences are not always made explicit or standardized, producing contradictory claims about which metric family (entailment vs QA) performs best. The paper documents that some strong entailment baselines had hyperparameters derived from Laban et al. (2021) while QAFactEval tuned hyperparameters on SummaC validation, which affects comparability.",
            "gap_location": "experimental setup and hyperparameters (input granularity, model checkpoints, thresholding)",
            "detection_method": "comparative meta-analysis and controlled experiments: authors re-ran and/or used original implementations and varied hyperparameters (ablation studies) to observe performance changes; they reference differences reported across prior work.",
            "measurement_method": "Performance on SummaC benchmark (balanced accuracy aggregated across six datasets) and dataset-specific test scores; use of threshold selection on validation sets; statistical testing via bootstrap resampling with Bonferroni correction to assess significance.",
            "impact_on_results": "Substantial: different hyperparameter and granularity choices change which metric appears best. The paper reports that tuning QA components and hyperparameters yields QAFactEval at 77.8 benchmark vs prior QA-metric QuestEval at 68.2 (≈14% relative improvement). It also notes individual metric score variability across datasets (Table 3).",
            "frequency_or_prevalence": "Described as common across the literature on factual-consistency metrics; the paper cites multiple prior works with differing choices but does not provide an aggregate prevalence percentage.",
            "root_cause": "Inconsistent or underspecified reporting of experimental details in natural-language descriptions (papers), and differing default choices in public code repositories and evaluation scripts.",
            "mitigation_approach": "Standardize evaluation by reusing common benchmarks (SummaC), apply uniform thresholding procedures drawn from validation sets, explicitly tune hyperparameters per benchmark, and release code/outputs for reproducibility (authors release QAFactEval code and follow SacreROUGE conventions).",
            "mitigation_effectiveness": "Effective: tuning and standardizing led to a stable metric combination (QAFactEval) that outperforms prior QA-based metrics on SummaC (QAFactEval 77.8 vs QuestEval 68.2); statistical testing shows significant improvement on the overall benchmark at the 0.01 level after Bonferroni correction.",
            "domain_or_field": "natural language processing (summarization evaluation), machine learning",
            "reproducibility_impact": true,
            "uuid": "e689.0",
            "source_info": {
                "paper_title": "QAFactEval: Improved QA-Based Factual Consistency Evaluation for Summarization",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "qg_description_vs_output",
            "name_full": "Question generation model description versus actual generated-question properties",
            "brief_description": "Mismatch between the conceptual description that better (e.g., more fluent or abstractive) QG models produce better evaluation questions and the empirical observation that longer, more extractive QG output (BART-QA2D) performed better downstream.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "QA-based factual consistency pipeline (QG + QA + overlap scoring)",
            "system_description": "Pipeline that extracts answers from a summary, generates questions conditioned on those answers, answers the questions from the source document, and computes an answer-overlap-based factuality score.",
            "nl_description_type": "research paper description of expected QG behavior / prior work claims",
            "code_implementation_type": "question generation model implementations (BART-large QA2D, T5-base SQuAD, MixQG models) used via model checkpoints and generation code",
            "gap_type": "algorithmic-behavior mismatch / description vs empirical output",
            "gap_description": "Although prior work and intuitions might suggest that more fluent or abstractive questions (shorter, human-like) are better, the authors find that BART-large (QA2D) produced longer (~17 tokens), more extractive questions (≈20% novel unigrams) that were easier for the QA model to answer and led to better metric performance than more abstractive QG outputs (e.g., T5-base SQuAD with ~47% novel unigrams). Thus the natural-language expectation (fluency equals better evaluation) misaligns with code-output consequences.",
            "gap_location": "question generation component in the QA pipeline",
            "detection_method": "Ablation studies across QG model variants and qualitative inspection of generated questions (Table 2 and Table 4 examples).",
            "measurement_method": "Downstream balanced accuracy on SummaC validation/test and ablation comparisons (e.g., QAFactEval with BART-large (QA2D) vs T5-base (SQuAD): reported validation scores 77.5 vs 67.0 in Table 2). Qualitative counts of question length and novel unigram rates were reported.",
            "impact_on_results": "Large: choice of QG model changes performance by many points. Example: swapping to T5-base (SQuAD) dropped validation performance to 67.0 from the best 77.5 (approx. 10.5 absolute points). Authors note that more extractive, longer questions improved downstream factual consistency detection.",
            "frequency_or_prevalence": "Observed within the authors' ablation experiments across multiple QG models; no cross-paper prevalence statistic given.",
            "root_cause": "Simplifying assumptions in natural-language claims about quality (e.g., fluency or human-like questions are best) and lack of attention to how QG surface properties affect downstream QA model performance in code/implementations.",
            "mitigation_approach": "Empirically evaluate multiple QG models and metrics, prefer QG models that produce extractive yet answerable questions for the downstream QA model (BART-large QA2D in this work), and include qualitative checks (length, extractiveness) when selecting QG implementations.",
            "mitigation_effectiveness": "Demonstrably effective in this paper: using BART-large (QA2D) as QG improved final metric performance relative to other QG variants (best validation score 77.5).",
            "domain_or_field": "natural language processing (question generation and evaluation), summarization",
            "reproducibility_impact": true,
            "uuid": "e689.1",
            "source_info": {
                "paper_title": "QAFactEval: Improved QA-Based Factual Consistency Evaluation for Summarization",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "answerability_handling_mismatch",
            "name_full": "Mismatch in handling question answerability between descriptions and implemented scoring",
            "brief_description": "Differences between conceptual treatments of unanswerable/generated questions and concrete implementation choices (IsAnsweredInput, IsAnsweredSumm filtering, and Answerability Penalty) substantially affect evaluation scores.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "QA-based metric pipeline (answerability filtering and scoring policy)",
            "system_description": "Mechanisms that decide whether a generated question is answerable from the input (source) and/or the summary, and how to score questions judged unanswerable (e.g., set overlap score to 0 or use QA model's top answer).",
            "nl_description_type": "paper description of intended scoring policy / metric specification",
            "code_implementation_type": "filtering and scoring code that applies Electra-large IsAnswered classifiers and Answerability Penalty logic in evaluation scripts",
            "gap_type": "incomplete specification / different default scoring policy",
            "gap_description": "Natural-language descriptions often omit detailed policies for unanswerable questions; the authors implement explicit filters: (1) IsAnsweredInput (Electra-large) labels whether a question is answerable on the source; (2) IsAnsweredSumm Filter removes questions the QA model deems unanswerable on the summary; and (3) the Answerability Penalty sets overlap scores to 0 for questions unanswerable on the source. Alternative implementations (e.g., scoring the QA model's most probable answer even when judged unanswerable) produce notably different results.",
            "gap_location": "question filtering and answer-overlap evaluation",
            "detection_method": "Empirical ablation experiments that toggle IsAnsweredSumm Filter and Answerability Penalty and compare resulting metric performance (Table 2 ablations).",
            "measurement_method": "Validation balanced-accuracy changes on SummaC when removing filters/penalties: with both filters performance best (77.5), no IsAnsweredSumm Filter -&gt; 73.8, no Answerability Penalty -&gt; 72.1, neither filter -&gt; 67.4.",
            "impact_on_results": "Substantial quantitative impact: removing both filters reduced validation score from 77.5 to 67.4 (approx. 10.1 absolute points). Not applying just the IsAnsweredSumm Filter decreased score by 3.7 points; not applying just the Answerability Penalty decreased by 5.4 points.",
            "frequency_or_prevalence": "Observed in the authors' QA-metric experiments; the paper cites prior work emphasizing answerability but finds that implementations that ignore explicit answerability handling perform worse.",
            "root_cause": "Ambiguous or underspecified descriptions of how to handle unanswerable questions in natural-language method sections and variance in code-level choices for scoring such cases.",
            "mitigation_approach": "Explicitly incorporate answerability classifiers and apply an Answerability Penalty (set overlap to 0 when question unanswerable on source) and apply summary-answerability filtering to remove noisy questions; choose QA models trained on unanswerable detection for consistency.",
            "mitigation_effectiveness": "Effective: inclusion of both IsAnsweredSumm filtering and Answerability Penalty yields the best reported configuration (validation 77.5). Removing these components produced multipoint drops (see measurement_method).",
            "domain_or_field": "natural language processing (QA-based evaluation), evaluation metrics",
            "reproducibility_impact": true,
            "uuid": "e689.2",
            "source_info": {
                "paper_title": "QAFactEval: Improved QA-Based Factual Consistency Evaluation for Summarization",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "answer_selection_misalignment",
            "name_full": "Misalignment between described answer-selection strategy and practical coverage/effect on metrics",
            "brief_description": "Differences between simple natural-language recommendations to use named entities or noun phrases and the empirical behavior where different answer-selection methods (NP Chunks, Max NP, NER, ALL) lead to large performance differences due to coverage and precision trade-offs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Answer selection component of QA-based factuality pipeline",
            "system_description": "Procedure to extract candidate 'answers' (information units) from the summary to be used as anchors for question generation (techniques: NER, NP Chunks, Max NP, ALL).",
            "nl_description_type": "paper method specification and prior-work recommendations",
            "code_implementation_type": "answer-extraction code using dependency parsing, NP chunking, and named-entity recognition libraries called during preprocessing",
            "gap_type": "incomplete specification / coverage mismatch",
            "gap_description": "Though many descriptions suggest using named entities as salient answers, the code-level outcome shows NER extracts too few answers (≈3 per summary) resulting in poor metric coverage and lower performance; NP Chunks provide higher coverage (&gt;10 answers per summary) and yield substantially better metric results. Thus an intuitive or commonly-stated choice (NER) misaligns with what the implementation needs for good evaluation coverage.",
            "gap_location": "data preprocessing / answer extraction",
            "detection_method": "Empirical ablation comparing answer-extraction strategies and reporting downstream metric performance (Table 2) and reporting average counts of extracted answers.",
            "measurement_method": "Validation balanced-accuracy for each answer-selection method: NP Chunks (best) -&gt; baseline 77.5; Max NP -&gt; 75.7; ALL -&gt; 75.7; NER -&gt; 66.4. Also reported average number of extracted answers (NER ~3, others &gt;10).",
            "impact_on_results": "Large: choosing NER over NP Chunks reduced validation balanced accuracy from 77.5 to 66.4 (≈11.1 absolute points). Reduced coverage also affects the sensitivity of QA-based metrics to factual errors.",
            "frequency_or_prevalence": "Observed within this paper's experiments across datasets; the authors cite prior protocols (Deutsch et al., 2020) recommending NP Chunks and validate this empirically.",
            "root_cause": "Simplified or generic natural-language guidance (e.g., 'use named entities') that omits practical consequences for coverage and precision in pipeline implementations.",
            "mitigation_approach": "Adopt NP Chunks for answer selection (empirically validated), or combine methods (ALL) if needed; report number of extracted answers and validate coverage on target datasets.",
            "mitigation_effectiveness": "Effective: NP Chunks yielded the best validation performance (77.5). Using NER alone performed much worse (66.4).",
            "domain_or_field": "natural language processing (preprocessing for QA-based evaluation)",
            "reproducibility_impact": true,
            "uuid": "e689.3",
            "source_info": {
                "paper_title": "QAFactEval: Improved QA-Based Factual Consistency Evaluation for Summarization",
                "publication_date_yy_mm": "2021-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Summac: Re-visiting nli-based models for inconsistency detection in summarization",
            "rating": 2
        },
        {
            "paper_title": "Towards question-answering as an automatic metric for evaluating the content quality of a summary",
            "rating": 2
        },
        {
            "paper_title": "FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization",
            "rating": 2
        },
        {
            "paper_title": "Questeval: Summarization asks for fact-based evaluation",
            "rating": 2
        },
        {
            "paper_title": "On faithfulness and factuality in abstractive summarization",
            "rating": 1
        },
        {
            "paper_title": "FFCI: A framework for interpretable automatic evaluation of summarization",
            "rating": 1
        }
    ],
    "cost": 0.014869249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>QAFactEval: Improved QA-Based Factual Consistency Evaluation for Summarization</h1>
<p>Alexander R. Fabbri Chien-Sheng Wu<br>Wenhao Liu Caiming Xiong<br>Salesforce AI Research<br>{afabbri, wu.jason, wenhao.liu, cxiong}@salesforce.com</p>
<h4>Abstract</h4>
<p>Factual consistency is an essential quality of text summarization models in practical settings. Existing work in evaluating this dimension can be broadly categorized into two lines of research, entailment-based and question answering (QA)-based metrics, and different experimental setups often lead to contrasting conclusions as to which paradigm performs the best. In this work, we conduct an extensive comparison of entailment and QA-based metrics, demonstrating that carefully choosing the components of a QA-based metric, especially question generation and answerability classification, is critical to performance. Building on those insights, we propose an optimized metric, which we call QAFactEval, that leads to a $14 \%$ average improvement over previous QA-based metrics on the SummaC factual consistency benchmark, and also outperforms the best-performing entailment-based metric. Moreover, we find that QA-based and entailment-based metrics can offer complementary signals and be combined into a single metric for a further performance boost.</p>
<h2>1 Introduction</h2>
<p>Text summarization aims to compress long document(s) into a short and fluent form that preserves salient information. The field has benefited from the application of pretrained methods (Liu and Lapata, 2019; Lewis et al., 2020; Zhang et al., 2020a). However, state-of-the-art models are not always factually consistent with the source documents they are conditioned on (Maynez et al., 2020; Fabbri et al., 2021). Thus, determining the factual consistency of a summary remains an essential task.</p>
<p>Recent metrics for summarization factual consistency can be broadly split into two categories: 1) Entailment-based metrics that determine whether the content in the summary is entailed by the input document (Kryscinski et al., 2020; Koto et al.,</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Document</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">The Knicks beat the Rockets. The fans were excited.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Summary</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">The Knicks beat the Bucks.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Entailment Matrix</td>
<td style="text-align: center;">Selected Answer</td>
</tr>
<tr>
<td style="text-align: center;">[Contra, Neutral, Support]</td>
<td style="text-align: center;">the Bucks</td>
</tr>
<tr>
<td style="text-align: center;">$\left[\begin{array}{llll}0.90 &amp; 0.07 &amp; 0.03 \ 0.02 &amp; 0.90 &amp; 0.08\end{array}\right]$</td>
<td style="text-align: center;">Generated Question</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Who did the Knicks beat?</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">QA Output</td>
</tr>
<tr>
<td style="text-align: center;">Max Support Score</td>
<td style="text-align: center;">the Rockets</td>
</tr>
<tr>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">Answer Overlap Score</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.20</td>
</tr>
</tbody>
</table>
<p>Table 1: Toy example of a factual inconsistency between a summary and a source document. Left: The entailment-based metric computes the level of contradiction, neutrality, and support between the summary and each source document sentence. The final factual consistency metric is calculated as the maximum support score over all source sentences. Right: The QA-based metric first selects a noun-phrase answer from the summary. A QG model then generates an associated question that a QA model answers based on the source document. The answer overlap score of the QA-based metric measures the semantic overlap between the QA model output and the selected answer as the final metric score.
2020) and 2) QA-based metrics that compute a factual consistency score based on a QA model's ability to answer, using the input document, questions generated from the summary (Wang et al., 2020a; Durmus et al., 2020). We provide an illustrative example in Table 1 in which both metric types correctly identify the factual inconsistency and output a low score.</p>
<p>Quantitative comparisons among entailmentbased and QA-based metrics, however, often differ in their choices of baseline model and input granularity, evaluating on single datasets and drawing differing conclusions as to the best paradigm. For example, some work reports entailment-based metrics as performing best (Koto et al., 2020; Maynez et al., 2020), while other work argues for QA metrics (Durmus et al., 2020; Wang et al., 2020b; Scialom et al., 2021). Recently, Laban et al. (2021) pro-</p>
<p>posed a benchmark called SummaC to compare metrics across six factual consistency datasets for the task of binary factual consistency classification, whether a summary is entirely factually consistent or not. This work unifies prior work on entailment-based metrics by studying the effect of input granularity, pretrained entailment model, and other hyperparameter choices on downstream evaluation performance. However, it does not study the components of QA-based metrics, which are more interpretable by their inherent decomposability.</p>
<p>To unify work in QA-based factual consistency evaluation, we do an extensive hyperparameter analysis of current metrics. We break down these metrics into four constituent components: 1) the selection of answers to ask questions about, 2) question generation (QG) conditioned upon these answers, 3) question answering (QA) based on the source document, and 4) answer overlap evaluation between QA model output and selected answers. We study the effect of each of these components on metric performance. Based on our insights, we propose an optimized metric, which we call QAFactEval, that outperforms the entailmentbased metrics of Laban et al. (2021).</p>
<p>Our contributions are the following: 1) We analyze all components of the QA-based metric pipeline, and our proposed solution improves performance over prior QA-based metrics by over $14 \%$ on a factual consistency benchmark consisting of 6 individual datasets, achieving state-of-the-art results. 2) We show that QA-based metrics and NLI-based metrics offer complementary signals and combine them into a new metric via a simple learned network, further improving performance. 3) We report results for 10 additional metrics across classification and correlation analysis, providing the most comprehensive benchmark results for factual consistency metrics and highlighting areas for future work in QA-based metrics ${ }^{1}$.</p>
<h2>2 Related Work</h2>
<p>Evaluating Factual Consistency Within entailment-based factual consistency evaluation, Falke et al. (2019) propose the task of ranking summary pairs for factual consistency based on entailment models, while Kryscinski et al. (2020) explore factual consistency classification jointly with source support or contradiction span extrac-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>tion. Other work on entailment-based metrics has examined input granularity (Goyal and Durrett, 2020), trained on adversarial datasets (Barrantes et al., 2020), and explored entailment-based models as the backbone of others metrics such as BERTScore (Zhang et al., 2020b) as in Koto et al. (2021). Metric comparisons, however, were often conducted on isolated datasets. Laban et al. (2021) unify work in entailment-based metrics for factual consistency, showing the effect of granularity, base models, and other hyperparameter choices. This work also proposes a learned metric built on top of the output of an entailment model, with parameters fine-tuned on synthetic data. While this work fills a gap in the use of entailment-based metrics for factual consistency, our work analogously unifies QA-based metrics for factual consistency and proposes to combine entailment and QA-based metrics in a single learned metric.</p>
<p>QA-based evaluation metrics have received attention for summary quality dimensions beyond factual consistency (Eyal et al., 2019; Scialom et al., 2019; Deutsch et al., 2020). Recent work has shown that QA-based metrics better measure the overlap of information units for determining summary relevance over embedding-based metrics (Deutsch and Roth, 2021), further driving our study of QA-based metrics for factual consistency. While several QA-based metrics with similar structures have been applied for factual consistency, (Durmus et al., 2020; Wang et al., 2020b; Scialom et al., 2021), they differ in their underlying answer selection, question generation, question answering, and answer overlap components, reporting different performances. We perform a comprehensive evaluation of QA-based metric components and propose improved model components for the task of answer overlap and question filtering.</p>
<p>Summarization Benchmarking A recent line of work aims to take stock of the current state of summarization models and progress, both within factual consistency and across summarization more broadly. Kryscinski et al. (2019) note biases and failure modes of abstractive summarization models, while other work analyzes and collects annotations over the output of recent summarization models across multiple dimensions, including factual consistency (Fabbri et al., 2021; Bhandari et al., 2020; Huang et al., 2020). Lux et al. (2020) propose a typology of errors found in summarization models, while Gabriel et al. (2021) propose a framework for</p>
<p>meta-evaluation of factual consistency metrics. Laban et al. (2021) propose to combine recent work in factual consistency evaluation for summarization through a single benchmark. Our work directly makes use of this benchmark while emphasizing QA-based metrics. We also include correlation analysis for a more comprehensive understanding of current factual consistency metrics.</p>
<h2>3 Evaluation Metrics</h2>
<p>In this section, we introduce the factual consistency metrics studied, which we divide into entailment metrics, QA-based metrics, and learned metrics.</p>
<h3>3.1 Entailment-based Metrics</h3>
<p>We include the following entailment-based metrics due to further understand differences in granularity and base entailment models. The metrics below produce a score for each summary sentence that is then averaged to compute the final metric score.</p>
<p>MNLI applies a RoBERTa large (Liu et al., 2019) model trained on MNLI (Williams et al., 2018). The score of a summary sentence is the maximum entailment score over all input sentences.</p>
<p>ANLI Barrantes et al. (2020) uses the same method as the MNLI metric with a model trained on the ANLI (Nie et al., 2020) dataset consisting of adversarial datapoints.</p>
<p>SCZeroShot Laban et al. (2021) works analogously to the above metrics with a base model trained on both MNLI and Vitamin-C data (Schuster et al., 2021), consisting of closely-related contrastive entailment examples.</p>
<p>BertScore-FFCI Koto et al. (2021) applies BertScore (Zhang et al., 2020b) with a backbone RoBERTa-MNLI model, averaging the three highest BertScore F1 scores over the input sentences.</p>
<p>DAE Goyal and Durrett (2020) computes entailment scores between a source document and summary dependency arcs, applying an entailment model trained on synthetic data.</p>
<p>FactCC Kryscinski et al. (2020) is a RoBERTabase model trained on FactCC synthetic data to compute a document-level score, and thus the scores need not be aggregated over input sentences.</p>
<p>DocNLI Yin et al. (2021) train a document-level entailment model, similar to the FactCC metric.</p>
<h3>3.2 QA Metric Components</h3>
<p>We now describe the components that constitute the QA-based pipeline for factual consistency. We refer to our metric, consisting of the best combination of the below components, as QAFactEval.</p>
<p>Answer Selection QA-based metrics compare information units between the summary and source, so it is thus necessary to first extract such units, or answers, from the given summary. We follow the protocols from Deutsch et al. (2020) and compare extracting the following answer types: named entities (NER), noun phrase chunks (NP Chunks), maximally sized noun phrases (Max NP), whereby the dependency subtrees of nouns reached by traversing a given sentence's dependency parse from the root are chosen as answers, and All, which combines answers from the above three techniques.</p>
<p>Question Generation Having selected answers, questions are generated conditioned upon these answers using the summary as context. Typically, this is an encoder-decoder model which inputs the answer and context separated by a special token. On the modeling side, we examine BART (Lewis et al., 2020) and T5 (Raffel et al., 2019) as the underlying generators. On the data side, we experiment with models trained for question generation on $S Q u A D$ (Rajpurkar et al., 2016), a standard QA dataset consisting of questions on Wikipedia articles, and on QA2D (Demszky et al., 2018), a dataset of declarative sentences with associated question/answer pairs derived from SQuAD. Furthermore, we experiment with the recently-introduced Mix $Q G$ models (Murakhovs'ka et al., 2021), which are T5 models trained on a combination of nine QA datasets with diverse answer types and which outperform other QG models across several tasks. We apply both the small and large versions of Mix $Q G$ to better understand the effect of QG model size.</p>
<p>Question Answering The QA component answers questions from the previous steps using the input document as context. We experiment with both extractive QA models, which extract a text span from the input as an answer, and abstractive QA models, which generate an answer token-bytoken. For extractive models, we ablate Electra (Clark et al., 2020), a model architecturally similar to BERT (Devlin et al., 2019) that achieves strong performance on the SQuAD 2.0 dataset and was previously used in measuring summary relevance (Deutsch et al., 2020). We also include MADE</p>
<p>(Friedman et al., 2021), which models multi-dataset QA with a collection of dataset-specific adapter modules sharing the same underlying RoBERTabase model. For abstractive QA, we experiment with T5 fine-tuned on SQuAD and UnifiedQA (Khashabi et al., 2020), an approach that trains a T5 QA model on 8 diverse, seed datasets and was shown to generalize across 20 datasets and 4 input formats. All QA models except MADE are trained on data containing unanswerable questions. Additional QA models can be included, although the above set of models allows us to inspect the aspects of interest in this study, namely extractive vs abstractive performance and multi-dataset training.</p>
<p>Answer Overlap Evaluation An answer overlap metric must be computed to determine the match between the initial answer selected in the first component and the QA model output. Typically, answer overlap in QA is measured through exact match $(E M)$ score or word $F 1$ score. We also test a learned metric, the $L E R C$ score proposed by Chen et al. (2020). This metric outputs a 1-5 answer overlap score conditioned on a question and context. The scorer is trained on their MOCHA dataset, consisting of 40 k crowdsourced judgments on QA model outputs. We include the BERT-base (Devlin et al., 2019) model from the original paper, which we call LERC (orig). We additionally experiment with two models trained from RoBERTa-large checkpoints, one trained from the original checkpoint, LERC (RoBERTa), and one initialized from Jia et al. (2021), which we call LERC (QuIP), for the task of jointly encoding passages and answers with question-infused pretraining. Lastly, we experiment with the IsAnsweredInput answer metric, which is a $0 / 1$ score of whether the question is answerable using the input document according to the QA model. We use the Electra-large QA model to determine whether a question is answerable, as this model shows strong performance on identifying unanswerable questions on SQuAD.</p>
<p>Question Filtering Model-generated questions may contain noise from the QG model itself or from disfluencies in the summary the QG model conditions upon. Such noisy questions can skew the overall metric score, as the QA component may be unable to correctly answer the question, regardless of the summary's factual consistency. We filter such questions through a step called IsAnsweredSumm Filter: the same Electra-large QA model
returns a $0 / 1$ score of whether the question is answerable, now using the summary as context, and questions labeled as unanswerable are filtered.</p>
<p>Overall For a given question, if IsAnsweredInput returns 0 , the question is unanswerable using the input, we label all the above answer overlap scores as 0 , and otherwise use the answer overlap score. We refer to this scoring of unanswerable questions as 0 as the Answerability Penalty. We also experiment with not setting the overlap score of these unanswerable questions to 0 but rather using the answer overlap score of the most probable answer from the QA model. Finally, the overall factual consistency score for each metric is computed as its average scores over all questions remaining following Question Filtering.</p>
<h3>3.3 Learned Metrics</h3>
<p>SCConv is a model introduced by Laban et al. (2021) that learns to aggregate entailment-model output scores across input sentences into a single score. More concretely, for a document consisting of $M$ sentences and a summary consisting of $N$ sentences, the entailment-based model produces an $M \times N$ matrix of entailment scores. The $M \times N$ matrix is then transformed to an $H \times N$ matrix by binning the $M$ sentences to create a histogram, where $H$ is the number of bins. This matrix is input to a 1-D convolution layer to produce a score for each summary sentence, and the scores are averaged across summary sentences. The parameters of this model are fine-tuned on synthetic data, detailed in Section 4.2</p>
<p>QAFactEval-NLI While SCConv captures sentence-level support, QAFactEval measures finer-grained answer overlap between the source and summary. Thus, we are able to combine these two into a single factual consistency metric, QAFactEval-NLI. Assume that $K$ answers are extracted from the summary. The pipeline described above will then output a single score per answer for the entire summary, resulting in an array of length $K$. We convert this to a histogram of size $H$ in a similar manner as SCConv and pass this histogram through a 1-D convolution layer to produce a single QA score. This score is concatenated with the NLI score produced by SCConv and input to a linear layer to produce the final metric score. The linear layer can be trained in either synthetic or supervised ways, detailed in Section 4.2.</p>
<h3>3.4 Additional Metrics</h3>
<p>We include the following metrics for completeness.
BARTScore Yuan et al. (2021) calculates the log-likelihood from BART fine-tuned on CNN/DailyMail (Hermann et al., 2015; Nallapati et al., 2016) of the summary conditioned upon the source text as a metric for factual consistency.</p>
<p>BLANC Vasilyev et al. (2020) is a reference-less metric of summary quality that measures the difference in masked language modeling performance with and without access to the summary.</p>
<p>QuestEval (Scialom et al., 2021) is the prior state-of-the-art QA-based metric for factual consistency. The T5-base (SQuAD) QG and T5-base QA models described above are applied directly from the QuestEval metric. QuestEval generates questions based on the input document and answers them using the summary in addition to following the above QA metric pipeline. QuestEval aggregates the score from these two pipelines. We believe that our described pipeline more closely measures factual consistency, while generating questions from the source may confound factual consistency with relevance.</p>
<h2>4 Methodology</h2>
<p>We present the datasets explored for binary classification and correlation analyses. We also describe settings for reporting ablation and final results.</p>
<h3>4.1 Data</h3>
<p>The SummaC benchmark (Laban et al., 2021) introduces a collection of datasets for binary factual consistency evaluation. A data point is labeled as positive if it contains no factual inconsistencies or is rated the highest possible score in the case of Likert scaling, and as negative otherwise. We now briefly describe the datasets in the benchmark and any departures from the original benchmark, and additional datasets we use for correlation analysis. We refer the reader to Laban et al. (2021) for further details regarding the benchmark creation.</p>
<p>CGS Falke et al. (2019) consists of paired summary sentences from CNN/DailyMail (Hermann et al., 2015; Nallapati et al., 2016), one correct sentence and one containing an error. Laban et al. (2021) treats the correct summaries as positive examples and the others as negative examples.</p>
<p>XSF Maynez et al. (2020) consists of summaries from the XSum dataset (Narayan et al., 2018) annotated for word-level factual consistency errors.</p>
<p>Polytope Huang et al. (2020) propose a typology of eight summarization errors consisting of both content and stylistic errors and annotate model outputs from 10 systems on CNN/DailyMail data. The original SummaC benchmark included the Omission and Addition errors of this proposed typology as factual inconsistencies, but these are largely extractive, factually consistent summaries. We thus label these examples as factually consistent and report results on this modified dataset.</p>
<p>FactCC Kryscinski et al. (2020) introduce a factual consistency dataset on CNN/DailyMail annotated by the authors of the paper to ensure the quality of the annotations.</p>
<p>SummEval Fabbri et al. (2021) analyze summaries from 17 models on CNN/DailyMail across the dimensions of factual consistency, coherence, fluency, and relevance.</p>
<p>FRANK Pagnoni et al. (2021) introduce an extensive typology of errors made by summarization systems across CNN/DailyMail and XSum.</p>
<p>QAGs Wang et al. (2020b) crowdsource sentence-level summary annotations for factual consistency across CNN/Daily Mail and XSum data. We only report correlation analysis for this dataset as it was not a part of SummaC.</p>
<h3>4.2 Experiment Setup</h3>
<p>Metric Implementation Metrics were applied directly from the original GitHub repository or by using the SacreRouge Library (Deutsch and Roth, 2020), which was also used in correlation analysis. The learned metrics make use of code released from Laban et al. (2021) for training, and all models are implemented in PyTorch (Li et al., 2020) and in the Transformers library (Wolf et al., 2019). The BART-large (QA2D) QG and Electra-large QA models are applied from the QAEval relevance modeling metric (Deutsch et al., 2020).</p>
<p>Ablation Settings Following Laban et al. (2021), a metric threshold score for binary classification is determined from the validation set of SummaC and applied to the test set. This threshold score is determined for every metric studied. Furthermore, we note that hyperparameter choices for several of</p>
<p>the strong entailment baselines, namely SCConv, SCZeroShot, and MNLI are derived from Laban et al. (2021), thus providing a reasonable comparison to QAFactEval, whose hyperparameters we tune on the SummaC validation set. For ablation studies, we both perform thresholding and evaluation on the validation set to preserve the integrity of the test set. For each benchmark dataset, we sample a random subset of $80 \%$ of the validation set to determine the threshold and evaluate on the remaining $20 \%$ of the validation set. The best performing combination of QA metric components constitutes our QAFactEval metric. We take the best performing combination of QA metric components and vary a given component, such as answer selection, while holding all other components constant and consistent with the best component combination.</p>
<p>Training Settings To tune the parameters of the learned metrics, we train on a subset of 50 k synthetic data points from FactCC, following Laban et al. (2021). We name these runs synthetic setting due to the lack of human-labeled data. We also experiment with a supervised setting by fine-tuning the parameters on the SummaC validation set for each individual dataset, choosing the threshold on this validation data, and applying the model to the test set. Training on such a small amount of data is feasible due to the small number of parameters of the learned metrics. Cross entropy loss with Adam (Kingma and Ba, 2015) optimizer is used, with a batch size of 32 and a learning rate of $1 \mathrm{e}-2$.</p>
<h2>5 Results</h2>
<p>In this section, we first study the effects of model component choices on QAFactEval. We then compare metric results across both the SummaC binary classification task and correlation analysis.</p>
<h3>5.1 Ablation Results</h3>
<p>We provide the results of our ablation studies on the components of QA-based metrics in Table 2 and show two illustrative examples in Table 4.</p>
<p>Effect of Answer Selection Selecting NP Chunks performs best, aligning with Deutsch et al. (2020), which shows that NP Chunks obtain the largest coverage of information units while retaining high precision. We find a large decrease in performance when selecting NER and only a slight decrease in performance when choosing Max NP</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Component</th>
<th style="text-align: center;">Model Choice</th>
<th style="text-align: center;">Benchmark</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">QAFactEval</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">77.5</td>
</tr>
<tr>
<td style="text-align: center;">Answer Selection</td>
<td style="text-align: center;">NP Chunks</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Max NP</td>
<td style="text-align: center;">75.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NER</td>
<td style="text-align: center;">66.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ALL</td>
<td style="text-align: center;">75.7</td>
</tr>
<tr>
<td style="text-align: center;">Question Generation</td>
<td style="text-align: center;">BART-large (QA2D)</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BART-large (SQuAD)</td>
<td style="text-align: center;">74.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5-base (SQuAD)</td>
<td style="text-align: center;">67.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MixQG-base</td>
<td style="text-align: center;">75.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MixQG-large</td>
<td style="text-align: center;">74.9</td>
</tr>
<tr>
<td style="text-align: center;">Question Answering</td>
<td style="text-align: center;">Electra-large</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Electra-base</td>
<td style="text-align: center;">77.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MADE</td>
<td style="text-align: center;">77.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">T5-base</td>
<td style="text-align: center;">76.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">UnifiedQA-base</td>
<td style="text-align: center;">75.7</td>
</tr>
<tr>
<td style="text-align: center;">Answer Overlap</td>
<td style="text-align: center;">LERC (QuIP)</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">68.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">71.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IsAnsweredInput</td>
<td style="text-align: center;">73.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LERC (orig)</td>
<td style="text-align: center;">71.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LERC (RoBERTa)</td>
<td style="text-align: center;">77.3</td>
</tr>
<tr>
<td style="text-align: center;">Filtering/Answerability</td>
<td style="text-align: center;">Both</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">No IsAnsweredSumm Filter</td>
<td style="text-align: center;">73.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">No Answerability Penalty</td>
<td style="text-align: center;">72.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Neither</td>
<td style="text-align: center;">67.4</td>
</tr>
</tbody>
</table>
<p>Table 2: Results of ablation studies on the SummaC benchmark validation set, showing the effect of the individual components of QAFactEval. The first row represents the performance of the best combination of components. Ablations are performed by swapping a given component while holding all others consistent with the best overall model, and the best setting is bolded.
or ALL answers together. Named entity selection likely performs worse due to the scarcity of extracted answers; only three entities are extracted on average across the benchmark, while all other approaches extract over 10 answers per summary.</p>
<p>Effect of QG Models The choice of the QG model notably affects downstream performance. BART-large (QA2D) works the best and produces much longer questions, about 17 tokens on average, versus about 10 from the other models. Deutsch et al. (2020) note how humans tend to produce shorter questions. However, longer questions may be preferable for this task to facilitate the QA model's ability to understand and answer the question. BART-large (QA2D) also is the most extractive, with only about $20 \%$ novel unigrams in the question, while T5-base (SQuAD) model is the most abstractive with about $47 \%$ novel unigrams, resulting in occasional hallucinations and questions that the QA model struggles to answer. As seen in Table 4, MixQG models do often produce highlyfluent questions, but the longer, highly-extractive output of BART-large (QA2D) improves downstream factual consistency performance.</p>
<p>Effect of QA Model Surprisingly, we do not find a large difference in the QA model compo-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model Type</th>
<th style="text-align: center;">Model Name</th>
<th style="text-align: center;">CGS</th>
<th style="text-align: center;">XSF</th>
<th style="text-align: center;">Polytope</th>
<th style="text-align: center;">FactCC</th>
<th style="text-align: center;">SummEval</th>
<th style="text-align: center;">FRANK</th>
<th style="text-align: center;">Benchmark</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Misc</td>
<td style="text-align: center;">BARTScore</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">68.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLANC</td>
<td style="text-align: center;">51.6</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">63.0</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">61.8</td>
</tr>
<tr>
<td style="text-align: center;">Entailment</td>
<td style="text-align: center;">FactCC</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">77.1</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">70.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BertScore-FFCI</td>
<td style="text-align: center;">56.9</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">57.9</td>
<td style="text-align: center;">67.4</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">65.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DAE</td>
<td style="text-align: center;">71.3</td>
<td style="text-align: center;">49.7</td>
<td style="text-align: center;">78.9</td>
<td style="text-align: center;">80.7</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">81.0</td>
<td style="text-align: center;">72.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ANLI</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">85.8</td>
<td style="text-align: center;">75.9</td>
<td style="text-align: center;">78.9</td>
<td style="text-align: center;">74.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MNLI</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">77.3</td>
<td style="text-align: center;">89.8</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">75.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DocNLI</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">57.0</td>
<td style="text-align: center;">84.7</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">68.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SCZeroShot</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">81.5</td>
<td style="text-align: center;">83.2</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">72.8</td>
</tr>
<tr>
<td style="text-align: center;">QA</td>
<td style="text-align: center;">QuestEval</td>
<td style="text-align: center;">59.4</td>
<td style="text-align: center;">61.9</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">68.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">QAFactEval</td>
<td style="text-align: center;">75.1</td>
<td style="text-align: center;">63.1</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">84.1</td>
<td style="text-align: center;">80.9</td>
<td style="text-align: center;">83.9</td>
<td style="text-align: center;">77.8</td>
</tr>
<tr>
<td style="text-align: center;">Learned</td>
<td style="text-align: center;">SCConv (synthetic)</td>
<td style="text-align: center;">60.8</td>
<td style="text-align: center;">60.9</td>
<td style="text-align: center;">76.0</td>
<td style="text-align: center;">88.1</td>
<td style="text-align: center;">78.1</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">74.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">QAFactEVAL-NLI (synthetic)</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">59.1</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">91.1</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">83.4</td>
<td style="text-align: center;">78.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">QAFactEVAL-NLI (supervised)</td>
<td style="text-align: center;">78.1</td>
<td style="text-align: center;">60.9</td>
<td style="text-align: center;">83.7</td>
<td style="text-align: center;">89.3</td>
<td style="text-align: center;">80.5</td>
<td style="text-align: center;">84.3*</td>
<td style="text-align: center;">79.5*</td>
</tr>
</tbody>
</table>
<p>Table 3: Balanced accuracy on the test set of the six SummaC benchmark datasets, and the average over the benchmark. Metrics are divided into entailment-based, QA-based, and learned metrics that are fine-tuned on synthetic or supervised data. An improvement over prior work with a $99 \%$ confidence interval is indicated by *.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Document</th>
<th style="text-align: center;">Paul Merson has restarted his row with Andros Townsend. ... '... it was a great goal,' Merson said. 'It's just a matter of opinion, and ... he got pulled off after half an hour .... in front of Roy Hodgson, so he shouldn't have been in the squad. ...' ... Sky Sports pundit Merson (centre) criticised Townsend's call-up to the England squad last week ....</th>
<th style="text-align: center;">They're not gonna take it anymore. Really. Twisted Sister says that its 2016 tour will be its last, according to a press release. ... The band will also perform two shows in Pero's honor: one at Las Vegas Hard Rock Hotel and Casino, the other at the Starland Ballroom in Sayreville, New Jersey.</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Summary</td>
<td style="text-align: center;">Paul Merson is not happy with Andros Townsend's call-up to the England squad last week</td>
<td style="text-align: center;">The band will perform two shows.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Selected Answer</td>
<td style="text-align: center;">Andros Townsend's call-up</td>
<td style="text-align: center;">the band</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Question Generation</td>
<td style="text-align: center;">BART-QA2D <br> What is Paul Merson not happy with to the England squad last week?</td>
<td style="text-align: center;">MixQG-large <br> What is Paul Merson not happy with?</td>
<td style="text-align: center;">BART-QA2D Question <br> Who will perform two shows?</td>
</tr>
<tr>
<td style="text-align: center;">QA Output</td>
<td style="text-align: center;">Townsend's call-up</td>
<td style="text-align: center;">he shouldn't have been in the squad</td>
<td style="text-align: center;">Unanswerable (Twisted Sister)</td>
</tr>
<tr>
<td style="text-align: center;">Answer Overlap</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">$0.00(0.80)$</td>
</tr>
</tbody>
</table>
<p>Table 4: Example source documents and summaries along with QA-based metric component outputs. Left: This example illustrates that the fluency of the QG model does not necessarily improve downstream factual consistency evaluation performance; the less fluent, more extractive BART-QA2D question is more-easily answerable by the QA model. Not shown, the entailment-based SCConv metric incorrectly labels this entity-centric example, likely due the introduction of novel unigrams. Right: The QA model incorrectly labels this question as unanswerable, perhaps due to the generality of the question or due to noise in the input document. The QA output and our learned overlap score if forced to extract an answer are in parenthesis. SCConv correctly labels this highly extractive example.
nent across model sizes or between extractive and abstractive QA models, implying that QA ability is not the bottleneck of our task. In this setting, we keep IsAnsweredInput from Electra-large constant, as not all QA models are trained with unanswerable questions; thus the only differences are in the answers to questions marked as answerable.</p>
<p>Effect of Answer Overlap Metric We observe a large difference between EM and other overlap metrics. We also see a notable gap between LERC (orig) and LERC (RoBERTa) along with a further
slight improvement with LERC (QuIP), showing the effect of the underlying model of the learned metric on factual consistency performance.</p>
<p>Effect of Question Filtering and Answerability Not filtering questions according to the QA model's ability to answer them conditioned upon the summary decreases performance. Furthermore, not applying the Answerability Penalty, and using the answer overlap score for the most probable answers for all questions, even those judged unanswerable by the QA model, also decreases perfor-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model Type</th>
<th style="text-align: center;">Model Name</th>
<th style="text-align: center;">XSF</th>
<th style="text-align: center;">SummEval</th>
<th style="text-align: center;">FRANK-CNNDM</th>
<th style="text-align: center;">FRANK-XSum</th>
<th style="text-align: center;">QAGs-CNNDM</th>
<th style="text-align: center;">QAGs-XSum</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Misc</td>
<td style="text-align: center;">BARTScore</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.17</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLANC</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.02</td>
</tr>
<tr>
<td style="text-align: center;">Entailment</td>
<td style="text-align: center;">FactCC</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.30</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BertScore-FFCI</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">0.21</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DAE</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">$-0.20$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ANLI</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.39</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MNLI</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.35</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DocNLI</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">$-0.34$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SCZeroShot</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.44</td>
</tr>
<tr>
<td style="text-align: center;">QA</td>
<td style="text-align: center;">QuestEval</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">0.23</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">QAFactEval</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.44</td>
</tr>
<tr>
<td style="text-align: center;">Learned</td>
<td style="text-align: center;">SCConv (synthetic)</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.06</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">QAFactEval-NLI(synthetic)</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.48</td>
</tr>
</tbody>
</table>
<p>Table 5: Instance-level Pearson correlation coefficients across factual consistency evaluation datasets. Metrics are divided into entailment-based, QA-based, and learned metrics that are fine-tuned on synthetic or supervised data. The two highest-correlated metrics for each dataset are shown in bold.
mance. While the answer overlap metric should capture unanswerable questions for information not found in the input (extrinsic error), the selected answer may appear in both the summary and source but in different contexts (intrinsic error). The QA model may return this as the most probable answer and be scored as correct by the answer overlap component despite a factual inconsistency. This finding demonstrates the importance of determining question answerability, a point also emphasized in Deutsch et al. (2020) for QA-based metrics of relevance. Removing both of these components results in a drastic performance decrease.</p>
<h3>5.2 Overall Results</h3>
<p>We present the results on the test set of SummaC in Table 3. QAFactEval shows a substantial improvement over the previous state-of-the-art QA metric for factual consistency, QuestEval. Furthermore, it outperforms all other entailment-based metrics. QAFactEval-NLI shows slight improvements on the synthetic data. Notable improvements in this synthetic setting can be observed on the FactCC dataset, likely as the synthetic FactCC data the model is trained on was designed to mirror the errors captured in annotations. This performance boost on FactCC motivated our use of supervised data for fine-tuning our learned metric. Supervised fine-tuning on validation data helps in most cases and QAFactEval-NLI (supervised) improves on the overall benchmark by a statistically significant margin, using bootstrap resampling (Efron, 1982) with Bonferroni correction (Bonferroni, 1935) to obtain 99\% confidence intervals (see Appendix for details). The performance drop on FactCC could be due to the proximity of the synthetic data to the labeled data and
the data size difference. BertScore-FFCI performs best on XSF perhaps due to the closeness between BertScore's token-level metric and XSF's wordlevel annotations, and DocNLI's Polytope performance may also be from training data similarity.</p>
<p>We find that QAFactEval and SCConv do offer complementary signals that can be learned from supervised data. Individually fine-tuning the learned SCConv or a learned variation of QAFactEval on supervised data did not improve results over the non-supervised metrics; this result suggests the necessity of combining the two for further improvements. Training on the validation sets combined, rather than on each individual dataset separately, did not give an improvement, likely due to the learnable combination of NLI and QAFactEval being dataset dependent.</p>
<h3>5.3 Correlation Analysis</h3>
<p>We provide instance-level Pearson correlation between aggregated human judgments and metric scores for each model to compare to previous work in factual consistency that reports correlation analysis. Results are shown in Table 5. We split FRANK into CNN/DailyMail and XSum subsets for finergrained analysis, as substantial differences have been noted in correlation performance across the two datasets (Durmus et al., 2020). We exclude Polytope, FactCC, and CGS here as prior work has only studied these datasets for binary classification.</p>
<p>We find that QAFactEval performs well across most datasets. As in the classification results, BertScore-FFCI's performs well on XSF, and we note that QuestEval's answerability classifier correlates more so with these fine-grained annotations than on other datasets. QAFactEval-NLI performs well on most datasets except XSF. Fine-</p>
<p>tuning on FactCC synthetic data for binary classification likely does not capture the aggregated, word-level factuality scores of XSF. We leave a study of fine-tuning this model on supervised data with a regression loss for future work.</p>
<h2>6 Conclusion</h2>
<p>In this work, we demonstrated that QA-based metrics, when its components are properly optimized, outperform entailment-based metrics on a comprehensive factual consistency evaluation benchmark. We identify question generation and answerability detection as key components for improving QAbased metrics in future work. Furthermore, we show that entailment and QA-based metrics offer complementary signals through a combined metric that achieves state-of-the-art performance on this benchmark. We believe that our work lays the foundation for future work in QA-based metrics for factual consistency by offering a fairer comparison to other metrics across datasets and settings.</p>
<h2>7 Ethical Considerations</h2>
<p>Dataset Biases The underlying models of the metrics presented in this work are trained on documents in English and thus mainly represent the culture of the English-speaking populace. Political or gender biases may also exist in the datasets, and models, and subsequently the metrics, trained on these datasets may propagate these biases. We did not stress test these metrics for such biases and request that the users of these metrics be aware of these potential issues in applying them.</p>
<p>Misuse Potential and Failure Mode When properly used, the metrics described in this paper can be a useful tool for detecting summarization model errors. However, the current metrics fail to detect all factual inconsistencies, which must be remembered when applying these metrics as a filter for downstream applications. Factual inconsistencies in summaries could contribute to misinformation on the internet.</p>
<p>Environmental Cost The experiments described in the paper primarily make use of A100 GPUs. Most of the metrics have already been trained, in which case we simply ran inference using the existing models. We typically used a single GPU per experiment. Training learned answer overlap components can take a couple of hours, while experiments for learned metrics on SummaC take
less than 10 minutes. These are the base models used in these experiments, with the number of parameters, in millions, in parentheses: BERTbase (110), BART-large (400), Electra-base (110), Electra-large (335), RoBERTa-large (355), T5-base (220), T5-large (770). Future work may analyze the effect of using distilled backbone models on factual consistency evaluation.</p>
<h2>References</h2>
<p>Mario Barrantes, Benedikt Herudek, and Richard Wang. 2020. Adversarial nli for factual correctness in text summarisation models. ArXiv preprint, abs/2005.11739.</p>
<p>Manik Bhandari, Pranav Narayan Gour, Atabak Ashfaq, Pengfei Liu, and Graham Neubig. 2020. Reevaluating evaluation in text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9347-9359, Online. Association for Computational Linguistics.</p>
<p>Carlo E Bonferroni. 1935. Il calcolo delle assicurazioni su gruppi di teste. Studi in onore del professore salvatore ortu carboni, pages 13-60.</p>
<p>Anthony Chen, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2020. MOCHA: A dataset for training and evaluating generative reading comprehension metrics. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6521-6532, Online. Association for Computational Linguistics.</p>
<p>Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020. ELECTRA: pretraining text encoders as discriminators rather than generators. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Dorottya Demszky, Kelvin Guu, and Percy Liang. 2018. Transforming question answering datasets into natural language inference datasets. ArXiv preprint, abs/1809.02922.</p>
<p>Daniel Deutsch, Tania Bedrax-Weiss, and Dan Roth. 2020. Towards question-answering as an automatic metric for evaluating the content quality of a summary.</p>
<p>Daniel Deutsch and Dan Roth. 2020. SacreROUGE: An open-source library for using and developing summarization evaluation metrics. In Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS), pages 120-125, Online. Association for Computational Linguistics.</p>
<p>Daniel Deutsch and Dan Roth. 2021. Understanding the extent to which content quality metrics measure the information quality of summaries. In Proceedings of</p>
<p>the 25th Conference on Computational Natural Language Learning, pages 300-309, Online. Association for Computational Linguistics.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Esin Durmus, He He, and Mona Diab. 2020. FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 50555070, Online. Association for Computational Linguistics.</p>
<p>Bradley Efron. 1982. The Jackknife, the bootstrap and other resampling plans. CBMS-NSF regional conference series in applied mathematics. SIAM, Philadelphia, PA. Lectures given at Bowling Green State Univ., June 1980.</p>
<p>Matan Eyal, Tal Baumel, and Michael Elhadad. 2019. Question answering as an automatic evaluation metric for news article summarization. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3938-3948, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Alexander R. Fabbri, Wojciech Kryściński, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir R. Radev. 2021. Summeval: Re-evaluating summarization evaluation. Trans. Assoc. Comput. Linguistics, 9:391-409.</p>
<p>Tobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie Utama, Ido Dagan, and Iryna Gurevych. 2019. Ranking generated summaries by correctness: An interesting but challenging application for natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2214-2220, Florence, Italy. Association for Computational Linguistics.</p>
<p>Dan Friedman, Ben Dodge, and Danqi Chen. 2021. Single-dataset experts for multi-dataset qa. In Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Saadia Gabriel, Asli Celikyilmaz, Rahul Jha, Yejin Choi, and Jianfeng Gao. 2021. GO FIGURE: A meta evaluation of factuality in summarization. In Findings of the Association for Computational Linguistics: ACLIJCNLP 2021, pages 478-487, Online. Association for Computational Linguistics.</p>
<p>Tanya Goyal and Greg Durrett. 2020. Evaluating factuality in generation with dependency-level entailment. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3592-3603, Online. Association for Computational Linguistics.</p>
<p>Karl Moritz Hermann, Tomás Kociský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 16931701 .</p>
<p>Dandan Huang, Leyang Cui, Sen Yang, Guangsheng Bao, Kun Wang, Jun Xie, and Yue Zhang. 2020. What have we achieved on text summarization? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 446-469, Online. Association for Computational Linguistics.</p>
<p>Robin Jia, Mike Lewis, and Luke Zettlemoyer. 2021. Question answering infused pre-training of generalpurpose contextualized representations.</p>
<p>Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. 2020. UNIFIEDQA: Crossing format boundaries with a single QA system. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1896-1907, Online. Association for Computational Linguistics.</p>
<p>Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.</p>
<p>Fajri Koto, Timothy Baldwin, and Jey Han Lau. 2021. Ffci: A framework for interpretable automatic evaluation of summarization.</p>
<p>Fajri Koto, Jey Han Lau, and Timothy Baldwin. 2020. FFCI: A framework for interpretable automatic evaluation of summarization. ArXiv preprint, abs/2011.13662.</p>
<p>Wojciech Kryscinski, Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Neural text summarization: A critical evaluation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 540-551, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the factual consistency of abstractive text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),</p>
<p>pages 9332-9346, Online. Association for Computational Linguistics.</p>
<p>Philippe Laban, Tobias Schnabel, Paul N. Bennett, and Marti A. Hearst. 2021. Summac: Re-visiting nlibased models for inconsistency detection in summarization.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics.</p>
<p>Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, and Soumith Chintala. 2020. Pytorch distributed: Experiences on accelerating data parallel training. Proc. VLDB Endow., 13(12):3005-3018.</p>
<p>Yang Liu and Mirella Lapata. 2019. Text summarization with pretrained encoders. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3730-3740, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. ArXiv preprint, abs/1907.11692.</p>
<p>Klaus-Michael Lux, Maya Sappelli, and Martha Larson. 2020. Truth or error? towards systematic analysis of factual errors in abstractive summaries. In Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems, pages 1-10, Online. Association for Computational Linguistics.</p>
<p>Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906-1919, Online. Association for Computational Linguistics.</p>
<p>Lidiya Murakhovs'ka, Chien-Sheng Wu, Tong Niu, Wenhao Liu, and Caiming Xiong. 2021. Mixqg: Neural question generation with mixed answer types.</p>
<p>Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Caglar Gulcehre, and Bing Xiang. 2016. Abstractive text summarization using sequence-to-sequence RNNs and beyond. In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, pages 280-290, Berlin, Germany. Association for Computational Linguistics.</p>
<p>Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797-1807, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. 2020. Adversarial NLI: A new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4885-4901, Online. Association for Computational Linguistics.</p>
<p>Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4812-4829, Online. Association for Computational Linguistics.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv e-prints.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics.</p>
<p>Tal Schuster, Adam Fisch, and Regina Barzilay. 2021. Get your vitamin C! robust fact verification with contrastive evidence. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 624-643, Online. Association for Computational Linguistics.</p>
<p>Thomas Scialom, Paul-Alexis Dray, Gallinari Patrick, Lamprier Sylvain, Piwowarski Benjamin, Staiano Jacopo, and Wang Alex. 2021. Questeval: Summarization asks for fact-based evaluation. ArXiv preprint, abs/2103.12693.</p>
<p>Thomas Scialom, Sylvain Lamprier, Benjamin Piwowarski, and Jacopo Staiano. 2019. Answers unite! unsupervised metrics for reinforced summarization models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3246-3256, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Oleg Vasilyev, Vedant Dhamidharka, and John Bohannon. 2020. Fill in the BLANC: Human-free quality estimation of document summaries. In Proceedings</p>
<p>of the First Workshop on Evaluation and Comparison of NLP Systems, pages 11-20, Online. Association for Computational Linguistics.</p>
<p>Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020a. Asking and answering questions to evaluate the factual consistency of summaries. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5008-5020, Online. Association for Computational Linguistics.</p>
<p>Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020b. Asking and answering questions to evaluate the factual consistency of summaries. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5008-5020, Online. Association for Computational Linguistics.</p>
<p>Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingface's transformers: State-of-the-art natural language processing. ArXiv preprint, abs/1910.03771.</p>
<p>Wenpeng Yin, Dragomir Radev, and Caiming Xiong. 2021. DocNLI: A large-scale dataset for documentlevel natural language inference. In Findings of the Association for Computational Linguistics: ACLIJCNLP 2021, pages 4913-4922, Online. Association for Computational Linguistics.</p>
<p>Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text generation.</p>
<p>Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. 2020a. PEGASUS: pre-training with extracted gap-sentences for abstractive summarization. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 11328-11339. PMLR.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020b. Bertscore: Evaluating text generation with BERT. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<h2>A Additional Data and Model Details</h2>
<p>In this section, we provide details regarding statistical testing, benchmark statistics, and miscellaneous details regarding our QA-based experiments.</p>
<h2>A. 1 Statistical Testing</h2>
<p>To determine whether the improvements on the SummaC benchmark are statistically significant, we perform significance tests using bootstrap resampling (Efron, 1982), following Laban et al. (2021). We compare our best model to the bestperforming model from prior work on a given subset of the benchmark. We compare confidence intervals at significance levels of 0.05 and 0.01 and apply the Bonferroni correction (Bonferroni, 1935). Statistically significant differences at the 0.01 level exist between QAFactEval-NLI (supervised) and the best prior work on the FRANK subset and for the overall benchmark result. We do not see statistically significant differences on the other datasets in the benchmark. However, the statistically significant difference at the overall benchmark is notable; while other metrics may perform comparably or better on a given dataset, our metric demonstrates consistent good performance across datasets.</p>
<h2>A. 2 Benchmark Statistics</h2>
<p>For completeness, we provide additional statistics for the SummaC benchmark in Table 6. Due to the exclusion of Omission and Addition as factual consistency errors in the Polytope dataset, our dataset contains benchmark replication contains many more positive examples for that dataset. For XSF, we restrict the dataset to those examples with labels for factual consistency with respect to the source, as opposed to more general factuality labels which take into account world knowledge, which results in fewer examples than the original SummaC benchmark. This is the same subset as was used in Koto et al. (2021).</p>
<p>Please see the following links for the licenses of the datasets and annotations: $\mathrm{CGS}^{2}, \mathrm{XSF}^{3}$, FactCC $^{4}$, SummEval $^{5}$. We did not find licenses for the remaining datasets analyzed in our study. The intended uses of these licenses align with our use for research purposes.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;"># Valid</th>
<th style="text-align: center;"># Test</th>
<th style="text-align: center;">\% Positive</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CGS</td>
<td style="text-align: center;">1281</td>
<td style="text-align: center;">400</td>
<td style="text-align: center;">49.7</td>
</tr>
<tr>
<td style="text-align: center;">XSF</td>
<td style="text-align: center;">996</td>
<td style="text-align: center;">996</td>
<td style="text-align: center;">9.4</td>
</tr>
<tr>
<td style="text-align: center;">Polytope</td>
<td style="text-align: center;">634</td>
<td style="text-align: center;">634</td>
<td style="text-align: center;">87.2</td>
</tr>
<tr>
<td style="text-align: center;">FactCC</td>
<td style="text-align: center;">931</td>
<td style="text-align: center;">503</td>
<td style="text-align: center;">85.8</td>
</tr>
<tr>
<td style="text-align: center;">SummEval</td>
<td style="text-align: center;">850</td>
<td style="text-align: center;">850</td>
<td style="text-align: center;">90.6</td>
</tr>
<tr>
<td style="text-align: center;">FRANK</td>
<td style="text-align: center;">671</td>
<td style="text-align: center;">1575</td>
<td style="text-align: center;">33.2</td>
</tr>
</tbody>
</table>
<p>Table 6: Statistics of the six datasets in the SummaC benchmark. We provide the number of validation and test set examples and the percentage of positive examples in the validation set.</p>
<h2>A. 3 Model Parameters</h2>
<p>Ablation experiments started from a combination that provided good initial validation results and then swapped components. Running every combination of QA-based metric components is expensive. We experimented with running an ablation of the QA models with a 2nd-best performing answer selection component $A L L$. This reduced all scores compared to using the NP Chunks component. This experiment supports our setup of keeping the best component constant when running ablations in order to determine the highest-performing combination of components, rather than experimenting with every combination.</p>
<p>Inference for the MADE QA model is run using the average of the six MADE adapters' parameters.</p>
<p>For Question Filtering with the IsAnsweredSumm Filter, in addition to if the Electra-large QA model labels the question as unanswerable, if the F1 overlap score between the selected answer and the QA model output is less than 0.60 , we remove this question. This filter was added only to IsAnsweredSumm and not IsAnsweredInput as answering questions based on the summary, from which the question was generated, should be an easy task. We reached this threshold based on a qualitative analysis of model outputs, although this number could have also been further tuned on the validation set.</p>
<h2>B Additional Correlation Results</h2>
<p>We provide additional correlation coefficients as a point of reference for future work. Instance-level correlations calculate the correlation between all instances, while the summary-level correlation computes the correlation between scores for each summary of the same input and then averages over inputs. Summary-level correlations are excluded
for QAGS as this dataset does not contain annotations for multiple models, which is necessary to compute this score.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model Type</th>
<th style="text-align: center;">Model Name</th>
<th style="text-align: center;">XSF</th>
<th style="text-align: center;">SummEval</th>
<th style="text-align: center;">FRANK-CNNDM</th>
<th style="text-align: center;">FRANK-XSum</th>
<th style="text-align: center;">QAGs-CNNDM</th>
<th style="text-align: center;">QAGs-XSum</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Misc</td>
<td style="text-align: center;">BARTScore</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.17</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLANC</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">0.03</td>
</tr>
<tr>
<td style="text-align: center;">Entailment</td>
<td style="text-align: center;">FactCC</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">0.26</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BertScore-FFCI</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.20</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DAE</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">$-0.14$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ANLI</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.36</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MNLI</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.35</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DocNLI</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">$-0.38$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SCZeroShot</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.44</td>
</tr>
<tr>
<td style="text-align: center;">QA</td>
<td style="text-align: center;">QuestEval</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.24</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">QAFactEval</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.44</td>
</tr>
<tr>
<td style="text-align: center;">Learned</td>
<td style="text-align: center;">SCConv (synthetic)</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.04</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">QAFactEval-NLI(synthetic)</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.47</td>
</tr>
</tbody>
</table>
<p>Table 7: Instance-level Spearman correlation coefficients across factual consistency evaluation datasets. Metrics are divided into entailment-based, QA-based, and learned metrics that are fine-tuned on synthetic or supervised data. The two highest-correlated metrics for each dataset are shown in bold.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model Type</th>
<th style="text-align: center;">Model Name</th>
<th style="text-align: center;">XSF</th>
<th style="text-align: center;">SummEval</th>
<th style="text-align: center;">FRANK-CNNDM</th>
<th style="text-align: center;">FRANK-XSum</th>
<th style="text-align: center;">QAGs-CNNDM</th>
<th style="text-align: center;">QAGs-XSum</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Misc</td>
<td style="text-align: center;">BARTScore</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.14</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLANC</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.02</td>
</tr>
<tr>
<td style="text-align: center;">Entailment</td>
<td style="text-align: center;">FactCC</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.21</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BertScore-FFCI</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.16</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DAE</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">$-0.11$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ANLI</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.30</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MNLI</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">0.28</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DocNLI</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">$-0.31$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SCZeroShot</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.36</td>
</tr>
<tr>
<td style="text-align: center;">QA</td>
<td style="text-align: center;">QuestEval</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">0.20</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">QAFactEval</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">0.36</td>
</tr>
<tr>
<td style="text-align: center;">Learned</td>
<td style="text-align: center;">SCConv (synthetic)</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.03</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">QAFactEval-NLI(synthetic)</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">0.38</td>
</tr>
</tbody>
</table>
<p>Table 8: Instance-level Kendall correlation coefficients across factual consistency evaluation datasets. Metrics are divided into entailment-based, QA-based, and learned metrics that are fine-tuned on synthetic or supervised data. The two highest-correlated metrics for each dataset are shown in bold.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model Type</th>
<th style="text-align: center;">Model Name</th>
<th style="text-align: center;">XSF</th>
<th style="text-align: center;">SummEval</th>
<th style="text-align: center;">FRANK-CNNDM</th>
<th style="text-align: center;">FRANK-XSum</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Misc</td>
<td style="text-align: center;">BARTScore</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.29</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLANC</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr>
<td style="text-align: center;">Entailment</td>
<td style="text-align: center;">FactCC</td>
<td style="text-align: center;">$-0.02$</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">$-0.07$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BertScore-FFCI</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.19</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DAE</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.32</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ANLI</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">0.18</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MNLI</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">0.17</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DocNLI</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.47</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SCZeroShot</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.52</td>
</tr>
<tr>
<td style="text-align: center;">QA</td>
<td style="text-align: center;">QuestEval</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.44</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">QAFactEval</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.53</td>
</tr>
<tr>
<td style="text-align: center;">Learned</td>
<td style="text-align: center;">SCConv (synthetic)</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.46</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">QAFactEval-NLI(synthetic)</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.48</td>
</tr>
</tbody>
</table>
<p>Table 9: Summary-level Pearson correlation coefficients across factual consistency evaluation datasets. Metrics are divided into entailment-based, QA-based, and learned metrics that are fine-tuned on synthetic or supervised data. The two highest-correlated metrics for each dataset are shown in bold.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model Type</th>
<th style="text-align: center;">Model Name</th>
<th style="text-align: center;">XSF</th>
<th style="text-align: center;">SummEval</th>
<th style="text-align: center;">FRANK-CNNDM</th>
<th style="text-align: center;">FRANK-XSum</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Misc</td>
<td style="text-align: center;">BARTScore</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.28</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLANC</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.06</td>
</tr>
<tr>
<td style="text-align: center;">Entailment</td>
<td style="text-align: center;">FactCC</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">$-0.01$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BertScore-FFCI</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.20</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DAE</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.30</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ANLI</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.17</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MNLI</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.15</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DocNLI</td>
<td style="text-align: center;">$-0.02$</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">0.41</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SCZeroShot</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">0.50</td>
</tr>
<tr>
<td style="text-align: center;">QA</td>
<td style="text-align: center;">QuestEval</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.45</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">QAFactEval</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.47</td>
</tr>
<tr>
<td style="text-align: center;">Learned</td>
<td style="text-align: center;">SCConv (synthetic)</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.44</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">QAFactEval-NLI(synthetic)</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.49</td>
</tr>
</tbody>
</table>
<p>Table 10: Summary-level Spearman correlation coefficients across factual consistency evaluation datasets. Metrics are divided into entailment-based, QA-based, and learned metrics that are fine-tuned on synthetic or supervised data. The two highest-correlated metrics for each dataset are shown in bold.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model Type</th>
<th style="text-align: center;">Model Name</th>
<th style="text-align: center;">XSF</th>
<th style="text-align: center;">SummEval</th>
<th style="text-align: center;">FRANK-CNNDM</th>
<th style="text-align: center;">FRANK-XSum</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Misc</td>
<td style="text-align: center;">BARTScore</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">0.25</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLANC</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.05</td>
</tr>
<tr>
<td style="text-align: center;">Entailment</td>
<td style="text-align: center;">FactCC</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">$-0.01$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BertScore-FFCI</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.18</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DAE</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.27</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ANLI</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.16</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MNLI</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.14</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DocNLI</td>
<td style="text-align: center;">$-0.01$</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">0.37</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SCZeroShot</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.45</td>
</tr>
<tr>
<td style="text-align: center;">QA</td>
<td style="text-align: center;">QuestEval</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.41</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">QAFactEval</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">0.45</td>
</tr>
<tr>
<td style="text-align: center;">Learned</td>
<td style="text-align: center;">SCConv (synthetic)</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">0.41</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">QAFactEval-NLI(synthetic)</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.44</td>
</tr>
</tbody>
</table>
<p>Table 11: Summary-level Kendall correlation coefficients across factual consistency evaluation datasets. Metrics are divided into entailment-based, QA-based, and learned metrics that are fine-tuned on synthetic or supervised data. The two highest-correlated metrics for each dataset are shown in bold.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://tudatalib.u1b.tu-darmstadt. de/handle/tudatalib/2002
${ }^{3}$ https://github.com/
google-research-datasets/xsum_
hallucination_annotations#license
${ }^{4}$ https://github.com/salesforce/factCC/
blob/master/LICENSE.txt
${ }^{5}$ https://github.com/Yale-LILY/
SummEval/blob/master/LICENSE&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
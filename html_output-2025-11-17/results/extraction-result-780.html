<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-780 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-780</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-780</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-21.html">extraction-schema-21</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <p><strong>Paper ID:</strong> paper-273323317</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.08328v1.pdf" target="_blank">Agents Thinking Fast and Slow: A Talker-Reasoner Architecture</a></p>
                <p><strong>Paper Abstract:</strong> Large language models have enabled agents of all kinds to interact with users through natural conversation. Consequently, agents now have two jobs: conversing and planning/reasoning. Their conversational responses must be informed by all available information, and their actions must help to achieve goals. This dichotomy between conversing with the user and doing multi-step reasoning and planning can be seen as analogous to the human systems of"thinking fast and slow"as introduced by Kahneman. Our approach is comprised of a"Talker"agent (System 1) that is fast and intuitive, and tasked with synthesizing the conversational response; and a"Reasoner"agent (System 2) that is slower, more deliberative, and more logical, and is tasked with multi-step reasoning and planning, calling tools, performing actions in the world, and thereby producing the new agent state. We describe the new Talker-Reasoner architecture and discuss its advantages, including modularity and decreased latency. We ground the discussion in the context of a sleep coaching agent, in order to demonstrate real-world relevance.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e780.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e780.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Talker-Reasoner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Talker-Reasoner dual-system agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dual-system agent splitting fast conversational behavior (Talker) and slow multi-step reasoning/planning (Reasoner); the Reasoner calls external tools and writes structured belief states to memory which the Talker reads to inform dialog.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Talker-Reasoner dual-agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A biologically-inspired two-agent architecture: (i) Talker — a fast, in-context learned LLM (instantiated in the paper with Gemini 1.5 Flash) that handles conversational responses, reads memory, and can wait for System 2 when needed; (ii) Reasoner — a slower, hierarchical, CoT-primed LLM pipeline that performs multi-step reasoning and planning, issues tool calls (e.g., SEARCH, APIs, database queries), collects tool observations, synthesizes intermediate beliefs, and writes a structured belief state (JSON/XML schema) into a shared memory. The agents communicate asynchronously via memory: the Reasoner stores b_t+1 in mem; the Talker retrieves b_mem when producing utterances. The Reasoner implements an augmented action space Â = A ∪ T ∪ B (tools, thoughts, belief extractions) and is goal-conditioned and hierarchical.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Conversational sleep-coaching dialog environment (user + knowledge-bases)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A partially-observable language-based environment where observations are natural-language utterances and feedback (O ⊂ L); the 'world' includes the human user and external knowledge bases (e.g., the Web, databases). The agent cannot see the full state and must form beliefs about the user's goals, barriers, habits and coaching phase from dialog and tool outputs, making planning and belief formation challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>APIs and external engines (e.g., SEARCH), functions, external databases/knowledge-bases (e.g., World Wide Web), media/resource links (YouTube recommendations shown as examples).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Textual search results and snippets, natural-language observations, structured database results, resource links (e.g., YouTube URLs), and intermediate textual reasoning traces which are converted into structured beliefs.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Structured language objects (JSON/XML) stored in persistent memory (mem); schemas encode fields such as sleep concern, goals, habits, barriers, environment, and coaching_phase. The Reasoner extracts and composes intermediate beliefs into a new structured belief state and writes it to mem for the Talker to retrieve.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>The Reasoner performs n reasoning/action steps: interleaving thoughts τ_i, tool calls a_i, tool observations o_i, and intermediate belief extractions b_i. When done it composes b_{t+1} from the intermediate beliefs b_1..b_n (which include tool outputs and extracted facts), and stores b_{t+1} in mem. The Talker reads the latest belief from mem (b_mem) when generating utterances; the Talker may operate with a stale b_mem if the Reasoner has not finished.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Hierarchical multi-step planning using in-context learning and chain-of-thought (CoT)-prompted LLMs; ReAct-like synergy between reasoning traces and tool calls; the Reasoner decomposes problems into subproblems and sequences tool calls and belief extractions to form plans.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Tools + structured belief states: the Reasoner uses tool calls to fetch external knowledge and composes tool outputs into structured beliefs stored in memory; the Talker can remain responsive by reading the latest stored belief but may act on stale beliefs causing 'snap-judgement' errors. When the coaching phase requires explicit planning, the Talker is instructed to wait for the Reasoner (and its tool-invoked belief updates) to avoid incorrect or incomplete responses. The architecture reduces latency via asynchronous Talker operation while preserving deliberative planning via the Reasoner.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agents Thinking Fast and Slow: A Talker-Reasoner Architecture', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e780.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e780.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Single-Language Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Single language-based agent with augmented actions (formal POMDP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A formalization of a single LLM-based agent operating in a partially-observable RL setting that can produce thoughts, call tools, extract structured beliefs, and emit utterances as augmented actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Single language-based augmented-action agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM-implemented policy π(â|c,x_mem;Θ) in a POMDP where augmented actions â ∈ Â include tools (A), thoughts/reasoning traces (T), belief extractions (B), and utterances (U). The agent receives natural-language observations o ∈ O and stores interaction history and the latest belief state in memory mem; it can retrieve x_mem to augment context. Tools are explicitly part of the action space and yield observations used in subsequent reasoning and belief formation.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Abstract partially-observable language environment (user + knowledge bases)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Formulated as a POMDP where observations are natural language utterances and rewards in language; environment is partially observable because the agent only sees language observations and must infer hidden aspects of the user's state and external knowledge via tool calls.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>External knowledge-fetching tools such as SEARCH, APIs, and functions (mentioned in the formalization as examples of A).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Textual observations/knowledge snippets and results from external databases; intermediate observations that feed into reasoning traces and belief extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Beliefs b ∈ SL are structured language objects (XML/JSON) stored in mem and used in context c; beliefs encode estimates of user goals, motivations, sentiments, and other structured fields.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Observations (ô) and tool outputs are concatenated into the context and used to update beliefs; the agent generates thoughts, issues tool calls, obtains observations, and performs extract actions to create/update structured belief objects stored in memory.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>LLM-driven policy with in-context learning; combines chain-of-thought-like reasoning traces with tool calls to build plans (augmented action sequences).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Formalizes tool use as part of an augmented action space and explicit belief extraction into structured objects; emphasizes using memory to store belief and plan state so future in-context reasoning and conversation can reference them.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agents Thinking Fast and Slow: A Talker-Reasoner Architecture', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e780.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e780.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct (reasoning + acting) paradigm</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A paradigm that combines chain-of-thought reasoning traces with interleaved actions (tool calls) produced by LLMs to solve tasks; cited as foundational related work for tool-enhanced language agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReAct-style LLM agent (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Mentioned as prior work that generates both reasoning traces and task-specific actions (e.g., tools to call) with LLMs, enabling synergy between deliberation and external action. The paper notes that ReAct lacks explicit belief modeling and continuous talking while reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Tool calls / APIs / search (general mention in the paper's related work).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Textual tool responses/observations (as mentioned).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Chain-of-thought reasoning interleaved with acting (tool calls).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as enabling synergistic reasoning+acting but noted to lack (in its original formulation) continuous dialog/talking while reasoning and explicit structured belief modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agents Thinking Fast and Slow: A Talker-Reasoner Architecture', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e780.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e780.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion (self-reflective ReAct extension)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of ReAct that adds self-reflection to improve reasoning quality; cited as related work on language-agent reasoning improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflexion (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Mentioned as an approach that extends ReAct with self-reflection to iteratively improve reasoning traces and outcomes; included in related work comparing to the proposed Talker-Reasoner framework.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Tool calls / APIs (implicitly via ReAct lineage; paper only mentions Reflexion in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Iterative self-reflection over reasoning traces (as mentioned).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as improving reasoning via self-reflection; contrasted with this work which emphasizes explicit structured belief modeling and simultaneous talking while reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agents Thinking Fast and Slow: A Talker-Reasoner Architecture', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e780.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e780.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoGPT (automated multi-step task agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A toolkit/approach for automating tasks by crafting sub-goal curricula and hierarchical reasoning; cited as related work that automates tool-based task execution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>AutoGPT (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Referenced as an example of an agent that automates tasks by creating a curriculum of sub-goals and connecting hierarchical reasoning with tool use; noted in related work but not used in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>APIs and tools (general mention in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Hierarchical sub-goal decomposition and automated tool execution (as mentioned).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as prior work on automated agents and hierarchical planning; paper contrasts its emphasis on explicit belief schemas and conversational interactivity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agents Thinking Fast and Slow: A Talker-Reasoner Architecture', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e780.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e780.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WebGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WebGPT (browser-assisted QA with human feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A browser-assisted LLM approach that uses web browsing as an external tool to answer questions; cited in related work on tool-augmented language agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>WebGPT (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Cited as an example of an LLM agent that is augmented with external browsing capabilities to fetch information; included in related work to motivate tool-enabled agents that query external knowledge bases.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Web browser / search (as mentioned).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Web search results and text snippets (implicitly).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Retrieval-augmented question answering via browsing (as mentioned).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited to illustrate successful use of external browsing tools with LLMs; used as part of the motivation for the Reasoner's tool-calling behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Agents Thinking Fast and Slow: A Talker-Reasoner Architecture', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ReAct: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>WebGPT: Browser-assisted question-answering with human feedback <em>(Rating: 2)</em></li>
                <li>Auto-gpt for online decision making: Benchmarks and additional opinions <em>(Rating: 1)</em></li>
                <li>Interactive Planning Using Large Language Models for Partially Observable Robotic Tasks <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-780",
    "paper_id": "paper-273323317",
    "extraction_schema_id": "extraction-schema-21",
    "extracted_data": [
        {
            "name_short": "Talker-Reasoner",
            "name_full": "Talker-Reasoner dual-system agent",
            "brief_description": "A dual-system agent splitting fast conversational behavior (Talker) and slow multi-step reasoning/planning (Reasoner); the Reasoner calls external tools and writes structured belief states to memory which the Talker reads to inform dialog.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Talker-Reasoner dual-agent",
            "agent_description": "A biologically-inspired two-agent architecture: (i) Talker — a fast, in-context learned LLM (instantiated in the paper with Gemini 1.5 Flash) that handles conversational responses, reads memory, and can wait for System 2 when needed; (ii) Reasoner — a slower, hierarchical, CoT-primed LLM pipeline that performs multi-step reasoning and planning, issues tool calls (e.g., SEARCH, APIs, database queries), collects tool observations, synthesizes intermediate beliefs, and writes a structured belief state (JSON/XML schema) into a shared memory. The agents communicate asynchronously via memory: the Reasoner stores b_t+1 in mem; the Talker retrieves b_mem when producing utterances. The Reasoner implements an augmented action space Â = A ∪ T ∪ B (tools, thoughts, belief extractions) and is goal-conditioned and hierarchical.",
            "environment_name": "Conversational sleep-coaching dialog environment (user + knowledge-bases)",
            "environment_description": "A partially-observable language-based environment where observations are natural-language utterances and feedback (O ⊂ L); the 'world' includes the human user and external knowledge bases (e.g., the Web, databases). The agent cannot see the full state and must form beliefs about the user's goals, barriers, habits and coaching phase from dialog and tool outputs, making planning and belief formation challenging.",
            "is_partially_observable": true,
            "external_tools_used": "APIs and external engines (e.g., SEARCH), functions, external databases/knowledge-bases (e.g., World Wide Web), media/resource links (YouTube recommendations shown as examples).",
            "tool_output_types": "Textual search results and snippets, natural-language observations, structured database results, resource links (e.g., YouTube URLs), and intermediate textual reasoning traces which are converted into structured beliefs.",
            "belief_state_mechanism": "Structured language objects (JSON/XML) stored in persistent memory (mem); schemas encode fields such as sleep concern, goals, habits, barriers, environment, and coaching_phase. The Reasoner extracts and composes intermediate beliefs into a new structured belief state and writes it to mem for the Talker to retrieve.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "The Reasoner performs n reasoning/action steps: interleaving thoughts τ_i, tool calls a_i, tool observations o_i, and intermediate belief extractions b_i. When done it composes b_{t+1} from the intermediate beliefs b_1..b_n (which include tool outputs and extracted facts), and stores b_{t+1} in mem. The Talker reads the latest belief from mem (b_mem) when generating utterances; the Talker may operate with a stale b_mem if the Reasoner has not finished.",
            "planning_approach": "Hierarchical multi-step planning using in-context learning and chain-of-thought (CoT)-prompted LLMs; ReAct-like synergy between reasoning traces and tool calls; the Reasoner decomposes problems into subproblems and sequences tool calls and belief extractions to form plans.",
            "uses_shortest_path_planning": false,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": false,
            "key_findings": "Tools + structured belief states: the Reasoner uses tool calls to fetch external knowledge and composes tool outputs into structured beliefs stored in memory; the Talker can remain responsive by reading the latest stored belief but may act on stale beliefs causing 'snap-judgement' errors. When the coaching phase requires explicit planning, the Talker is instructed to wait for the Reasoner (and its tool-invoked belief updates) to avoid incorrect or incomplete responses. The architecture reduces latency via asynchronous Talker operation while preserving deliberative planning via the Reasoner.",
            "uuid": "e780.0",
            "source_info": {
                "paper_title": "Agents Thinking Fast and Slow: A Talker-Reasoner Architecture",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Single-Language Agent",
            "name_full": "Single language-based agent with augmented actions (formal POMDP)",
            "brief_description": "A formalization of a single LLM-based agent operating in a partially-observable RL setting that can produce thoughts, call tools, extract structured beliefs, and emit utterances as augmented actions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Single language-based augmented-action agent",
            "agent_description": "An LLM-implemented policy π(â|c,x_mem;Θ) in a POMDP where augmented actions â ∈ Â include tools (A), thoughts/reasoning traces (T), belief extractions (B), and utterances (U). The agent receives natural-language observations o ∈ O and stores interaction history and the latest belief state in memory mem; it can retrieve x_mem to augment context. Tools are explicitly part of the action space and yield observations used in subsequent reasoning and belief formation.",
            "environment_name": "Abstract partially-observable language environment (user + knowledge bases)",
            "environment_description": "Formulated as a POMDP where observations are natural language utterances and rewards in language; environment is partially observable because the agent only sees language observations and must infer hidden aspects of the user's state and external knowledge via tool calls.",
            "is_partially_observable": true,
            "external_tools_used": "External knowledge-fetching tools such as SEARCH, APIs, and functions (mentioned in the formalization as examples of A).",
            "tool_output_types": "Textual observations/knowledge snippets and results from external databases; intermediate observations that feed into reasoning traces and belief extraction.",
            "belief_state_mechanism": "Beliefs b ∈ SL are structured language objects (XML/JSON) stored in mem and used in context c; beliefs encode estimates of user goals, motivations, sentiments, and other structured fields.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "Observations (ô) and tool outputs are concatenated into the context and used to update beliefs; the agent generates thoughts, issues tool calls, obtains observations, and performs extract actions to create/update structured belief objects stored in memory.",
            "planning_approach": "LLM-driven policy with in-context learning; combines chain-of-thought-like reasoning traces with tool calls to build plans (augmented action sequences).",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Formalizes tool use as part of an augmented action space and explicit belief extraction into structured objects; emphasizes using memory to store belief and plan state so future in-context reasoning and conversation can reference them.",
            "uuid": "e780.1",
            "source_info": {
                "paper_title": "Agents Thinking Fast and Slow: A Talker-Reasoner Architecture",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct (reasoning + acting) paradigm",
            "brief_description": "A paradigm that combines chain-of-thought reasoning traces with interleaved actions (tool calls) produced by LLMs to solve tasks; cited as foundational related work for tool-enhanced language agents.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "ReAct-style LLM agent (as cited)",
            "agent_description": "Mentioned as prior work that generates both reasoning traces and task-specific actions (e.g., tools to call) with LLMs, enabling synergy between deliberation and external action. The paper notes that ReAct lacks explicit belief modeling and continuous talking while reasoning.",
            "environment_name": null,
            "environment_description": null,
            "is_partially_observable": null,
            "external_tools_used": "Tool calls / APIs / search (general mention in the paper's related work).",
            "tool_output_types": "Textual tool responses/observations (as mentioned).",
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": "Chain-of-thought reasoning interleaved with acting (tool calls).",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Cited as enabling synergistic reasoning+acting but noted to lack (in its original formulation) continuous dialog/talking while reasoning and explicit structured belief modeling.",
            "uuid": "e780.2",
            "source_info": {
                "paper_title": "Agents Thinking Fast and Slow: A Talker-Reasoner Architecture",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion (self-reflective ReAct extension)",
            "brief_description": "An extension of ReAct that adds self-reflection to improve reasoning quality; cited as related work on language-agent reasoning improvements.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Reflexion (as cited)",
            "agent_description": "Mentioned as an approach that extends ReAct with self-reflection to iteratively improve reasoning traces and outcomes; included in related work comparing to the proposed Talker-Reasoner framework.",
            "environment_name": null,
            "environment_description": null,
            "is_partially_observable": null,
            "external_tools_used": "Tool calls / APIs (implicitly via ReAct lineage; paper only mentions Reflexion in related work).",
            "tool_output_types": null,
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": "Iterative self-reflection over reasoning traces (as mentioned).",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Cited as improving reasoning via self-reflection; contrasted with this work which emphasizes explicit structured belief modeling and simultaneous talking while reasoning.",
            "uuid": "e780.3",
            "source_info": {
                "paper_title": "Agents Thinking Fast and Slow: A Talker-Reasoner Architecture",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "AutoGPT",
            "name_full": "AutoGPT (automated multi-step task agent)",
            "brief_description": "A toolkit/approach for automating tasks by crafting sub-goal curricula and hierarchical reasoning; cited as related work that automates tool-based task execution.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "AutoGPT (as cited)",
            "agent_description": "Referenced as an example of an agent that automates tasks by creating a curriculum of sub-goals and connecting hierarchical reasoning with tool use; noted in related work but not used in the paper's experiments.",
            "environment_name": null,
            "environment_description": null,
            "is_partially_observable": null,
            "external_tools_used": "APIs and tools (general mention in related work).",
            "tool_output_types": null,
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": "Hierarchical sub-goal decomposition and automated tool execution (as mentioned).",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Mentioned as prior work on automated agents and hierarchical planning; paper contrasts its emphasis on explicit belief schemas and conversational interactivity.",
            "uuid": "e780.4",
            "source_info": {
                "paper_title": "Agents Thinking Fast and Slow: A Talker-Reasoner Architecture",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "WebGPT",
            "name_full": "WebGPT (browser-assisted QA with human feedback)",
            "brief_description": "A browser-assisted LLM approach that uses web browsing as an external tool to answer questions; cited in related work on tool-augmented language agents.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "WebGPT (as cited)",
            "agent_description": "Cited as an example of an LLM agent that is augmented with external browsing capabilities to fetch information; included in related work to motivate tool-enabled agents that query external knowledge bases.",
            "environment_name": null,
            "environment_description": null,
            "is_partially_observable": null,
            "external_tools_used": "Web browser / search (as mentioned).",
            "tool_output_types": "Web search results and text snippets (implicitly).",
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": "Retrieval-augmented question answering via browsing (as mentioned).",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Cited to illustrate successful use of external browsing tools with LLMs; used as part of the motivation for the Reasoner's tool-calling behavior.",
            "uuid": "e780.5",
            "source_info": {
                "paper_title": "Agents Thinking Fast and Slow: A Talker-Reasoner Architecture",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "WebGPT: Browser-assisted question-answering with human feedback",
            "rating": 2,
            "sanitized_title": "webgpt_browserassisted_questionanswering_with_human_feedback"
        },
        {
            "paper_title": "Auto-gpt for online decision making: Benchmarks and additional opinions",
            "rating": 1,
            "sanitized_title": "autogpt_for_online_decision_making_benchmarks_and_additional_opinions"
        },
        {
            "paper_title": "Interactive Planning Using Large Language Models for Partially Observable Robotic Tasks",
            "rating": 2,
            "sanitized_title": "interactive_planning_using_large_language_models_for_partially_observable_robotic_tasks"
        }
    ],
    "cost": 0.01404075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Agents Thinking Fast and Slow: A Talker-Reasoner Architecture
10 Oct 2024</p>
<p>Konstantina Christakopoulou 
Shibl Mourad 
Maja Matarić 
Google Deepmind 
Agents Thinking Fast and Slow: A Talker-Reasoner Architecture
10 Oct 20244C9CCD38CA89E4885AB1E71876D69D1EarXiv:2410.08328v1[cs.AI]
Large language models have enabled agents of all kinds to interact with users through natural conversation.Consequently, agents now have two jobs: conversing and planning/reasoning.Their conversational responses must be informed by all available information, and their actions must help to achieve goals.This dichotomy between conversing with the user and doing multi-step reasoning and planning can be seen as analogous to the human systems of "thinking fast and slow" as introduced by Kahneman [14].Our approach is comprised of a "Talker" agent (System 1) that is fast and intuitive, and tasked with synthesizing the conversational response; and a "Reasoner" agent (System 2) that is slower, more deliberative, and more logical, and is tasked with multi-step reasoning and planning, calling tools, performing actions in the world, and thereby producing the new agent state.We describe the new Talker-Reasoner architecture and discuss its advantages, including modularity and decreased latency.We ground the discussion in the context of a sleep coaching agent, in order to demonstrate real-world relevance.</p>
<p>Introduction</p>
<p>Humans have the ability to do two very different kinds of thinking.On the one hand, we can form snap judgements, such as reacting to a speeding car or recognizing the emotional cues of an upset coworker.On the other hand, we can solve complicated problems, like planning a vacation and doing complex multiplications.The well-known behavioral science theory proposes that two different systems drive those abilities: the fast and intuitive System 1 and the slow and deliberative System 2 [14].Daniel Kahneman, who introduced the theory, described the two systems for the two modes of thinking as follows: "System 1 operates automatically and quickly, with little or no effort and no sense of voluntary control.System 2 allocates attention to the effortful mental activities that demand it, including complex computations.It represents the conscious reasoning self that has beliefs, makes choices, and decides what to think about and what to do."</p>
<p>Although difficult problems might rely more on System 2 and everyday skills more on System 1, most cognitive processes are a mix of both kinds of reasoning.System 1 continuously generates suggestions for System 2: impressions, intuitions, intentions, and feelings.If endorsed by System 2, impressions and intuitions form the basis of the explicit beliefs of System 2, and intentions turn into the deliberate choices of System 2.</p>
<p>Many reinforcement learning (RL) problems can also benefit from a similar dual-system approach.The rapid advances in large language models (LLMs) [1,6,32] have enabled artificial intelligence (AI) agents of all kinds, from AI coding buddies, to tutors and health coaches.These agents are expected to understand the complex patterns of the world via language and potentially perceive other heterogeneous multimodal signals, generating impressions, creating coherent patterns of ideas, and producing dialog (with other modalities being actively added).This resembles the coherent-seeking System 1. On the other hand, AI agents are supposed to perform complex multi-step reasoning, and make decisions that involve calling tools, actively retrieving information from external data sources, and solving complex problems.This is similar to the slower and more deliberative System 2. In the context of enabling agents to converse, reason and plan, in this work we consider a dual-system approach that enables those abilities through the two modes of thinking.We therefore divide the agent into two agents: a fast and intuitive Talker agent and a slower and deliberative Reasoner agent.The Talker agent focuses on generating natural and coherent conversation with the user and interacts with the environment, while the Reasoner agent focuses on performing multi-step planning, reasoning, and forming beliefs, grounded in the environment information provided by the Talker.The Talker agent, a la System 1, can access memory, priming its responses.</p>
<p>Similarly to the System 1 and 2 modes of thinking, the division of labor between the Talker and Reasoner agents is efficient: it minimizes effort and optimizes performance.An added benefit of this division is that the Talker can carry out the conversation, while getting more observations from the environment, without needing to wait for the slow reasoning and belief forming of the Reasoner agent.This is analogous to behavioral science dual-System approach, with System 1 always being on while System 2 operates at a fraction of its capacity.Similarly, the Talker is always on and interacting with the environment, while the Reasoner updates beliefs informing the Talker only when the Talker waits for it, or can read it from memory.This division of labor works well most of the time, as the Talker is typically very good at what it does: it can automatically fetch information from memory, effectively priming its underlying model to respond well to familiar situations.However, the framework has limitations.The Talker operates with a more outdated view of the world, which has inherent biases, and can sometimes answer easier questions than the ones asked.Also, it has little understanding of complex problem solving and planning.So, we introduce a variable allowing the Talker to wait the Reasoner, in cases when System 2-thinking is necessary before the Talker forms its response.</p>
<p>To evaluate the proposed dual-system Talker-Reasoner framework, we ground our work on the real world setting of a sleep coaching agent interacting with users through dialog.We discuss success cases of this division of labor, including fast and intuitive conversations driven by the Talker and complex plans and belief states developed by the Reasoner.We also discuss cases where, similar to the dual-system thought machinery, the Reasoner (System 2) might need to override the Talker (System 1).In the AI coaching context, this could be useful when the user is requesting a complex coaching plan the Reasoner needs to finish before the Talker is able to respond.</p>
<p>Related Work</p>
<p>Large Language Models for Agent Planning.Inspired by the strong emergent capabilities of LLMs [6], such as zero-shot prompting [15], in-context learning [5], and complex reasoning [37,43], research into LLM-driven agents is receiving a great deal of attention [36,40,2,25,21,34,35,24,4].The work most relevant to this paper is on text-based agents [25,39,22], although a great deal of work on real-world embodied agents [16] is increasingly relevant as models become truly multimodal [36,2,34,19,27,30,28].ReAct [39] uses chain-of-thought (CoT) prompting [37] and generates both reasoning traces and task-specific actions (e.g., tools to call) with LLMs.Reflexion [26] extends ReAct with self-reflection to improve reasoning.AutoGPT [38] is a tool for automating tasks by crafting a curriculum of sub-goals for completing a high-level goal, connecting to hierarchical reasoning.All these works lack (i) talking while reasoning/planning and (ii) explicit belief modeling, which are crucial components in our framework.The (i) talking aspect of our agent connects our work to prior work on natural-language based feedback agents [26,40].Similarly to [40], our agent iteratively incorporates environment feedback to modify subsequent plans in a closed-loop process.However, we do not use RL to update all future plans, but instead only to adapt the cross-session plan for the single user, by augmenting the context of the in-context learned LLM with the feedback.The (ii) explicit belief modeling aspect of our work relates to the theory of mind [23,8,18] and the large body of work on world modeling [7,10,12,42], as the world in our case encompasses the user, and the agent builds a proxy model of other agents (i.e., humans) to reason about their behaviors.In particular, the Reasoner continuously updates its belief about the user's goals, plans, barriers, motivations, in the form of a structured object or schema [43], similarly to world and user models in past work [11,10,42].We do not yet model beliefs about beliefs (e.g., what the user thinks the agent thinks, and so on) or use world models to predict future actions; this is left for future work.</p>
<p>The Talker-Reasoner Agent Model</p>
<p>Before we introduce the dual-system Talker-Reasoner agent framework corresponding to the fast and slow thinking respectively (Section 3.2), we start with formalizing a single language-based agent capable of talking and System 1 reasoning, as well as System 2 multi-step reasoning and planning useful for complex problem solving (Section 3.1).</p>
<p>Single Language-Based Agent Interacting With Humans: Synergizing Talking and Extracting Beliefs With Reasoning and Planning</p>
<p>Let us consider a language-based AI agent that can interact with users through natural language conversation to help them accomplish some task.The agent should be capable of multi-step reasoning and planning to be able to solve the task and also capable of generating a conversational response to the user.This paradigm of agents reasoning/planning and conversing has become more prevalent as a result of the introduction of large language models [41,32,1,31,33,29].We present a Reinforcement Learning (RL) formulation of this talking-and-reasoning paradigm.We also extend the paradigm to include explicit modeling of the beliefs the agent has about the user, such as the user's motivations, emotions, and goals, which guide the talking and reasoning.Figure 2 shows an overview of the overall language-based agent interacting with a user, which we will describe in detail in what follows.</p>
<p>We formulate the language-based agent that can reason, talk, and do explicit belief modeling in a partially-observable RL framework [29].The agent is continuously interacting with the world E. The world encompasses both the user the agent is interacting with, and the knowledge bases (such as the World Wide Web) that allow the agent to retrieve real-world knowledge.The agent only has a partial view of the world, thus formulating beliefs b ∈ B about the current state of the world.It can learn more about the user by interacting with them via language (future work will add other modalities).Assume that L represents the language space; the agent receives from the user observations o ∈ O which live in the language space O ⊂ L. Observations can contain both information and feedback/rewards in natural language (e.g., "I don't like this", and "Can you add something else to my plan?").We formalize this as Ô = O ∪ R, with observations o ∈ L and rewards r ∈ L. The observations ô ∈ Ô are then used to update both the agent's beliefs and the subsequent planning/reasoning performed by the agent.This can be seen as a form of online policy learning via natural language feedback and relates to natural-language based feedback agents [26].</p>
<p>We now focus on the agent's actions a ∈ A. The agent can (i) formulate thoughts τ ∈ L around actions it can take, and (ii) decide which tools a ∈ A to select (e.g., APIs, engines like SEARCH, functions) to fetch external knowledge-this expands the space of tasks it can accomplish.By combining a series of thoughts and tools/actions, along with the results fetched via the tools, the agent can create a plan p for solving a problem.Furthermore, the agent can (iii) formulate beliefs about the user (and potentially other aspects of the world); thus, another key action is extracting leading to a new agent belief state.The beliefs are represented as structured language objects living in XML or JSON space, with b ∈ SL [42,43], where structured language can be seen as a subset of the L language space.The belief state could encode the agent's estimate of the user's goals, needs, thoughts, sentiments, motivations, and barriers (depending on the agent context and use case), also relating this work with theory of minds [18,42].Given the plan formed via multi-step reasoning/planning ((i) and (ii) above) and the beliefs formed ((iii) above), the agent can (iv) talk, i.e., provide a natural language utterance u ∈ L to the user.Putting these together, the space of actions A, which in a classic act-only paradigm contains only the tools, is expanded to Â.We refer to this as the augmented space of actions.The space Â encompasses tools a ∈ A, thoughts/reasoning traces τ ∈ T (as considered in the ReAct [39] paradigm), and also beliefs b and utterances u, i.e., Â = A ∪ T ∪ B ∪ U.</p>
<p>While thoughts and beliefs do not affect the world and lead to no observational feedback, tools and utterances interact with the world, namely the external knowledge bases and the user, respectively.</p>
<p>The language-based agent chooses its augmented action according to a policy π implemented via a large language model (LLM) with parameters Θ, instructed via its prompt/constitution to follow a set of instructions I ∈ L. The instructions encode domain knowledge, desired behavior in interactions with the user, and the constitution [3] the agent is supposed to follow.The LLM implementing the policy uses the instructions, the last user's natural-language feedback/utterance o, along with the interaction history H ∈ L in its context window c.Besides the context window, the agent is memory-enabled, allowing it to record all agent-user interactions along with other user information across sessions in its memory mem.That is where the agent also stores the most recent belief state b ∈ B and the current plan p for how to solve the task.The agent can, at any point, retrieve relevant information from memory x mem , augmenting the information in its context window.</p>
<p>We formulate this in a POMDP [20,29], as follows.At time step t + 1, the language-based agent chooses actions â ∈ Â according to a Θ-parameterized LLM-based policy: ât+1 ∼ π(â|c t , x mem ; Θ) where the context c t = Concat(ô t , H t , I) is the concatenation of the last user's utterance, the interaction history, and the overall instructions; x mem represents any information the agent might need to retrieve from memory, including the previous belief state b.For each user-agent interaction, when the agent needs to generate an utterance to talk to the user, there might be series of augmented actions the agent may take before it produces its response.The context captures the series of thoughts/tool selections/results obtained before it generates the action:
c t = (τ 1 , a 1 , o 1 , b 1 , . . . , τ t−1 , a t−1 , o t−1 , b t−1 ).</p>
<p>Proposed Dual-System Talker-Reasoner Agent Model</p>
<p>So far, we have formalized an agent that can interact with users to solve tasks via its ability to do multi-step reasoning and planning, talking, and extracting beliefs about the user.However, this can be hard for a single LLM to do, as there are different requirements for talking vs. multi-step reasoning/planning and forming beliefs.In what follows, we propose the dual-system architecture, inspired by the fast and slow thinking Systems 1 and 2, respectively, consisting of: 1.The Talker: The fast agent that interacts with the user via language, perceives the world, gets observations and feedback from the user, interacts with memory to prime its responses, and generates the conversational response.2. The Reasoner: The slow and deliberative agent responsible for complex problem solving, which involves synergizing reasoning with taking actions augmenting its knowledge from the real world, such as calling tools or fetching information from external databases [17].The Reasoner is also responsible for making and updating beliefs that drive its decisions, and the Talker's subsequent utterances.The Reasoner is typically goal-conditioned, primed to solve a specific problem or goal [9], and hierarchical [36], dividing problems into sub-problems.</p>
<p>As shown in Figure 3, the main way the Talker (System 1) and Reasoner (System 2) interact is through memory.The Reasoner is responsible for generating the next belief state and deriving multi-step reasoning and planning, and storing them in memory.Any time the Talker needs the belief, it retrieves the latest one from memory.The Talker therefore might operate with a delayed view of the world, as the Reasoner might not have had time to generate the new belief and store it in memory.However, because the Talker is meant to be intuitive and fast and takes into account what the user just said and the conversation history, the conversational response will still be coherent.</p>
<p>In fact, the conversation driven by the Talker is processed by the Reasoner so that the quick impressions and responses of the Talker become sources of explicit beliefs and choices (plans) of the Reasoner.The Talker can also wait for the Reasoner before generating a response; this is equivalent to System 2 taking over and overruling System 1's impulses.</p>
<p>The Talker (Thinking Fast) Agent</p>
<p>The Talker interacts with the world, including the user, and needs to understand language and the interaction/conversation history, and be able to generate natural human-level language to do the needed talk action.These criteria are met by implementing the Talker agent with a powerful, in-context learned [5] language model.Similar to System 1, the Talker strives for coherence, and acts as an associative machine.To ensure the coherence of the Talker and a good user experience, instructions I ∈ L are given to the language model to follow, encoding the Talker's constitution [3].</p>
<p>The Talker also interacts with memory mem to prime its responses with relevant information x mem , including the latest beliefs that have been formed by the Reasoner and stored in mem.At every interaction with the user, the Talker takes the Talk action, and generates a conversational response, i.e., utterance u, conditioned on the context c and the instructions I:
u (t+1) ∼ Talker(u|c t+1 , I(•|b mem ); Φ)(1)
where Φ are the parameters of the Talker.The context c t+1 can include the latest user utterance ô which acts as both observation and natural language feedback, the b mem is the latest belief produced by the Reasoner and stored in memory, and H mem is the interaction history:
c t+1 = Concat(ô t+1 , b mem , H mem ) (2)
The instructions I the Talker follows can depend on the belief state.Overall, the Talker is supposed to be fast and conversational, minimizing latency.It is "on" whenever the user converses with the system, similar to System 1.It may use beliefs b mem that are not the latest b t+1 of the Reasoner in order to ensure fast interactivity, meaning that the two systems may at times be decoupled.</p>
<p>The Reasoner (Thinking Slow) Agent</p>
<p>The Reasoner agent acts like System 2: it enables complex problem solving, deliberate belief forming, and choice making.</p>
<p>The Reasoner performs multi-step reasoning and planning, entailing series of calls to various incontext learned [5] or Chain-of-Thought (CoT)-prompted language models [37], and calls to different tools [39] or databases [17] for external knowledge fetching.This requires it to synergize reasoning (producing thoughts) and acting (calling tools to fetch observations), as in retrieval-augmented or tool-enhanced ReAct-type agents [39].The agent can develop plans (e.g., series of tools to call) and reasoning traces to solve complex tasks.It typically decomposes the problem into sub-problems in a hierarchical fashion, and tasks each sub-problem to different modules, tools, or LLMs.</p>
<p>It also forms beliefs about the state of the world, which can combine multiple intermediate results of multi-step reasoning, and extract from past interaction history all interesting facts about the user model in a structured language object to be stored in mem.This aspect of deliberate belief forming is what distinguishes the Reasoner from typical ReAct-style agents, as it includes deliberate attempt in modeling the world/human, as described in the extract action.</p>
<p>Concretely, the actions the Reasoner can take are: reason, act, and extract, each resulting in thoughts τ ∈ L, intermediate observations as a result of tool use o fetching external knowledge, and beliefs b ∈ SL in the form of structured language objects.Thus, the augmented space of actions includes thoughts, tool actions, and belief extractions: Â = A ∪ T ∪ B. Since the augmented action space lives in the unlimited language space, learning a policy is difficult and requires strong language priors.Thus, we implement the Reasoner's policy via an in-context learned language model parameterized by Z.The Reasoner selects an augmented action:
â ∼ Reasoner(b, â|c Reasoner ; Z).(3)
The context c Reasoner involves the interleaving of a series of n thoughts, actions (e.g., calling tools like SEARCH), observations after these actions, and belief extractions, along with the newest observations/language feedback ôt provided by the Talker:
c Reasoner = Concat(τ 1 , a 1 , o 1 , b 1 , . . . , τ n , a n , o n , b n ; ôt ).(4)
When the Reasoner finishes its series of n reasoning/planning steps, where n can vary per round depending on the problem, it constructs the belief state b t+1 as a combination of intermediate beliefs b 1 , . . ., b n and stores it in mem.Therefore, between two steps of user-Talker interaction, there are n steps of slower "thinking" by the Reasoner.</p>
<p>Evaluation Case Study: Sleep Coaching Agent</p>
<p>We instantiated and validated the Talker-Reasoner dual-agent architecture in a sleep coach use case: an AI language agent interacting with users to provide help with sleeping behaviors and challenges.</p>
<p>Grounding in a Real-World Scenario of AI Coaching for Sleep</p>
<p>We use this real-world scenario to ground the evaluation of our dual-agent architecture.We chose AI coaching because it requires having a model of the user being coached, using sleep coaching expert knowledge to ensure scientifically-supported advice, providing a multi-step coaching plan for the user, and being conversational and empathetic much as a human coach would be.This instantiation allows us to qualitatively test the planning and reasoning capabilities of the Reasoner and the interactivity of the Talker.We also chose sleep because it is a critically important component of human health, with impact on nutrition, activity, and mental health [13].The AI coach needs to continuously understand the user's needs through dialog, and to accompany the user from understanding them, to helping them set goals, to providing a multi-step plan they can follow, connecting them with resources.</p>
<p>Instantiating a Talker-Reasoner Dual-Agent Model for Sleep Coaching</p>
<p>Sleep Coaching Talker Agent: We encode expert knowledge about sleep obtained from clinical experts in a set of instructions I that describe the agent's constitution (e.g., being empathetic, conversational, providing accurate advice) and the desired phases of sleep coaching (understanding, goal-setting, and coaching-plan) with separate instructions for each: I understanding , I goal-setting , I coaching-plan , to guide the Talker through the expert clinically-informed coaching process.We implemented the Talker via a Gemini 1.5 Flash [31] model, conditioned on the instructions, the context including the last user utterance, the interaction history, and the latest available belief state stored in mem, as in Equations 1, 2. The model's strong language prior along with set of instructions allows it to perform complex pattern understanding and provide thoughtful conversational responses akin to System 1, as illustrated in Section 4.3.1.</p>
<p>Sleep Coaching Reasoner Agent: The AI Coaching Reasoner explicitly models beliefs about the user.To do so, with clinician expert input, we encoded a JSON/XML schema of the belief, including fields such as sleep concern, goals, habits, barriers, and sleep environment.The Reasoner infers and updates the schema fields while it performs its multi-step reasoning/planning, thereby creating/updating its belief state.We implemented a hierarchical [36] Reasoner-at every turn, it infers which of the three discrete coaching phases the user is in, and employs the corresponding of the three mini-Reasoners, each instantiated by a different combination of CoT-prompted Gemini 1.5 Flash models [31] along with corresponding schemas to extract.</p>
<p>Talker-Reasoner Coordination: Whether the Talker waits for the Reasoner to finish is determined by the belief-in the planning coaching phase the Talker waits, otherwise it does not.</p>
<p>Qualitative Results</p>
<p>Example Conversation</p>
<p>The following is an example conversation illustrating the interaction between Reasoner and Talker.Absolutely! Here's an "Explore Natural Sounds" segment of your plan, along with some relevant YouTube content:</p>
<p>• Explore Natural Sounds: This segment is designed to help you create a calming and serene ambience in your space using sounds of nature.By incorporating these sounds, you can promote relaxation and reduce stress.Recommended YouTube videos: ... • Discover Healing Frequencies: In this segment of your plan, we'll explore the world of theurapeutic frequencies, which are believed to have a profound impact on relaxation and emotional wellbeing.... YouTube recommendations:</p>
<p>Discussion</p>
<p>As mentioned in Section 3, to minimize latency, the Talker uses the latest available belief state b from memory, rather than waiting for the Reasoner to finish its thinking process.The qualitative results in Section 4.3 illustrate two distinct success and failure modes of this approach:</p>
<p>"Intuitive Talker": The asynchronous approach can be effective for tasks where the Talker is sufficient even if it operates with an older belief state.These are typically System 1 tasks.For example, when the coaching phase is "understanding", the Talker can successfully carry out the conversation without the need for the Reasoner to finish the belief updating.</p>
<p>"Snap judgement Talker": However, the Reasoner must update its belief state before the Talker proceeds in complex problem-solving scenarios e.g., when the user is asking for an explicit multistep plan or for specific resources that require tool calling.In those cases, without waiting for the Reasoner to finish, the Talker makes snap judgements.We can see some examples of such "snap judgement Talker" behavior when the belief extracted by the Reasoner does not yet capture the correct coaching phase, and does not fetch resources.To address this, when the Talker reads that the coaching phase is "planning", it is instructed to wait for the Reasoner to finish.This corresponds to System 2 taking over and overruling the impulses of System 1.</p>
<p>Finally, although there is a growing interest in AI agents performing more complex System 2 reasoning [14], we believe that our work is the first to formalize the duality of System 1 and System 2 reasoning that our Talker-Reasoner architecture offers.</p>
<p>Conclusions</p>
<p>This paper introduces the dual-system agent framework as a possible biologically-inspired architecture for foundation-model driven intelligent agents.Inspired by the behavioral science principles behind this framework, directions for future research include deciding when not to probe the Reasoner and how to utilize it in a lower capacity most of the time, when the Talker can handle most situations.Ideally, given a user query, the Talker should automatically determine whether it requires System 2 reasoning, and therefore the Reasoner, or whether it can safely proceed with its System 1 thinking.Another direction is to extend the Talker-Reasoner architecture to multiple Reasoners, each writing belief states to different part of the memory, for different types of reasoning.</p>
<p>Figure 1 :
1
Figure 1: Illustration of the proposed dual-system Talker-Reasoner approach.</p>
<p>Figure 2 :
2
Figure 2: Single LLM-based agent that talks and extracts belief states while multi-step reasoning.</p>
<p>Figure 3 :
3
Figure 3: Diagram of Talker-Reasoner architecture.</p>
<p>Absolutely!Here is a plan to eliminate distractions in your bedroom: Noise:• Identify noise sources: Make a list of all the noise sources in your bedroom • Block outside noise: Use blackout curtains to block light and reduce noise; Consider noisecancelling curtains or soundproofing panels.;Tryearplugsor a white noise machine.This video from roomdsignTV provides visual examples of calming bedroom color palettes.It showcases real-life transformations that demonstrate the impact of soothing colors on creating a serene atmosphere.2.YouTube Playlist: (link) Title: Berger paints colour combination.Channel: Hresun Interiors.Reasoning:This playlist from Hresun Interiors presents a collection of bedroom color combination ideas.It features a wide range of styles, from traditional to modern, to help you find the perfect color scheme for your bedroom.The following is an example of how the Reasoner agent adapted its plan based on the feedback collected from user by the Talker agent.
TALKER: copying Reasoner's response4.3.2 Adapting planning from feedbackUSER: thank you, this is quite useful. Could you please add in my plan more steps around anyrelaxing sounds I should add to my space?(BELIEF in mem): unchangedPLAN by REASONER:USER: Hey, I need your help planning re: how to create a relaxing bedtime environment.(BELIEF in mem) {updated-context: starting context, updated-title: Sleeping Coaching, coaching-phase: UNDERSTANDING}TALKER: Great, let's start working on creating a relaxing bedtime environment for you :-) What'sone thing in your bedroom that might be disrupting your sleep?USER: I think noises and light can be too distracting. Can you help create a plan for me for howto eliminate these distractions?(BELIEF in mem) unchangedTALKER: Light: ... Remember, consistency is key. Stick to this plan every night to create a peaceful anddark sleep environment. Is there anything else I can help you with?USER: sure, that's useful. However, I would prefer to take it more step-by-step. And also seeresources of how to create a serene and relaxing atmosphere in my bedroom.(BELIEF in mem): {user context: UserCollectedContextData(primary-sleep-concern: Noise and lightdistractions in the bedroom), coaching phase: CoachingPhase(value=PLANNING)}REASONER's PLAN: Certainly! Let's take a more in-depth look at the first step of your plan:
• Step: Choose a calming color palette.Description: When selecting colors for your bedroom, opt for soothing, neutral tones like soft blues, greens, or grays.These shades have been scientifically proven to promote relaxation and tranquility.1. YouTube Video: (link) Title: Wall Color Ideas to Create a Relaxing and Stress-free Bedroom.Channel: roomdsignTV.Reasoning:</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Chuyuan Finn, Keerthana Fu, Karol Gopalakrishnan, Hausman, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022. 2022arXiv preprint</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron Mckinnon, arXiv:2212.08073Constitutional ai: Harmlessness from ai feedback. 2022. 2022arXiv preprint</p>
<p>Large language models can implement policy iteration. Ethan Brooks, Logan Walls, Richard L Lewis, Satinder Singh, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Language models are few-shot learners. Tom B Brown, arXiv:2005.141652020. 2020arXiv preprint</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023. 2023arXiv preprint</p>
<p>Counterintuitive behavior of social systems. Jay W Forrester, Theory and decision. 21971. 1971</p>
<p>Development and neurophysiology of mentalizing. Uta Frith, Christopher D Frith, Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences. 3582003. 2003</p>
<p>Dibya Ghosh, Abhishek Gupta, Sergey Levine, arXiv:1811.07819Learning actionable representations with goal-conditioned policies. 2018. 2018arXiv preprint</p>
<p>. David Ha, Jürgen Schmidhuber, arXiv:1803.101222018. 2018World models. arXiv preprint</p>
<p>Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu, arXiv:2305.14992Reasoning with language model is planning with world model. 2023. 2023arXiv preprint</p>
<p>Language models, agent models, and world models: The law for machine reasoning and planning. Zhiting Hu, Tianmin Shu, arXiv:2312.052302023. 2023arXiv preprint</p>
<p>The sleep revolution: Transforming your life. Arianna Huffington, 2016</p>
<p>Thinking, fast and slow. Daniel Kahneman, 2011. 2011Farrar, Straus and Giroux</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 352022. 2022</p>
<p>Robotic systems architectures and programming. David Kortenkamp, Reid Simmons, Davide Brugali, 2016. 2016Springer handbook of robotics</p>
<p>Retrievalaugmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 332020. 2020</p>
<p>Huao Li, Yu Quan Chong, Simon Stepputtis, Joseph Campbell, Dana Hughes, Michael Lewis, Katia Sycara, arXiv:2310.10701Theory of mind for multi-agent collaboration via large language models. 2023. 2023arXiv preprint</p>
<p>Code as policies: Language model programs for embodied control. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>A survey of POMDP solution techniques. Kevin P Murphy, 2000. 2000210</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, arXiv:2112.09332Webgpt: Browserassisted question-answering with human feedback. 2021. 2021arXiv preprint</p>
<p>Sung Joon, Park, O' Joseph, Carrie Jun Brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Generative agents: Interactive simulacra of human behavior. 2023. 2023</p>
<p>Does the chimpanzee have a theory of mind?. David Premack, Guy Woodruff, Behavioral and brain sciences. 11978. 1978</p>
<p>Francesco Sp Sharan, Manmohan Pittaluga, Chandraker, arXiv:2401.00125Llm-assist: Enhancing closed-loop planning with language-based reasoning. 2023. 2023arXiv preprint</p>
<p>Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Progprompt: Generating situated robot task plans using large language models. Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, Animesh Garg, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Interactive Planning Using Large Language Models for Partially Observable Robotic Tasks. Lingfeng Sun, K Devesh, Chiori Jha, Siddarth Hori, Radu Jain, Xinghao Corcodel, Masayoshi Zhu, Diego Tomizuka, Romeres, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>. Andrew G Richard S Sutton, Barto, Reinforcement learning. Journal of Cognitive Neuroscience. 111999. 1999</p>
<p>Large language models as generalizable policies for embodied tasks. Andrew Szot, Max Schwarzer, Harsh Agrawal, Bogdan Mazoure, Rin Metcalf, Walter Talbott, Natalie Mackraz, Devon Hjelm, Alexander T Toshev, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.11805Gemini: a family of highly capable multimodal models. 2023. 2023arXiv preprint</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Du, arXiv:2201.08239Lamda: Language models for dialog applications. 2022. 2022arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023. 2023arXiv preprint</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, arXiv:2305.162912023. 2023arXiv preprint</p>
<p>A survey on large language model based autonomous agents. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Frontiers of Computer Science. 181863452024. 2024</p>
<p>Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, Yitao Liang, arXiv:2302.015602023. 2023arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 352022. 2022</p>
<p>Auto-gpt for online decision making: Benchmarks and additional opinions. Hui Yang, Sifu Yue, Yunzhong He, arXiv:2306.022242023. 2023arXiv preprint</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022. 2022arXiv preprint</p>
<p>Haiteng Zhao, Chang Ma, Guoyin Wang, Jing Su, Lingpeng Kong, Jingjing Xu, Zhi-Hong Deng, Hongxia Yang, arXiv:2402.15809Empowering Large Language Model Agents through Action Learning. 2024. 2024arXiv preprint</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223A survey of large language models. 2023. 2023arXiv preprint</p>
<p>How FaR Are Large Language Models From Agents with Theory. Pei Zhou, Aman Madaan, Pranavi Srividya, Aditya Potharaju, Kevin R Gupta, Ari Mckee, Jay Holtzman, Xiang Pujara, Swaroop Ren, Aida Mishra, Nematzadeh, arXiv:2310.030512023. 2023arXiv preprint</p>
<p>Self-discover: Large language models self-compose reasoning structures. Pei Zhou, Jay Pujara, Xiang Ren, Xinyun Chen, Heng-Tze Cheng, Quoc V Le, Ed H Chi, Denny Zhou, Swaroop Mishra, Huaixiu Steven, Zheng , arXiv:2402.036202024. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
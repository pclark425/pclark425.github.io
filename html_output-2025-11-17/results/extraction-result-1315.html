<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1315 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1315</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1315</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-954b01151ff13aef416d27adc60cd9a076753b1a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/954b01151ff13aef416d27adc60cd9a076753b1a" target="_blank">RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes to represent a "fast" reinforcement learning algorithm as a recurrent neural network (RNN) and learn it from data, encoded in the weights of the RNN, which are learned slowly through a general-purpose ("slow") RL algorithm.</p>
                <p><strong>Paper Abstract:</strong> Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a "fast" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL$^2$, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose ("slow") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the "fast" RL algorithm on the current (previously unseen) MDP. We evaluate RL$^2$ experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL$^2$ is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL$^2$ on a vision-based navigation task and show that it scales up to high-dimensional problems.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1315.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1315.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RL^2 (bandits)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RL-squared (meta-learned recurrent policy) on Multi-armed Bandits</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A meta-reinforcement-learning agent implemented as a GRU-based recurrent policy whose hidden state is preserved across episodes within a trial; trained with TRPO to learn an adaptive exploration/exploitation algorithm for new bandit instances.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RL^2</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Recurrent neural network policy (GRU, 256 units) that takes (state placeholder, previous action, previous reward, termination flag) as input each timestep; hidden state preserved across episodes within a trial so activations encode the 'fast' learned learning algorithm; outer-loop optimization via TRPO with an RNN baseline and optional GAE.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Meta-learned sequential experimental design / learned exploration (meta-RL via recurrent policy).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts by integrating previous actions, observed rewards and termination flags into RNN hidden state across timesteps and episodes within a trial; hidden activations represent beliefs/strategy and determine next actions without an explicit Bayesian update — adaptation is learned end-to-end to maximize cumulative trial reward.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Multi-armed bandits (Bernoulli arms sampled p_i ~ Uniform[0,1])</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Stateless (no dynamics), stochastic Bernoulli rewards, unknown arm success probabilities, discrete finite action set (arms), fully observed rewards but unknown parameters — not partially observable in sense of POMDP but parameters unknown.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>k in {5,10,50} arms; adaptation evaluated over trials with n in {10,100,500} pulls/episodes; evaluation averaged over 1000 random bandit instances.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Cumulative reward (mean over 1000 instances) for RL^2: n=10: k=5 -> 6.7; k=10 -> 6.7; k=50 -> 6.8. n=100: k=5 -> 78.7; k=10 -> 83.5; k=50 -> 84.9. n=500: k=5 -> 401.6; k=10 -> 432.5; k=50 -> 438.9. (Units: total reward accumulated over the n pulls/episodes in each trial.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Random baseline totals (same settings): n=10 -> ~5.0; n=100 -> ~49.8-49.9; n=500 -> ~249.0-249.8. Best classical algorithms (for comparison): Gittins / UCB1 often score higher or comparable depending on setting (e.g., Gittins: n=500,k=50 -> 463.7).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>At test time RL^2 adapts within the trial length n (e.g., effective adaptation observed within n=10,100,500 pulls); training required large outer-loop samples (TRPO batch size 250,000, policy iterations up to 1000 indicated), but adaptation to new bandits occurs within the evaluated n episodes per trial.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Implicit, learned trade-off encoded in RNN hidden state that conditions action selection on past rewards/counts; no explicit UCB/Thompson formula — balance is discovered by optimizing expected trial return across MDP distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Random, Gittins index (approximate implementations), Thompson sampling (TS), Optimistic Thompson sampling (OTS), UCB1, epsilon-Greedy, Greedy.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>RL^2 achieved performance nearly matching or tied with hand-designed bandit algorithms across many finite-horizon settings; competitive with UCB1 and Gittins in many cases and outperformed naive baselines; however RL^2 underperformed best methods in the most challenging case (k=50, n=500) indicating optimization/outer-loop limitations rather than representational limits (supervised training on Gittins trajectories showed the same architecture can match Gittins performance).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Noticeable gap in the hardest setting (50 arms, 500 pulls) suggesting outer-loop RL optimization is the bottleneck; significant training resource requirements; method does not use explicit Bayesian posterior updates and relies on learned heuristics which may generalize imperfectly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning', 'publication_date_yy_mm': '2016-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1315.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1315.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RL^2 (tabular MDPs)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RL-squared (meta-learned recurrent policy) on Random Tabular MDPs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same GRU-based RL^2 agent applied to small tabular MDPs to meta-learn adaptive exploration/exploitation strategies across randomly sampled MDPs; outer-loop training via TRPO.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RL^2</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>GRU-based recurrent policy (256 units) that receives one-hot state embeddings, previous action/reward/termination and preserves hidden state across episodes of a fixed MDP; optimized with TRPO (batch size 250k, up to 10000 policy iterations).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Meta-learned adaptive exploration via recurrent policy (implicit posterior/strategy encoded in hidden state); analogous to Bayesian adaptive control but learned end-to-end.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Across episodes within a trial the policy uses accumulated transitions and rewards to alter action selection, effectively learning which states/actions to explore early vs exploit later to maximize cumulative trial reward; learned heuristics may emphasize earlier exploitation when sample budgets are small.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Random tabular MDPs (|Σ|=10 states, |A|=5 actions), Gaussian rewards and Dirichlet-sampled transitions</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Fully observable states during episodes, stochastic Gaussian rewards (unit variance), unknown transition probabilities drawn from flat Dirichlet; finite discrete state and action spaces; episodic with horizon T=10.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>State space size 10, action space size 5, episode horizon 10, trials composed of n in {10,25,50,75,100} episodes; evaluation averaged over many randomized MDPs.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Cumulative returns (mean): n=10 -> RL^2: 156.2; n=25 -> 445.7; n=50 -> 936.1; n=75 -> 1428.8; n=100 -> 1913.7. (Units: total reward summed over episodes within trial; compared against baselines in Table 2.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Random baseline: n=10 -> 100.1; PSRL: n=10 -> 138.1, PSRL and other Bayesian algorithms generally outperform random; for small n (10,25) RL^2 outperforms PSRL/OPSRL/UCRL2/BEB, but for large n (>=75) classical optimistic/Bayesian methods catch up or slightly outperform RL^2.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>RL^2 shows strong sample-efficiency for small n (e.g., n=10 and n=25) where it outperforms established methods, indicating it learns to exploit aggressively when the episode budget is small; training required large outer-loop data (TRPO settings), but at test-time adaptation occurs across the few episodes in each trial.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Learned implicit trade-off: RL^2 tends to exploit earlier when total episode budget n is small (because directly optimizing trial return encourages aggressive exploitation when full model estimation is infeasible); contrast: PSRL-like methods aim to explore more for long-run regret minimization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Random, PSRL (posterior sampling for RL), Optimistic PSRL (OPSRL), UCRL2, BEB, epsilon-Greedy, Greedy.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>RL^2 outperforms several theoretically-grounded Bayesian/optimistic RL algorithms when the number of episodes per MDP is small, demonstrating its ability to learn task-specific adaptive exploration strategies; advantage decreases or reverses as n grows, suggesting outer-loop optimization and/or asymptotics limit performance for long horizons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Performance advantage diminishes for larger n (>=75); authors attribute this to difficulty in solving the outer-loop RL optimization for very long horizons and to the need for better architectures that exploit episodic structure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning', 'publication_date_yy_mm': '2016-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1315.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1315.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RL^2 (visual navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RL-squared (meta-learned recurrent policy) on Vision-based Navigation (ViZDoom) POMDP</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GRU-based recurrent policy trained end-to-end from raw pixel observations in randomly generated mazes to meta-learn exploratory behaviors (first episode) and exploitative behaviors (subsequent episodes) using preserved hidden state across episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RL^2</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Policy uses convolutional encoder (two conv layers, 16 filters, 5x5 stride 2), concatenated with action embedding and a fully connected layer into a GRU (256 units); hidden state preserved across episodes within a maze configuration; trained with TRPO (batch size 50k, up to 5000 policy iterations) and shared baseline network.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Meta-learned active exploration and memory-based adaptation (learned mapping from episode-1 observations to improved episode-2 behavior) — an implicit adaptive experimental design to locate target quickly.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>During the first episode the agent explores the maze, stores relevant visual/positional information in RNN activations; in subsequent episodes it conditions action selection on stored information to navigate more directly to the target, reducing trajectory length.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Vision-based maze navigation (ViZDoom custom mazes)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Partially observable (first-person pixel observations), high-dimensional visual input (RGB images rescaled to 40x30), stochastic (random maze layouts/target positions across trials), sparse rewards (+1 on reaching target, -0.001 on wall hit, -0.04 per timestep), discrete action space, episodic; POMDP because the maze identity is unobserved but fixed across episodes in a trial.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Training mazes: small 5x5 grid, 2 episodes per trial, episode horizon up to 250 timesteps; evaluation includes extrapolation to large 9x9 mazes and up to 5 episodes; 1000 randomly generated configurations used for train/test sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Key metrics (best run): Average length of successful trajectories (mean ± std): Small maze: Episode 1 = 52.4 ± 1.3; Episode 2 = 39.1 ± 0.9. Large maze: Episode 1 = 180.1 ± 6.0; Episode 2 = 151.8 ± 5.9. Success rates: Small episode 1 = 99.3%, episode 2 = 99.6%; Large episode 1 = 97.1%, episode 2 = 96.7%. %Improved (trajectory length in ep2 <= ep1): Small = 91.7%, Large = 71.4%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>No-adaptation baseline (random/exploratory) not given numerically here, but first-episode performance serves as a within-trial baseline; RL^2 shows large reduction in path length between ep1 and ep2 indicating adaptation (e.g., small maze length reduction from 52.4 to 39.1).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Agent learns to exploit information across just 2 episodes per trial in the training regime; at test time it reduces trajectory length between episode 1 and 2 consistently, indicating adaptation occurs within one exploratory episode.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Learned: first episode behaviors are more exploratory to locate target (despite sparse reward), second episode behaviors exploit stored information to traverse to target more directly; exploration is driven by maximizing expected trial return during meta-training rather than hand-designed curiosity bonuses.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>No direct algorithmic baselines reported for visual navigation in this paper (comparison is mainly across random initializations and extrapolation tests), but environment implemented with ViZDoom referenced.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>RL^2 learns to use episodic memory (RNN hidden state) to reduce navigation path lengths in subsequent episodes, showing that meta-learned policies can perform adaptive experimental-design-like exploration in partially observable visual tasks; extrapolates somewhat to larger mazes but with degraded improvement rates.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Agent sometimes forgets target location and re-explores in episode 2 (visualized failure cases); performance degrades on larger mazes and across random initializations; training is sensitive to initialization and outer-loop optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning', 'publication_date_yy_mm': '2016-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1315.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1315.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Thompson sampling (TS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Thompson Sampling (posterior sampling) for bandits</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Bayesian adaptive sampling method that at each time samples arm parameters from the posterior and selects the arm with the sampled highest mean; used here as a baseline for bandit experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Thompson sampling (TS)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>At each timestep sample means from posterior (Beta posteriors for Bernoulli arms), pick arm with highest sampled mean; an approximate Bayesian adaptive experimental design strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Posterior sampling / Thompson sampling</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Maintains posterior over arm success probabilities, updates posterior after each pull, samples from posterior each time to choose arms (stochastic exploration guided by posterior uncertainty).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Multi-armed bandits (Bernoulli arms)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Stateless, stochastic Bernoulli rewards with unknown parameters sampled from Uniform[0,1].</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>k in {5,10,50}; n in {10,100,500} pulls; evaluation averaged over 1000 instances.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Reported cumulative rewards (mean over 1000 instances): Examples: n=10,k=5 -> 5.7; n=100,k=5 -> 74.7; n=500,k=5 -> 402.0; see Table 1 for full matrix.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Random baseline: e.g., n=100,k=5 -> 49.9; shows TS substantially improves over random.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>TS efficiently balances exploration and exploitation and performs well across horizons; empirical performance shown across finite horizons in table.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Intrinsic via posterior sampling: exploration occurs when posterior uncertainty is high, exploitation when posterior concentrates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against Gittins index approximations, UCB1, OTS, epsilon-Greedy, Greedy, RL^2 in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>TS is a strong baseline; RL^2 matches or exceeds TS in many finite-horizon settings, but TS remains competitive especially at larger horizons in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>TS performance depends on posterior model correctness and can be outperformed by problem-specific optimal algorithms (Gittins) in some settings; in high-arm/high-horizon settings RL^2 and Gittins may outperform TS depending on approximation choices.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning', 'publication_date_yy_mm': '2016-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1315.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1315.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UCB1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UCB1 (Upper Confidence Bound algorithm)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A frequentist adaptive exploration algorithm for multi-armed bandits that selects the arm maximizing an upper confidence bound on the estimated mean; used as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>UCB1</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Maintains empirical mean and count per arm and selects arm with largest ucb_i(t)=hat_mu_i + c * sqrt((2 log t)/T_i(t-1)); hyperparameter c tuned per setting.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>UCB-style optimism in the face of uncertainty (confidence-bound driven exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Selects actions that maximize an optimism-adjusted estimate of expected reward, where exploration decays as arms are sampled more.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Multi-armed bandits (Bernoulli arms)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Stateless, stochastic Bernoulli rewards with unknown parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>k in {5,10,50}; n in {10,100,500} pulls.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Reported cumulative rewards (mean over 1000 instances): e.g., n=10,k=5 -> 6.7; n=100,k=50 -> 84.3; n=500,k=50 -> 457.6. See Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Random baseline substantially worse (see Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Designed for logarithmic regret in the long run; empirically performs well across finite horizons in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Explicit via confidence bounds; exploration decreases as counts increase.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Benchmarked against Thompson sampling, Gittins, OTS, epsilon-Greedy, Greedy, RL^2.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>UCB1 is one of the top hand-designed baselines and ties or outperforms RL^2 in some finite-horizon settings; RL^2 is competitive with UCB1 in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Performance sensitive to horizon and hyperparameter tuning; may be suboptimal for finite-horizon settings optimized for Bayesian priors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning', 'publication_date_yy_mm': '2016-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1315.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1315.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PSRL / OPSRL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Posterior Sampling Reinforcement Learning (and optimistic variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A posterior-sampling-based algorithm for MDPs (generalization of Thompson sampling) that samples an MDP from the posterior at episode start and follows its optimal policy for that episode; OPSRL adds optimism.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PSRL / OPSRL</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Maintain posterior over MDP models (transitions/rewards), sample a model at the start of each episode and act optimally for that sampled MDP (OPSRL applies optimistic adjustments).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Posterior sampling (Thompson-like) for MDPs / optimistic posterior variants.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Updates posterior after each episode; exploration arises from sampling variability in early episodes, exploitation as posterior concentrates.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Random tabular MDPs (|Σ|=10, |A|=5) with Gaussian rewards and Dirichlet transitions</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Fully observable finite MDPs, stochastic transitions and rewards, unknown dynamics and reward means drawn from priors.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>State space 10, action space 5, episode horizon T=10, trials with n in {10,25,50,75,100}.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Reported cumulative returns (means): PSRL: n=10 -> 138.1; n=25 -> 408.8; n=50 -> 904.4; n=75 -> 1417.1; n=100 -> 1939.5. OPSRL often slightly higher (e.g., n=50 -> 930.7).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Random baseline: e.g., n=10 -> 100.1; shows substantial improvement from adaptive posterior sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>PSRL is sample-efficient in these tabular settings with modest episode budgets; RL^2 outperforms PSRL at small n in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Exploration via sampling of MDPs from posterior; optimism in OPSRL increases exploration on uncertain regions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to RL^2, UCRL2, BEB, epsilon-Greedy, Greedy in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>PSRL and OPSRL perform very well across increasing n; RL^2 outperforms them for small n but PSRL/OPSRL overtake for larger n, indicating differing inductive biases (PSRL is more conservative and aimed at long-run regret minimization).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Performance depends on correctness of priors and computational tractability of solving sampled MDPs; OPSRL requires optimistic adjustments that may be costly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning', 'publication_date_yy_mm': '2016-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1315.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1315.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gittins index</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gittins index (Bayes-optimal index policy for bandits)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A theoretically Bayes-optimal index policy for discounted infinite-horizon multi-armed bandits; in practice approximations are used and tested as a baseline in the bandit experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Gittins index (approx.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Computes an index per arm based on Bayesian posterior that yields Bayes-optimal choice under certain discounted, infinite-horizon assumptions; practical implementations use approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayes-optimal index policy (index-based adaptive selection)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Computes per-arm indices from posterior information and selects arm with highest index; indices encode optimal exploration/exploitation trade-offs under the assumed infinite-horizon discounted objective.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Multi-armed bandits (Bernoulli arms with known prior)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Stateless, stochastic Bernoulli rewards, unknown arm parameters; experiments supply the true prior.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>k in {5,10,50}, various horizons including finite n in experiments though Gittins is derived for infinite-horizon discounted case; approximations used to apply it.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Often the top baseline in many bandit settings; example from table: n=500,k=50 -> 463.7 (mean cumulative reward) which is higher than RL^2's 438.9 in that setting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Not applicable (it is an adaptive method); compared to random baseline which is much lower.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Designed for discounted infinite-horizon optimality; in finite-horizon experiments it is often very effective, sometimes outperforming other methods.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Implicitly optimal under the model assumptions via the computed indices.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to RL^2, TS, UCB1, OTS, epsilon-Greedy, Greedy in bandit experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Gittins approximations often achieve the best cumulative rewards in many finite-horizon bandit experiments reported; RL^2 can approach or match these in many settings but can fall behind in the hardest settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Exact computation intractable in general; requires approximations; derived for discounted infinite-horizon setting so finite-horizon performance depends on approximation choices.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning', 'publication_date_yy_mm': '2016-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. <em>(Rating: 2)</em></li>
                <li>Using confidence bounds for exploitation-exploration trade-offs. <em>(Rating: 2)</em></li>
                <li>Bandit processes and dynamic allocation indices. <em>(Rating: 2)</em></li>
                <li>A bayesian framework for reinforcement learning. <em>(Rating: 2)</em></li>
                <li>Near-bayesian exploration in polynomial time. <em>(Rating: 2)</em></li>
                <li>Near-optimal regret bounds for reinforcement learning. <em>(Rating: 2)</em></li>
                <li>Vizdoom: A doom-based ai research platform for visual reinforcement learning. <em>(Rating: 1)</em></li>
                <li>Dual control theory. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1315",
    "paper_id": "paper-954b01151ff13aef416d27adc60cd9a076753b1a",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "RL^2 (bandits)",
            "name_full": "RL-squared (meta-learned recurrent policy) on Multi-armed Bandits",
            "brief_description": "A meta-reinforcement-learning agent implemented as a GRU-based recurrent policy whose hidden state is preserved across episodes within a trial; trained with TRPO to learn an adaptive exploration/exploitation algorithm for new bandit instances.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "RL^2",
            "agent_description": "Recurrent neural network policy (GRU, 256 units) that takes (state placeholder, previous action, previous reward, termination flag) as input each timestep; hidden state preserved across episodes within a trial so activations encode the 'fast' learned learning algorithm; outer-loop optimization via TRPO with an RNN baseline and optional GAE.",
            "adaptive_design_method": "Meta-learned sequential experimental design / learned exploration (meta-RL via recurrent policy).",
            "adaptation_strategy_description": "Adapts by integrating previous actions, observed rewards and termination flags into RNN hidden state across timesteps and episodes within a trial; hidden activations represent beliefs/strategy and determine next actions without an explicit Bayesian update — adaptation is learned end-to-end to maximize cumulative trial reward.",
            "environment_name": "Multi-armed bandits (Bernoulli arms sampled p_i ~ Uniform[0,1])",
            "environment_characteristics": "Stateless (no dynamics), stochastic Bernoulli rewards, unknown arm success probabilities, discrete finite action set (arms), fully observed rewards but unknown parameters — not partially observable in sense of POMDP but parameters unknown.",
            "environment_complexity": "k in {5,10,50} arms; adaptation evaluated over trials with n in {10,100,500} pulls/episodes; evaluation averaged over 1000 random bandit instances.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Cumulative reward (mean over 1000 instances) for RL^2: n=10: k=5 -&gt; 6.7; k=10 -&gt; 6.7; k=50 -&gt; 6.8. n=100: k=5 -&gt; 78.7; k=10 -&gt; 83.5; k=50 -&gt; 84.9. n=500: k=5 -&gt; 401.6; k=10 -&gt; 432.5; k=50 -&gt; 438.9. (Units: total reward accumulated over the n pulls/episodes in each trial.)",
            "performance_without_adaptation": "Random baseline totals (same settings): n=10 -&gt; ~5.0; n=100 -&gt; ~49.8-49.9; n=500 -&gt; ~249.0-249.8. Best classical algorithms (for comparison): Gittins / UCB1 often score higher or comparable depending on setting (e.g., Gittins: n=500,k=50 -&gt; 463.7).",
            "sample_efficiency": "At test time RL^2 adapts within the trial length n (e.g., effective adaptation observed within n=10,100,500 pulls); training required large outer-loop samples (TRPO batch size 250,000, policy iterations up to 1000 indicated), but adaptation to new bandits occurs within the evaluated n episodes per trial.",
            "exploration_exploitation_tradeoff": "Implicit, learned trade-off encoded in RNN hidden state that conditions action selection on past rewards/counts; no explicit UCB/Thompson formula — balance is discovered by optimizing expected trial return across MDP distribution.",
            "comparison_methods": "Random, Gittins index (approximate implementations), Thompson sampling (TS), Optimistic Thompson sampling (OTS), UCB1, epsilon-Greedy, Greedy.",
            "key_results": "RL^2 achieved performance nearly matching or tied with hand-designed bandit algorithms across many finite-horizon settings; competitive with UCB1 and Gittins in many cases and outperformed naive baselines; however RL^2 underperformed best methods in the most challenging case (k=50, n=500) indicating optimization/outer-loop limitations rather than representational limits (supervised training on Gittins trajectories showed the same architecture can match Gittins performance).",
            "limitations_or_failures": "Noticeable gap in the hardest setting (50 arms, 500 pulls) suggesting outer-loop RL optimization is the bottleneck; significant training resource requirements; method does not use explicit Bayesian posterior updates and relies on learned heuristics which may generalize imperfectly.",
            "uuid": "e1315.0",
            "source_info": {
                "paper_title": "RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning",
                "publication_date_yy_mm": "2016-11"
            }
        },
        {
            "name_short": "RL^2 (tabular MDPs)",
            "name_full": "RL-squared (meta-learned recurrent policy) on Random Tabular MDPs",
            "brief_description": "Same GRU-based RL^2 agent applied to small tabular MDPs to meta-learn adaptive exploration/exploitation strategies across randomly sampled MDPs; outer-loop training via TRPO.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "RL^2",
            "agent_description": "GRU-based recurrent policy (256 units) that receives one-hot state embeddings, previous action/reward/termination and preserves hidden state across episodes of a fixed MDP; optimized with TRPO (batch size 250k, up to 10000 policy iterations).",
            "adaptive_design_method": "Meta-learned adaptive exploration via recurrent policy (implicit posterior/strategy encoded in hidden state); analogous to Bayesian adaptive control but learned end-to-end.",
            "adaptation_strategy_description": "Across episodes within a trial the policy uses accumulated transitions and rewards to alter action selection, effectively learning which states/actions to explore early vs exploit later to maximize cumulative trial reward; learned heuristics may emphasize earlier exploitation when sample budgets are small.",
            "environment_name": "Random tabular MDPs (|Σ|=10 states, |A|=5 actions), Gaussian rewards and Dirichlet-sampled transitions",
            "environment_characteristics": "Fully observable states during episodes, stochastic Gaussian rewards (unit variance), unknown transition probabilities drawn from flat Dirichlet; finite discrete state and action spaces; episodic with horizon T=10.",
            "environment_complexity": "State space size 10, action space size 5, episode horizon 10, trials composed of n in {10,25,50,75,100} episodes; evaluation averaged over many randomized MDPs.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Cumulative returns (mean): n=10 -&gt; RL^2: 156.2; n=25 -&gt; 445.7; n=50 -&gt; 936.1; n=75 -&gt; 1428.8; n=100 -&gt; 1913.7. (Units: total reward summed over episodes within trial; compared against baselines in Table 2.)",
            "performance_without_adaptation": "Random baseline: n=10 -&gt; 100.1; PSRL: n=10 -&gt; 138.1, PSRL and other Bayesian algorithms generally outperform random; for small n (10,25) RL^2 outperforms PSRL/OPSRL/UCRL2/BEB, but for large n (&gt;=75) classical optimistic/Bayesian methods catch up or slightly outperform RL^2.",
            "sample_efficiency": "RL^2 shows strong sample-efficiency for small n (e.g., n=10 and n=25) where it outperforms established methods, indicating it learns to exploit aggressively when the episode budget is small; training required large outer-loop data (TRPO settings), but at test-time adaptation occurs across the few episodes in each trial.",
            "exploration_exploitation_tradeoff": "Learned implicit trade-off: RL^2 tends to exploit earlier when total episode budget n is small (because directly optimizing trial return encourages aggressive exploitation when full model estimation is infeasible); contrast: PSRL-like methods aim to explore more for long-run regret minimization.",
            "comparison_methods": "Random, PSRL (posterior sampling for RL), Optimistic PSRL (OPSRL), UCRL2, BEB, epsilon-Greedy, Greedy.",
            "key_results": "RL^2 outperforms several theoretically-grounded Bayesian/optimistic RL algorithms when the number of episodes per MDP is small, demonstrating its ability to learn task-specific adaptive exploration strategies; advantage decreases or reverses as n grows, suggesting outer-loop optimization and/or asymptotics limit performance for long horizons.",
            "limitations_or_failures": "Performance advantage diminishes for larger n (&gt;=75); authors attribute this to difficulty in solving the outer-loop RL optimization for very long horizons and to the need for better architectures that exploit episodic structure.",
            "uuid": "e1315.1",
            "source_info": {
                "paper_title": "RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning",
                "publication_date_yy_mm": "2016-11"
            }
        },
        {
            "name_short": "RL^2 (visual navigation)",
            "name_full": "RL-squared (meta-learned recurrent policy) on Vision-based Navigation (ViZDoom) POMDP",
            "brief_description": "GRU-based recurrent policy trained end-to-end from raw pixel observations in randomly generated mazes to meta-learn exploratory behaviors (first episode) and exploitative behaviors (subsequent episodes) using preserved hidden state across episodes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "RL^2",
            "agent_description": "Policy uses convolutional encoder (two conv layers, 16 filters, 5x5 stride 2), concatenated with action embedding and a fully connected layer into a GRU (256 units); hidden state preserved across episodes within a maze configuration; trained with TRPO (batch size 50k, up to 5000 policy iterations) and shared baseline network.",
            "adaptive_design_method": "Meta-learned active exploration and memory-based adaptation (learned mapping from episode-1 observations to improved episode-2 behavior) — an implicit adaptive experimental design to locate target quickly.",
            "adaptation_strategy_description": "During the first episode the agent explores the maze, stores relevant visual/positional information in RNN activations; in subsequent episodes it conditions action selection on stored information to navigate more directly to the target, reducing trajectory length.",
            "environment_name": "Vision-based maze navigation (ViZDoom custom mazes)",
            "environment_characteristics": "Partially observable (first-person pixel observations), high-dimensional visual input (RGB images rescaled to 40x30), stochastic (random maze layouts/target positions across trials), sparse rewards (+1 on reaching target, -0.001 on wall hit, -0.04 per timestep), discrete action space, episodic; POMDP because the maze identity is unobserved but fixed across episodes in a trial.",
            "environment_complexity": "Training mazes: small 5x5 grid, 2 episodes per trial, episode horizon up to 250 timesteps; evaluation includes extrapolation to large 9x9 mazes and up to 5 episodes; 1000 randomly generated configurations used for train/test sampling.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Key metrics (best run): Average length of successful trajectories (mean ± std): Small maze: Episode 1 = 52.4 ± 1.3; Episode 2 = 39.1 ± 0.9. Large maze: Episode 1 = 180.1 ± 6.0; Episode 2 = 151.8 ± 5.9. Success rates: Small episode 1 = 99.3%, episode 2 = 99.6%; Large episode 1 = 97.1%, episode 2 = 96.7%. %Improved (trajectory length in ep2 &lt;= ep1): Small = 91.7%, Large = 71.4%.",
            "performance_without_adaptation": "No-adaptation baseline (random/exploratory) not given numerically here, but first-episode performance serves as a within-trial baseline; RL^2 shows large reduction in path length between ep1 and ep2 indicating adaptation (e.g., small maze length reduction from 52.4 to 39.1).",
            "sample_efficiency": "Agent learns to exploit information across just 2 episodes per trial in the training regime; at test time it reduces trajectory length between episode 1 and 2 consistently, indicating adaptation occurs within one exploratory episode.",
            "exploration_exploitation_tradeoff": "Learned: first episode behaviors are more exploratory to locate target (despite sparse reward), second episode behaviors exploit stored information to traverse to target more directly; exploration is driven by maximizing expected trial return during meta-training rather than hand-designed curiosity bonuses.",
            "comparison_methods": "No direct algorithmic baselines reported for visual navigation in this paper (comparison is mainly across random initializations and extrapolation tests), but environment implemented with ViZDoom referenced.",
            "key_results": "RL^2 learns to use episodic memory (RNN hidden state) to reduce navigation path lengths in subsequent episodes, showing that meta-learned policies can perform adaptive experimental-design-like exploration in partially observable visual tasks; extrapolates somewhat to larger mazes but with degraded improvement rates.",
            "limitations_or_failures": "Agent sometimes forgets target location and re-explores in episode 2 (visualized failure cases); performance degrades on larger mazes and across random initializations; training is sensitive to initialization and outer-loop optimization.",
            "uuid": "e1315.2",
            "source_info": {
                "paper_title": "RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning",
                "publication_date_yy_mm": "2016-11"
            }
        },
        {
            "name_short": "Thompson sampling (TS)",
            "name_full": "Thompson Sampling (posterior sampling) for bandits",
            "brief_description": "A Bayesian adaptive sampling method that at each time samples arm parameters from the posterior and selects the arm with the sampled highest mean; used here as a baseline for bandit experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Thompson sampling (TS)",
            "agent_description": "At each timestep sample means from posterior (Beta posteriors for Bernoulli arms), pick arm with highest sampled mean; an approximate Bayesian adaptive experimental design strategy.",
            "adaptive_design_method": "Posterior sampling / Thompson sampling",
            "adaptation_strategy_description": "Maintains posterior over arm success probabilities, updates posterior after each pull, samples from posterior each time to choose arms (stochastic exploration guided by posterior uncertainty).",
            "environment_name": "Multi-armed bandits (Bernoulli arms)",
            "environment_characteristics": "Stateless, stochastic Bernoulli rewards with unknown parameters sampled from Uniform[0,1].",
            "environment_complexity": "k in {5,10,50}; n in {10,100,500} pulls; evaluation averaged over 1000 instances.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Reported cumulative rewards (mean over 1000 instances): Examples: n=10,k=5 -&gt; 5.7; n=100,k=5 -&gt; 74.7; n=500,k=5 -&gt; 402.0; see Table 1 for full matrix.",
            "performance_without_adaptation": "Random baseline: e.g., n=100,k=5 -&gt; 49.9; shows TS substantially improves over random.",
            "sample_efficiency": "TS efficiently balances exploration and exploitation and performs well across horizons; empirical performance shown across finite horizons in table.",
            "exploration_exploitation_tradeoff": "Intrinsic via posterior sampling: exploration occurs when posterior uncertainty is high, exploitation when posterior concentrates.",
            "comparison_methods": "Compared against Gittins index approximations, UCB1, OTS, epsilon-Greedy, Greedy, RL^2 in this paper.",
            "key_results": "TS is a strong baseline; RL^2 matches or exceeds TS in many finite-horizon settings, but TS remains competitive especially at larger horizons in some settings.",
            "limitations_or_failures": "TS performance depends on posterior model correctness and can be outperformed by problem-specific optimal algorithms (Gittins) in some settings; in high-arm/high-horizon settings RL^2 and Gittins may outperform TS depending on approximation choices.",
            "uuid": "e1315.3",
            "source_info": {
                "paper_title": "RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning",
                "publication_date_yy_mm": "2016-11"
            }
        },
        {
            "name_short": "UCB1",
            "name_full": "UCB1 (Upper Confidence Bound algorithm)",
            "brief_description": "A frequentist adaptive exploration algorithm for multi-armed bandits that selects the arm maximizing an upper confidence bound on the estimated mean; used as a baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "UCB1",
            "agent_description": "Maintains empirical mean and count per arm and selects arm with largest ucb_i(t)=hat_mu_i + c * sqrt((2 log t)/T_i(t-1)); hyperparameter c tuned per setting.",
            "adaptive_design_method": "UCB-style optimism in the face of uncertainty (confidence-bound driven exploration)",
            "adaptation_strategy_description": "Selects actions that maximize an optimism-adjusted estimate of expected reward, where exploration decays as arms are sampled more.",
            "environment_name": "Multi-armed bandits (Bernoulli arms)",
            "environment_characteristics": "Stateless, stochastic Bernoulli rewards with unknown parameters.",
            "environment_complexity": "k in {5,10,50}; n in {10,100,500} pulls.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Reported cumulative rewards (mean over 1000 instances): e.g., n=10,k=5 -&gt; 6.7; n=100,k=50 -&gt; 84.3; n=500,k=50 -&gt; 457.6. See Table 1.",
            "performance_without_adaptation": "Random baseline substantially worse (see Table 1).",
            "sample_efficiency": "Designed for logarithmic regret in the long run; empirically performs well across finite horizons in experiments.",
            "exploration_exploitation_tradeoff": "Explicit via confidence bounds; exploration decreases as counts increase.",
            "comparison_methods": "Benchmarked against Thompson sampling, Gittins, OTS, epsilon-Greedy, Greedy, RL^2.",
            "key_results": "UCB1 is one of the top hand-designed baselines and ties or outperforms RL^2 in some finite-horizon settings; RL^2 is competitive with UCB1 in many cases.",
            "limitations_or_failures": "Performance sensitive to horizon and hyperparameter tuning; may be suboptimal for finite-horizon settings optimized for Bayesian priors.",
            "uuid": "e1315.4",
            "source_info": {
                "paper_title": "RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning",
                "publication_date_yy_mm": "2016-11"
            }
        },
        {
            "name_short": "PSRL / OPSRL",
            "name_full": "Posterior Sampling Reinforcement Learning (and optimistic variant)",
            "brief_description": "A posterior-sampling-based algorithm for MDPs (generalization of Thompson sampling) that samples an MDP from the posterior at episode start and follows its optimal policy for that episode; OPSRL adds optimism.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "PSRL / OPSRL",
            "agent_description": "Maintain posterior over MDP models (transitions/rewards), sample a model at the start of each episode and act optimally for that sampled MDP (OPSRL applies optimistic adjustments).",
            "adaptive_design_method": "Posterior sampling (Thompson-like) for MDPs / optimistic posterior variants.",
            "adaptation_strategy_description": "Updates posterior after each episode; exploration arises from sampling variability in early episodes, exploitation as posterior concentrates.",
            "environment_name": "Random tabular MDPs (|Σ|=10, |A|=5) with Gaussian rewards and Dirichlet transitions",
            "environment_characteristics": "Fully observable finite MDPs, stochastic transitions and rewards, unknown dynamics and reward means drawn from priors.",
            "environment_complexity": "State space 10, action space 5, episode horizon T=10, trials with n in {10,25,50,75,100}.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Reported cumulative returns (means): PSRL: n=10 -&gt; 138.1; n=25 -&gt; 408.8; n=50 -&gt; 904.4; n=75 -&gt; 1417.1; n=100 -&gt; 1939.5. OPSRL often slightly higher (e.g., n=50 -&gt; 930.7).",
            "performance_without_adaptation": "Random baseline: e.g., n=10 -&gt; 100.1; shows substantial improvement from adaptive posterior sampling.",
            "sample_efficiency": "PSRL is sample-efficient in these tabular settings with modest episode budgets; RL^2 outperforms PSRL at small n in experiments.",
            "exploration_exploitation_tradeoff": "Exploration via sampling of MDPs from posterior; optimism in OPSRL increases exploration on uncertain regions.",
            "comparison_methods": "Compared to RL^2, UCRL2, BEB, epsilon-Greedy, Greedy in Table 2.",
            "key_results": "PSRL and OPSRL perform very well across increasing n; RL^2 outperforms them for small n but PSRL/OPSRL overtake for larger n, indicating differing inductive biases (PSRL is more conservative and aimed at long-run regret minimization).",
            "limitations_or_failures": "Performance depends on correctness of priors and computational tractability of solving sampled MDPs; OPSRL requires optimistic adjustments that may be costly.",
            "uuid": "e1315.5",
            "source_info": {
                "paper_title": "RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning",
                "publication_date_yy_mm": "2016-11"
            }
        },
        {
            "name_short": "Gittins index",
            "name_full": "Gittins index (Bayes-optimal index policy for bandits)",
            "brief_description": "A theoretically Bayes-optimal index policy for discounted infinite-horizon multi-armed bandits; in practice approximations are used and tested as a baseline in the bandit experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Gittins index (approx.)",
            "agent_description": "Computes an index per arm based on Bayesian posterior that yields Bayes-optimal choice under certain discounted, infinite-horizon assumptions; practical implementations use approximations.",
            "adaptive_design_method": "Bayes-optimal index policy (index-based adaptive selection)",
            "adaptation_strategy_description": "Computes per-arm indices from posterior information and selects arm with highest index; indices encode optimal exploration/exploitation trade-offs under the assumed infinite-horizon discounted objective.",
            "environment_name": "Multi-armed bandits (Bernoulli arms with known prior)",
            "environment_characteristics": "Stateless, stochastic Bernoulli rewards, unknown arm parameters; experiments supply the true prior.",
            "environment_complexity": "k in {5,10,50}, various horizons including finite n in experiments though Gittins is derived for infinite-horizon discounted case; approximations used to apply it.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Often the top baseline in many bandit settings; example from table: n=500,k=50 -&gt; 463.7 (mean cumulative reward) which is higher than RL^2's 438.9 in that setting.",
            "performance_without_adaptation": "Not applicable (it is an adaptive method); compared to random baseline which is much lower.",
            "sample_efficiency": "Designed for discounted infinite-horizon optimality; in finite-horizon experiments it is often very effective, sometimes outperforming other methods.",
            "exploration_exploitation_tradeoff": "Implicitly optimal under the model assumptions via the computed indices.",
            "comparison_methods": "Compared to RL^2, TS, UCB1, OTS, epsilon-Greedy, Greedy in bandit experiments.",
            "key_results": "Gittins approximations often achieve the best cumulative rewards in many finite-horizon bandit experiments reported; RL^2 can approach or match these in many settings but can fall behind in the hardest settings.",
            "limitations_or_failures": "Exact computation intractable in general; requires approximations; derived for discounted infinite-horizon setting so finite-horizon performance depends on approximation choices.",
            "uuid": "e1315.6",
            "source_info": {
                "paper_title": "RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning",
                "publication_date_yy_mm": "2016-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples.",
            "rating": 2
        },
        {
            "paper_title": "Using confidence bounds for exploitation-exploration trade-offs.",
            "rating": 2
        },
        {
            "paper_title": "Bandit processes and dynamic allocation indices.",
            "rating": 2
        },
        {
            "paper_title": "A bayesian framework for reinforcement learning.",
            "rating": 2
        },
        {
            "paper_title": "Near-bayesian exploration in polynomial time.",
            "rating": 2
        },
        {
            "paper_title": "Near-optimal regret bounds for reinforcement learning.",
            "rating": 2
        },
        {
            "paper_title": "Vizdoom: A doom-based ai research platform for visual reinforcement learning.",
            "rating": 1
        },
        {
            "paper_title": "Dual control theory.",
            "rating": 1
        }
    ],
    "cost": 0.01913225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>RL²: FAST REINFORCEMENT LEARNING VIA SLOW REINFORCEMENT LEARNING</h1>
<p>Yan Duan ${ }^{\dagger \ddagger}$, John Schulman ${ }^{\dagger \ddagger}$, Xi Chen ${ }^{\dagger \ddagger}$, Peter L. Bartlett ${ }^{\dagger}$, Ilya Sutskever ${ }^{\ddagger}$, Pieter Abbeel ${ }^{\dagger \ddagger}$<br>${ }^{\dagger}$ UC Berkeley, Department of Electrical Engineering and Computer Science<br>${ }^{\ddagger}$ OpenAI<br>{rocky, joschu, peter}@openai.com, peter@berkeley.edu, {ilyasu, pieter}@openai.com</p>
<h4>Abstract</h4>
<p>Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a "fast" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, $\mathrm{RL}^{2}$, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose ("slow") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the "fast" RL algorithm on the current (previously unseen) MDP. We evaluate $\mathrm{RL}^{2}$ experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-armed bandit problems and finite MDPs. After $\mathrm{RL}^{2}$ is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the largescale side, we test $\mathrm{RL}^{2}$ on a vision-based navigation task and show that it scales up to high-dimensional problems.</p>
<h2>1 INTRODUCTION</h2>
<p>In recent years, deep reinforcement learning has achieved many impressive results, including playing Atari games from raw pixels (Guo et al., 2014; Mnih et al., 2015; Schulman et al., 2015), and acquiring advanced manipulation and locomotion skills (Levine et al., 2016; Lillicrap et al., 2015; Watter et al., 2015; Heess et al., 2015; Schulman et al., 2015; 2016). However, many of the successes come at the expense of high sample complexity. For example, the state-of-the-art Atari results require tens of thousands of episodes of experience (Mnih et al., 2015) per game. To master a game, one would need to spend nearly 40 days playing it with no rest. In contrast, humans and animals are capable of learning a new task in a very small number of trials. Continuing the previous example, the human player in Mnih et al. (2015) only needed 2 hours of experience before mastering a game. We argue that the reason for this sharp contrast is largely due to the lack of a good prior, which results in these deep RL agents needing to rebuild their knowledge about the world from scratch.</p>
<p>Although Bayesian reinforcement learning provides a solid framework for incorporating prior knowledge into the learning process (Strens, 2000; Ghavamzadeh et al., 2015; Kolter \&amp; Ng, 2009), exact computation of the Bayesian update is intractable in all but the simplest cases. Thus, practical reinforcement learning algorithms often incorporate a mixture of Bayesian and domain-specific ideas to bring down sample complexity and computational burden. Notable examples include guided policy search with unknown dynamics (Levine \&amp; Abbeel, 2014) and PILCO (Deisenroth \&amp; Rasmussen, 2011). These methods can learn a task using a few minutes to a few hours of real experience, compared to days or even weeks required by previous methods (Schulman et al., 2015; 2016; Lillicrap et al., 2015). However, these methods tend to make assumptions about the environment (e.g., instrumentation for access to the state at learning time), or become computationally intractable in high-dimensional settings (Wahlström et al., 2015).</p>
<p>Rather than hand-designing domain-specific reinforcement learning algorithms, we take a different approach in this paper: we view the learning process of the agent itself as an objective, which can be optimized using standard reinforcement learning algorithms. The objective is averaged across all possible MDPs according to a specific distribution, which reflects the prior that we would like to distill into the agent. We structure the agent as a recurrent neural network, which receives past rewards, actions, and termination flags as inputs in addition to the normally received observations. Furthermore, its internal state is preserved across episodes, so that it has the capacity to perform learning in its own hidden activations. The learned agent thus also acts as the learning algorithm, and can adapt to the task at hand when deployed.</p>
<p>We evaluate this approach on two sets of classical problems, multi-armed bandits and tabular MDPs. These problems have been extensively studied, and there exist algorithms that achieve asymptotically optimal performance. We demonstrate that our method, named RL^{2}, can achieve performance comparable with these theoretically justified algorithms. Next, we evaluate RL^{2} on a vision-based navigation task implemented using the ViZDoom environment <em>(Kempka et al., 2016)</em>, showing that RL^{2} can also scale to high-dimensional problems.</p>
<h2>2 Method</h2>
<h3>2.1 Preliminaries</h3>
<p>We define a discrete-time finite-horizon discounted Markov decision process (MDP) by a tuple $M=$ $(\mathcal{S},\mathcal{A},\mathcal{P},r,\rho_{0},\gamma,T)$, in which $\mathcal{S}$ is a state set, $\mathcal{A}$ an action set, $\mathcal{P}:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow\mathbb{R}<em _max="\max">{+}$ a transition probability distribution, $r:\mathcal{S}\times\mathcal{A}\rightarrow[-R</em>},R_{\max}]$ a bounded reward function, $\rho_{0}:\mathcal{S}\rightarrow\mathbb{R<em _theta="\theta">{+}$ an initial state distribution, $\gamma\in[0,1]$ a discount factor, and $T$ the horizon. In policy search methods, we typically optimize a stochastic policy $\pi</em>}:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R<em _theta="\theta">{+}$ parametrized by $\theta$. The objective is to maximize its expected discounted return, $\eta(\pi</em>})=\mathbb{E<em t="0">{\tau}[\sum</em>)$.}^{T}\gamma^{t}r(s_{t},a_{t})]$, where $\tau=(s_{0},a_{0},\ldots)$ denotes the whole trajectory, $s_{0}\sim\rho_{0}(s_{0})$, $a_{t}\sim\pi_{\theta}(a_{t}|s_{t})$, and $s_{t+1}\sim\mathcal{P}(s_{t+1}|s_{t},a_{t</p>
<h3>2.2 Formulation</h3>
<p>We now describe our formulation, which casts learning an RL algorithm as a reinforcement learning problem, and hence the name RL^{2}. We assume knowledge of a set of MDPs, denoted by $\mathcal{M}$, and a distribution over them: $\rho_{\mathcal{M}}:\mathcal{M}\rightarrow\mathbb{R}_{+}$. We only need to sample from this distribution. We use $n$ to denote the total number of episodes allowed to spend with a specific MDP. We define a trial to be such a series of episodes of interaction with a fixed MDP.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Procedure of agent-environment interaction</p>
<p>This process of interaction between an agent and the environment is illustrated in Figure 1. Here, each trial happens to consist of two episodes, hence $n=2$. For each trial, a separate MDP is drawn from $\rho_{\mathcal{M}}$, and for each episode, a fresh $s_{0}$ is drawn from the initial state distribution specific to the corresponding MDP. Upon receiving an action $a_{t}$ produced by the agent, the environment computes reward $r_{t}$, steps forward, and computes the next state $s_{t+1}$. If the episode has terminated, it sets termination flag $d_{t}$ to 1, which otherwise defaults to 0. Together, the next state $s_{t+1}$, action</p>
<p>$a_{t}$, reward $r_{t}$, and termination flag $d_{t}$, are concatenated to form the input to the policy, which, conditioned on the hidden state $h_{t+1}$, generates the next hidden state $h_{t+2}$ and action $a_{t+1}$. At the end of an episode, the hidden state of the policy is preserved to the next episode, but not preserved between trials.</p>
<p>The objective under this formulation is to maximize the expected total discounted reward accumulated during a single trial rather than a single episode. Maximizing this objective is equivalent to minimizing the cumulative pseudo-regret (Bubeck &amp; Cesa-Bianchi, 2012). Since the underlying MDP changes across trials, as long as different strategies are required for different MDPs, the agent must act differently according to its belief over which MDP it is currently in. Hence, the agent is forced to integrate all the information it has received, including past actions, rewards, and termination flags, and adapt its strategy continually. Hence, we have set up an end-to-end optimization process, where the agent is encouraged to learn a “fast” reinforcement learning algorithm.</p>
<p>For clarity of exposition, we have defined the “inner” problem (of which the agent sees $n$ each trial) to be an MDP rather than a POMDP. However, the method can also be applied in the partially-observed setting without any conceptual changes. In the partially observed setting, the agent is faced with a sequence of POMDPs, and it receives an observation $o_{t}$ instead of state $s_{t}$ at time $t$. The visual navigation experiment in Section 3.3 is actually an instance of the this POMDP setting.</p>
<h3>2.3 Policy Representation</h3>
<p>We represent the policy as a general recurrent neural network. Each timestep, it receives the tuple $(s,a,r,d)$ as input, which is embedded using a function $\phi(s,a,r,d)$ and provided as input to an RNN. To alleviate the difficulty of training RNNs due to vanishing and exploding gradients (Bengio et al., 1994), we use Gated Recurrent Units (GRUs) (Cho et al., 2014) which have been demonstrated to have good empirical performance (Chung et al., 2014; Józefowicz et al., 2015). The output of the GRU is fed to a fully connected layer followed by a softmax function, which forms the distribution over actions.</p>
<p>We have also experimented with alternative architectures which explicitly reset part of the hidden state each episode of the sampled MDP, but we did not find any improvement over the simple architecture described above.</p>
<h3>2.4 Policy Optimization</h3>
<p>After formulating the task as a reinforcement learning problem, we can readily use standard off-the-shelf RL algorithms to optimize the policy. We use a first-order implementation of Trust Region Policy Optimization (TRPO) (Schulman et al., 2015), because of its excellent empirical performance, and because it does not require excessive hyperparameter tuning. For more details, we refer the reader to the original paper. To reduce variance in the stochastic gradient estimation, we use a baseline which is also represented as an RNN using GRUs as building blocks. We optionally apply Generalized Advantage Estimation (GAE) (Schulman et al., 2016) to further reduce the variance.</p>
<h2>3 Evaluation</h2>
<p>We designed experiments to answer the following questions:</p>
<ul>
<li>Can RL^{2} learn algorithms that achieve good performance on MDP classes with special structure, relative to existing algorithms tailored to this structure that have been proposed in the literature?</li>
<li>Can RL^{2} scale to high-dimensional tasks?</li>
</ul>
<p>For the first question, we evaluate RL^{2} on two sets of tasks, multi-armed bandits (MAB) and tabular MDPs. These problems have been studied extensively in the reinforcement learning literature, and this body of work includes algorithms with guarantees of asymptotic optimality. We demonstrate that our approach achieves comparable performance to these theoretically justified algorithms.</p>
<p>To make sure that the inputs have a consistent dimension, we use placeholder values for the initial input to the policy.</p>
<p>For the second question, we evaluate RL ${ }^{2}$ on a vision-based navigation task. Our experiments show that the learned policy makes effective use of the learned visual information and also short-term information acquired from previous episodes.</p>
<h1>3.1 Multi-armed bandits</h1>
<p>Multi-armed bandit problems are a subset of MDPs where the agent's environment is stateless. Specifically, there are $k$ arms (actions), and at every time step, the agent pulls one of the arms, say $i$, and receives a reward drawn from an unknown distribution: our experiments take each arm to be a Bernoulli distribution with parameter $p_{i}$. The goal is to maximize the total reward obtained over a fixed number of time steps. The key challenge is balancing exploration and exploitation"exploring" each arm enough times to estimate its distribution $\left(p_{i}\right)$, but eventually switching over to "exploitation" of the best arm. Despite the simplicity of multi-arm bandit problems, their study has led to a rich theory and a collection of algorithms with optimality guarantees.
Using RL ${ }^{2}$, we can train an RNN policy to solve bandit problems by training it on a given distribution $\rho_{\mathcal{M}}$. If the learning is successful, the resulting policy should be able to perform competitively with the theoretically optimal algorithms. We randomly generated bandit problems by sampling each parameter $p_{i}$ from the uniform distribution on $[0,1]$. After training the RNN policy with RL ${ }^{2}$, we compared it against the following strategies:</p>
<ul>
<li>Random: this is a baseline strategy, where the agent pulls a random arm each time.</li>
<li>Gittins index (Gittins, 1979): this method gives the Bayes optimal solution in the discounted infinite-horizon case, by computing an index separately for each arm, and taking the arm with the largest index. While this work shows it is sufficient to independently compute an index for each arm (hence avoiding combinatorial explosion with the number of arms), it doesn't show how to tractably compute these individual indices exactly. We follow the practical approximations described in Gittins et al. (2011), Chakravorty \&amp; Mahajan (2013), and Whittle (1982), and choose the best-performing approximation for each setup.</li>
<li>UCB1 (Auer, 2002): this method estimates an upper-confidence bound, and pulls the arm with the largest value of $\operatorname{ucb}<em i="i">{i}(t)=\hat{\mu}</em>}(t-1)+c \sqrt{\frac{2 \log t}{T_{i}(t-1)}}$, where $\hat{\mu<em i="i">{i}(t-1)$ is the estimated mean parameter for the $i$ th arm, $T</em>(1,1)$ prior.}(t-1)$ is the number of times the $i$ th arm has been pulled, and $c$ is a tunable hyperparameter (Audibert \&amp; Munos, 2011). We initialize the statistics with exactly one success and one failure, which corresponds to a $\operatorname{Beta</li>
<li>Thompson sampling (TS) (Thompson, 1933): this is a simple method which, at each time step, samples a list of arm means from the posterior distribution, and choose the best arm according to this sample. It has been demonstrated to compare favorably to UCB1 empirically (Chapelle \&amp; Li, 2011). We also experiment with an optimistic variant (OTS) (May et al., 2012), which samples $N$ times from the posterior, and takes the one with the highest probability.</li>
<li>$\epsilon$-Greedy: in this strategy, the agent chooses the arm with the best empirical mean with probability $1-\epsilon$, and chooses a random arm with probability $\epsilon$. We use the same initialization as UCB1.</li>
<li>Greedy: this is a special case of $\epsilon$-Greedy with $\epsilon=0$.</li>
</ul>
<p>The Bayesian methods, Gittins index and Thompson sampling, take advantage of the distribution $\rho_{\mathcal{M}}$; and we provide these methods with the true distribution. For each method with hyperparameters, we maximize the score with a separate grid search for each of the experimental settings. The hyperparameters used for TRPO are shown in the appendix.</p>
<p>The results are summarized in Table 1. Learning curves for various settings are shown in Figure 2. We observe that our approach achieves performance that is almost as good as the the reference methods, which were (human) designed specifically to perform well on multi-armed bandit problems. It is worth noting that the published algorithms are mostly designed to minimize asymptotic regret (rather than finite horizon regret), hence there tends to be a little bit of room to outperform them in the finite horizon settings.</p>
<p>Table 1: MAB Results. Each grid cell records the total reward averaged over 1000 different instances of the bandit problem. We consider $k \in{5,10,50}$ bandits and $n \in{10,100,500}$ episodes of interaction. We highlight the best-performing algorithms in each setup according to the computed mean, and we also highlight the other algorithms in that row whose performance is not significantly different from the best one (determined by a one-sided $t$-test with $p=0.05$ ).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Setup</th>
<th style="text-align: center;">Random</th>
<th style="text-align: center;">Gittins</th>
<th style="text-align: center;">TS</th>
<th style="text-align: center;">OTS</th>
<th style="text-align: center;">UCB1</th>
<th style="text-align: center;">$\epsilon$-Greedy</th>
<th style="text-align: center;">Greedy</th>
<th style="text-align: center;">RL $^{2}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$n=10, k=5$</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">$\mathbf{6 . 6}$</td>
<td style="text-align: center;">5.7</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">$\mathbf{6 . 7}$</td>
<td style="text-align: center;">$\mathbf{6 . 6}$</td>
<td style="text-align: center;">$\mathbf{6 . 6}$</td>
<td style="text-align: center;">$\mathbf{6 . 7}$</td>
</tr>
<tr>
<td style="text-align: center;">$n=10, k=10$</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">$\mathbf{6 . 6}$</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">$\mathbf{6 . 7}$</td>
<td style="text-align: center;">$\mathbf{6 . 6}$</td>
<td style="text-align: center;">$\mathbf{6 . 6}$</td>
<td style="text-align: center;">$\mathbf{6 . 7}$</td>
</tr>
<tr>
<td style="text-align: center;">$n=10, k=50$</td>
<td style="text-align: center;">5.1</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">5.2</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">$\mathbf{6 . 6}$</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">$\mathbf{6 . 8}$</td>
</tr>
<tr>
<td style="text-align: center;">$n=100, k=5$</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">$\mathbf{7 8 . 3}$</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">$\mathbf{7 7 . 9}$</td>
<td style="text-align: center;">$\mathbf{7 8 . 0}$</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">$\mathbf{7 8 . 7}$</td>
</tr>
<tr>
<td style="text-align: center;">$n=100, k=10$</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">$\mathbf{8 2 . 8}$</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: center;">82.4</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">77.1</td>
<td style="text-align: center;">$\mathbf{8 3 . 5}$</td>
</tr>
<tr>
<td style="text-align: center;">$n=100, k=50$</td>
<td style="text-align: center;">49.8</td>
<td style="text-align: center;">$\mathbf{8 5 . 2}$</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">67.7</td>
<td style="text-align: center;">84.3</td>
<td style="text-align: center;">78.3</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">$\mathbf{8 4 . 9}$</td>
</tr>
<tr>
<td style="text-align: center;">$n=500, k=5$</td>
<td style="text-align: center;">249.8</td>
<td style="text-align: center;">$\mathbf{4 0 5 . 8}$</td>
<td style="text-align: center;">$\mathbf{4 0 2 . 0}$</td>
<td style="text-align: center;">$\mathbf{4 0 6 . 7}$</td>
<td style="text-align: center;">$\mathbf{4 0 5 . 8}$</td>
<td style="text-align: center;">388.2</td>
<td style="text-align: center;">380.6</td>
<td style="text-align: center;">$\mathbf{4 0 1 . 6}$</td>
</tr>
<tr>
<td style="text-align: center;">$n=500, k=10$</td>
<td style="text-align: center;">249.0</td>
<td style="text-align: center;">$\mathbf{4 3 7 . 8}$</td>
<td style="text-align: center;">429.5</td>
<td style="text-align: center;">$\mathbf{4 3 8 . 9}$</td>
<td style="text-align: center;">$\mathbf{4 3 7 . 1}$</td>
<td style="text-align: center;">408.0</td>
<td style="text-align: center;">395.0</td>
<td style="text-align: center;">432.5</td>
</tr>
<tr>
<td style="text-align: center;">$n=500, k=50$</td>
<td style="text-align: center;">249.6</td>
<td style="text-align: center;">$\mathbf{4 6 3 . 7}$</td>
<td style="text-align: center;">427.2</td>
<td style="text-align: center;">437.6</td>
<td style="text-align: center;">457.6</td>
<td style="text-align: center;">413.6</td>
<td style="text-align: center;">402.8</td>
<td style="text-align: center;">438.9</td>
</tr>
</tbody>
</table>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: RL $^{2}$ learning curves for multi-armed bandits. Performance is normalized such that Gittins index scores 1 , and random policy scores 0 .</p>
<p>We observe that there is a noticeable gap between Gittins index and $\mathrm{RL}^{2}$ in the most challenging scenario, with 50 arms and 500 episodes. This raises the question whether better architectures or better (slow) RL algorithms should be explored. To determine the bottleneck, we trained the same policy architecture using supervised learning, using the trajectories generated by the Gittins index approach as training data. We found that the learned policy, when executed in test domains, achieved the same level of performance as the Gittins index approach, suggesting that there is room for improvement by using better RL algorithms.</p>
<h1>3.2 TAbular MDPs</h1>
<p>The bandit problem provides a natural and simple setting to investigate whether the policy learns to trade off between exploration and exploitation. However, the problem itself involves no sequential decision making, and does not fully characterize the challenges in solving MDPs. Hence, we perform further experiments using randomly generated tabular MDPs, where there is a finite number of possible states and actions-small enough that the transition probability distribution can be explicitly given as a table. We compare our approach with the following methods:</p>
<ul>
<li>Random: the agent chooses an action uniformly at random for each time step;</li>
<li>PSRL (Strens, 2000; Osband et al., 2013): this is a direct generalization of Thompson sampling to MDPs, where at the beginning of each episode, we sample an MDP from the posterior distribution, and take actions according to the optimal policy for the entire episode. Similarly, we include an optimistic variant (OPSRL), which has also been explored in Osband \&amp; Van Roy (2016).</li>
<li>
<p>BEB (Kolter \&amp; Ng, 2009): this is a model-based optimistic algorithm that adds an exploration bonus to (thus far) infrequently visited states and actions.</p>
</li>
<li>
<p>UCRL2 (Jaksch et al., 2010): this algorithm computes, at each iteration, the optimal policy against an optimistic MDP under the current belief, using an extended value iteration procedure.</p>
</li>
<li>$\epsilon$-Greedy: this algorithm takes actions optimal against the MAP estimate according to the current posterior, which is updated once per episode.</li>
<li>Greedy: a special case of $\epsilon$-Greedy with $\epsilon=0$.</li>
</ul>
<p>Table 2: Random MDP Results</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Setup</th>
<th style="text-align: center;">Random</th>
<th style="text-align: center;">PSRL</th>
<th style="text-align: center;">OPSRL</th>
<th style="text-align: center;">UCRL2</th>
<th style="text-align: center;">BEB</th>
<th style="text-align: center;">$\epsilon$-Greedy</th>
<th style="text-align: center;">Greedy</th>
<th style="text-align: center;">RL $^{2}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$n=10$</td>
<td style="text-align: center;">100.1</td>
<td style="text-align: center;">138.1</td>
<td style="text-align: center;">144.1</td>
<td style="text-align: center;">146.6</td>
<td style="text-align: center;">150.2</td>
<td style="text-align: center;">132.8</td>
<td style="text-align: center;">134.8</td>
<td style="text-align: center;">$\mathbf{1 5 6 . 2}$</td>
</tr>
<tr>
<td style="text-align: center;">$n=25$</td>
<td style="text-align: center;">250.2</td>
<td style="text-align: center;">408.8</td>
<td style="text-align: center;">425.2</td>
<td style="text-align: center;">424.1</td>
<td style="text-align: center;">427.8</td>
<td style="text-align: center;">377.3</td>
<td style="text-align: center;">368.8</td>
<td style="text-align: center;">$\mathbf{4 4 5 . 7}$</td>
</tr>
<tr>
<td style="text-align: center;">$n=50$</td>
<td style="text-align: center;">499.7</td>
<td style="text-align: center;">904.4</td>
<td style="text-align: center;">$\mathbf{9 3 0 . 7}$</td>
<td style="text-align: center;">918.9</td>
<td style="text-align: center;">917.8</td>
<td style="text-align: center;">823.3</td>
<td style="text-align: center;">769.3</td>
<td style="text-align: center;">$\mathbf{9 3 6 . 1}$</td>
</tr>
<tr>
<td style="text-align: center;">$n=75$</td>
<td style="text-align: center;">749.9</td>
<td style="text-align: center;">1417.1</td>
<td style="text-align: center;">$\mathbf{1 4 4 9 . 2}$</td>
<td style="text-align: center;">1427.6</td>
<td style="text-align: center;">1422.6</td>
<td style="text-align: center;">1293.9</td>
<td style="text-align: center;">1172.9</td>
<td style="text-align: center;">1428.8</td>
</tr>
<tr>
<td style="text-align: center;">$n=100$</td>
<td style="text-align: center;">999.4</td>
<td style="text-align: center;">1939.5</td>
<td style="text-align: center;">$\mathbf{1 9 7 3 . 9}$</td>
<td style="text-align: center;">1942.1</td>
<td style="text-align: center;">1935.1</td>
<td style="text-align: center;">1778.2</td>
<td style="text-align: center;">1578.5</td>
<td style="text-align: center;">1913.7</td>
</tr>
</tbody>
</table>
<p>The distribution over MDPs is constructed with $|\mathcal{S}|=10,|\mathcal{A}|=5$. The rewards follow a Gaussian distribution with unit variance, and the mean parameters are sampled independently from $\operatorname{Normal}(1,1)$. The transitions are sampled from a flat Dirichlet distribution. This construction matches the commonly used prior in Bayesian RL methods. We set the horizon for each episode to be $T=10$, and an episode always starts on the first state.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: RL $^{2}$ learning curves for tabular MDPs. Performance is normalized such that OPSRL scores 1 , and random policy scores 0 .</p>
<p>The results are summarized in Table 2, and the learning curves are shown in Figure 3. We follow the same evaluation procedure as in the bandit case. We experiment with $n \in{10,25,50,75,100}$. For fewer episodes, our approach surprisingly outperforms existing methods by a large margin. The advantage is reversed as $n$ increases, suggesting that the reinforcement learning problem in the outer loop becomes more challenging to solve. We think that the advantage for small $n$ comes from the need for more aggressive exploitation: since there are 140 degrees of freedom to estimate in order to characterize the MDP, and by the 10th episode, we will not have enough samples to form a good estimate of the entire dynamics. By directly optimizing the RNN in this setting, our approach should be able to cope with this shortage of samples, and decides to exploit sooner compared to the reference algorithms.</p>
<h1>3.3 VisUAl NAVIGATION</h1>
<p>The previous two tasks both only involve very low-dimensional state spaces. To evaluate the feasibility of scaling up $\mathrm{RL}^{2}$, we further experiment with a challenging vision-based task, where the</p>
<p>agent is asked to navigate a randomly generated maze to find a randomly placed target. The agent receives a $+1$ reward when it reaches the target, $-0.001$ when it hits the wall, and $-0.04$ per time step to encourage it to reach targets faster. It can interact with the maze for multiple episodes, during which the maze structure and target position are held fixed. The optimal strategy is to explore the maze efficiently during the first episode, and after locating the target, act optimally against the current maze and target based on the collected information. An illustration of the task is given in Figure 4.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Visual navigation. The target block is shown in red, and occupies an entire grid in the maze layout.</p>
<p>Visual navigation alone is a challenging task for reinforcement learning. The agent only receives very sparse rewards during training, and does not have the primitives for efficient exploration at the beginning of training. It also needs to make efficient use of memory to decide how it should explore the space, without forgetting about where it has already explored. Previously, Oh et al. (2016) have studied similar vision-based navigation tasks in Minecraft. However, they use higher-level actions for efficient navigation. Similar high-level actions in our task would each require around 5 low-level actions combined in the right way. In contrast, our RL ${ }^{2}$ agent needs to learn these higher-level actions from scratch.</p>
<p>We use a simple training setup, where we use small mazes of size $5 \times 5$, with 2 episodes of interaction, each with horizon up to 250 . Here the size of the maze is measured by the number of grid cells along each wall in a discrete representation of the maze. During each trial, we sample 1 out of 1000 randomly generated configurations of map layout and target positions. During testing, we evaluate on 1000 separately generated configurations. In addition, we also study its extrapolation behavior along two axes, by (1) testing on large mazes of size $9 \times 9$ (see Figure 4c) and (2) running the agent for up to 5 episodes in both small and large mazes. For the large maze, we also increase the horizon per episode by 4 x due to the increased size of the maze.</p>
<p>Table 3: Results for visual navigation. These metrics are computed using the best run among all runs shown in Figure 5. In 3c, we measure the proportion of mazes where the trajectory length in the second episode does not exceed the trajectory length in the first episode.
(a) Average length of successful trajectories
(b) \%Success
(c) \%Improved</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Episode</th>
<th style="text-align: center;">Small</th>
<th style="text-align: center;">Large</th>
<th style="text-align: center;">Episode</th>
<th style="text-align: center;">Small</th>
<th style="text-align: center;">Large</th>
<th style="text-align: center;">Small</th>
<th style="text-align: center;">Large</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$52.4 \pm 1.3$</td>
<td style="text-align: center;">$180.1 \pm 6.0$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$99.3 \%$</td>
<td style="text-align: center;">$97.1 \%$</td>
<td style="text-align: center;">$91.7 \%$</td>
<td style="text-align: center;">$71.4 \%$</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$39.1 \pm 0.9$</td>
<td style="text-align: center;">$151.8 \pm 5.9$</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$99.6 \%$</td>
<td style="text-align: center;">$96.7 \%$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$42.6 \pm 1.0$</td>
<td style="text-align: center;">$169.3 \pm 6.3$</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$99.7 \%$</td>
<td style="text-align: center;">$95.8 \%$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$43.5 \pm 1.1$</td>
<td style="text-align: center;">$162.3 \pm 6.4$</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$99.4 \%$</td>
<td style="text-align: center;">$95.6 \%$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">$43.9 \pm 1.1$</td>
<td style="text-align: center;">$169.3 \pm 6.5$</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">$99.6 \%$</td>
<td style="text-align: center;">$96.1 \%$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: $\mathrm{RL}^{2}$ learning curves for visual navigation. Each curve shows a different random initialization of the RNN weights. Performance varies greatly across different initializations.</p>
<p>The results are summarized in Table 3, and the learning curves are shown in Figure 5. We observe that there is a significant reduction in trajectory lengths between the first two episodes in both the smaller and larger mazes, suggesting that the agent has learned how to use information from past episodes. It also achieves reasonable extrapolation behavior in further episodes by maintaining its performance, although there is a small drop in the rate of success in the larger mazes. We also observe that on larger mazes, the ratio of improved trajectories is lower, likely because the agent has not learned how to act optimally in the larger mazes.</p>
<p>Still, even on the small mazes, the agent does not learn to perfectly reuse prior information. An illustration of the agent's behavior is shown in Figure 6. The intended behavior, which occurs most frequently, as shown in 6 a and 6 b , is that the agent should remember the target's location, and utilize it to act optimally in the second episode. However, occasionally the agent forgets about where the target was, and continues to explore in the second episode, as shown in 6 c and 6 d . We believe that better reinforcement learning techniques used as the outer-loop algorithm will improve these results in the future.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Visualization of the agent's behavior. In each scenario, the agent starts at the center of the blue block, and the goal is to reach anywhere in the red block.</p>
<h1>4 Related Work</h1>
<p>The concept of using prior experience to speed up reinforcement learning algorithms has been explored in the past in various forms. Earlier studies have investigated automatic tuning of hyperparameters, such as learning rate and temperature (Ishii et al., 2002; Schweighofer \&amp; Doya, 2003), as a form of meta-learning. Wilson et al. (2007) use hierarchical Bayesian methods to maintain a posterior over possible models of dynamics, and apply optimistic Thompson sampling according to the posterior. Many works in hierarchical reinforcement learning propose to extract reusable skills from previous tasks to speed up exploration in new tasks (Singh, 1992; Perkins et al., 1999). We refer the reader to Taylor \&amp; Stone (2009) for a more thorough survey on the multi-task and transfer learning aspects.</p>
<p>More recently, Fu et al. (2015) propose a model-based approach on top of iLQG with unknown dynamics (Levine \&amp; Abbeel, 2014), which uses samples collected from previous tasks to build a neural network prior for the dynamics, and can perform one-shot learning on new, but related tasks thanks to reduced sample complexity. There has been a growing interest in using deep neural networks for multi-task learning and transfer learning (Parisotto et al., 2015; Rusu et al., 2015; 2016a; Devin et al., 2016; Rusu et al., 2016b).</p>
<p>In the broader context of machine learning, there has been a lot of interest in one-shot learning for object classification (Vilalta \&amp; Drissi, 2002; Fei-Fei et al., 2006; Larochelle et al., 2008; Lake et al., 2011; Koch, 2015). Our work draws inspiration from a particular line of work (Younger et al., 2001; Santoro et al., 2016; Vinyals et al., 2016), which formulates meta-learning as an optimization problem, and can thus be optimized end-to-end via gradient descent. While these work applies to the supervised learning setting, our work applies in the more general reinforcement learning setting. Although the reinforcement learning setting is more challenging, the resulting behavior is far richer: our agent must not only learn to exploit existing information, but also learn to explore, a problem that is usually not a factor in supervised learning. Another line of work (Hochreiter et al., 2001; Younger et al., 2001; Andrychowicz et al., 2016; Li \&amp; Malik, 2016) studies meta-learning over the optimization process. There, the meta-learner makes explicit updates to a parametrized model. In comparison, we do not use a directly parametrized policy; instead, the recurrent neural network agent acts as the meta-learner and the resulting policy simultaneously.</p>
<p>Our formulation essentially constructs a partially observable MDP (POMDP) which is solved in the outer loop, where the underlying MDP is unobserved by the agent. This reduction of an unknown MDP to a POMDP can be traced back to dual control theory (Feldbaum, 1960), where "dual" refers to the fact that one is controlling both the state and the state estimate. Feldbaum pointed out that the solution can in principle be computed with dynamic programming, but doing so is usually impractical. POMDPs with such structure have also been studied under the name "mixed observability MDPs" (Ong et al., 2010). However, the method proposed there suffers from the usual challenges of solving POMDPs in high dimensions.</p>
<h1>5 DISCUSSION</h1>
<p>This paper suggests a different approach for designing better reinforcement learning algorithms: instead of acting as the designers ourselves, learn the algorithm end-to-end using standard reinforcement learning techniques. That is, the "fast" RL algorithm is a computation whose state is stored in the RNN activations, and the RNN's weights are learned by a general-purpose "slow" reinforcement learning algorithm. Our method, $\mathrm{RL}^{2}$, has demonstrated competence comparable with theoretically optimal algorithms in small-scale settings. We have further shown its potential to scale to high-dimensional tasks.</p>
<p>In the experiments, we have identified opportunities to improve upon $\mathrm{RL}^{2}$ : the outer-loop reinforcement learning algorithm was shown to be an immediate bottleneck, and we believe that for settings with extremely long horizons, better architecture may also be required for the policy. Although we have used generic methods and architectures for the outer-loop algorithm and the policy, doing this also ignores the underlying episodic structure. We expect algorithms and policy architectures that exploit the problem structure to significantly boost the performance.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>We would like to thank our colleagues at Berkeley and OpenAI for insightful discussions. This research was funded in part by ONR through a PECASE award. Yan Duan was also supported by a Berkeley AI Research lab Fellowship and a Huawei Fellowship. Xi Chen was also supported by a Berkeley AI Research lab Fellowship. We gratefully acknowledge the support of the NSF through grant IIS-1619362 and of the ARC through a Laureate Fellowship (FL110100281) and through the ARC Centre of Excellence for Mathematical and Statistical Frontiers.</p>
<h2>REFERENCES</h2>
<p>Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, and Nando de Freitas. Learning to learn by gradient descent by gradient descent. arXiv preprint</p>
<p>arXiv:1606.04474, 2016.
Jean-Yves Audibert and Rémi Munos. Introduction to bandits: Algorithms and theory. ICML Tutorial on bandits, 2011.</p>
<p>Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Research, 3(Nov):397-422, 2002.</p>
<p>Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE transactions on neural networks, 5(2):157-166, 1994.</p>
<p>Sébastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multiarmed bandit problems. arXiv preprint arXiv:1204.5721, 2012.</p>
<p>Jhelum Chakravorty and Aditya Mahajan. Multi-armed bandits, gittins index, and its calculation. Methods and Applications of Statistics in Clinical Trials: Planning, Analysis, and Inferential Methods, 2:416-435, 2013.</p>
<p>Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. In Advances in neural information processing systems, pp. 2249-2257, 2011.</p>
<p>Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259, 2014.</p>
<p>Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.</p>
<p>Marc Deisenroth and Carl E Rasmussen. Pilco: A model-based and data-efficient approach to policy search. In Proceedings of the 28th International Conference on machine learning (ICML-11), pp. $465-472,2011$.</p>
<p>Coline Devin, Abhishek Gupta, Trevor Darrell, Pieter Abbeel, and Sergey Levine. Learning modular neural network policies for multi-task and multi-robot transfer. arXiv preprint arXiv:1609.07088, 2016.</p>
<p>Li Fei-Fei, Rob Fergus, and Pietro Perona. One-shot learning of object categories. IEEE transactions on pattern analysis and machine intelligence, 28(4):594-611, 2006.</p>
<p>AA Feldbaum. Dual control theory. i. Avtomatika i Telemekhanika, 21(9):1240-1249, 1960.
Justin Fu, Sergey Levine, and Pieter Abbeel. One-shot learning of manipulation skills with online dynamics adaptation and neural network priors. arXiv preprint arXiv:1509.06841, 2015.</p>
<p>Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, Aviv Tamar, et al. Bayesian reinforcement learning: a survey. World Scientific, 2015.</p>
<p>John Gittins, Kevin Glazebrook, and Richard Weber. Multi-armed bandit allocation indices. John Wiley \&amp; Sons, 2011.</p>
<p>John C Gittins. Bandit processes and dynamic allocation indices. Journal of the Royal Statistical Society. Series B (Methodological), pp. 148-177, 1979.</p>
<p>Xiaoxiao Guo, Satinder Singh, Honglak Lee, Richard L Lewis, and Xiaoshi Wang. Deep learning for real-time atari game play using offline monte-carlo tree search planning. In Advances in neural information processing systems, pp. 3338-3346, 2014.</p>
<p>Nicolas Heess, Gregory Wayne, David Silver, Tim Lillicrap, Tom Erez, and Yuval Tassa. Learning continuous control policies by stochastic value gradients. In Advances in Neural Information Processing Systems, pp. 2944-2952, 2015.</p>
<p>Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent. In International Conference on Artificial Neural Networks, pp. 87-94. Springer, 2001.</p>
<p>Shin Ishii, Wako Yoshida, and Junichiro Yoshimoto. Control of exploitation-exploration metaparameter in reinforcement learning. Neural networks, 15(4):665-687, 2002.</p>
<p>Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning. Journal of Machine Learning Research, 11(Apr):1563-1600, 2010.</p>
<p>Rafal Józefowicz, Wojciech Zaremba, and Ilya Sutskever. An empirical exploration of recurrent network architectures. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pp. 2342-2350, 2015. URL http: //jmlr.org/proceedings/papers/v37/jozefowicz15.html.</p>
<p>Michał Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech Jaśkowski. Vizdoom: A doom-based ai research platform for visual reinforcement learning. arXiv preprint arXiv:1605.02097, 2016.</p>
<p>Gregory Koch. Siamese neural networks for one-shot image recognition. PhD thesis, University of Toronto, 2015.</p>
<p>J Zico Kolter and Andrew Y Ng. Near-bayesian exploration in polynomial time. In Proceedings of the 26th Annual International Conference on Machine Learning, pp. 513-520. ACM, 2009.</p>
<p>Brenden M Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua B Tenenbaum. One shot learning of simple visual concepts. In Proceedings of the 33rd Annual Conference of the Cognitive Science Society, volume 172, pp. 2, 2011.</p>
<p>Hugo Larochelle, Dumitru Erhan, and Yoshua Bengio. Zero-data learning of new tasks. In AAAI, volume 1, pp. 3, 2008.</p>
<p>Sergey Levine and Pieter Abbeel. Learning neural network policies with guided policy search under unknown dynamics. In Advances in Neural Information Processing Systems, pp. 1071-1079, 2014.</p>
<p>Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. Journal of Machine Learning Research, 17(39):1-40, 2016.</p>
<p>Ke Li and Jitendra Malik. Learning to optimize. arXiv preprint arXiv:1606.01885, 2016.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.</p>
<p>Benedict C May, Nathan Korda, Anthony Lee, and David S Leslie. Optimistic bayesian sampling in contextual-bandit problems. Journal of Machine Learning Research, 13(Jun):2069-2106, 2012.</p>
<p>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.</p>
<p>Junhyuk Oh, Valliappa Chockalingam, Satinder Singh, and Honglak Lee. Control of memory, active perception, and action in minecraft. arXiv preprint arXiv:1605.09128, 2016.</p>
<p>Sylvie CW Ong, Shao Wei Png, David Hsu, and Wee Sun Lee. Planning under uncertainty for robotic tasks with mixed observability. The International Journal of Robotics Research, 29(8): $1053-1068,2010$.</p>
<p>Ian Osband and Benjamin Van Roy. Why is posterior sampling better than optimism for reinforcement learning. arXiv preprint arXiv:1607.00215, 2016.</p>
<p>Ian Osband, Dan Russo, and Benjamin Van Roy. (more) efficient reinforcement learning via posterior sampling. In Advances in Neural Information Processing Systems, pp. 3003-3011, 2013.</p>
<p>Emilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. Actor-mimic: Deep multitask and transfer reinforcement learning. arXiv preprint arXiv:1511.06342, 2015.</p>
<p>Theodore J Perkins, Doina Precup, et al. Using options for knowledge transfer in reinforcement learning. University of Massachusetts, Amherst, MA, USA, Tech. Rep, 1999.</p>
<p>Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy distillation. arXiv preprint arXiv:1511.06295, 2015.</p>
<p>Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016a.</p>
<p>Andrei A Rusu, Matej Vecerik, Thomas Rothörl, Nicolas Heess, Razvan Pascanu, and Raia Hadsell. Sim-to-real robot learning from pixels with progressive nets. arXiv preprint arXiv:1610.04286, 2016b.</p>
<p>Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Oneshot learning with memory-augmented neural networks. arXiv preprint arXiv:1605.06065, 2016.</p>
<p>John Schulman, Sergey Levine, Philipp Moritz, Michael I Jordan, and Pieter Abbeel. Trust region policy optimization. CoRR, abs/1502.05477, 2015.</p>
<p>John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. In International Conference on Learning Representations (ICLR2016), 2016.</p>
<p>Nicolas Schweighofer and Kenji Doya. Meta-learning in reinforcement learning. Neural Networks, 16(1):5-9, 2003.</p>
<p>Satinder Pal Singh. Transfer of learning by composing solutions of elemental sequential tasks. Machine Learning, 8(3-4):323-339, 1992.</p>
<p>Malcolm Strens. A bayesian framework for reinforcement learning. In ICML, pp. 943-950, 2000.
Matthew E Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey. Journal of Machine Learning Research, 10(Jul):1633-1685, 2009.</p>
<p>William R Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4):285-294, 1933.</p>
<p>Ricardo Vilalta and Youssef Drissi. A perspective view and survey of meta-learning. Artificial Intelligence Review, 18(2):77-95, 2002.</p>
<p>Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Matching networks for one shot learning. arXiv preprint arXiv:1606.04080, 2016.</p>
<p>Niklas Wahlström, Thomas B Schön, and Marc Peter Deisenroth. From pixels to torques: Policy learning with deep dynamical models. arXiv preprint arXiv:1502.02251, 2015.</p>
<p>Manuel Watter, Jost Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control: A locally linear latent dynamics model for control from raw images. In Advances in Neural Information Processing Systems, pp. 2746-2754, 2015.</p>
<p>Peter Whittle. Optimization over time. John Wiley \&amp; Sons, Inc., 1982.
Aaron Wilson, Alan Fern, Soumya Ray, and Prasad Tadepalli. Multi-task reinforcement learning: a hierarchical bayesian approach. In Proceedings of the 24th international conference on Machine learning, pp. 1015-1022. ACM, 2007.</p>
<p>A Steven Younger, Sepp Hochreiter, and Peter R Conwell. Meta-learning with backpropagation. In Neural Networks, 2001. Proceedings. IJCNN'01. International Joint Conference on, volume 3. IEEE, 2001.</p>
<h1>APPENDIX</h1>
<h2>A DETAILED EXPERIMENT SETUP</h2>
<p>Common to all experiments: as mentioned in Section 2.2, we use placeholder values when necessary. For example, at $t=0$ there is no previous action, reward, or termination flag. Since all of our experiments use discrete actions, we use the embedding of the action 0 as a placeholder for actions, and 0 for both the rewards and termination flags. To form the input to the GRU, we use the values for the rewards and termination flags as-is, and embed the states and actions as described separately below for each experiments. These values are then concatenated together to form the joint embedding.</p>
<p>For the neural network architecture, We use rectified linear units throughout the experiments as the hidden activation, and we apply weight normalization without data-dependent initialization (Salimans \&amp; Kingma, 2016) to all weight matrices. The hidden-to-hidden weight matrix uses an orthogonal initialization (Saxe et al., 2013), and all other weight matrices use Xavier initialization (Glorot \&amp; Bengio, 2010). We initialize all bias vectors to 0 . Unless otherwise mentioned, the policy and the baseline uses separate neural networks with the same architecture until the final layer, where the number of outputs differ.</p>
<p>All experiments are implemented using TensorFlow (Abadi et al., 2016) and rllab (Duan et al., 2016). We use the implementations of classic algorithms provided by the TabulaRL package (Osband, 2016).</p>
<h2>A. 1 Multi-armed bandits</h2>
<p>The parameters for TRPO are shown in Table 1. Since the environment is stateless, we use a constant embedding 0 as a placeholder in place of the states, and a one-hot embedding for the actions.</p>
<p>Table 1: Hyperparameters for TRPO: multi-armed bandits</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Discount</th>
<th style="text-align: left;">0.99</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GAE $\lambda$</td>
<td style="text-align: left;">0.3</td>
</tr>
<tr>
<td style="text-align: left;">Policy Iters</td>
<td style="text-align: left;">Up to 1000</td>
</tr>
<tr>
<td style="text-align: left;">#GRU Units</td>
<td style="text-align: left;">256</td>
</tr>
<tr>
<td style="text-align: left;">Mean KL</td>
<td style="text-align: left;">0.01</td>
</tr>
<tr>
<td style="text-align: left;">Batch size</td>
<td style="text-align: left;">250000</td>
</tr>
</tbody>
</table>
<h2>A. 2 TAbular MDPs</h2>
<p>The parameters for TRPO are shown in Table 2. We use a one-hot embedding for the states and actions separately, which are then concatenated together.</p>
<p>Table 2: Hyperparameters for TRPO: tabular MDPs</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Discount</th>
<th style="text-align: left;">0.99</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GAE $\lambda$</td>
<td style="text-align: left;">0.3</td>
</tr>
<tr>
<td style="text-align: left;">Policy Iters</td>
<td style="text-align: left;">Up to 10000</td>
</tr>
<tr>
<td style="text-align: left;">#GRU Units</td>
<td style="text-align: left;">256</td>
</tr>
<tr>
<td style="text-align: left;">Mean KL</td>
<td style="text-align: left;">0.01</td>
</tr>
<tr>
<td style="text-align: left;">Batch size</td>
<td style="text-align: left;">250000</td>
</tr>
</tbody>
</table>
<h2>A. 3 VISUAL NAVIGATION</h2>
<p>The parameters for TRPO are shown in Table 3. For this task, we use a neural network to form the joint embedding. We rescale the images to have width 40 and height 30 with RGB channels preserved, and we recenter the RGB values to lie within range $[-1,1]$. Then, this preprocessed</p>
<p>image is passed through 2 convolution layers, each with 16 filters of size $5 \times 5$ and stride 2 . The action is first embedded into a 256 -dimensional vector where the embedding is learned, and then concatenated with the flattened output of the final convolution layer. The joint vector is then fed to a fully connected layer with 256 hidden units.</p>
<p>Unlike previous experiments, we let the policy and the baseline share the same neural network. We found this to improve the stability of training baselines and also the end performance of the policy, possibly due to regularization effects and better learned features imposed by weight sharing. Similar weight-sharing techniques have also been explored in (Mnih et al., 2016).</p>
<p>Table 3: Hyperparameters for TRPO: visual navigation</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Discount</th>
<th style="text-align: left;">0.99</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GAE $\lambda$</td>
<td style="text-align: left;">0.99</td>
</tr>
<tr>
<td style="text-align: left;">Policy Iters</td>
<td style="text-align: left;">Up to 5000</td>
</tr>
<tr>
<td style="text-align: left;">#GRU Units</td>
<td style="text-align: left;">256</td>
</tr>
<tr>
<td style="text-align: left;">Mean KL</td>
<td style="text-align: left;">0.01</td>
</tr>
<tr>
<td style="text-align: left;">Batch size</td>
<td style="text-align: left;">50000</td>
</tr>
</tbody>
</table>
<h1>REFERENCES</h1>
<p>Martın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.</p>
<p>Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. arXiv preprint arXiv:1604.06778, 2016.</p>
<p>Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Aistats, volume 9, pp. 249-256, 2010.</p>
<p>Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy P Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. arXiv preprint arXiv:1602.01783, 2016.</p>
<p>Ian Osband. TabulaRL. https://github.com/iosband/TabulaRL, 2016.
Tim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. arXiv preprint arXiv:1602.07868, 2016.</p>
<p>Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Videos for the task are available at https://goo.gl/rDDBpb.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
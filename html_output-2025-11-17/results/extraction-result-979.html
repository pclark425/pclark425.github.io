<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-979 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-979</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-979</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-23.html">extraction-schema-23</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <p><strong>Paper ID:</strong> paper-a8c3c55a246b8656268e766602f6748f2d04ccbb</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a8c3c55a246b8656268e766602f6748f2d04ccbb" target="_blank">Can Language Models Serve as Text-Based World Simulators?</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work builds and uses a new benchmark, called ByteSized32-State-Prediction, containing a dataset of text game state transitions and accompanying game tasks, to directly quantify, for the first time, how well LLMs can serve as text-based world simulators.</p>
                <p><strong>Paper Abstract:</strong> Virtual environments play a key role in benchmarking advances in complex planning and decision-making tasks but are expensive and complicated to build by hand. Can current language models themselves serve as world simulators, correctly predicting how actions change different world states, thus bypassing the need for extensive manual coding? Our goal is to answer this question in the context of text-based simulators. Our approach is to build and use a new benchmark, called ByteSized32-State-Prediction, containing a dataset of text game state transitions and accompanying game tasks. We use this to directly quantify, for the first time, how well LLMs can serve as text-based world simulators. We test GPT-4 on this dataset and find that, despite its impressive performance, it is still an unreliable world simulator without further innovations. This work thus contributes both new insights into current LLM's capabilities and weaknesses, as well as a novel benchmark to track future progress as new models appear.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e979.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e979.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reasoning via Planning (RAP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neurosymbolic approach that constructs a symbolic world model using LLM-derived priors and then applies a dedicated planning algorithm to derive agent policies; cited in this paper as an example of LLMs being used to build symbolic world models for downstream planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reasoning with language model is planning with world model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Reasoning via Planning (RAP)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>According to this paper's citation, RAP uses a language model to produce priors or structured knowledge that are used to instantiate a formal world model; a separate/pluggable planner then performs planning over that constructed model to produce actions. The paper cites RAP as an example of neurosymbolic pipelines where LLMs produce symbolic artifacts for classical planners rather than serving as end-to-end planners themselves.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>neurosymbolic / symbolic world model (LLM-derived priors)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>Described here as a symbolic world model constructed from LLM outputs (i.e., LLM priors). The paper does not report the internal representation (e.g., PDDL/PPDDL/belief states) used by RAP; it only notes that RAP constructs a world model from LLM priors and then uses a dedicated planner. Probabilistic vs deterministic status is not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td>world model construction (provide priors / generate symbolic descriptions used to build the world model)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td>dedicated planner (not specified in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as a neurosymbolic example: constructs a symbolic world model from LLM priors and uses a planner. The present paper contrasts this approach with direct LLM-as-simulator approaches and notes that LLMs alone are unreliable simulators; RAP exemplifies the alternative of using LLMs to create structured models for formal planning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Language Models Serve as Text-Based World Simulators?', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e979.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e979.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM+P</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM+P</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method (cited) that aims to combine large language models with planning algorithms to give LLMs better planning proficiency by leveraging symbolic planning structures alongside LLM reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLM+P: Empowering large language models with optimal planning proficiency</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM+P</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as an example of neurosymbolic work that pairs LLM reasoning with formal planning. The paper lists LLM+P among works that use LLMs to generate symbolic representations or otherwise augment formal planners, but does not detail the representation format or how uncertainty is handled.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>symbolic planning augmentation (exact formalism not specified in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>Not specified in this paper; implied to be a symbolic planning representation used in conjunction with LLM outputs. No claim in this paper that PPDDL/PDDL or belief states are used by LLM+P (not described here).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td>planning augmentation / provide structured guidance or priors for planning</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as an example of combining LLMs with formal planning. The current paper notes such neurosymbolic techniques as alternatives to direct LLM simulation but provides no experimental details on LLM+P's uncertainty treatment or evaluation in text-based environments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Language Models Serve as Text-Based World Simulators?', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e979.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e979.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WorldCoder</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Worldcoder (model-based LLM agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited recent method that builds world models by writing code and interacting with the environment; exemplifies approaches where LLMs synthesize executable symbolic/state representations (code) that can be used for planning or model-based control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Worldcoder, a model-based llm agent: Building world models by writing code and interacting with the environment</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Worldcoder</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Described in this paper's related-work as an approach that has an LLM generate code to instantiate a world model and then uses that code/model to interact with the environment. This is given as an example of neurosymbolic strategies where language models output structured artifacts (executable code) serving as the world model.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>executable symbolic world model (code-based representation)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>World model is represented as generated code (i.e., symbolic/executable representation). The paper does not state whether the generated code encodes probabilistic transitions (e.g., PPDDL) or deterministic semantics, nor whether it maintains explicit belief-state distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td>world model construction (generate executable code that implements a simulator/world model)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as a code-generating LLM approach to produce world models. The present paper positions such methods versus direct LLM simulation and emphasizes that constructing explicit symbolic models (e.g., via code) is an active alternative to relying on LLMs as simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Language Models Serve as Text-Based World Simulators?', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e979.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e979.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Nottingham et al. 2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Do embodied agents dream of pixelated sheep: Embodied decision making using language guided world modelling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited work that uses language-guided world modeling for embodied decision making; referenced as an example of leveraging language models to create structured world representations for planning or decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Do embodied agents dream of pixelated sheep: Embodied decision making using language guided world modelling</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Language-guided world modelling (Nottingham et al. 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>This cited work is presented as employing language guidance to build world models for embodied decision making. In the context of this paper it's an example of neurosymbolic approaches where LLMs help produce symbolic/world-model artifacts that can be used by planners or decision-makers.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>language-guided symbolic world model (specific formalism not specified here)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>The paper does not report the internal symbolic formalism used by Nottingham et al.; it frames the work as constructing or guiding world models via language, potentially enabling formal planning, but does not state whether models are probabilistic (PPDDL/belief states) or deterministic.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td>world model construction / language-guided modelling</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as representative of language-guided world modelling for embodied decision-making. The present paper contrasts such structured approaches with direct simulation by LLMs, noting the latter's unreliability as simulators in single-step prediction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Language Models Serve as Text-Based World Simulators?', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e979.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e979.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wong et al. 2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>From word models to world models: Translating from natural language to the probabilistic language of thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced work that translates natural language into a probabilistic symbolic formalism (a 'probabilistic language of thought'), and thus directly connects language understanding to probabilistic symbolic world representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>From word models to world models: Translating from natural language to the probabilistic language of thought</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Probabilistic Language-of-Thought translation (Wong et al. 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>This cited work focuses on translating natural-language descriptions into a probabilistic symbolic formalism (described as a probabilistic 'language of thought'), thereby producing probabilistic symbolic world models that can represent uncertainty explicitly. The paper is cited among neurosymbolic literature bridging language and symbolic/probabilistic models.</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_type</strong></td>
                            <td>probabilistic symbolic representation (probabilistic language-of-thought)</td>
                        </tr>
                        <tr>
                            <td><strong>world_model_description</strong></td>
                            <td>Represents states and inferences in a probabilistic symbolic formalism (a probabilistic language of thought). This explicitly encodes uncertainty over symbolic hypotheses; the present paper cites Wong et al. as an example of probabilistic symbolic world modelling derived from language.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_llm</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>llm_role</strong></td>
                            <td>translation from natural language to probabilistic symbolic representation / world model construction</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_modeling</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_type</strong></td>
                            <td>probabilistic state uncertainty / epistemic uncertainty over symbolic hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_method</strong></td>
                            <td>probabilistic language-of-thought formalism (probability distributions over symbolic hypotheses); exact methods not detailed in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>planning_algorithm</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_integrates_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>text_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_uncertainty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an explicit example linking language to probabilistic symbolic representations (a probabilistic language-of-thought). The present paper highlights such works as alternatives to direct LLM simulation; Wong et al. are noted as explicitly probabilistic in their symbolic modeling, making them directly relevant to planning under uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can Language Models Serve as Text-Based World Simulators?', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reasoning with language model is planning with world model <em>(Rating: 2)</em></li>
                <li>LLM+P: Empowering large language models with optimal planning proficiency <em>(Rating: 2)</em></li>
                <li>Worldcoder, a model-based llm agent: Building world models by writing code and interacting with the environment <em>(Rating: 2)</em></li>
                <li>Do embodied agents dream of pixelated sheep: Embodied decision making using language guided world modelling <em>(Rating: 2)</em></li>
                <li>From word models to world models: Translating from natural language to the probabilistic language of thought <em>(Rating: 2)</em></li>
                <li>Do embodied agents dream of pixelated sheep: Embodied decision making using language guided world modelling <em>(Rating: 1)</em></li>
                <li>TextWorld: A learning environment for text-based games <em>(Rating: 1)</em></li>
                <li>ALFWorld: Aligning text and embodied environments for interactive learning <em>(Rating: 1)</em></li>
                <li>ScienceWorld: Is your agent smarter than a 5th grader? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-979",
    "paper_id": "paper-a8c3c55a246b8656268e766602f6748f2d04ccbb",
    "extraction_schema_id": "extraction-schema-23",
    "extracted_data": [
        {
            "name_short": "RAP",
            "name_full": "Reasoning via Planning (RAP)",
            "brief_description": "A neurosymbolic approach that constructs a symbolic world model using LLM-derived priors and then applies a dedicated planning algorithm to derive agent policies; cited in this paper as an example of LLMs being used to build symbolic world models for downstream planning.",
            "citation_title": "Reasoning with language model is planning with world model",
            "mention_or_use": "mention",
            "system_name": "Reasoning via Planning (RAP)",
            "system_description": "According to this paper's citation, RAP uses a language model to produce priors or structured knowledge that are used to instantiate a formal world model; a separate/pluggable planner then performs planning over that constructed model to produce actions. The paper cites RAP as an example of neurosymbolic pipelines where LLMs produce symbolic artifacts for classical planners rather than serving as end-to-end planners themselves.",
            "world_model_type": "neurosymbolic / symbolic world model (LLM-derived priors)",
            "world_model_description": "Described here as a symbolic world model constructed from LLM outputs (i.e., LLM priors). The paper does not report the internal representation (e.g., PDDL/PPDDL/belief states) used by RAP; it only notes that RAP constructs a world model from LLM priors and then uses a dedicated planner. Probabilistic vs deterministic status is not specified in this paper.",
            "uses_llm": true,
            "llm_role": "world model construction (provide priors / generate symbolic descriptions used to build the world model)",
            "llm_model_name": null,
            "uncertainty_modeling": null,
            "uncertainty_type": null,
            "uncertainty_method": null,
            "planning_algorithm": "dedicated planner (not specified in this paper)",
            "planning_integrates_uncertainty": null,
            "text_environment_name": null,
            "text_environment_description": null,
            "performance_metric": null,
            "performance_value": null,
            "baseline_comparison": null,
            "has_ablation_uncertainty": null,
            "ablation_results": null,
            "key_findings": "Cited as a neurosymbolic example: constructs a symbolic world model from LLM priors and uses a planner. The present paper contrasts this approach with direct LLM-as-simulator approaches and notes that LLMs alone are unreliable simulators; RAP exemplifies the alternative of using LLMs to create structured models for formal planning.",
            "uuid": "e979.0",
            "source_info": {
                "paper_title": "Can Language Models Serve as Text-Based World Simulators?",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LLM+P",
            "name_full": "LLM+P",
            "brief_description": "A method (cited) that aims to combine large language models with planning algorithms to give LLMs better planning proficiency by leveraging symbolic planning structures alongside LLM reasoning.",
            "citation_title": "LLM+P: Empowering large language models with optimal planning proficiency",
            "mention_or_use": "mention",
            "system_name": "LLM+P",
            "system_description": "Referenced as an example of neurosymbolic work that pairs LLM reasoning with formal planning. The paper lists LLM+P among works that use LLMs to generate symbolic representations or otherwise augment formal planners, but does not detail the representation format or how uncertainty is handled.",
            "world_model_type": "symbolic planning augmentation (exact formalism not specified in this paper)",
            "world_model_description": "Not specified in this paper; implied to be a symbolic planning representation used in conjunction with LLM outputs. No claim in this paper that PPDDL/PDDL or belief states are used by LLM+P (not described here).",
            "uses_llm": true,
            "llm_role": "planning augmentation / provide structured guidance or priors for planning",
            "llm_model_name": null,
            "uncertainty_modeling": null,
            "uncertainty_type": null,
            "uncertainty_method": null,
            "planning_algorithm": null,
            "planning_integrates_uncertainty": null,
            "text_environment_name": null,
            "text_environment_description": null,
            "performance_metric": null,
            "performance_value": null,
            "baseline_comparison": null,
            "has_ablation_uncertainty": null,
            "ablation_results": null,
            "key_findings": "Mentioned as an example of combining LLMs with formal planning. The current paper notes such neurosymbolic techniques as alternatives to direct LLM simulation but provides no experimental details on LLM+P's uncertainty treatment or evaluation in text-based environments.",
            "uuid": "e979.1",
            "source_info": {
                "paper_title": "Can Language Models Serve as Text-Based World Simulators?",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "WorldCoder",
            "name_full": "Worldcoder (model-based LLM agent)",
            "brief_description": "A cited recent method that builds world models by writing code and interacting with the environment; exemplifies approaches where LLMs synthesize executable symbolic/state representations (code) that can be used for planning or model-based control.",
            "citation_title": "Worldcoder, a model-based llm agent: Building world models by writing code and interacting with the environment",
            "mention_or_use": "mention",
            "system_name": "Worldcoder",
            "system_description": "Described in this paper's related-work as an approach that has an LLM generate code to instantiate a world model and then uses that code/model to interact with the environment. This is given as an example of neurosymbolic strategies where language models output structured artifacts (executable code) serving as the world model.",
            "world_model_type": "executable symbolic world model (code-based representation)",
            "world_model_description": "World model is represented as generated code (i.e., symbolic/executable representation). The paper does not state whether the generated code encodes probabilistic transitions (e.g., PPDDL) or deterministic semantics, nor whether it maintains explicit belief-state distributions.",
            "uses_llm": true,
            "llm_role": "world model construction (generate executable code that implements a simulator/world model)",
            "llm_model_name": null,
            "uncertainty_modeling": null,
            "uncertainty_type": null,
            "uncertainty_method": null,
            "planning_algorithm": null,
            "planning_integrates_uncertainty": null,
            "text_environment_name": null,
            "text_environment_description": null,
            "performance_metric": null,
            "performance_value": null,
            "baseline_comparison": null,
            "has_ablation_uncertainty": null,
            "ablation_results": null,
            "key_findings": "Cited as a code-generating LLM approach to produce world models. The present paper positions such methods versus direct LLM simulation and emphasizes that constructing explicit symbolic models (e.g., via code) is an active alternative to relying on LLMs as simulators.",
            "uuid": "e979.2",
            "source_info": {
                "paper_title": "Can Language Models Serve as Text-Based World Simulators?",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Nottingham et al. 2023",
            "name_full": "Do embodied agents dream of pixelated sheep: Embodied decision making using language guided world modelling",
            "brief_description": "A cited work that uses language-guided world modeling for embodied decision making; referenced as an example of leveraging language models to create structured world representations for planning or decision-making.",
            "citation_title": "Do embodied agents dream of pixelated sheep: Embodied decision making using language guided world modelling",
            "mention_or_use": "mention",
            "system_name": "Language-guided world modelling (Nottingham et al. 2023)",
            "system_description": "This cited work is presented as employing language guidance to build world models for embodied decision making. In the context of this paper it's an example of neurosymbolic approaches where LLMs help produce symbolic/world-model artifacts that can be used by planners or decision-makers.",
            "world_model_type": "language-guided symbolic world model (specific formalism not specified here)",
            "world_model_description": "The paper does not report the internal symbolic formalism used by Nottingham et al.; it frames the work as constructing or guiding world models via language, potentially enabling formal planning, but does not state whether models are probabilistic (PPDDL/belief states) or deterministic.",
            "uses_llm": true,
            "llm_role": "world model construction / language-guided modelling",
            "llm_model_name": null,
            "uncertainty_modeling": null,
            "uncertainty_type": null,
            "uncertainty_method": null,
            "planning_algorithm": null,
            "planning_integrates_uncertainty": null,
            "text_environment_name": null,
            "text_environment_description": null,
            "performance_metric": null,
            "performance_value": null,
            "baseline_comparison": null,
            "has_ablation_uncertainty": null,
            "ablation_results": null,
            "key_findings": "Cited as representative of language-guided world modelling for embodied decision-making. The present paper contrasts such structured approaches with direct simulation by LLMs, noting the latter's unreliability as simulators in single-step prediction tasks.",
            "uuid": "e979.3",
            "source_info": {
                "paper_title": "Can Language Models Serve as Text-Based World Simulators?",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Wong et al. 2023",
            "name_full": "From word models to world models: Translating from natural language to the probabilistic language of thought",
            "brief_description": "A referenced work that translates natural language into a probabilistic symbolic formalism (a 'probabilistic language of thought'), and thus directly connects language understanding to probabilistic symbolic world representations.",
            "citation_title": "From word models to world models: Translating from natural language to the probabilistic language of thought",
            "mention_or_use": "mention",
            "system_name": "Probabilistic Language-of-Thought translation (Wong et al. 2023)",
            "system_description": "This cited work focuses on translating natural-language descriptions into a probabilistic symbolic formalism (described as a probabilistic 'language of thought'), thereby producing probabilistic symbolic world models that can represent uncertainty explicitly. The paper is cited among neurosymbolic literature bridging language and symbolic/probabilistic models.",
            "world_model_type": "probabilistic symbolic representation (probabilistic language-of-thought)",
            "world_model_description": "Represents states and inferences in a probabilistic symbolic formalism (a probabilistic language of thought). This explicitly encodes uncertainty over symbolic hypotheses; the present paper cites Wong et al. as an example of probabilistic symbolic world modelling derived from language.",
            "uses_llm": true,
            "llm_role": "translation from natural language to probabilistic symbolic representation / world model construction",
            "llm_model_name": null,
            "uncertainty_modeling": true,
            "uncertainty_type": "probabilistic state uncertainty / epistemic uncertainty over symbolic hypotheses",
            "uncertainty_method": "probabilistic language-of-thought formalism (probability distributions over symbolic hypotheses); exact methods not detailed in this paper",
            "planning_algorithm": null,
            "planning_integrates_uncertainty": null,
            "text_environment_name": null,
            "text_environment_description": null,
            "performance_metric": null,
            "performance_value": null,
            "baseline_comparison": null,
            "has_ablation_uncertainty": null,
            "ablation_results": null,
            "key_findings": "Cited as an explicit example linking language to probabilistic symbolic representations (a probabilistic language-of-thought). The present paper highlights such works as alternatives to direct LLM simulation; Wong et al. are noted as explicitly probabilistic in their symbolic modeling, making them directly relevant to planning under uncertainty.",
            "uuid": "e979.4",
            "source_info": {
                "paper_title": "Can Language Models Serve as Text-Based World Simulators?",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reasoning with language model is planning with world model",
            "rating": 2
        },
        {
            "paper_title": "LLM+P: Empowering large language models with optimal planning proficiency",
            "rating": 2
        },
        {
            "paper_title": "Worldcoder, a model-based llm agent: Building world models by writing code and interacting with the environment",
            "rating": 2
        },
        {
            "paper_title": "Do embodied agents dream of pixelated sheep: Embodied decision making using language guided world modelling",
            "rating": 2
        },
        {
            "paper_title": "From word models to world models: Translating from natural language to the probabilistic language of thought",
            "rating": 2
        },
        {
            "paper_title": "Do embodied agents dream of pixelated sheep: Embodied decision making using language guided world modelling",
            "rating": 1
        },
        {
            "paper_title": "TextWorld: A learning environment for text-based games",
            "rating": 1
        },
        {
            "paper_title": "ALFWorld: Aligning text and embodied environments for interactive learning",
            "rating": 1
        },
        {
            "paper_title": "ScienceWorld: Is your agent smarter than a 5th grader?",
            "rating": 1
        }
    ],
    "cost": 0.014275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Can Language Models Serve as Text-Based World Simulators?</h1>
<p>Ruoyao Wang ${ }^{\ddagger}$, Graham Todd ${ }^{\ddagger}$, Ziang Xiao<em>, Xingdi Yuan ${ }^{\diamond}$<br>Marc-Alexandre Ct</em>, Peter Clark ${ }^{\text {A }}$, Peter Jansen ${ }^{\dagger \Delta}$<br>${ }^{\dagger}$ University of Arizona ${ }^{\diamond}$ Microsoft Research Montral<br>${ }^{\ddagger}$ New York University ${ }^{\Delta}$ Johns Hopkins University ${ }^{\Delta}$ Allen Institute for AI<br>{ruoyaowang, pajansen}@arizona.edu gdrtodd@nyu.edu<br>ziang.xiao@jhu.edu {eric.yuan,macote}@microsoft.com<br>PeterC@allenai.org</p>
<h4>Abstract</h4>
<p>Virtual environments play a key role in benchmarking advances in complex planning and decision-making tasks but are expensive and complicated to build by hand. Can current language models themselves serve as world simulators, correctly predicting how actions change different world states, thus bypassing the need for extensive manual coding? Our goal is to answer this question in the context of text-based simulators. Our approach is to build and use a new benchmark, called BYTE-SIZED32-State-Prediction, containing a dataset of text game state transitions and accompanying game tasks. We use this to directly quantify, for the first time, how well LLMs can serve as text-based world simulators. We test GPT-4 on this dataset and find that, despite its impressive performance, it is still an unreliable world simulator without further innovations. This work thus contributes both new insights into current LLM's capabilities and weaknesses, as well as a novel benchmark to track future progress as new models appear.</p>
<h2>1 Introduction and Related Work</h2>
<p>Simulating the world is crucial for studying and understanding it. In many cases, however, the breadth and depth of available simulations are limited by the fact that their implementation requires extensive work from a team of human experts over weeks or months. Recent advances in large language models (LLMs) have pointed towards an alternate approach by leveraging the huge amount of knowledge contained in their pre-training datasets. But are they ready to be used directly as simulators?</p>
<p>We examine this question in the domain of textbased games, which naturally express the environment and its dynamics in natural language and have long been used as part of advances in decision making processes (Ct et al., 2018; Fan et al., 2020; Urbanek et al., 2019; Shridhar et al., 2020; Hausknecht et al., 2020; Jansen, 2022; Wang et al.,
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An overview of our two approaches using an LLM as a text game simulator. The example shows the process that a cup in the sink is filled by water after turning on the sink. The full state prediction includes all objects in the game including the unrelated stove, while the state difference prediction excludes the unrelated stove. State changes caused by $\mathcal{F}<em _env="{env" _text="\text">{\text {act }}$ and $\mathcal{F}</em>$ are highlighted in yellow and green, respectively.
2023), information extraction (Ammanabrolu and Hausknecht, 2020; Adhikari et al., 2020), and artificial reasoning (Wang et al., 2022).}</p>
<p>Broadly speaking, there are two ways to leverage LLMs in the context of world modeling and simulation. The first is neurosymbolic: a number of efforts use language models to generate code in a symbolic representation that allows for formal planning or inference (Liu et al., 2023; Nottingham et al., 2023; Wong et al., 2023; Tang et al., 2024). Reasoning via Planning (RAP) (Hao et al., 2023) is one such approach - it constructs a world model using LLM priors and then uses a</p>
<p>dedicated planning algorithm to decide on agent policies (LLMs themselves continue to struggle to act directly as planners <em>Valmeekam et al. (2023)</em>). Similarly, BYTESIZED32 <em>Wang et al. (2023)</em> tasks LLMs with instantiating simulations of scientific reasoning concepts in the form of large PYTHON programs. These efforts are in contrast to the second, and comparatively less studied, approach of direct simulation. For instance, AI-DUNGEON represents a game world purely through the generated output of a language model, with inconsistent results <em>Walton (2020)</em>. In this work, we provide the first quantitative analysis of the abilities of LLMs to directly simulate virtual environments. We make use of structured representations in the JSON schema as a scaffold that both improves simulation accuracy and allows for us to directly probe the LLMs abilities across a variety of conditions.</p>
<p>In a systematic analysis of GPT-4 <em>Achiam et al. (2023)</em>, we find that LLMs broadly fail to capture state transitions not directly related to agent actions, as well as transitions that require arithmetic, common-sense, or scientific reasoning. Across a variety of conditions, model accuracy does not exceed 59.9% for transitions in which a non-trivial change in the world state occurs. These results suggest that, while promising and useful for downstream tasks, LLMs are not yet ready to act as reliable world simulators without further innovation.</p>
<h2>2 Methodology</h2>
<p>We examine the abilities of LLMs to serve as world simulators in text-based virtual environments, in which an agent receives observations and proposes actions in natural language in order to complete certain objectives. Each text environment can be formally represented as a goal-conditioned partially observable Markov decision process (POMDP) <em>Kaelbling et al. (1998)</em> with the 7-tuple $(S, A, \mathcal{T}, O, R, C, D)$, where $S$ denotes the state space, $A$ denotes the action space, $\mathcal{T}: S \times A \rightarrow S$ denotes the transition function, $O$ denotes the observation function, $R: S \times A \rightarrow \mathbb{R}$ denotes the reward function, $C$ denotes a natural language "context message" that describes the goal and action semantics, and $D: S \times A \rightarrow{0,1}$ denotes the binary completion indicator function.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 1: Corpus statistics of BYTESIZED32-SP.</p>
<h3>2.1 LLM-Sim Task</h3>
<p>We propose a prediction task, which we call LLM-as-a-Simulator (LLM-Sim), as a way of quantitatively evaluating the capacity of language models to serve as reliable simulators. The LLMSim task is defined as implementing a function $\mathcal{F}: C \times S \times A \rightarrow S \times \mathbb{R} \times{0,1}$ as a world simulator that maps from a given context, state, and action (i.e. $c, s_{t}, a_{t}$ ) to the subsequent state, reward, and game completion status (i.e. $s_{t+1}, r_{t+1}, d_{t+1}$ ).</p>
<p>In practice, the whole state transition simulator $\mathcal{F}$ should consider two types of state transitions: action-driven transitions and environment-driven transitions. For the example in Figure 1, the action-driven transition is that the sink is turned on (isOn=true) after taking the action turn on sink, and the environment-driven transition is that water fills up the cup in the sink when the sink is on. To better understand LLMs ability to model each of these transitions, we further decompose the simulator function $\mathcal{F}$ into three steps:</p>
<p>$$
\begin{aligned}
s_{t+1}^{\text{act}} &amp; =\mathcal{F}<em t="t">{\text{act}}\left(c, s</em>\right) \
s_{t+1} &amp; =\mathcal{F}}, a_{t<em t_1="t+1">{\text{env}}\left(c, s</em>\right) \
r_{t+1}, d_{t+1} &amp; =\mathcal{F}}^{\text{act}<em t="t">{R}\left(c, a</em>\right)
\end{aligned}
$$}, s_{t+1</p>
<ol>
<li>Action-driven transition simulator $\mathcal{F}<em t_1="t+1">{\text {act }}:$ $C \times S \times A \rightarrow S$ predicts $s</em>$ represents the direct state change caused by actions.}^{\text {act }}$ given $c, s_{t}$, and $a_{t}$, where $s_{t+1}^{\text {act }</li>
<li>Environment-driven transition simulator $\mathcal{F}<em t_1="t+1">{\text {env }}: C \times S \rightarrow S$ predicts $s</em>$ is the state that results after any environment-driven transitions.}$ given $c$ and $s_{t+1}^{\text {act }}$, where $s_{t+1</li>
<li>Game progress simulator $\mathcal{F}<em t_1="t+1">{R}: C \times S \times A \rightarrow$ $\mathbb{R} \times{0,1}$ predicts the reward $r</em>$.}$ and the game completion status $d_{t+1}$ given $c$, $s_{t+1}$, and $a_{t</li>
</ol>
<p>In our experiments, we measure the ability for LLMs to model $\mathcal{F}<em _env="{env" _text="\text">{\text {act }}, \mathcal{F}</em>$ (i.e. in which all transitions are captured in a single step). We consider two variants of the LLM-Sim task:}}$, and $\mathcal{F}_{R}$ separately, as well as the complete $\mathcal{F</p>
<table>
<thead>
<tr>
<th>Rules</th>
<th>State Change</th>
<th>$\mathcal{F}$</th>
<th></th>
<th>$\mathcal{F}_{\text {act }}$</th>
<th></th>
<th>$\mathcal{F}_{\text {env }}$</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Full</td>
<td>Diff</td>
<td>Full</td>
<td>Diff</td>
<td>Full</td>
<td>Diff</td>
</tr>
<tr>
<td>LLM</td>
<td>dynamic</td>
<td>59.0</td>
<td>59.5</td>
<td>76.1</td>
<td>75.2</td>
<td>44.1</td>
<td>49.7</td>
</tr>
<tr>
<td></td>
<td>static</td>
<td>62.8</td>
<td>72.2</td>
<td>73.0</td>
<td>89.5</td>
<td>61.9</td>
<td>93.8</td>
</tr>
<tr>
<td>Human</td>
<td>dynamic</td>
<td>59.9</td>
<td>51.6</td>
<td>77.1</td>
<td>68.4</td>
<td>38.6</td>
<td>22.2</td>
</tr>
<tr>
<td></td>
<td>static</td>
<td>63.5</td>
<td>73.9</td>
<td>77.5</td>
<td>90.2</td>
<td>73.8</td>
<td>92.3</td>
</tr>
<tr>
<td>No rule</td>
<td>dynamic</td>
<td>54.1</td>
<td>52.2</td>
<td>70.8</td>
<td>67.7</td>
<td>24.4</td>
<td>22.3</td>
</tr>
<tr>
<td></td>
<td>static</td>
<td>56.6</td>
<td>70.4</td>
<td>65.3</td>
<td>84.6</td>
<td>73.0</td>
<td>91.7</td>
</tr>
</tbody>
</table>
<p>Full State Prediction: The LLM outputs the complete state. For example, when functioning as $\mathcal{F}$, given $c$, $s_{t}$ and $a_{t}$, the model generates the full game state $s_{t+1}$ alongside $r_{t+1}$ and $d_{t+1}$.
State Difference Prediction: The LLM outputs only the difference between the input and output states. For example, when functioning as $\mathcal{F}$, given $c, s_{t}$ and $a_{t}$, the model generates only the difference between the current and subsequent game states, $\Delta\left(\left(s_{t}, r_{t}, d_{t}\right),\left(s_{t+1}, r_{t+1}, d_{t+1}\right)\right)$, as a way to reduce the need to generate redundant or unchanging information. We do not apply state difference prediction to the game progress simulator $\mathcal{F}<em t_1="t+1">{R}$ as its output ( $r</em>$ ) is not complex.}$ and $d_{t+1</p>
<h3>2.2 Data</h3>
<p>To facilitate evaluation on the LLM-Sim task, we introduce a novel dataset of text game state transitions. Our dataset, BYteSIZED32-State-Prediction (BYteSIZED32-SP), consists of 76,369 transitions represented as $\left(c, s_{t}, r_{t}, d_{t}, a_{t}, s_{t+1}^{\text {act }}, s_{t+1}, r_{t+1}, d_{t+1}\right)$ tuples collected from 31 distinct text games. Additional corpus statistics are summarized in Table 1.</p>
<p>Data Collection: Our dataset is derived from the open BYteSIZED32 corpus (Wang et al., 2023), which consists of 32 human-authored text games that each simulate a different scientific or commonsense reasoning concept. We first modify each BYteSIZED32 game to dump the game state $\left(s_{t}, r_{t}, d_{t}\right)$ as well as its intermediate state $s_{t+1}^{\text {act }}$ at each time step $t$ as a JSON object. We hold out one game as an example and seed our dataset of transitions by first following the gold-label goalfollowing trajectory provided with each game. We then deterministically collect every valid transition that is at most one step away from the gold-label trajectory by querying the game for the set of valid actions at each step.</p>
<p>Additional Context: Each game also includes a context message, $c$, that provides additional information to the model. The context consists of four parts: action rules describing the effect of each action on the game state, object rules describing the meaning of each object property and whether they are affected by the game's underlying dynamics, scoring rules describing how an agent earns reward and the conditions under which the game is won or lost, and one or two example transitions (see Appendix B for details) from the held-out game mentioned above. For each game we generate three</p>
<p>Table 2: Average accuracy per game of GPT-4 predicting the whole state transitions $(\mathcal{F})$ as well as action-driven transitions $\left(\mathcal{F}<em _env="{env" _text="\text">{\text {act }}\right)$ and environment-driven transitions $\left(\mathcal{F}</em>\right)$. We report settings that use LLM generated rules, human written rules, or no rules. Dynamic and static denote whether the game object properties and game progress should be changed; Full and diff denote whether the prediction outcome is the full game state or state differences. Numbers are shown in percentage.}</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Rules</th>
<th style="text-align: center;">Game Progress</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LLM</td>
<td style="text-align: center;">92.1</td>
</tr>
<tr>
<td style="text-align: left;">Human</td>
<td style="text-align: center;">81.8</td>
</tr>
<tr>
<td style="text-align: left;">No rule</td>
<td style="text-align: center;">61.5</td>
</tr>
</tbody>
</table>
<p>Table 3: GPT-4 game progress prediction results
versions of the context, one where the rules are written by a human expert (one of the game authors), and one where they are produced by an LLM with access to the game code, and one where no rules are provided. See Appendix C for additional details.</p>
<h3>2.3 Evaluation</h3>
<p>Performance on LLM-Sim is determined by the model's prediction accuracy w.r.t. the ground truth labels over a dataset of test samples. Depending on the experimental condition, the LLM must model object properties (when simulating $\mathcal{F}<em _env="{env" _text="\text">{\text {act }}, \mathcal{F}</em>$ ), defined as:}}$, or $\mathcal{F}$ ) and / or game progress (when simulating $\mathcal{F}_{R}$ or $\mathcal{F</p>
<p>Object Properties: a list of all objects in the game, along with each object's properties (e.g., temperature, size) and relationships to other objects (e.g., being within or on top of another object).
Game Progress: the status of the agent w.r.t. the overall goal, consisting of the current accumulated reward, whether the game has terminated, and whether the overall goal has been achieved.</p>
<p>We note that in each case the LLM is provided with the ground truth previous state (when functions as $\mathcal{F}<em t_1="t+1">{\text {env }}$ the previous state is $s</em>$ ) as well as the overall task context. That is to say, the LLM always performs a single-step prediction.}^{\text {act }</p>
<h2>3 Experiments</h2>
<p>Figure 1 demonstrates how we evaluate the performance of a model on the LLM-Sim task using</p>
<table>
<thead>
<tr>
<th>Game</th>
<th>Avg. Annotator</th>
<th>GPT-4</th>
</tr>
</thead>
<tbody>
<tr>
<td>bath-tub-water-temperature</td>
<td>0.99</td>
<td>0.60</td>
</tr>
<tr>
<td>clean-energy</td>
<td>0.50</td>
<td>0.35</td>
</tr>
<tr>
<td>take-photo</td>
<td>0.83</td>
<td>0.00</td>
</tr>
<tr>
<td>metal-detector</td>
<td>0.86</td>
<td>0.50</td>
</tr>
<tr>
<td>mix-paint</td>
<td>0.85</td>
<td>0.50</td>
</tr>
<tr>
<td>Average</td>
<td>0.80</td>
<td>0.49</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparison between accuracy of human annotators and GPT-4 on a subset of the BYTESIZED32-SP dataset. Transitions were sampled to normalize GPT-4 performance at 50% (if possible) and annotators were tasked with modeling the complete transition function $\mathcal{F}$ and outputting the full state.
in-context learning. We evaluate the accuracy of GPT-4 in both the Full State and State Difference prediction regimes. The model receives the previous state (encoded as a JSON object), previous action, and context message, it produces the subsequent state (either as a complete JSON object or as a diff). See Appendix A for details.</p>
<p>We note that the transition dynamics between states depend primarily on the verb used in the action (e.g., take, put, cook, ...). In addition, some state-action pairs do not result in any changes to object properties or game progress. To ensure balance across these conditions (and increase the tractability of our experiments), we sub-sample a dataset $\mathcal{D}$ from the full BYTESIZED32-SP set. Formally, let $s_{\text {in }}$ be the input state of a simulator function and $s_{\text {out }}$ be the output state of the simulator function (e.g. $s_{\text {in }}=s_{\mathrm{f}}$ and $s_{\text {out }}=s_{\mathrm{f}+1}^{\text {act }}$ for $\mathcal{F}<em _out="{out" _text="\text">{\text {act }}$ ). We call any transition in which $s</em>$ by randomly sampling 10 dynamic transitions and 10 static transitions from BYTESIZED32-SP for each possible action verb (taking as many as possible if fewer than 10 exist) w.r.t action-driven transitions. The resulting experimental dataset consists of 2954 transition tuples.}}=s_{\text {in }}$ (according to the ground-truth) static and call each other transition dynamic. Note that the environment-driven transition following a dynamic action-driven transition is not necessarily dynamic. For example, a state in which the agent takes an apple while the remaining objects in the environment remain the same is a dynamic action-driven transition and a static environment-driven transition. We construct $\mathcal{D</p>
<h2>4 Results</h2>
<p>Table 2 presents the accuracy of GPT-4 simulating the whole state transitions as well as its accuracy of simulating action-driven transitions and environment-driven transitions alone. ${ }^{2}$ We report</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>some major observations below:
Predicting action-driven transitions is easier than predicting environment-driven transitions: At best, GPT-4 is able to simulate $77.1 \%$ of $d y$ namic action-driven transitions correctly. In contrast, GPT-4 simulates at most $49.7 \%$ of dynamic environment-driven transitions correctly. This indicates that the most challenging part of the LLMSim task is likely simulating the underlying environmental dynamics.</p>
<p>Predicting static transitions is easier than dynamic transitions: Unsurprisingly, modeling a static transition is substantially easier than a dynamic transition across most conditions. While the LLM needs to determine whether a given initial state and action will result in a state change in either case, dynamic transitions also require simulating the dynamics in exactly the same way as the underlying game engine by leveraging the information in the context message.</p>
<p>Predicting full game states is easier for dynamic states, whereas predicting state difference is easier for static states: Predicting the state difference for dynamic state significantly improves the performance ( $&gt;10 \%$ ) of simulating static transitions, while decreases the performance when simulating dynamic transitions. This may be because state difference prediction is aimed at reducing potential format errors. However, GPT-4 is able to get the response format correct in most cases, while introducing the state difference increases the complexity of the output format of the task.</p>
<p>Game rules matter, and LLMs are able to generate good enough game rules: Performance of GPT-4 on all three simulation tasks drops in most conditions when game rules are not provided in the context message. However, we fail to find obvious performance differences between game rules generated by human experts and by LLMs themselves.</p>
<p>GPT-4 can predict game progress in most cases: Table 3 presents the results of GPT-4 predicting game progress. With game rules information in the context, GPT-4 can predict the game progress correctly in $92.1 \%$ test cases. The presence of these rules in context is crucial: without them, GPT-4's prediction accuracy drops to $61.5 \%$.</p>
<p>Humans outperform GPT-4 on the LLM-Sim task: We provide a preliminary human study on the LLM-Sim task. In particular, we take the 5 games</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />Figure 2: Simulation performance of whole state transition (top), action-driven transitions (middle) and environment-driven transitions (bottom) as a function of the property being modified, in the GPT-4, full state prediction, with human written rules condition. The $x$-axis represents specific object properties, and $y$-axis represents performance (0-100%). Errors are broken down into incorrect value and unaltered value. Refer to Table 7 for the meaning of each property.
from the BYTESIZED32-SP dataset in which GPT- 4 produced the worst accuracy at modeling $\mathcal{F}<em _act="{act" _text="\text">{\text {act }}$. For each game, we randomly sample 20 games with the aim of having 10 transitions where GPT-4 succeeded and 10 transitions where GPT-4 failed (note that this is not always possible because on some games GPT-4 fails/succeeds on most transitions). In addition, we balance each set of 10 transitions to have 5 dynamic transitions and 5 static transitions. We instruct four human annotators (4 authors of this paper) to model as $\mathcal{F}</em>$ using the humangenerated rules as context in a full game state prediction setting. Results are reported in Table 4. The overall human accuracy is $80 \%$, compared to the sampled LLM accuracy of $50 \%$, and the variation among annotators is small. This suggests that while our task is generally straightforward and relatively easy for humans, there is still a significant room for improvement for LLMs.}</p>
<p>GPT-4 is more likely to make an error when arithmetic, common-sense, or scientific knowledge is needed: Because most errors occur in modeling dynamic transitions, we conduct an additional analysis to better understand failure modes. We use the setting with the best performance on $d y$ namic transitions (GPT-4, Human-written context, full state prediction) and further break down the results according to the specific object properties that are changed during the transition. Figure 2 shows, for the whole state transitions, action-driven transitions, and environment-driven transitions, the proportion of predictions that are either correct, set the property to an incorrect value, or fail to change the property value (empty columns means the property is not changed in its corresponding condition). We observe that GPT-4 is able to handle most simple boolean value properties well. The errors are concentrated on non-trivial properties that requires arithmetic (e.g., temperature, timeAboveMaxTemp), common-sense (e.g., current_aperture, current_focus), or scientific knowledge (e.g., on). We also observe that when predicting the action-driven and environment-driven transitions in a single step, GPT-4 tends to focus more on action-driven transitions, resulting in more unaltered value errors on states that it can predict correctly when solely simulating environment-driven transitions.</p>
<h2>5 Conclusion</h2>
<p>We propose BYTESIZED32-State-Prediction, a benchmark of 76,369 virtual text environment state transitions for testing LLMs as simulators. We evaluate GPT-4 on this world modeling task. Across models and conditions, the best recorded performance is $59.9 \%$ on accurately simulating state transitions that involve non-trivial changes. Because simulation errors accumulate across steps, a simulator with modest single-step accuracy has limited utility in practice - for example, after 10 steps, average simulation accuracy would reduce to $0.599^{10}$, or less than $1 \%$. Our results indicate that LLMs are not yet able to reliably act as text world simulators. Further error analysis shows that while LLMs are better at simulating the results of user actions, it is difficult for LLMs to handle environment-driven transitions and transitions that require arithmetic, common sense, or scientific knowledge.</p>
<h2>6 Limitations and Ethical Concerns</h2>
<h3>6.1 Limitations</h3>
<p>This work considers two strong in-context learning LLMs, GPT-3.5 and GPT-4, in their ability to act as explicit formal simulators. We adopt these models because they are generally the most performant off-the-shelf models across a variety of benchmarks. While we observe that even GPT-3.5 and GPT-4 achieve a modest score at the proposed task, we acknowledge that we did not exhaustively evaluate a large selection of large language models, and other models may perform better. We provide this work as a benchmark to evaluate the performance of existing and future models on the task of accurately simulating state space transitions.</p>
<p>In this work, we propose two representational formalisms for representing state spaces, one that includes full state space, while the other focuses on state difference, both represented using JSON objects. We have chosen these representations based on their popularity and compatibility with the input and output formats of most LLM pretraining data (e.g. Fakhoury et al., 2023), as well as being able to directly compare against gold standard simulator output for evaluation, though it is possible that other representational formats may be more performant at the simulation task.</p>
<p>Finally, the state spaces produced in this work are focused around the domain of common-sense and early (elementary) scientific reasoning. These tasks, such as opening containers or activating devices, were chosen because the results of these actions are common knowledge, and models are likely to be most performant in simulating these actions. While this work does address a selection of less frequent actions and properties, it does not address using LLMs as simulators for highly domain-specific areas, such as physical or medical simulation. A long term goal of this work is to facilitate using language models as simulators for high-impact domains, and we view this work as a stepping-stone to developing progressively more capable language model simulators.</p>
<h3>6.2 Ethical Concerns</h3>
<p>We do not foresee an immediate ethical or societal impact resulting from our work. However, we acknowledge that as an LLM application, the proposed LLM-Sim task could be affected in some way by misinformation and hallucinations introduced by the specific LLM selected by the user.</p>
<p>Our work highlights the issue with using LLMs as text-based world simulators. In downstream tasks, such as game simulation, LLMs may generate misleading or non-factual information. For example, if the simulator suggests burning a house to boil water, our work does not prevent this, nor do we evaluate the ethical implications of such potentially dangerous suggestions. As a result, we believe such applications are neither suitable nor safe to be deployed to a setting where they directly interact with humans, especially children, e.g., in an educational setting. We urge researchers and practitioners to use our proposed task and dataset in a mindful manner.</p>
<h2>Acknowledgements</h2>
<p>We wish to thank the three anonymous reviewers for their helpful comments on an earlier draft of this paper.</p>
<h2>References</h2>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.</p>
<p>Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Ct, Mikul Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, and Will Hamilton. 2020. Learning dynamic belief graphs to generalize on text-based games. Advances in Neural Information Processing Systems, 33:30453057.</p>
<p>Prithviraj Ammanabrolu and Matthew Hausknecht. 2020. Graph constrained reinforcement learning for natural language action spaces. arXiv preprint arXiv:2001.08837.</p>
<p>Marc-Alexandre Ct, kos Kdr, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Ruo Yu Tao, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, and Adam Trischler. 2018. Textworld: A learning environment for textbased games. CoRR, abs/1806.11532.</p>
<p>Sarah Fakhoury, Saikat Chakraborty, Madan Musuvathi, and Shuvendu K Lahiri. 2023. Towards generating functionally correct code edits from natural language issue descriptions. arXiv preprint arXiv:2304.03816.</p>
<p>Angela Fan, Jack Urbanek, Pratik Ringshia, Emily Dinan, Emma Qian, Siddharth Karamcheti, Shrimai Prabhumoye, Douwe Kiela, Tim Rocktaschel, Arthur Szlam, and Jason Weston. 2020. Generating interactive worlds with text. Proceedings of the AAAI Conference on Artificial Intelligence, 34(02):16931700 .</p>
<p>Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu. 2023. Reasoning with language model is planning with world model. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 8154-8173.</p>
<p>Matthew Hausknecht, Prithviraj Ammanabrolu, MarcAlexandre Ct, and Xingdi Yuan. 2020. Interactive fiction games: A colossal adventure. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7903-7910.</p>
<p>Peter Jansen. 2022. A systematic survey of text worlds as embodied natural language environments. In Proceedings of the 3rd Wordplay: When Language Meets Games Workshop (Wordplay 2022), pages 1-15.</p>
<p>Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. 1998. Planning and acting in partially observable stochastic domains. Artificial intelligence, 101(1-2):99-134.</p>
<p>Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. 2023. Llm+ p: Empowering large language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477.</p>
<p>Kolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh Hajishirzi, Sameer Singh, and Roy Fox. 2023. Do embodied agents dream of pixelated sheep: Embodied decision making using language guided world modelling. In International Conference on Machine Learning, pages 26311-26325. PMLR.</p>
<p>Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Ct, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. 2020. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768.</p>
<p>Hao Tang, Darren Key, and Kevin Ellis. 2024. Worldcoder, a model-based llm agent: Building world models by writing code and interacting with the environment. arXiv preprint arXiv:2402.12275.</p>
<p>Jack Urbanek, Angela Fan, Siddharth Karamcheti, Saachi Jain, Samuel Humeau, Emily Dinan, Tim Rocktschel, Douwe Kiela, Arthur Szlam, and Jason Weston. 2019. Learning to speak and act in a fantasy text adventure game.</p>
<p>Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. 2023. On the planning abilities of large language models-a critical investigation. Advances in Neural Information Processing Systems, 36:75993-76005.</p>
<p>Nick Walton. 2020. How we scaled AI Dungeon 2 to support over 1,000,000 users.</p>
<p>Ruoyao Wang, Peter Jansen, Marc-Alexandre Ct, and Prithviraj Ammanabrolu. 2022. Scienceworld: Is your agent smarter than a 5th grader? In Proceedings
of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11279-11298.</p>
<p>Ruoyao Wang, Graham Todd, Xingdi Yuan, Ziang Xiao, Marc-Alexandre Ct, and Peter Jansen. 2023. ByteSized32: A corpus and challenge task for generating task-specific world models expressed as text games. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 13455-13471, Singapore. Association for Computational Linguistics.</p>
<p>Lionel Wong, Gabriel Grand, Alexander K Lew, Noah D Goodman, Vikash K Mansinghka, Jacob Andreas, and Joshua B Tenenbaum. 2023. From word models to world models: Translating from natural language to the probabilistic language of thought. arXiv preprint arXiv:2306.12672.</p>
<h2>A Model details</h2>
<p>For the GPT-3.5 model, we use the gpt-3.5-turbo-0125 model. For the GPT-4 model, we use the gpt-4-0125-preview model. For both models, the temperature is set to 0 to get deterministic results. We also turn on the JSON mode of both models, which ensures that the model gives a valid JSON response. Our experiments cost approximately $\$ 5,000$ for OpenAI API usage.</p>
<h2>B Game transition examples</h2>
<p>We manually pick the wash-clothes game in BYtESIZED32 as the example game as it contains both state transitions driven by actions and game's underlying dynamics. In tasks where the model predicts action transition, environment-driven transitions, or the game progress alone, we provide one corresponding in-context example. In the task that requires the model to predict everything, we offer two in-context examples in the prompt. The two examples are manually picked such that in one example the game state is changed directly by the action taken while in the other example the game state is changed by the game's underlying dynamics.</p>
<h2>C Game rules generation</h2>
<h2>C. 1 LLM generated rules</h2>
<p>For LLM generated rules, we manually check all of them to avoid misinformation and offensive content.</p>
<p>We prompt GPT-4 (gpt-4-0125-preview) with the code of each object class to acquire the rules of each object. We also provide one in-context example. We ask GPT-4 to describe the meaning of each critical property (i.e. properties that do not inherit from parent) of the object and the tick function of the object (i.e. a function that defines how object properties may change at each time step regardless of the action taken). Below is an example of our prompt of object rule generation:</p>
<h2>Object Rule Generation Prompt</h2>
<p>You will be given a Python class which defines an object in a text game. List the classes inherited by this class and explain the properties of the object based on your understanding of the code. The properties you need to explain are commented as critical properties in the init function. If the class contains a tick method function, you should also decribe how the object properties will be changed at each game tick. Otherwise, do not explain any property. Your response should follow the format of the example below:
Here is the code for the example:
(0BJECT_CLASS_CODE)
The expected output is:
Object: Stove
Inherits: Container, Device
Properties:
maxTemperature: the maximum temperature of the stove in degrees Celsius
tempIncreasePerTick: the temperature increases per tick for objects on the stove if the stove is on.</p>
<p>Now here is another object class that needs you to explain: (0BJECT_CLASS_CODE)</p>
<p>For action rules generation, we prompt GPT-4 (gpt-4-0125-preview) with the code of the whole game, but unlike object rules, we do not offer any in-context example. We ask GPT-4 to describe each of the actions in the game. Below is an example of our prompt for action rule generation:</p>
<h2>Action Rule Generation Prompt</h2>
<p>You will be given a Python program which defines an a text game. Describe the all actions based on your understanding of the code. You can find all actions listed in the comments at the beginning of the program. You should describe all constraints of each action and how game states will be changed by taking each action. Here is the code of the game:
(GAME_CODE)
Similar to action rules, we generate score rules by prompting GPT-4 (gpt-4-0125-preview) with the code of the game and ask GPT-4 to describe how the game can be won or lose and how rewards can be earned. Below is an example of our prompt for score rule generation:</p>
<h2>Score Rule Generation Prompt</h2>
<p>You will be given a Python program which defines an a text game. Describe how the game can be won or lose, and how game scores can be earned based on your understanding of the calculateScore function in the TextGame class. Here is the code of the game. Do not describe the main function. (GAME_CODE)</p>
<h2>C. 2 Human-Written Action Rules</h2>
<p>The action rules describe how each action can change the game states. The expert annotator reads the game description and source code for each game. They went through the list of available actions in the game and their corresponding functions in the game. Each action rule has three main parts: Action, Description, and Rules. The Action specifies the name of the action (e.g., action). The Description explains the general purpose of the ac-</p>
<p>tion (e.g., connect two objects with input terminals). The Rules is an unordered list of rule descriptions that describe the constraints of the action when interacting with different objects (e.g., At least one of the objects should be a wire or a multimeter) or how the rule might function under different conditions (e.g., Disconnect terminal if the terminal is already connected to other objects). To ensure accuracy, the annotator plays through the game and checks if the written object rules were correctly reflected in the gameplay.</p>
<h3>C. 3 Human-Written Object Rules</h3>
<p>The object rules describe the meaning of each object property (e.g., temperature, size, weight, etc.) and how they will be changed at each time step. The expert annotators read the game description and source code for each game. They went through the object classes in the code script and wrote the object rules. Each object rule has three main parts: Object, Description, and Properties. The Object specifies the name of the object. The Description explains the general purpose of the object (e.g., GarbageCan is a container that can hold garbage). In the Description, the inheritance of the object class has been noted. The Properties is an unordered list of property descriptions that describe each property of that object (e.g., A Mold has its shape.) and their default value (e.g., By default, a GameObject is not combustible.) if the object is an abstract class. For objects with tick function, there is another property describing how an object may change under each tick. To ensure accuracy, the annotator plays through the game and checks if the written object rules were correctly reflected in the gameplay.</p>
<h3>C. 4 Human-Written Score Rules</h3>
<p>Score rules describe the conditions to win or lose the game and how rewards can be earned. An expert annotator (one of the BYTESIZED32 game authors) creates the rules by reading the game description and the code of the score function.</p>
<h2>D Prompts</h2>
<p>The prompts introduced in this section includes game rules that can either be human written rules or LLM generated rules. For experiments without game rules, we simply remove the rules from the corresponding prompts.</p>
<h3>1.1 Prompt Example: $\mathcal{F}_{\text {act }}$</h3>
<h3>1.1.1 Full State Prediction</h3>
<h2>Full State Prediction Prompt $\left(\mathcal{F}_{\text {act }}\right)$</h2>
<p>You are a simulator of a text game. Read the task description of a text game. Given the current game state in JSON, you need to decide the new game state after taking an action.
Your response should be in the same JSON format as the given game state.
Here is an example:
Example game task description:
Your task is to wash the dirty dishes.
Here are the descriptions of all game objects properties in the example game:
(OBJECT_RULES)
Here are the descriptions of all game actions in the example game:
(ACTION_RULES)
Here is the game state:
(GAME_STATE)
The action to take is put plate (ID: 5) in dirty cup (ID: 4)
The expected response is:
(GAME_STATE)
Here is the game that you need to simulate:
Task Description:
Your task is to figure out the weight of the cube. Use the answer action to give your answer.
Here are the descriptions of all game objects properties:
(OBJECT_RULES)
Here are the descriptions of all game actions:
(ACTION_RULES)
Here is the game state:
(GAME_STATE)
The action to take is:
look</p>
<h3>1.1.2 State Difference Prediction</h3>
<h2>State Difference Prediction Prompt $\left(\mathcal{F}_{\text {dif }}\right)$</h2>
<p>You are a simulator of a text game. Read the task description of a text game. Given the current game state in JSON, you need to decide the new game state after taking an action.
Your response should be in the JSON format. It should have two keys: 'modified' and 'removed'. The 'modified' key stores a list of all the object states that are added or changed after taking the action. Keep it an empty list if no object is added or modified. The ' removed' key stores a list of uuids of the objects that are removed. Keep it an empty list if no object is removed.
Here is an example:
Example game task description:
Your task is to wash the dirty dishes.
Here are the descriptions of all game objects properties in the example game:
(OBJECT_RULES)
Here are the descriptions of all game actions in the example game:
(ACTION_RULES)
Here is the game state:
(GAME_STATE)
The action to take is put plate (ID: 5) in dirty cup (ID: 4)
The expected response is:
(GAME_STATE_DIFFERENCE)
Here is the game that you need to simulate:
Task Description:
Your task is to figure out the weight of the cube. Use the answer action to give your answer.
Here are the descriptions of all game objects properties:
(OBJECT_RULES)
Here are the descriptions of all game actions:
(ACTION_RULES)
Here is the game state:
(GAME_STATE)
The action to take is:
look</p>
<h2>D. 2 Prompt Example: $\mathcal{F}_{\text {env }}$</h2>
<h2>D.2.1 Full State Prediction</h2>
<h2>Full State Prediction Prompt $\left(\mathcal{F}_{\text {env }}\right)$</h2>
<p>You are a simulator of a text game. Read the task description. Given the current game state in JSON, you need to decide how the game state changes in the next time step (without considering the agent actions). Rules for such changes are described as the tick function of each object.
Your response should be in the same JSON format as the given game state.
Here is an example:
Example game task description:
Your task is to wash the dirty dishes.
Here are the descriptions of all game objects properties in the example game:
[OBJECT_RULES]
Here is the game state:
[GAME_STATE]
The expected response is:
[GAME_STATE]
Here is the game that you need to simulate:
Task Description:
Your task is to figure out the weight of the cube. Use the answer action to give your answer.
Here are the descriptions of all game objects properties:
[OBJECT_RULES]
Here is the game state:
[GAME_STATE]</p>
<h2>D.2.2 State Difference Prediction</h2>
<h2>State Difference Prediction Prompt $\left(\mathcal{F}_{\text {std }}\right)$</h2>
<p>You are a simulator of a text game. Read the task description. Given the current game state in JSON, you need to decide how the game state changes in the next time step (without considering the agent actions). Rules for such changes are described as the tick function of each object.
Your response should be in the JSON format. It should have two keys: 'modified' and 'removed'. The 'modified' key stores a list of all the object states that are added or changed after taking the action. Keep it an empty list if no object is added or modified. The ' removed' key stores a list of uuids of the objects that are removed. Keep it an empty list if no object is removed.
Here is an example:
Example game task description:
Your task is to wash the dirty dishes.
Here are the descriptions of all game objects properties in the example game:
[OBJECT_RULES]
Here is the game state:
[GAME_STATE]
The expected response is:
[GAME_STATE_DIFFERENCE]
Here is the game that you need to simulate:
Task Description:
Your task is to figure out the weight of the cube. Use the answer action to give your answer.
Here are the descriptions of all game objects properties:
[OBJECT_RULES]
Here is the game state:
[GAME_STATE]</p>
<h2>Game Progress Prediction Prompt $\left(\mathcal{F}_{p}\right)$</h2>
<p>You are a simulator of a text game. Read the task description of a text game. Given the current game state in JSON, you need to predict the current game score, whether the game is over, and whether the agent wins the game.
Your response should be a JSON with three keys: 'score', ' gameOver', and 'gameWon'. 'score' stores the current game score, 'gameOver' stores a bool value on whether the game is over, and gameWon' stores a bool value on whether the game is won. Here is an example:
Example game task description:
Your task is to wash the dirty dishes.
Here are the descriptions of all game objects properties in the example game:
[OBJECT_RULES]
Here is a description of the game score function:
[SCORE_RULES]
Here is the previous game state:
[GAME_STATE]
The game score of the preivous state is:
['score': -1, 'gameOver': False, 'gameWon': False]
The action to take is use dish soap (ID: 12) on glass (ID: 8)
[GAME_STATE]
The expected response is:
['score': 3, 'gameOver': True, 'gameWon': True]
Here is the game that you need to simulate:
Task Description:
Your task is to figure out the weight of the cube. Use the answer action to give your answer.
Here are the descriptions of all game objects properties:
[OBJECT_RULES]
Here is a description of the game score function:
[SCORE_RULES]
Here is the previous game state:
[GAME_STATE]
The game score of the preivous state is:
['score': 0, 'gameOver': False, 'gameWon': False]
The action to take is:
look
Here is the current game state after taking the action:
[GAME_STATE]</p>
<h2>D. 4 Prompt Example: $\mathcal{F}$</h2>
<h2>D.4.1 Full State Prediction</h2>
<h2>Full State Prediction Prompt (F)</h2>
<p>You are a simulator of a text game. Read the task description of a text game. Given the current game state in JSON, you need to decide the new game state after taking an action including the game score.
You may need to create new objects when you predict the new game state. You should assign the uuid of new objects starting from the UUID base given in the instructions. Your response should be in the same JSON format as the given game state.
Note that while game states can be changed by actions, some game states may change over the time, which is described in the tick function of each object class.
Here are two examples of both cases. Both examples are from the same example game.
Example game task description:
Your task is to wash the dirty dishes.
Here are the descriptions of all game objects properties in the example game:
(OBJECT_RULES)
Here are the descriptions of all game actions in the example game:
(ACTION_RULES)
Here is a description of the score function of the example game: (SCORE_RULES)
In the first example, the game state is changed by an action:
Here is the game state:
(GAME_STATE)
The current game UUID base is 12
The action to take is: put plate (ID: 5) in dirty cup (ID: 4)
The expected response is:
(GAME_STATE)
In the second example from the same example game, the game state is changed over the time. Note that while in this example the game state is changed by time only, it is possible that a game state is changed by both an action and time.
Here is the game state:
(GAME_STATE)
The current game UUID base is 13
The action to take is: eat dishwasher (ID: 2) with dirty plate (ID: 5)
The expected response is:
(GAME_STATE)
Here is the game that you need to simulate:
(OBJECT_RULES)
Here are the descriptions of all game actions:
(ACTION_RULES)
Here is a description of the game score function:
(SCORE_RULES)
Here is the game state:
(GAME_STATE)
The current game UUID base is 12
The action to take is:
look</p>
<h2>D.4.2 State Difference Prediction</h2>
<h2>State Difference Prediction Prompt (F)</h2>
<p>You are a simulator of a text game. Read the task description and the current environment observation description. Given the current game state in 'textsc(JSON), you need to decide the new game state after taking an action.
Your response should be in the 'textsc(JSON) format. It should have three keys: 'modified', 'removed', and 'score'. The 'modified' key stores a list of all the object states that are added or changed after taking the action. Keep it an empty list if no object is added or modified. The 'removed' key stores a list of uuids of the objects that are removed. Keep it an empty list if no object is removed. The 'score' key stores a dictionary with three keys: 'score' is the current game score, 'gameOver' is a boolean of whether the game is over, and 'gameWon' is a boolean of whether the agent won the game. If a player earns a score or wins/losses the game, you should reflect that change in the dictionary saved under the 'score' key. Otherwise, you should set value of the 'score' key to an empty dictionary. Note that while game states can be changed by actions, some game states may change over the time, which is described in the tick function of each object class.
Note that while game states can be changed by actions, some game states may change over the time, which is described in the tick function of each object class.
Here are two examples of both cases. Both examples are from the same example game.
Example game task description:
Your task is to wash the dirty dishes.
Here are the descriptions of all game objects properties in the example game:
(OBJECT_RULES)
Here are descriptions of all game actions in the example game: (ACTION_RULES)
Here is a description of the score function of the example game: (SCORE_RULES)
In the first example, the game state is changed by an action: Current observation:
(GAME_OBSERVATION)
Here is the game state:
(GAME_STATE)
The action to take is put dirty plate (ID: 5) in mug (ID: 6)
The expected response is:
(GAME_STATE_DIFFERENCE)
In the second example from the same example game, the game state is changed over the time. Note that while in this example the game state is changed by time only, it is possible that a game state is changed by both an action and time.
Current observation:
(Example_2 observation)
Here is the game state:
(GAME_STATE)
The action to take is eat dishwasher (ID: 2) with dirty plate (ID: 5)
The expected response is:
(GAME_STATE_DIFFERENCE)
Here is the game that you need to simulate:
Task Description:
Your task is to boil water.
Here are the descriptions of all game objects properties:
(OBJECT_RULES)
Here are the descriptions of all game actions:
(ACTION_RULES)
Here is a description of the score function of the game:
(SCORE_RULES)
Current observation:
(GAME_OBSERVATION)
Here is the game state:
(GAME_STATE)
The current game UUID base is 12
The action to take is:
look</p>
<h2>D. 5 Other Examples</h2>
<p>Below is an example of the rule of an action:</p>
<h2>Action Rule Example</h2>
<div class="codehilite"><pre><span></span><code><span class="n">put</span><span class="o">:</span>
<span class="n">Description</span><span class="o">:</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">object</span><span class="w"> </span><span class="n">into</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">target</span><span class="w"> </span><span class="n">container</span>
<span class="n">Rules</span><span class="o">:</span>
<span class="mi">1</span><span class="o">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">target</span><span class="w"> </span><span class="n">must</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">container</span><span class="w"> </span><span class="o">(</span><span class="n">Container</span><span class="o">)</span>
<span class="mi">2</span><span class="o">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">target</span><span class="w"> </span><span class="n">container</span><span class="w"> </span><span class="n">must</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">open</span>
<span class="mi">3</span><span class="o">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">object</span><span class="w"> </span><span class="n">must</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">inventory</span>
<span class="mi">4</span><span class="o">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">object</span><span class="w"> </span><span class="n">must</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">moveable</span><span class="w"> </span><span class="o">(</span><span class="n">isMoveable</span><span class="o">)</span>
</code></pre></div>

<p>Below is an example of the rule of an object:</p>
<h2>Object Rule Example</h2>
<p>Object: Container
Description: Abstract class for things that can be considered ' containers' (e.g. a drawer, a box, a table, a shelf, etc.)
Properties:</p>
<ul>
<li>A Container is a container.</li>
<li>A Container could be opened (e.g., e.g. a drawer, a door, a box, etc.), or is it always 'open' (e.g. a table, a shelf, etc.).</li>
<li>A Container has a property indicating if it is opened.</li>
<li>A Container has a property indicating the prefix to use when referring to the container (e.g. "in the drawer", "on the table", etc.). By default, the prefix is 'in'.</li>
</ul>
<p>Below is an example of the score rule:</p>
<h2>Score Rule Example</h2>
<p>The player wins the game by getting all dishes clean. The player gets one point for each dish that is cleaned. The player loses one point for each dish that is made dirty.</p>
<p>Below is an example of a game state:</p>
<h2>Game State Example</h2>
<p>| game_state' [[name': 'agent (ID: 0)', 'uuid': 0, 'type': 'Agent', 'properties': ['isContainer': True, 'isMoveable': True, 'isOpenable': False, 'isOpen': True, 'containerPrefix': 'in'), 'contains': ['plate (ID: 5)', 'mug (ID: 6)', 'knife (ID: 7)']], ['name': 'plate (ID: 5)', 'uuid': 5, 'type': 'Dish', 'properties': ['isContainer': True, 'isMoveable': True, ' isOpenable': False, 'isOpen': True, 'containerPrefix': 'on', 'dishType ': 'plate', 'isDirty': True, 'foodMessName': 'orange'], 'contains': [], [ name': 'mug (ID: 6)', 'uuid': 6, 'type': 'Dish', 'properties': [' isContainer': True, 'isMoveable': True, 'isOpenable': False, 'isOpen ': True, 'containerPrefix': 'in', 'dishType': 'mug', 'isDirty': True, ' foodMessName': 'sandwhich'], 'contains': [], ['name': 'knife (ID: 7) ', 'uuid': 7, 'type': 'Dish', 'properties': ['isContainer': True, 'isMoveable': True, 'isOpenable': False, 'isOpen': True, ' containerPrefix': 'in', 'dishType': 'knife', 'isDirty': True, ' foodMessName': 'apple (ID: 11)'], 'contains': [], ['name': ' dishwasher (ID: 2)', 'uuid': 2, 'type': 'DishWasher', 'properties': [' isContainer': True, 'isMoveable': False, 'isOpenable': True, 'isOpen ': True, 'containerPrefix': 'in', 'isDevice': True, 'isActivatable': True, ' isOn': False, 'cycleStage': 0, 'finishedCycle': False], 'contains': [' cup (ID: 4)']], ['name': 'cup (ID: 4)', 'uuid': 4, 'type': 'Dish', 'properties': ['isContainer': True, 'isMoveable': True, 'isOpenable': False, 'isOpen': True, 'containerPrefix': 'in', 'dishType': 'cup', ' isDirty': True, 'foodMessName': 'peanut butter'], 'contains': [], [' name': 'bottle of dish soap (ID: 3)', 'uuid': 3, 'type': 'DishSoapBottle ', 'properties': ['isContainer': False, 'isMoveable': True, 'isDevice': True, 'isActivatable': True, 'isOn': False], 'contains': [], ['name': ' glass (ID: 8)', 'uuid': 8, 'type': 'Dish', 'properties': ['isContainer': True, 'isMoveable': True, 'isOpenable': False, 'isOpen': True, ' containerPrefix': 'in', 'dishType': 'glass', 'isDirty': False], 'contains': [], ['name': 'bowl (ID: 9)', 'uuid': 9, 'type': 'Dish', 'properties': [' isContainer': True, 'isMoveable': True, 'isOpenable': False, 'isOpen ', True, 'containerPrefix': 'in', 'dishType': 'bowl', 'isDirty': False], ' contains': [], ['name': 'banana (ID: 10)', 'uuid': 10, 'type': 'Food', ' properties': ['isContainer': False, 'isMoveable': True, 'isFood': True], 'contains': [], ['score': -1, 'gameOver': False, 'gameWon': False]]]</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Rules</th>
<th style="text-align: center;">State <br> Change</th>
<th style="text-align: center;">$\mathcal{F}$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\mathcal{F}_{\text {set }}$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\mathcal{F}_{\text {set }}$</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Diff</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Diff</td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">Diff</td>
</tr>
<tr>
<td style="text-align: center;">LLM</td>
<td style="text-align: center;">dynamic static</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">2.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">54.0</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">65.9</td>
<td style="text-align: center;">41.8</td>
<td style="text-align: center;">63.1</td>
</tr>
<tr>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">dynamic static</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">0.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">42.3</td>
<td style="text-align: center;">64.7</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">74.2</td>
</tr>
<tr>
<td style="text-align: center;">No rule</td>
<td style="text-align: center;">dynamic static</td>
<td style="text-align: center;">15.4</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">43.8</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">0.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">26.9</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">35.2</td>
<td style="text-align: center;">63.0</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">54.8</td>
</tr>
</tbody>
</table>
<p>Table 5: Average accuracy per game of GPT-3.5 predicting the whole state transitions ( $\mathcal{F}$ ) as well as action-driven transitions ( $\mathcal{F}<em _env="{env" _text="\text">{\text {act }}$ ) and environment-driven transitions ( $\mathcal{F}</em>$ ). We report settings that use LLM generated rules, human written rules, or no rules. Dynamic and static denote whether the game object properties and game progress should be changed; Full and diff denote whether the prediction outcome is the full game state or state differences. Numbers shown in percentage.}</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Rules</th>
<th style="text-align: left;">Game Progress</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LLM</td>
<td style="text-align: left;">73.9</td>
</tr>
<tr>
<td style="text-align: left;">Human</td>
<td style="text-align: left;">63.3</td>
</tr>
<tr>
<td style="text-align: left;">No rule</td>
<td style="text-align: left;">64.2</td>
</tr>
</tbody>
</table>
<p>Table 6: GPT-3.5 game progress prediction results</p>
<p>Below is an example of a JSON that describes the difference of two game states:</p>
<h2>Game State Difference Example</h2>
<p>['modified': [['name': 'agent (ID: 0)', 'uuid': 0, 'type': 'Agent', ' properties': ['isContainer': True, 'isMoveable': True, 'isOpenable': False, 'isOpen': True, 'containerPrefix': 'in'], 'contains': ['mug (ID: 6)', 'knife (ID: 7)']], ['name': 'mug (ID: 6)', 'uuid': 6, 'type': 'Dish', ' properties': ['isContainer': True, 'isMoveable': True, 'isOpenable': False, 'isOpen': True, 'containerPrefix': 'in', 'dishType': 'mug', 'isDirty': True, 'foodMessName': 'sandwhich'], 'contains': ['plate (ID: 5)']]], 'removed': [], 'score': []]</p>
<h2>E GPT-3.5 results</h2>
<p>Table 5 and Table 6 shows the performance of a GPT-3.5 simulator predicting objects properties and game progress respectively. There is a huge gap between the GPT-4 performance and GPT-3.5 performance, providing yet another example of how fast LLM develops in the two years. It is also worth notices that the performance difference is larger when no rules is provided, indicating that GPT-3.5 is especially weak at applying common sense knowledge to this few-shot world simulation task.</p>
<h2>F Histograms</h2>
<ol>
<li>In Figure 3, we show detailed experimental results on the full state prediction task performed by GPT-4.</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align: left;">Property Name</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">buried</td>
<td style="text-align: left;">Objects buried in the room</td>
</tr>
<tr>
<td style="text-align: left;">combustionTimeRemaining</td>
<td style="text-align: left;">Number of time steps remaining to combust of a combusting object</td>
</tr>
<tr>
<td style="text-align: left;">connects</td>
<td style="text-align: left;">Electrical objects connecting to the current object</td>
</tr>
<tr>
<td style="text-align: left;">contains</td>
<td style="text-align: left;">Objects in the current object</td>
</tr>
<tr>
<td style="text-align: left;">cook</td>
<td style="text-align: left;">How an ingredient is cooked</td>
</tr>
<tr>
<td style="text-align: left;">current_aperture</td>
<td style="text-align: left;">Current aperture of a camera</td>
</tr>
<tr>
<td style="text-align: left;">current_focus</td>
<td style="text-align: left;">The object that the camera is currently focusing on</td>
</tr>
<tr>
<td style="text-align: left;">current_iso</td>
<td style="text-align: left;">Current ISO of a camera</td>
</tr>
<tr>
<td style="text-align: left;">current_shutter_speed</td>
<td style="text-align: left;">Current shutter speed of a camera</td>
</tr>
<tr>
<td style="text-align: left;">cut</td>
<td style="text-align: left;">How an ingredient is cut</td>
</tr>
<tr>
<td style="text-align: left;">cycleStage</td>
<td style="text-align: left;">The current stage of the washing machine's cycle (running/washing/finished).</td>
</tr>
<tr>
<td style="text-align: left;">durability</td>
<td style="text-align: left;">Number of times left for a shovel to dig something</td>
</tr>
<tr>
<td style="text-align: left;">finishedCycle</td>
<td style="text-align: left;">A boolean indicator of whether the washing machine has finished</td>
</tr>
<tr>
<td style="text-align: left;">food</td>
<td style="text-align: left;">The food level of a young bird. Reduce 1 if the young bird is not fed at each time step.</td>
</tr>
<tr>
<td style="text-align: left;">grow</td>
<td style="text-align: left;">Number of time steps that a young bird has grown</td>
</tr>
<tr>
<td style="text-align: left;">hatch</td>
<td style="text-align: left;">Number of time steps that an egg is hatched</td>
</tr>
<tr>
<td style="text-align: left;">isAboveMaxTemp</td>
<td style="text-align: left;">Whether the temperature of the current food is above its maximum preservation temperature</td>
</tr>
<tr>
<td style="text-align: left;">isActivated</td>
<td style="text-align: left;">Whether a device is activated</td>
</tr>
<tr>
<td style="text-align: left;">isChoppable</td>
<td style="text-align: left;">Whether an object is choppable</td>
</tr>
<tr>
<td style="text-align: left;">isCombusting</td>
<td style="text-align: left;">Whether an object is combusting</td>
</tr>
<tr>
<td style="text-align: left;">isDirty</td>
<td style="text-align: left;">Whether a dish is dirty</td>
</tr>
<tr>
<td style="text-align: left;">isMoveable</td>
<td style="text-align: left;">Whether the current object is moveable</td>
</tr>
<tr>
<td style="text-align: left;">isOn</td>
<td style="text-align: left;">Whether a device is turned on</td>
</tr>
<tr>
<td style="text-align: left;">isOpen</td>
<td style="text-align: left;">Whether a container is open</td>
</tr>
<tr>
<td style="text-align: left;">isWet</td>
<td style="text-align: left;">Whether a clothes is wet</td>
</tr>
<tr>
<td style="text-align: left;">is_open</td>
<td style="text-align: left;">Whether a door is open</td>
</tr>
<tr>
<td style="text-align: left;">liquid</td>
<td style="text-align: left;">Whether there is liquid in a container</td>
</tr>
<tr>
<td style="text-align: left;">mode</td>
<td style="text-align: left;">Mode of a multimeter</td>
</tr>
<tr>
<td style="text-align: left;">objects</td>
<td style="text-align: left;">Record of the number of time steps that each object is on the inclined plane</td>
</tr>
<tr>
<td style="text-align: left;">on</td>
<td style="text-align: left;">Whether a light bulb is on</td>
</tr>
<tr>
<td style="text-align: left;">photo</td>
<td style="text-align: left;">The object that the camera has taken a picture of</td>
</tr>
<tr>
<td style="text-align: left;">prefix</td>
<td style="text-align: left;">Prefix abstract to describe the object. E.g., a tree and some firewood</td>
</tr>
<tr>
<td style="text-align: left;">stage</td>
<td style="text-align: left;">Life stage of a bird</td>
</tr>
<tr>
<td style="text-align: left;">stateOfMatter</td>
<td style="text-align: left;">State of matter of a substance</td>
</tr>
<tr>
<td style="text-align: left;">sunburn</td>
<td style="text-align: left;">Whether the player's skin is burnt by the sun</td>
</tr>
<tr>
<td style="text-align: left;">temperature</td>
<td style="text-align: left;">Object temperature</td>
</tr>
<tr>
<td style="text-align: left;">tick</td>
<td style="text-align: left;">Number of ticks that an object is placed on an inclined plane</td>
</tr>
<tr>
<td style="text-align: left;">timeAboveMaxTemp</td>
<td style="text-align: left;">Number of time steps that a food is above its maximum preservation temperature</td>
</tr>
<tr>
<td style="text-align: left;">use_sunscreen</td>
<td style="text-align: left;">Whether the player has used the sunscreen</td>
</tr>
<tr>
<td style="text-align: left;">volume</td>
<td style="text-align: left;">Volume of an object</td>
</tr>
<tr>
<td style="text-align: left;">warm</td>
<td style="text-align: left;">The warmth received by an egg during its hatching stage</td>
</tr>
<tr>
<td style="text-align: left;">wearSpaceSuit</td>
<td style="text-align: left;">Whether the agent wears the spacesuit</td>
</tr>
</tbody>
</table>
<p>Table 7: Description of object properties mentioned in Figure 2
2. In Figure 4, we show detailed experimental results on the state difference prediction task performed by GPT-4.
3. In Figure 5, we show detailed experimental results on the full state prediction task performed by GPT-3.5.
4. In Figure 6, we show detailed experimental results on the state difference prediction task performed by GPT-3.5.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: GPT-4 - Full State prediction from a) Human-generated rules, b) LLM-generated rules, and c) No rules.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: GPT-4 - Difference prediction from a) Human-generated rules, b) LLM-generated rules, and c) No rules.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: GPT-3.5 - Full State prediction from a) Human-generated rules, b) LLM-generated rules, and c) No rules.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: GPT-3.5 - Difference prediction from a) Human-generated rules, b) LLM-generated rules, and c) No rules.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ See Appendix E for the results of GPT-3.5.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
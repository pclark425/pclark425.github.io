<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-496 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-496</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-496</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-232146616</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2103.04514v3.pdf" target="_blank">Nondeterminism and Instability in Neural Network Optimization</a></p>
                <p><strong>Paper Abstract:</strong> Nondeterminism in neural network optimization produces uncertainty in performance, making small improvements difficult to discern from run-to-run variability. While uncertainty can be reduced by training multiple model copies, doing so is time-consuming, costly, and harms reproducibility. In this work, we establish an experimental protocol for understanding the effect of optimization nondeterminism on model diversity, allowing us to isolate the effects of a variety of sources of nondeterminism. Surprisingly, we find that all sources of nondeterminism have similar effects on measures of model diversity. To explain this intriguing fact, we identify the instability of model training, taken as an end-to-end procedure, as the key determinant. We show that even one-bit changes in initial parameters result in models converging to vastly different values. Last, we propose two approaches for reducing the effects of instability on run-to-run variability.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e496.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e496.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Nondeterminism Sources</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sources of Nondeterminism in Neural Network Training</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Enumerates and experimentally controls multiple sources of nondeterminism that cause run-to-run variability in model training, including parameter initialization, data shuffling, data augmentation, stochastic regularization, asynchronous training, and low-level library nondeterminism (e.g., cuDNN).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ResNet-14 (image classification) and QRNN (language modeling)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine learning (image classification and language modeling)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Study and quantify run-to-run variability of trained neural networks under controlled variations of nondeterministic factors</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Random parameter initialization; training data shuffling (per-epoch shuffling); data augmentation randomness; stochastic regularization (e.g., dropout, DropConnect, variable-length BPTT); asynchronous parallelism; low-level library nondeterminism (cuDNN non-deterministic kernels); explicit random operations in training code; single-bit changes to initial weights</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Standard deviation of accuracy; standard deviation of cross-entropy; perplexity SD (for language modeling); pairwise disagreement (% of examples where argmax differs); pairwise prediction correlation (Spearman's rho over flattened logits); improvement from ensembling (ensemble metric minus mean of members); centered kernel alignment (CKA) for representation similarity</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Across ResNet-14 CIFAR-10 experiments: Accuracy SD ≈ 0.26% ± 0.02, Cross-Entropy SD ≈ 0.0072 ± 0.0005 (100 runs). For QRNN on Penn Treebank: PPL SD ≈ 0.20 ± 0.01, pairwise disagreement ≈ 17.3%. Different nondeterminism sources produced variability within ~20% (relative) of each other.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Same as variability_metrics above; controlled-seed protocol to isolate sources; comparisons of distributions across runs and seeds</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Different sources (init, shuffle, augmentation, cuDNN) produced similar variability magnitudes; disabling cuDNN nondeterminism reduces that component but other sources still produce similar variability; extremely small perturbations (one-bit change) produced variability comparable to all other sources combined (see Random Bit Change entry)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Multiple independent randomized components; low-level library nondeterminism that cannot be seeded (only toggled); dependence on seed selection and sampling variability; variability covaries with absolute model performance making interpretation harder</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Typically 100 runs (CIFAR-10, QRNN experiments use 100 runs); ImageNet experiments used 20 runs</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Many distinct nondeterministic sources (init, shuffle, augmentation, cuDNN, stochastic ops) produce very similar magnitudes of run-to-run variability in performance and representations; thus reproducibility problems are not dominated solely by parameter initialization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Nondeterminism and Instability in Neural Network Optimization', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e496.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e496.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Protocol for Testing Nondeterminism</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Seed-controlled Protocol to Isolate Effects of Individual Nondeterminism Sources</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An experimental protocol that holds all nondeterministic seeds constant except one, varying that one over R independent runs to measure the isolated effect of that source on performance and representation diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ResNet-14 and QRNN (applied across experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine learning experimentation methodology</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Isolate and quantify the per-source contribution to run-to-run variability by controlled seeding and R repeated runs</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Same list as above; protocol supports toggling deterministic mode for libraries like cuDNN (deterministic vs nondeterministic)</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Standard deviation across R runs for chosen evaluation metric (accuracy, cross-entropy, PPL); pairwise disagreement; pairwise prediction correlation; ensemble improvement; CKA</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Used R = 100 for CIFAR-10 experiments; variability statistics (e.g., accuracy SD 0.26% ±0.02) computed across those runs; protocol shows similar SDs when varying each source independently</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Comparative SDs across controlled seed settings, pairwise statistics between models trained under same source-of-interest</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Protocol demonstrates that individually varying each seed/source yields nearly identical variability profiles, supporting claim that instability amplifies tiny differences</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Full cross-product sampling of all seeds is computationally infeasible, and different choices of held-constant seeds introduce minor sampling differences</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>R typically 100 (experiments commonly use 100 models per tested source)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A simple seed-isolation protocol reliably measures each source's contribution to variability and shows that all tested sources have comparable effects under controlled evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Nondeterminism and Instability in Neural Network Optimization', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e496.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e496.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Variability Metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Set of Quantitative Metrics for Run-to-run Variability and Representation Diversity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Defines a suite of metrics to measure variability in performance and learned representations: per-metric standard deviations, pairwise disagreement, pairwise Spearman correlation over logits, ensemble improvement, and CKA for representation similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ResNet-14 (CIFAR-10) and QRNN (Penn Treebank)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Evaluation methodology in ML</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Measure run-to-run variability and representation diversity across multiple independent trainings</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Accuracy SD (%), Cross-Entropy SD, Perplexity SD (PPL SD), Pairwise Disagreement (% of differing argmax), Pairwise Spearman correlation of flattened logits, Ensemble improvement metric defined as mean over pairs of (f(ensemble) - mean(fmembers)), Linear CKA between intermediate layer representations</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Examples: Accuracy SD = 0.26% ±0.02 and Cross-Entropy SD = 0.0072 ±0.0005 for ResNet-14 single-model baseline (100 runs); QRNN PPL SD = 0.20 ±0.01 (100 runs). Pairwise disagreement ≈ 10.7% (ResNet) and ≈ 17.3% (QRNN).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Same as variability_metrics above</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Metrics reveal comparable magnitudes of variability across multiple nondeterminism sources; CKA shows representational differences are generally small and layer-dependent but consistent across sources</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Typically 100 runs (per experimental condition) for CIFAR-10 and QRNN experiments; 20 runs for ImageNet experiments</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A combination of simple performance SDs and pairwise representation metrics provides a robust view of both behavioral and internal representational variability; these metrics show similar variability magnitudes across different nondeterminism sources.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Nondeterminism and Instability in Neural Network Optimization', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e496.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e496.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instability / Random Bit Change</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Optimization Instability Demonstrated by Single Floating-Point Bit Perturbation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Shows that changing a single bit (least significant floating-point increment/decrement) in one initial weight yields final models with variability comparable to reinitializing the entire network, indicating extreme instability in SGD-like optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ResNet-14 (CIFAR-10); QRNN (Penn Treebank) experiments also tested random-bit perturbations</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Neural network optimization; reproducibility in ML</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Introduce the smallest possible perturbation to initial weights (one-bit change) in repeated runs and measure resulting variability</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Single random bit change to a single weight (initialization perturbation) used as an experimental source of nondeterminism</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Same set as other experiments: Accuracy SD, Cross-Entropy SD, PPL SD, pairwise disagreement, ensemble PPL ∆</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Random bit change produces variability similar to 'All Nondeterminism Sources'; e.g., ResNet-14 width=1.0: Random Bit Change Accuracy SD ≈ 0.21% ± 0.01 vs All Sources Accuracy SD ≈ 0.26% ± 0.02. QRNN PPL SD for Random Bit Change ≈ 0.19 ± 0.01 (comparable to stochastic ops/initialization). Condition numbers estimated: δf/δx ≈ 1.8e7 for cross-entropy and 2.6e8 for accuracy in one example.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Comparison of SDs across controlled-source experiments and random-bit-change runs; time-evolution of divergence during training (epoch-wise SD/range)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Even minimal representational perturbations at initialization lead to rapidly diverging optimization trajectories within a few epochs, and produce final-model variability comparable to that caused by full reinitialization or other nondeterministic sources.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Optimization is highly sensitive (ill-conditioned) end-to-end; small machine-precision changes are amplified by non-linear dynamics of training</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Typically 100 runs for CIFAR-10 experiments; some width/control experiments used N=30</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Neural network training with SGD-like optimizers is unstable: a one-bit change in a single initial weight can create as much run-to-run variability as all commonly-considered nondeterministic sources combined.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Nondeterminism and Instability in Neural Network Optimization', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e496.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e496.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Accelerated Ensembling (Snapshot Ensemble)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Accelerated Model Ensembling via Snapshot Ensembles (Cyclic LR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An accelerated ensembling technique that collects multiple model snapshots from one training run using a cyclic learning rate schedule, reducing run-to-run variability with no extra training cost compared to independent ensembles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ResNet-14 (CIFAR-10); also tested on other ResNets and QRNN</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Model ensembling for robustness and reproducibility in ML</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Use Snapshot Ensemble (5 cycles) to form accelerated ensemble members and measure reduction in variability compared to single and independently-trained ensembles</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Accuracy SD; Cross-Entropy SD; Pairwise disagreement; pairwise correlation; ensemble improvement ∆</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Snapshot accelerated ensembling (Acc. Ens.) on CIFAR-10 (ResNet-14) reduced variability on average by 48% relative to single-model baseline: Accuracy SD lowered from 0.26% ±0.02 to 0.19% ±0.02; Cross-Entropy SD from 0.0072 ±0.0005 to 0.0044 ±0.0003. Accelerated ensemble variability comparable to an ensemble of two independently-trained models.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Same variability metrics above compared across Single Model, standard Ensembles (N=2..20), and Acc. Ens.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Accelerated ensembling substantially reduced measured run-to-run variability without increasing training cost, producing reductions comparable to small standard ensembles (e.g., similar to ensemble of 2 models for some metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Snapshot ensembles can underperform true independent ensembles in absolute performance; in language modeling, snapshot ensembles sometimes had higher average PPL than regular models</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Snapshot Ensembles (cyclic learning rate with snapshots at learning-rate minima)</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Average variability reduction ~48% relative to single-model baseline on CIFAR-10; specific numbers: Accuracy SD from 0.26% to 0.19%; Cross-Entropy SD from 0.0072 to 0.0044</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>100 runs per reported condition (CIFAR-10 experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Accelerated ensembling is an effective, low-cost mitigation that halves much of run-to-run variability, providing variability reductions comparable to small independent ensembles without extra training runs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Nondeterminism and Instability in Neural Network Optimization', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e496.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e496.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Test-Time Augmentation (TTA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Test-Time Data Augmentation (Data-space Ensembling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ensembling predictions over multiple augmented versions of each test input reduces variability by averaging out differences across models and augmentations, acting as a data-space ensemble.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ResNet variants on CIFAR-10 and ImageNet; also applied with QRNN experiments where applicable</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Evaluation and robustness in image classification (and general ML)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Apply horizontal flips and multiple crops at test-time, average predictions, and measure reductions in run-to-run variability</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Accuracy SD; Cross-Entropy SD; pairwise disagreement; pairwise correlation; ensemble improvement ∆</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>On CIFAR-10 (ResNet-14): Simple flip TTA reduced variability by ~21% (Accuracy SD from 0.26% to 0.24% and Cross-Entropy SD from 0.0072 to 0.0061), Flip+Crop81 combined with Acc. Ens. yielded up to ~61% reduction relative to single-model baseline. Different TTA variants yielded 16–37% reductions depending on configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>As above, comparisons across Single, Acc. Ens., and TTA configurations</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>TTA consistently reduced variability across architectures and datasets (CIFAR-10, ImageNet, MNIST), sometimes more effective for pairwise representation metrics and larger models; on MNIST TTA was the main effective mitigation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>TTA increases test-time computation; TTA effectiveness can depend on the dataset and degree of overfitting</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Horizontal flips, multi-crop TTA (25 or 81 crops on CIFAR-10; flip+crop combinations on ImageNet), averaging predictions</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Flip TTA ~21% avg reduction; Crop81 + Flip combined with Acc. Ens. up to ~61% reduction (CIFAR-10 ResNet-14); absolute numbers depend on metric/configuration</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>100 runs per reported condition for CIFAR-10; 20 runs for ImageNet</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>TTA is a broadly effective mitigation reducing run-to-run variability without additional training cost, particularly helpful on some datasets and for pairwise diversity metrics, though it increases inference cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Nondeterminism and Instability in Neural Network Optimization', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e496.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e496.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Failed Mitigations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Approaches Tested but Not Effective at Reducing Variability</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of attempted interventions that did not reduce run-to-run variability while maintaining performance, including varying learning rate or training duration, optimizer choice (momentum/Adam), aggressive SWA variants, gradient clipping, and weight-augmentation-like averaging.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ResNet-14 (CIFAR-10) and variants</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Optimization and reproducibility in ML</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Test multiple interventions hypothesized to improve stability and measure their effect on run-to-run variability</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Accuracy SD; Cross-Entropy SD; pairwise metrics</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>No tested modifications (varying epochs 50–2000, SGD momentum up to 0.999, Adam, ASWA variants, gradient clipping, weight-augmentation sampling) produced significant reductions in variability while preserving performance; sometimes variability increased slightly with extreme training durations</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Comparative SDs across modified training regimes vs baseline</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>None of these approaches yielded notable stability improvements; ASWA failed to converge in original formulation and modified ASWA did not reduce output variance</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Stability appears intrinsic to non-linear optimization trajectories and not trivially mitigated by standard optimizer or hyperparameter changes</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Learning-rate schedule changes, longer/shorter training, optimizer changes (momentum, Adam), aggressive SWA variants, gradient clipping, stochastic weight perturbation averaging</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Multiple experiments with 100-run baselines; some variants tested with similar run counts</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Many intuitive optimization or regularization adjustments do not improve run-to-run reproducibility, suggesting the problem stems from deep instability in non-linear optimization trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Nondeterminism and Instability in Neural Network Optimization', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>On model stability as a function of random seed <em>(Rating: 2)</em></li>
                <li>The impact of nondeterminism on reproducibility in deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Snapshot ensembles: Train 1, get m for free <em>(Rating: 2)</em></li>
                <li>Critical learning periods in deep neural networks <em>(Rating: 1)</em></li>
                <li>The early phase of neural network training <em>(Rating: 1)</em></li>
                <li>Similarity of neural network representations revisited <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-496",
    "paper_id": "paper-232146616",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "Nondeterminism Sources",
            "name_full": "Sources of Nondeterminism in Neural Network Training",
            "brief_description": "Enumerates and experimentally controls multiple sources of nondeterminism that cause run-to-run variability in model training, including parameter initialization, data shuffling, data augmentation, stochastic regularization, asynchronous training, and low-level library nondeterminism (e.g., cuDNN).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ResNet-14 (image classification) and QRNN (language modeling)",
            "model_size": null,
            "scientific_domain": "Machine learning (image classification and language modeling)",
            "experimental_task": "Study and quantify run-to-run variability of trained neural networks under controlled variations of nondeterministic factors",
            "variability_sources": "Random parameter initialization; training data shuffling (per-epoch shuffling); data augmentation randomness; stochastic regularization (e.g., dropout, DropConnect, variable-length BPTT); asynchronous parallelism; low-level library nondeterminism (cuDNN non-deterministic kernels); explicit random operations in training code; single-bit changes to initial weights",
            "variability_measured": true,
            "variability_metrics": "Standard deviation of accuracy; standard deviation of cross-entropy; perplexity SD (for language modeling); pairwise disagreement (% of examples where argmax differs); pairwise prediction correlation (Spearman's rho over flattened logits); improvement from ensembling (ensemble metric minus mean of members); centered kernel alignment (CKA) for representation similarity",
            "variability_results": "Across ResNet-14 CIFAR-10 experiments: Accuracy SD ≈ 0.26% ± 0.02, Cross-Entropy SD ≈ 0.0072 ± 0.0005 (100 runs). For QRNN on Penn Treebank: PPL SD ≈ 0.20 ± 0.01, pairwise disagreement ≈ 17.3%. Different nondeterminism sources produced variability within ~20% (relative) of each other.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Same as variability_metrics above; controlled-seed protocol to isolate sources; comparisons of distributions across runs and seeds",
            "reproducibility_results": "Different sources (init, shuffle, augmentation, cuDNN) produced similar variability magnitudes; disabling cuDNN nondeterminism reduces that component but other sources still produce similar variability; extremely small perturbations (one-bit change) produced variability comparable to all other sources combined (see Random Bit Change entry)",
            "reproducibility_challenges": "Multiple independent randomized components; low-level library nondeterminism that cannot be seeded (only toggled); dependence on seed selection and sampling variability; variability covaries with absolute model performance making interpretation harder",
            "mitigation_methods": null,
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": true,
            "number_of_runs": "Typically 100 runs (CIFAR-10, QRNN experiments use 100 runs); ImageNet experiments used 20 runs",
            "key_findings": "Many distinct nondeterministic sources (init, shuffle, augmentation, cuDNN, stochastic ops) produce very similar magnitudes of run-to-run variability in performance and representations; thus reproducibility problems are not dominated solely by parameter initialization.",
            "uuid": "e496.0",
            "source_info": {
                "paper_title": "Nondeterminism and Instability in Neural Network Optimization",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "Protocol for Testing Nondeterminism",
            "name_full": "Seed-controlled Protocol to Isolate Effects of Individual Nondeterminism Sources",
            "brief_description": "An experimental protocol that holds all nondeterministic seeds constant except one, varying that one over R independent runs to measure the isolated effect of that source on performance and representation diversity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ResNet-14 and QRNN (applied across experiments)",
            "model_size": null,
            "scientific_domain": "Machine learning experimentation methodology",
            "experimental_task": "Isolate and quantify the per-source contribution to run-to-run variability by controlled seeding and R repeated runs",
            "variability_sources": "Same list as above; protocol supports toggling deterministic mode for libraries like cuDNN (deterministic vs nondeterministic)",
            "variability_measured": true,
            "variability_metrics": "Standard deviation across R runs for chosen evaluation metric (accuracy, cross-entropy, PPL); pairwise disagreement; pairwise prediction correlation; ensemble improvement; CKA",
            "variability_results": "Used R = 100 for CIFAR-10 experiments; variability statistics (e.g., accuracy SD 0.26% ±0.02) computed across those runs; protocol shows similar SDs when varying each source independently",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Comparative SDs across controlled seed settings, pairwise statistics between models trained under same source-of-interest",
            "reproducibility_results": "Protocol demonstrates that individually varying each seed/source yields nearly identical variability profiles, supporting claim that instability amplifies tiny differences",
            "reproducibility_challenges": "Full cross-product sampling of all seeds is computationally infeasible, and different choices of held-constant seeds introduce minor sampling differences",
            "mitigation_methods": null,
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": true,
            "number_of_runs": "R typically 100 (experiments commonly use 100 models per tested source)",
            "key_findings": "A simple seed-isolation protocol reliably measures each source's contribution to variability and shows that all tested sources have comparable effects under controlled evaluation.",
            "uuid": "e496.1",
            "source_info": {
                "paper_title": "Nondeterminism and Instability in Neural Network Optimization",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "Variability Metrics",
            "name_full": "Set of Quantitative Metrics for Run-to-run Variability and Representation Diversity",
            "brief_description": "Defines a suite of metrics to measure variability in performance and learned representations: per-metric standard deviations, pairwise disagreement, pairwise Spearman correlation over logits, ensemble improvement, and CKA for representation similarity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ResNet-14 (CIFAR-10) and QRNN (Penn Treebank)",
            "model_size": null,
            "scientific_domain": "Evaluation methodology in ML",
            "experimental_task": "Measure run-to-run variability and representation diversity across multiple independent trainings",
            "variability_sources": null,
            "variability_measured": true,
            "variability_metrics": "Accuracy SD (%), Cross-Entropy SD, Perplexity SD (PPL SD), Pairwise Disagreement (% of differing argmax), Pairwise Spearman correlation of flattened logits, Ensemble improvement metric defined as mean over pairs of (f(ensemble) - mean(fmembers)), Linear CKA between intermediate layer representations",
            "variability_results": "Examples: Accuracy SD = 0.26% ±0.02 and Cross-Entropy SD = 0.0072 ±0.0005 for ResNet-14 single-model baseline (100 runs); QRNN PPL SD = 0.20 ±0.01 (100 runs). Pairwise disagreement ≈ 10.7% (ResNet) and ≈ 17.3% (QRNN).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Same as variability_metrics above",
            "reproducibility_results": "Metrics reveal comparable magnitudes of variability across multiple nondeterminism sources; CKA shows representational differences are generally small and layer-dependent but consistent across sources",
            "reproducibility_challenges": null,
            "mitigation_methods": null,
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": true,
            "number_of_runs": "Typically 100 runs (per experimental condition) for CIFAR-10 and QRNN experiments; 20 runs for ImageNet experiments",
            "key_findings": "A combination of simple performance SDs and pairwise representation metrics provides a robust view of both behavioral and internal representational variability; these metrics show similar variability magnitudes across different nondeterminism sources.",
            "uuid": "e496.2",
            "source_info": {
                "paper_title": "Nondeterminism and Instability in Neural Network Optimization",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "Instability / Random Bit Change",
            "name_full": "Optimization Instability Demonstrated by Single Floating-Point Bit Perturbation",
            "brief_description": "Shows that changing a single bit (least significant floating-point increment/decrement) in one initial weight yields final models with variability comparable to reinitializing the entire network, indicating extreme instability in SGD-like optimization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ResNet-14 (CIFAR-10); QRNN (Penn Treebank) experiments also tested random-bit perturbations",
            "model_size": null,
            "scientific_domain": "Neural network optimization; reproducibility in ML",
            "experimental_task": "Introduce the smallest possible perturbation to initial weights (one-bit change) in repeated runs and measure resulting variability",
            "variability_sources": "Single random bit change to a single weight (initialization perturbation) used as an experimental source of nondeterminism",
            "variability_measured": true,
            "variability_metrics": "Same set as other experiments: Accuracy SD, Cross-Entropy SD, PPL SD, pairwise disagreement, ensemble PPL ∆",
            "variability_results": "Random bit change produces variability similar to 'All Nondeterminism Sources'; e.g., ResNet-14 width=1.0: Random Bit Change Accuracy SD ≈ 0.21% ± 0.01 vs All Sources Accuracy SD ≈ 0.26% ± 0.02. QRNN PPL SD for Random Bit Change ≈ 0.19 ± 0.01 (comparable to stochastic ops/initialization). Condition numbers estimated: δf/δx ≈ 1.8e7 for cross-entropy and 2.6e8 for accuracy in one example.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Comparison of SDs across controlled-source experiments and random-bit-change runs; time-evolution of divergence during training (epoch-wise SD/range)",
            "reproducibility_results": "Even minimal representational perturbations at initialization lead to rapidly diverging optimization trajectories within a few epochs, and produce final-model variability comparable to that caused by full reinitialization or other nondeterministic sources.",
            "reproducibility_challenges": "Optimization is highly sensitive (ill-conditioned) end-to-end; small machine-precision changes are amplified by non-linear dynamics of training",
            "mitigation_methods": null,
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": true,
            "number_of_runs": "Typically 100 runs for CIFAR-10 experiments; some width/control experiments used N=30",
            "key_findings": "Neural network training with SGD-like optimizers is unstable: a one-bit change in a single initial weight can create as much run-to-run variability as all commonly-considered nondeterministic sources combined.",
            "uuid": "e496.3",
            "source_info": {
                "paper_title": "Nondeterminism and Instability in Neural Network Optimization",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "Accelerated Ensembling (Snapshot Ensemble)",
            "name_full": "Accelerated Model Ensembling via Snapshot Ensembles (Cyclic LR)",
            "brief_description": "An accelerated ensembling technique that collects multiple model snapshots from one training run using a cyclic learning rate schedule, reducing run-to-run variability with no extra training cost compared to independent ensembles.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ResNet-14 (CIFAR-10); also tested on other ResNets and QRNN",
            "model_size": null,
            "scientific_domain": "Model ensembling for robustness and reproducibility in ML",
            "experimental_task": "Use Snapshot Ensemble (5 cycles) to form accelerated ensemble members and measure reduction in variability compared to single and independently-trained ensembles",
            "variability_sources": null,
            "variability_measured": true,
            "variability_metrics": "Accuracy SD; Cross-Entropy SD; Pairwise disagreement; pairwise correlation; ensemble improvement ∆",
            "variability_results": "Snapshot accelerated ensembling (Acc. Ens.) on CIFAR-10 (ResNet-14) reduced variability on average by 48% relative to single-model baseline: Accuracy SD lowered from 0.26% ±0.02 to 0.19% ±0.02; Cross-Entropy SD from 0.0072 ±0.0005 to 0.0044 ±0.0003. Accelerated ensemble variability comparable to an ensemble of two independently-trained models.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Same variability metrics above compared across Single Model, standard Ensembles (N=2..20), and Acc. Ens.",
            "reproducibility_results": "Accelerated ensembling substantially reduced measured run-to-run variability without increasing training cost, producing reductions comparable to small standard ensembles (e.g., similar to ensemble of 2 models for some metrics).",
            "reproducibility_challenges": "Snapshot ensembles can underperform true independent ensembles in absolute performance; in language modeling, snapshot ensembles sometimes had higher average PPL than regular models",
            "mitigation_methods": "Snapshot Ensembles (cyclic learning rate with snapshots at learning-rate minima)",
            "mitigation_effectiveness": "Average variability reduction ~48% relative to single-model baseline on CIFAR-10; specific numbers: Accuracy SD from 0.26% to 0.19%; Cross-Entropy SD from 0.0072 to 0.0044",
            "comparison_with_without_controls": true,
            "number_of_runs": "100 runs per reported condition (CIFAR-10 experiments)",
            "key_findings": "Accelerated ensembling is an effective, low-cost mitigation that halves much of run-to-run variability, providing variability reductions comparable to small independent ensembles without extra training runs.",
            "uuid": "e496.4",
            "source_info": {
                "paper_title": "Nondeterminism and Instability in Neural Network Optimization",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "Test-Time Augmentation (TTA)",
            "name_full": "Test-Time Data Augmentation (Data-space Ensembling)",
            "brief_description": "Ensembling predictions over multiple augmented versions of each test input reduces variability by averaging out differences across models and augmentations, acting as a data-space ensemble.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ResNet variants on CIFAR-10 and ImageNet; also applied with QRNN experiments where applicable",
            "model_size": null,
            "scientific_domain": "Evaluation and robustness in image classification (and general ML)",
            "experimental_task": "Apply horizontal flips and multiple crops at test-time, average predictions, and measure reductions in run-to-run variability",
            "variability_sources": null,
            "variability_measured": true,
            "variability_metrics": "Accuracy SD; Cross-Entropy SD; pairwise disagreement; pairwise correlation; ensemble improvement ∆",
            "variability_results": "On CIFAR-10 (ResNet-14): Simple flip TTA reduced variability by ~21% (Accuracy SD from 0.26% to 0.24% and Cross-Entropy SD from 0.0072 to 0.0061), Flip+Crop81 combined with Acc. Ens. yielded up to ~61% reduction relative to single-model baseline. Different TTA variants yielded 16–37% reductions depending on configuration.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "As above, comparisons across Single, Acc. Ens., and TTA configurations",
            "reproducibility_results": "TTA consistently reduced variability across architectures and datasets (CIFAR-10, ImageNet, MNIST), sometimes more effective for pairwise representation metrics and larger models; on MNIST TTA was the main effective mitigation",
            "reproducibility_challenges": "TTA increases test-time computation; TTA effectiveness can depend on the dataset and degree of overfitting",
            "mitigation_methods": "Horizontal flips, multi-crop TTA (25 or 81 crops on CIFAR-10; flip+crop combinations on ImageNet), averaging predictions",
            "mitigation_effectiveness": "Flip TTA ~21% avg reduction; Crop81 + Flip combined with Acc. Ens. up to ~61% reduction (CIFAR-10 ResNet-14); absolute numbers depend on metric/configuration",
            "comparison_with_without_controls": true,
            "number_of_runs": "100 runs per reported condition for CIFAR-10; 20 runs for ImageNet",
            "key_findings": "TTA is a broadly effective mitigation reducing run-to-run variability without additional training cost, particularly helpful on some datasets and for pairwise diversity metrics, though it increases inference cost.",
            "uuid": "e496.5",
            "source_info": {
                "paper_title": "Nondeterminism and Instability in Neural Network Optimization",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "Failed Mitigations",
            "name_full": "Approaches Tested but Not Effective at Reducing Variability",
            "brief_description": "A set of attempted interventions that did not reduce run-to-run variability while maintaining performance, including varying learning rate or training duration, optimizer choice (momentum/Adam), aggressive SWA variants, gradient clipping, and weight-augmentation-like averaging.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ResNet-14 (CIFAR-10) and variants",
            "model_size": null,
            "scientific_domain": "Optimization and reproducibility in ML",
            "experimental_task": "Test multiple interventions hypothesized to improve stability and measure their effect on run-to-run variability",
            "variability_sources": null,
            "variability_measured": true,
            "variability_metrics": "Accuracy SD; Cross-Entropy SD; pairwise metrics",
            "variability_results": "No tested modifications (varying epochs 50–2000, SGD momentum up to 0.999, Adam, ASWA variants, gradient clipping, weight-augmentation sampling) produced significant reductions in variability while preserving performance; sometimes variability increased slightly with extreme training durations",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Comparative SDs across modified training regimes vs baseline",
            "reproducibility_results": "None of these approaches yielded notable stability improvements; ASWA failed to converge in original formulation and modified ASWA did not reduce output variance",
            "reproducibility_challenges": "Stability appears intrinsic to non-linear optimization trajectories and not trivially mitigated by standard optimizer or hyperparameter changes",
            "mitigation_methods": "Learning-rate schedule changes, longer/shorter training, optimizer changes (momentum, Adam), aggressive SWA variants, gradient clipping, stochastic weight perturbation averaging",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": true,
            "number_of_runs": "Multiple experiments with 100-run baselines; some variants tested with similar run counts",
            "key_findings": "Many intuitive optimization or regularization adjustments do not improve run-to-run reproducibility, suggesting the problem stems from deep instability in non-linear optimization trajectories.",
            "uuid": "e496.6",
            "source_info": {
                "paper_title": "Nondeterminism and Instability in Neural Network Optimization",
                "publication_date_yy_mm": "2021-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "On model stability as a function of random seed",
            "rating": 2,
            "sanitized_title": "on_model_stability_as_a_function_of_random_seed"
        },
        {
            "paper_title": "The impact of nondeterminism on reproducibility in deep reinforcement learning",
            "rating": 2,
            "sanitized_title": "the_impact_of_nondeterminism_on_reproducibility_in_deep_reinforcement_learning"
        },
        {
            "paper_title": "Snapshot ensembles: Train 1, get m for free",
            "rating": 2,
            "sanitized_title": "snapshot_ensembles_train_1_get_m_for_free"
        },
        {
            "paper_title": "Critical learning periods in deep neural networks",
            "rating": 1,
            "sanitized_title": "critical_learning_periods_in_deep_neural_networks"
        },
        {
            "paper_title": "The early phase of neural network training",
            "rating": 1,
            "sanitized_title": "the_early_phase_of_neural_network_training"
        },
        {
            "paper_title": "Similarity of neural network representations revisited",
            "rating": 1,
            "sanitized_title": "similarity_of_neural_network_representations_revisited"
        }
    ],
    "cost": 0.01565825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Nondeterminism and Instability in Neural Network Optimization</p>
<p>Cecilia Summers 
Michael J Dinneen 
Nondeterminism and Instability in Neural Network Optimization</p>
<p>Nondeterminism in neural network optimization produces uncertainty in performance, making small improvements difficult to discern from runto-run variability. While uncertainty can be reduced by training multiple model copies, doing so is time-consuming, costly, and harms reproducibility. In this work, we establish an experimental protocol for understanding the effect of optimization nondeterminism on model diversity, allowing us to isolate the effects of a variety of sources of nondeterminism. Surprisingly, we find that all sources of nondeterminism have similar effects on measures of model diversity. To explain this intriguing fact, we identify the instability of model training, taken as an end-to-end procedure, as the key determinant. We show that even onebit changes in initial parameters result in models converging to vastly different values. Last, we propose two approaches for reducing the effects of instability on run-to-run variability.</p>
<p>Introduction</p>
<p>Consider this common scenario: you have a baseline "current best" model, and are trying to improve it. One of your experiments has produced a model whose metrics are slightly better than the baseline. Yet you have your reservations -how do you know the improvement is "real" and not due to run-to-run variability?</p>
<p>Similarly, consider hyperparameter optimization, in which many possible values exist for a set of hyperparameters, with minor differences in performance between them. How do you pick the best hyperparameters, and how can you be sure that you've actually picked wisely?</p>
<p>In both scenarios, the standard practice is to train multiple independent copies of your model to understand its variability. While this helps address the problem, it is extremely 1 Department of Computer Science, University of Auckland, Auckland, New Zealand. Correspondence to: Cecilia Summers <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#99;&#101;&#99;&#105;&#108;&#105;&#97;&#46;&#115;&#117;&#109;&#109;&#101;&#114;&#115;&#46;&#48;&#55;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;">&#99;&#101;&#99;&#105;&#108;&#105;&#97;&#46;&#115;&#117;&#109;&#109;&#101;&#114;&#115;&#46;&#48;&#55;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;</a>.</p>
<p>Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s). wasteful, using more computing power, increasing the time required for effective research, and making reproducibility difficult, all while still leaving some uncertainty.</p>
<p>Ultimately, the source of this problem is nondeterminism in model optimization -randomized components of model training that cause each run to produce different models with their own performance characteristics. Nondeterminism itself occurs due to many factors: while the most salient source is the random initialization of parameters, other sources exist, including random shuffling of training data, stochasticity in data augmentation, explicit random operations (e.g. dropout (Srivastava et al., 2014)), asynchronous training (Recht et al., 2011), and even nondeterminism in low-level libraries such as cuDNN (Chetlur et al., 2014).</p>
<p>Despite the clear impact nondeterminism has on the efficacy of modeling, relatively little attention has been paid towards understanding its mechanisms. In this work, we establish an experimental protocol for analyzing the impact of nondeterminism in model training, allowing us to quantify the independent effect of each source of nondeterminism. In doing so, we make a surprising discovery: each source has nearly the same effect on the variability of final model performance. Further, we find each source produces models of similar diversity, as measured by correlations between model predictions, functional changes in model performance while ensembling, and state-of-the-art methods of model similarity (Kornblith et al., 2019). To emphasize one particularly interesting result: nondeterminism in low-level libraries like cuDNN can matter just as much with respect to model diversity and variability as varying the entire network initialization.</p>
<p>We explain this mystery by demonstrating that it can be attributed to instability in optimizing neural networkswhen training with SGD-like approaches, we show that small changes to initial parameters result in large changes to final parameter values. In fact, the instabilities in the optimization process are extreme: changing the initialization of a single weight by the smallest possible amount within machine precision (∼6 · 10 −11 ) produces nearly as much variability as all other sources combined. Therefore, any source of nondeterminism with any effect at all on model weights inherits at least this level of variability.</p>
<p>Last, we present promising results in reducing the effects of arXiv:2103.04514v3 [cs.LG] 10 Jul 2021 instability on run-to-run variability. While we find that many approaches result in no apparent change, we propose and demonstrate two approaches that reduce model variability without any increase in model training time: accelerated model ensembling and test-time augmentation. Together, these provide the first encouraging signs for the tractability of this problem. Code has been made publicly available. 1</p>
<p>Related Work</p>
<p>Nondeterminism. Relatively little prior work has studied the effects of nondeterminism on model optimization. While nondeterminism is recognized as a significant barrier to reproducibility and evaluating progress in some subfields of machine learning, such as reinforcement learning (Nagarajan et al., 2018;Henderson et al., 2018;Islam et al., 2017;Machado et al., 2018), in the setting of supervised learning, the focus of this work, the problem is much less studied. Madhyastha and Jain (Madhyastha &amp; Jain, 2019) aggregate all sources of nondeterminism together into a single random seed and analyze the variability of model attention and accuracy across various NLP datasets. They also propose a method for reducing this variability (see Appendix Sec. E for details of our reproduction attempt). More common in the field, results across multiple random seeds are reported (Erhan et al., 2010), but the precise nature of nondeterminism's influence on variability goes unstudied.</p>
<p>Instability. We use the term "stability" in a manner analogous to numerical stability (Higham, 2002), where a stable algorithm is one for which the final output (converged model) does not vary much as the input (initial parameters) are changed. In other contexts, the term "stability" has been used both in learning theory (Bousquet &amp; Elisseeff, 2002) and in reference to vanishing and exploding gradients (Haber &amp; Ruthotto, 2017).</p>
<p>Nondeterminism</p>
<p>Many sources of nondeterminism exist in neural network optimization, each of which affects the variability of trained models. We begin with a very brief overview:</p>
<p>Parameter Initialization. When training a model, parameters without preset values are initialized randomly according to a given distribution, e.g. a zero-mean Gaussian with variance determined by the number of input connections to the layer (Glorot &amp; Bengio, 2010;He et al., 2015).</p>
<p>Data Shuffling. In stochastic gradient descent, the gradient is approximated on a random subset of examples, com-1 https://github.com/ceciliaresearch/ nondeterminism_instability monly implemented by using small batches of data iteratively in a shuffled training dataset (Bottou, 2012). Shuffling may happen either once, before training, or in between each epoch of training, the variant we use in this work.</p>
<p>Data Augmentation. A common practice, data augmentation refers to randomly altering each training example to artificially expand the training dataset (Shorten &amp; Khoshgoftaar, 2019). For example, randomly flipping images encourages invariance to left/right orientation.</p>
<p>Stochastic Regularization. Some types of regularization, such as Dropout (Srivastava et al., 2014), take the form of stochastic operations internal to a model during training. Other instances of this include DropConnect (Wan et al., 2013) and variable length backpropagation through time (Merity et al., 2017), among many others.</p>
<p>Low-level Operations. Often underlooked, many libraries that deep learning frameworks are built on, such as cuDNN (Chetlur et al., 2014), typically run nondeterministically in order to increase the speed of their operations. This nondeterminism is small when evaluated in the context of a single operation -in one test we performed it caused an output difference of 0.003%. In the case of cuDNN, the library we test, it is possible to disable nondeterministic behavior at a speed penalty on the order of ∼15%. However, unlike other nondeterminism sources, it is not possible to "seed" this; it is only possible to turn it on or off.</p>
<p>Protocol for Testing Effects of Nondeterminism</p>
<p>Performance Variability. Our protocol for testing the effects of sources of nondeterminism is based on properly controlling for each source. Formally, suppose there are N sources of nondeterminism, with source i controlled by seed S i . To test the effect of source i, we keep all values {S j } j =i set to a constant, and vary S i with R different values, where R is the number of independent training runs performed. For sources of nondeterminism which cannot be effectively seeded, such as cuDNN, we indicate one of these values as the deterministic value, which it must be set to when varying the other sources of nondeterminism.</p>
<p>For example, denote S 1 the seed for random parameter initialization, S 2 for training data shuffling, and S 3 for cuDNN, where S 3 = 1 is the deterministic value for cuDNN. To test the effect of random parameter initialization, with a budget of R = 100 training runs, we set S 3 to the deterministic value of 1, S 2 to an arbitrary constant (typically 1 for simplicity), and test 100 different values of S 1 . All together, this corresponds to training models for each of (S 1 , S 2 , S 3 ) ∈ {(i, 1, 1)} 100 i=1 . To measure variability of a particular evaluation metric (e.g. cross-entropy or accuracy for classification), we calculate the standard devia-tion (across all R = 100 models) of the metric. Note that it is also possible to test the effect of several sources of nondeterminism in tandem this way, e.g. by considering
(S 1 , S 2 , S 3 ) ∈ {(i, i, 0)} R i=1
to measure the joint effect of all three sources in this example.</p>
<p>Representation Diversity. We also examine differences in the representation of trained models, complementary to variability in test set performance -this allows us to differentiate cases where two sources of nondeterminism have similar performance variability but actually produce models with disparate amounts of representational similarity. In order to rigorously examine this, we consider four distinct analyses of the functional behavior of models:</p>
<p>The first and simplest metric we consider is the average disagreement between pairs of models, with higher disagreement corresponding to higher diversity and variability. In contrast to our other metrics, this considers only the argmax of a model's predictions, which makes it the most limited but also the most interpretable of the group. This metric has also been used recently to compare similarity in the context of network ensembles (Fort et al., 2019).</p>
<p>Second, we consider the average correlation between the predictions of two models, i.e. the expectation (across pairs of models from the same nondeterminism source) of the correlation of predictions, calculated across examples and classes. Concretely, for a classification task, the predicted logits from each of R models are flattened into vectors of length N * C (with N test examples and C classes), and we calculate the mean correlation coefficient of the predictions across all R 2 pairs of models. We use Spearman's ρ for the correlation coefficient, but note that other metrics are possible and yield similar conclusions. For this metric, a lower score indicates a more diverse set of models.</p>
<p>The third analysis we perform examines the change in performance in ensembling two models from the same source of nondeterminism. Intuitively, if a pair of models are completely redundant, then ensembling them would result in no change in performance. However, if models actually learn different representations, then ensembling should create an improvement, with a greater improvement the greater the diversity in a set of models. Denoting by f (S i ) some particular evaluation metric f calculated on the predictions of model S i , and</p>
<p>Si+Sj 2 the ensemble of models S i and S j , this metric is formally determined by:
1 R 2 R i=1 R j=i+1 f S i + S j 2 − f (S i ) + f (S j ) 2(1)
Last, for a more detailed view of learned representations internal to a network, we consider a state-of-the-art method for measuring the similarity of neural network representations, centered kernel alignment (CKA) (Kornblith et al., 2019), which has previously been used to analyze models trained with different random initializations, widths, and even entirely different architectures. We use the linear version of CKA, which Kornblith et al. found to perform similarly to more complicated RBF kernels.</p>
<p>Experiments in Image Classification</p>
<p>We begin our study of nondeterminism with the fundamental task of image classification. We execute our protocol with CIFAR-10 (Krizhevsky et al., 2009) as a testbed, a 10-way classification dataset with 50,000 training images of resolution 32 × 32 pixels and 10,000 images for testing. In these initial experiments, we use a 14-layer ResNet model (He et al., 2016), trained with a cosine learning rate decay (Loshchilov &amp; Hutter, 2016) for 500 epochs with a maximum learning rate of .40, three epochs of linear learning rate warmup, a batch size of 512, momentum of 0.9, and weight decay of 5 · 10 −4 , obtaining a baseline accuracy of 90.0%. Data augmentation consists of random crops and horizontal flips. All experiments were done on two NVIDIA Tesla V100 GPUs with pytorch (Paszke et al., 2019).</p>
<p>We show the results of our protocol in this setting in Table 1. Across all measures of performance variability and representation diversity, what we find is surprising and clear -while there are slight differences, each source of nondeterminism has very similar effects on the variability of final trained models. In fact, random parameter initialization, arguably the form of nondeterminism that variability in performance is most commonly attributed to, does not stand out based on any metric, and even combinations of multiple sources of nondeterminism produce remarkably little difference -all are within a maximum of 20% (relative) of each other.</p>
<p>Turning toward CKA and representational diversity on a per-layer level, we plot average CKA values across 6 representative layers in Fig. 1, done for pairwise combinations of 25 models (due to the cost of CKA). Consistent with other analyses, CKA reveals that while some differences in representational similarity exist between nondeterminism sources, particularly in the output of the first residual block, by and large these differences are small, easily dwarfed in size by representational differences across layers.</p>
<p>Experiments in Language Modeling</p>
<p>Here we show that this phenomenon is not unique to image classification by applying the same experimental protocol to language modeling. For these experiments, we employ a small quasi-recurrent neural network (QRNN) (Bradbury et al., 2016)   For this task, two sources of nondeterminism are relevant: random parameter initialization, and stochastic operations, including a variation of dropout and variable length backpropagation through time, which share a common seed. To measure performance variability, PPL is the most widelyaccepted metric, and for diversity in representation we focus on only two metrics (pairwise disagreement and benefits from ensembling) because CKA was not designed for variable-length input and standard computing libraries (Virtanen et al., 2020) are not efficient enough to calculate O(R 2 ) correlation coefficients with such large inputs.</p>
<p>We show results in Table 2, where we find almost no difference across all diversity metrics, showing the phenomenon generalizes beyond image classification and ResNets.</p>
<p>Nondeterminism Throughout Training</p>
<p>One hypothesis for the this phenomenon's cause is the sensitivity of optimization in the initial phase of learning, which recent work has demonstrated in other contexts (Achille et al., 2019;Frankle et al., 2020). With our experimental protocol, this is straightforward to test: If this were the case, then training models identically for the first N epochs and only then introducing nondeterminism would result in significantly less variability in final trained models, measured across all metrics. Furthermore, by varying N , we can actually determine when in training each source of nondeterminism has its effect (for sources that vary over the course of training, i.e. not random parameter initialization).</p>
<p>We perform this experiment for the ResNet-14 model on CIFAR-10 in Fig. 2, where we find that the beginning of training is not particularly sensitive to nondeterminism. Instead, model variability is nearly as high when enabling nondeterminism even after 50 epochs, and we see only a gradual reduction in final model variability as the onset of nondeterminism is moved later and later.</p>
<p>Instability</p>
<p>Why does each source of nondeterminism have similar effects on model variability? We approach this question by finding the smallest possible change that produces the same amount of variability. In doing so, we find that only an extremely tiny change is necessary, thereby demonstrating the instability in optimizing neural networks. </p>
<p>Instability and Nondeterminism</p>
<p>To demonstrate, we perform a simple experiment: First we deterministically train a simple ResNet-14 model on CIFAR-10, achieving a test cross-entropy of 0.3519 and accuracy of 90.0%. Then, we train another model in an identical fashion, with exactly equal settings for all sources of nondeterminism, but one extremely small change: we randomly pick a single weight in the first layer and change its value by the smallest possible amount in a 32-bit floating point representation, i.e. an addition or subtraction of a single bit in the least-significant digit. As an example, this could change a value from −0.0066514308 to −0.0066514313, a difference on the order of 5 · 10 −10 . Viewing the optimization process end-to-end, with the initial parameters as the input and a given performance metric as the output, this demonstrates a condition number δf δx of 1.8 · 10 7 for cross-entropy and 2.6 · 10 8 for accuracy.</p>
<p>We can more rigorously test this using our protocol from Sec. 3 -this time, our source of nondeterminism is randomly picking a different weight to change in each model training run, then either incrementing or decrementing it to the next available floating-point value. We show the results in Table 3 for image classification on CIFAR-10 (c.f. Table 1 for comparison) and Table 4 for language modeling on Penn Treebank (c.f. Table 2), where we find that even this small change produces roughly as much variability in model performance as every other source of nondeterminism.</p>
<p>From this, it is easy to see why every other source of nondeterminism has similar effects -so long as nondeterminism produces any change in model weights, whether by changing the input slightly, altering the gradient in some way, or any other effect, it will produce at least as much model variability as caused by the instability of model optimization.</p>
<p>Instability and Depth</p>
<p>Instability occurs in networks of more than a single layer.</p>
<p>Due to convexity, linear models optimized with a crossentropy loss and an appropriate learning rate schedule always converge to a global minimum. However, in practice we find an even stronger property: when initial weights are modified by a single bit, beyond simply converging to the same final value, the entire optimization trajectory stays close to that of an unperturbed model, never differing by more than a vanishingly small amount. At convergence, a set of linear models trained in this way with only single random bit changes had a final accuracy SD of 0 (i.e. no changes in any test set predictions) and cross-entropy SD of ∼1 · 10 −7 , far below that of any deeper model.</p>
<p>In contrast, instability occurs as soon as a single hidden layer was added, with an accuracy SD of 0.28 and crossentropy SD of 0.0051 for a model whose hidden layer is fully-connected, and an accuracy SD of 0.14 and crossentropy SD of 0.0022 when the hidden layer is convolutional, both a factor of 10,000 greater than the linear model. See Appendix Secs. A and B for full details and a visualization of the effects of instability during training.</p>
<p>Reducing Variability</p>
<p>Here we identify and demonstrate two approaches that partially mitigate the variability caused by nondeterminism and instability. See the Appendix Sec. E for learnings on approaches which were unsuccessful in reducing variability.</p>
<p>Accelerated Ensembling. As previously mentioned, the standard practice for mitigating run-to-run variability is to train multiple independent copies of a model, gaining a more robust performance estimate by measuring a metric of interest over multiple trials. Ensembling is a similar alternative approach, which shares the intuition of multiple independent training runs, but differs in that the predictions themselves are averaged and the performance of the ensembled model  To that end, we propose the use of recent accelerated ensembling techniques to reduce variability. Accelerated ensembling is a new research direction in which only one training run is needed (Huang et al., 2017;Garipov et al., 2018;Wen et al., 2020). While such techniques typically underperform ensembles composed out of truly independent models, the nature of their accelerated training can reduce variability without incurring additional cost during training. The approach we focus on is the Snapshot Ensemble (Huang et al., 2017), which uses a cyclic learning rate schedule, creating the members of an ensemble out of models where the learning rate is 0 in the cyclic learning rate schedule.</p>
<p>In Table 5 (bottom), we compare a snapshot ensemble ("Acc. Ens.") with 5 cycles in its learning rate (i.e. model snapshots are taken after every 100 epochs of training) to ordinary ensembling on CIFAR-10 with all sources of nondeterminism enabled. Despite training only a single model, the accelerated ensemble had variability in accuracy and cross-entropy comparable to an ensemble of two independently-trained models, with other metrics comparable to those of even larger ensembles. Across measures, accelerated ensembling reduces variability by an average of 48% relative.</p>
<p>Test-Time Data Augmentation. Test-time data augmentation (TTA) is the practice of augmenting test set examples using data augmentation, averaging model predictions made on each augmented example, and is typically used to improve generalization (Szegedy et al., 2015). Beyond improved generalization, though, TTA can be thought of as a form of ensembling in data-space (as opposed to the modelspace averaging of standard ensembling), giving it potential for mitigating the variability due to nondeterminism.</p>
<p>In </p>
<p>Generalization Experiments</p>
<p>In this section we detail additional experiments showing the generalization of our results on nondeterminism, instability, and methods for reducing variability to other datasets (MNIST, ImageNet) and model architectures. We compile our main generalization results in Table 6, with additional results in the Appendix.  Table 6, the observations around instability and its relationship to nondeterminism generally hold for these architectures, with a close correspondence between the magnitude of effects for a random bit change and each of the five metrics considered.</p>
<p>Turning towards our proposals (Sec. 5) for mitigating the effects of nondeterminism and instability on model variability, we find across all model architectures that both accelerated ensembling and test-time augmentation reduce variability across nearly all metrics, with perhaps larger relative reductions for larger models and the pairwise metrics. Only for the intersection of the smallest model (ResNet-6) and metrics of performance variability (Accuracy SD and Cross-Entropy SD) was there no benefit.</p>
<p>MNIST. Experiments on MNIST (LeCun et al., 1998), allow us to test whether our observations hold for tasks with very high accuracy -99.14% for our relatively simple baseline model, which has two convolution and fully-connected layers. As before, we find similar effects of nondeterminism for parameter initialization and all nondeterminism sources, including a comparable effect (albeit smaller) from a single random bit change, highlighting that the instability of training extends even to datasets where the goal is simpler and model performance is higher. Of note, though, is the relative smaller effect of a single bit change on pairwise metrics of diversity, further suggesting that the magnitude of instability might be at least partially related to the interplay of model architecture, capacity, and degree of overfitting.</p>
<p>In terms of the mitigations against variability, only test-time augmentation appeared to significantly help. For MNIST, the only augmentation employed was cropping, with a small 1-pixel padding (models were trained with no data augmentation). While the fact that accelerated ensembling did not result in improvements is not particularly important in practice (since MNIST models are fast to train), it is an interesting result, which we hypothesize is also related to the degree of overfitting (similar to ResNet-6 on CIFAR-10).</p>
<p>ImageNet. We perform larger-scale tests on ImageNet using 20 runs of a ResNet-18 (He et al., 2016), trained for 120 epochs, obtaining an average top-1 accuracy of 71.9% on the ImageNet validation set. Again, we find evidence supporting instability, with "Random Bit Change" having levels of variability comparable to models trained with all nondeterminism sources. For reducing variability, we find modest improvements for both flipping-based and crop-based TTA on all metrics other than Accuracy SD, noting the large error bars of Accuracy and Cross-Entropy SD relative to their point estimates. Accelerated ensembling follows a similar but stronger trend, with particularly large reductions in variability for pairwise metrics.</p>
<p>Conclusion</p>
<p>In this work, we have shown two surprising facts: First, though conventional wisdom holds that run-to-run variability in model performance is primarily determined by random parameter initialization, many sources of nondeterminism actually result in similar levels of variability. Second, a key driver of this phenomenon is the instability of model optimization, in which changes on the order of 10 −10 in a single weight at initialization can have as much effect as reinitializing all weights to completely random values. We have also identified two approaches for reducing the variability in model performance and representation without incurring any additional training cost: ensembling in model-space via accelerated model ensembling, and ensembling in dataspace via the application of test-time data augmentation.</p>
<p>Many promising directions for future work exist. One important line of inquiry is in developing stronger theoretic understanding of the instability in optimization, beyond the largely empirical evidence in our work. Another natural direction is improving upon the algorithms for reducing the effects of instability on model variability -although both accelerated ensembling and TTA help, they are far from solving the problem entirely and incur additional computation during test time. Last, it would be interesting to examine our findings on even larger models (e.g. transformers for NLP and image recognition) and problems outside the fully supervised setting. We hope that our work has shed light on a complex phenomenon that affects all deep learning researchers and inspires further research.  Table 6. Generalization experiments of nondeterminism and instability with other architectures on CIFAR-10, ImageNet, and MNIST. For CIFAR-10 and MNIST, each row is computed from the statistics of 100 trained models, and for ImageNet, each row is computed from 20 trained models. Within each section the most relevant comparisons to make are between "Random Bit Change" and "All Nondeterminism Sources" to evaluate instability, and between "All Nondeterminism Sources", "Acc. Ens.", and each TTA method to evaluate the efficacy of our proposals to mitigate the effects of nondeterminism and instability (all TTA models have all sources of nondeterminism enabled). Notation follows Tables 1 and 5 Wen, Y., Tran, D., and Ba, J. Batchensemble: an alternative approach to efficient ensemble and lifelong learning. arXiv preprint arXiv:2002.06715, 2020.</p>
<p>A. Linear, 2-Layer, and ResNet-10 Results</p>
<p>In Table 7 we include results for linear networks, 2-layer networks (1 hidden layer), and a ResNet-10 on CIFAR-10, omitted from the main text due to space constraints. As noted in the main text, parameter initialization generally has less effect for linear models, and random bit changes in particular have nearly no effect on linear models, highlighting the stability of SGD in optimizing linear models. Also of note is the relative smaller effect of a single bit change for a 2-layer network where the hidden layer is convolutionalstill much larger than for the linear model, but significantly smaller than for any other non-linear model. This suggests a similar effect to what was previously observed on MNIST (Table 6) in that degree of instability might be related to the interplay of model, dataset, and the degree of overfitting.</p>
<p>Otherwise, these results follow the results in the main text, wherein each source of nondeterminism has roughly the same effect as each other, and a majority of this is due to the instability of optimization, evidenced by the high variability in models with only random bit changes at initialization. Test-time augmentation also remains effective in reducing model variability as compared to "All Nondeterminism Sources", the setting TTA is applied to.</p>
<p>B. Impact of Random Bit Changes Over Time</p>
<p>In Fig. 3 we plot the effect of a random bit change for a linear and single hidden layer model on CIFAR-10, illustrating the effect of instability as described in Sec. 4. In the first few epochs of training, we observe that the standard deviation and range of cross-entropy for the model with one hidden layer quickly grows, only eventually decreasing much later in training as the model's parameters converges toward their final values. On the other hand, for linear models, the standard deviation consistently remains 5 or more orders of magnitude lower throughout training.</p>
<p>C. Impact of Network Width</p>
<p>Here we present results studying the interaction of instability and model width, training variants of the ResNet-14 on CIFAR-10 with width modifiers ranging from 0.125 to 8 in powers of two. Results are given in Table 8. Across model widths, we observe that instability still produces roughly as much variability as all other nondeterminism sources combined, as shown by the relative variability of models trained with "Random Bit Change" as a source of nondeterminism compared with those trained with all other sources ("All Sources"). The only minor outlier was the width multiplier of 0.125, for which random bit changes produced somewhat less variability, reminiscent of the experiments on MNIST (Table 6). We also observe that variability generally decreases as model width increases (except for "Pairwise Corr."), though it is difficult to say whether this is due to a unique property of model width or simply wider models having higher test set performance. </p>
<p>D. Test-Time Augmentation</p>
<p>E. Approaches That Don't Reduce Instability</p>
<p>In the process of finding an approach that reduces run-to-run variability of models (Sec. 5), we experimented with many approaches which all failed to make a dent in improving variability and stability. For the benefit of the field, here we provide our experiences with these approaches which did not succeed in improving stability, despite the intuitive arguments for why they might help.</p>
<p>Learning Rate and Duration of Training. Noticing that the effects of nondeterminism seemed to accumulate during the course of training (Fig. 2), it seemed reasonable that varying the learning rate or duration of training might have an effect. However, varying the duration of training from anywhere between 50 and 2,000 epochs on CIFAR-10 all produced models with a similar variance in performance as the results in the rest of this work (which used 500 epochs), even though the absolute performance differed by up to ∼2%. Cross-Entropy SD Figure 3. The impact of a random bit change during initialization for linear models vs 2-layer models with a single fully-connected hidden layer, where row 1 considers the full 500 epochs of training, and row 2 zooms in on the first 10 epochs. The left column of each row gives the range of cross-entropy values for 100 models in the middle 95th percentile of cross-entropy, plotted at each epoch. The right column of each row presents the standard deviation of these models, roughly corresponding to half the width of the corresponding range in the left-hand plots.</p>
<p>We show these results in Table 9. In general, increasing the number of epochs or changing the learning rate did not change the variability in performance (Accuracy SD; Cross-Entropy SD) much, with only a very slight increase in variability as the number of epochs grew to extremely large values (i.e. 2,000 epochs). There were slightly larger changes in pairwise representation-based metrics, where training longer again increased run-to-run variability. However, none of these attempts actually reduced variability while maintaining performance; they only served to potentially make it larger.</p>
<p>As part of these experiments, we also verified the effects of instability with only 200 epochs of training and the effectiveness of accelerated ensembling techniques ("Acc. Ens.") with this reduced training time, given in the last three rows of Table 9.</p>
<p>Choice of Optimizer. Since instability and nondeterminism are both a property of optimization, it is conceivable that use of a different optimizer might be able to lessen the degree of instability in model training. We experimented with SGD using various values of momentum, ranging from 0 for pure SGD to 0.999 for a momentum-driven optimizer, but none succeeded in reduce instability. In addition, we experimented with Adam (Kingma &amp; Ba, 2014), picked as a representative of the class of adaptive learning rate algorithms, but this, too, had no effect on stability.</p>
<p>Aggressive Stochastic Weight Averaging. Inspired by the success found by Madhyastha &amp; Jain (2019), we tried Aggressive Stochastic Weight Averaging (ASWA), a variant of SWA (Izmailov et al., 2018). However, we could not get the model to converge to a reasonable degree of performance with the original formulation due to update sizes that decreased too rapidly, and though we were able to modify it to converge successfully, the output variance remained as high as the other models.</p>
<p>Gradient Clipping. With the intuition that instability might be caused by spurious gradients of large magnitude, we experimented with clipping the norm of gradients (using pytorch's implementation of torch.nn.utils.clip_grad.clip_grad_norm_. Like other approaches, though, this had no effect on model variability.</p>
<p>Weight Augmentation. A very experimental approach, to reduce instability we experimented with taking an averaged gradient around the current set of parameters at each step, approximated by sampling a random weight offset before doing a forward or backward pass through the model. Intuitively, this might encourage optimization to not be too sensitive to the current value of weights; however, in prac-tice this simply didn't affect the variance or stability of the model.</p>
<p>F. Accelerated Ensembling in Language Modeling</p>
<p>Although the accelerated ensembling technique we employed in the main text, Snapshot Ensembles (Huang et al., 2017), was only designed for image classification, we have also experimented with its usage for language modeling (see Sec. 3.3 for problem setup). We present results in Table 10, where we additionally compare models trained for 500 and 1000 epochs. In both cases, accelerated ensembling resulted in lower model variability when considering pairwise metrics (reducing the fraction of tokens models disagreed on and reducing the PPL improvement from ensembling). However, the variability in PPL was more mixed, and in fact we note that the accelerated ensembles actually had higher average PPL than their counterparts (e.g. 75.0 for the accelerated ensemble vs 73.0 for the regular model), indicating that alternative accelerated ensembling techniques may be warranted for language modeling.</p>
<p>G. Subtleties in Evaluation</p>
<p>Choice of Seeds. While we have done our best to make our experimental protocol straightforward and easy to interpret, one subtlety related to seed selection arises when examining results. First, recall that in our example from Sec. 3.1, testing the effects of random initialization corresponded to training models for (S 1 , S 2 , S 3 ) ∈ {(i, 1, 1)} R i=1 , where S 1 was the seed for random initialization, S 2 was the seed for training data shuffling, and S 3 was set to 1 to indicate the deterministic mode for cuDNN. The subtlety arises in that the resulting distribution of (S 1 , S 2 , S 3 ) ∈ {(i, 1, 1)} R i=1 is not necessarily the same as the distribution where S 2 is set to a different arbitrary constant value, e.g. S 2 = 2. Due to this, there may be minor discrepancies when comparing the diversity in performance between two different sources of nondeterminism (though unlikely to change general conclusions unless the magnitude of the discrepancy is very large). Combined with the natural sampling variability implicit in only training a finite number of models, this can lead to paradoxical results such as the standard deviation for a particular metric being slightly higher for a random bit change as compared to an entirely different random parameter initialization. While we have separately validated that the general conclusions of our results hold when varying a few of these constant factors (i.e. running experiments where S 2 is set to 2 and 3, in this example), it is difficult to resolve the discrepancy entirely without models according to the full cross-product of random seeds, which is prohibitive due to the exponential amount of required computation.  Variability vs Model Performance. Another challenge when interpreting results is that model variability covaries with model performance, which impacts validating any approach that affects both model variability and performance.</p>
<p>Though this is a difficult evaluation issue to solve in general, it is possible to show that the reductions in variability from Accelerated Ensembling and Test-Time Augmentation are not simply due to this: For example, "Acc. Ens./Flip-Crop81-TTA" for ResNet-14 (Table 5) has lower variability across all metrics than "All Nondeterminism Sources" for ResNet-18 (Table 6), despite the ResNet-18 having higher accuracy (92.0% vs 94.9%). Similar trends hold for ResNet-10 with Acc. Ens. and TTA (Table 7) vs ResNet-14 (88.7% vs 89.8%) and ResNet-6 vs ResNet-10 (77.9% vs 86.1%), albeit only visible on certain variability metrics in the latter case.</p>
<p>Figure 1 .
1Average CKA representation similarity (Kornblith et al., 2019) for pairs of ResNet-14 models on CIFAR-10 across nondeterminism sources and a variety of network layers.</p>
<p>Figure 2 .
2The effect of the onset of nondeterminism on the variability of accuracy in converged models. Each point corresponds to training 100 models deterministically for a certain number of epochs (x-axis), then enabling a given source of nondeterminism by varying its seed starting from that epoch and continuing through to the end of training, then measuring the accuracy SD (y-axis).</p>
<p>CIFAR- 10 .
10On CIFAR-10, in addition to the ResNet-14 employed throughout this work, we experiment with a smaller 6-layer variant, larger 18-layer variant, VGG-11 (Simonyan &amp; Zisserman, 2014), and a 50%-capacity Shuf-fleNetv2(Ma et al., 2018), with even more architectures in the Appendix. As shown in</p>
<p>, and all TTA cropping for CIFAR-10 uses the 81-crop variant.Islam, R.,Henderson, P., Gomrokchi, M., and Precup,  D.  Reproducibility of benchmarked deep reinforcement learning tasks for continuous control. arXiv preprint arXiv:1708.04133, 2017.Izmailov, P., Podoprikhin, D.,Garipov, T., Vetrov, D.,  and Wilson, A. G.   Averaging weights leads to wider optima and better generalization. arXiv preprint arXiv:1803.05407, 2018.Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.Kornblith, S., Norouzi, M., Lee, H., and Hinton, G. Similarity of neural network representations revisited. arXiv preprint arXiv:1905.00414, 2019.Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009. LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradientbased learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998. Loshchilov, I. and Hutter, F. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016. Ma, N., Zhang, X., Zheng, H.-T., and Sun, J. Shufflenet v2: Practical guidelines for efficient cnn architecture design. In Proceedings of the European conference on computer vision (ECCV), pp. 116-131, 2018. Machado, M. C., Bellemare, M. G., Talvitie, E., Veness, J., Hausknecht, M., and Bowling, M. Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. Journal of Artificial Intelligence Research, 61:523-562, 2018. Madhyastha, P. and Jain, R. On model stability as a function of random seed. In Conference on Computational Natural Language Learning, pp. 929-939, 2019. Marcus, M., Santorini, B., and Marcinkiewicz, M. A. Building a large annotated corpus of english: The penn treebank. 1993. Merity, S., Keskar, N. S., and Socher, R. Regularizing and optimizing lstm language models. arXiv preprint arXiv:1708.02182, 2017. Nagarajan, P., Warnell, G., and Stone, P. The impact of nondeterminism on reproducibility in deep reinforcement learning. 2018. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems, pp. 8024-8035, 2019. Recht, B., Re, C., Wright, S., and Niu, F. Hogwild: A lockfree approach to parallelizing stochastic gradient descent. In Advances in neural information processing systems, pp. 693-701, 2011. Shorten, C. and Khoshgoftaar, T. M. A survey on image data augmentation for deep learning. Journal of Big Data, 6(1):1-48, 2019. Simonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929-1958, 2014. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1-9, 2015. Virtanen, P., Gommers, R., Oliphant, T. E., Haberland, M., Reddy, T., Cournapeau, D., Burovski, E., Peterson, P., Weckesser, W., Bright, J., et al. Scipy 1.0: fundamental algorithms for scientific computing in python. Nature methods, 17(3):261-272, 2020. Wan, L., Zeiler, M., Zhang, S., Le Cun, Y., and Fergus, R. Regularization of neural networks using dropconnect. In International conference on machine learning, pp. 1058-1066, 2013.</p>
<p>Details CIFAR-10. On CIFAR-10, in addition to TTA with horizontal flipping (i.e. ensembling model predictions on the original image with its horizontally-flipped version), we also used a form of TTA with cropping. Our usage of crop-based TTA was based on the version of cropping used as data augmentation during model training, in which each image was zero-padded by four pixels along each side, after which a random 32 × 32 crop was drawn. In the main text, weexperimented with 25-and 81-crop TTA variants, where the 81-crop variant uses all possible crops [(4 · 2 + 1) 2 = 81], and the 25-crop variant uses a stride of 2 when sampling the possible crops [(2 · 2 + 1) 2 = 25]. When adding in horizontally-flipped versions, the number of TTA examples doubles, for a maximum of 162 augmented versions of the original image.ImageNet. On ImageNet, the standard evaluation protocol we use for our experiments first resizes each image to have its smaller side be length 256, after which the central 224 × 224 crop is taken. For TTA, besides the horizontal image flipping used in CIFAR-10, we also experimented with crops ('Crop-TTA' and 'Flip-Crop-TTA') as follows: after each image is resized to have its smaller side length 256, a central 256 × 256 crop is taken, and then 9 crops of size 224 × 224 are taken in a 3 × 3 grid, starting from the top-left, and where the spacing between crops in the grid is 16 pixels.</p>
<p>Table 2. The effect of each source of nondeterminism for a QRNN on Penn Treebank; 100 runs per row. Note that lower PPL is better for language modeling tasks, so changes in PPL from ensembling are negative.on Penn Treebank (Marcus et al., 1993), using 
the publicly available code of (Merity et al., 2017). This 
model uses a 256-dimensional word embedding, 512 hidden 
units per layer, and 3 layers of recurrent units, obtaining a 
perplexity (PPL) of 75.49 on the Penn Treebank test set. 
Nondeterminism Source 
Accuracy 
Cross-Entropy 
Pairwise 
Pairwise Ensemble 
SD (%) 
SD 
Disagree (%) 
Corr. 
∆ (%) </p>
<p>Parameter Initialization 
0.23 ± 0.02 0.0074 ± 0.0005 
10.7 
0.872 
1.82 
Data Shuffling 
0.25 ± 0.02 0.0082 ± 0.0005 
10.6 
0.871 
1.81 
Data Augmentation 
0.23 ± 0.02 0.0072 ± 0.0005 
10.7 
0.872 
1.83 
cuDNN 
0.22 ± 0.01 0.0083 ± 0.0007 
10.5 
0.873 
1.76 
Data Shuffling + cuDNN 
0.21 ± 0.01 0.0077 ± 0.0005 
10.6 
0.871 
1.80 
Data Shuffling + Aug. + cuDNN 0.22 ± 0.01 0.0074 ± 0.0005 
10.7 
0.871 
1.84 
All Nondeterminism Sources 
0.26 ± 0.02 0.0072 ± 0.0005 
10.7 
0.871 
1.82 </p>
<p>Table 1. The effect of each source of nondeterminism and several combinations of nondeterminism sources for ResNet-14 on CIFAR-10. 
The second and third columns give the standard deviation of accuracy and cross-entropy across 100 runs, varying only the nondeterminism 
source (700 trained models total). Also given are error bars, corresponding to the standard deviation of each standard deviation. The 
fourth, fifth, and sixth columns give the average percentage of examples models disagree on, the average pairwise Spearman's correlation 
coefficient between predictions, and the average change in accuracy from ensembling two models, respectively (Sec. 3.1). </p>
<p>Nondeterminism Source 
PPL SD 
Pairwise Disagree (%) Ensemble PPL ∆ </p>
<p>Parameter Initialization 
0.20 ± 0.01 
17.3 
-2.07 
Stochastic Operations 
0.19 ± 0.01 
17.3 
-2.08 
All Nondeterminism Sources 0.18 ± 0.01 
17.4 
-2.07 </p>
<p>Conv1 ResBlock1 ResBlock2 ResBlock3 AvgPool 
Logits </p>
<p>0.86 </p>
<p>0.88 </p>
<p>0.90 </p>
<p>0.92 </p>
<p>0.94 </p>
<p>0.96 </p>
<p>0.98 </p>
<p>1.00 
Param. Init 
Data Shuffle 
Data Aug. 
cuDNN 
Shuffle + cuDNN 
Shuffle + Aug. + cuDNN 
All sources </p>
<p>What happens when we optimize this model, different from the original by only a single bit? By the end of the first epoch of training, with the learning rate still warming up, the new model already differs in accuracy by 0.18% (25.74% vs 25.56%). In one more epoch the difference is a larger 2.33% (33.45% vs 31.12%), and after three epochs, the difference is a staggering 10.42% (41.27% vs 30.85%). Finally, at the end of training the model weights converge, with the new model obtaining an accuracy of 90.12% and a crossentropy of 0.34335, substantially different from the original despite only a tiny change in initialization.</p>
<p>Table 3 .Table 4 .
34The effect of instability -randomly changing a single weight by one bit during initialization for ResNet-14 on CIFAR-10. The effect of instability for a QRNN on Penn Treebank. Also seeTable 2for comparison.Nondeterminism Source 
PPL SD 
Pairwise Disagree (%) Ensemble PPL ∆ </p>
<p>Random Bit Change 
0.19 ± 0.01 
17.7 
-2.07 </p>
<p>itself is measured. Indeed, as demonstrated in Table 5 (top), 
ensembles of larger models have less variability. However, 
since ensembling still requires training multiple copies of 
models, is does not reduce the computational burden caused 
by nondeterminism and instability. </p>
<p>Table 5 (
5bottom), we show results on CIFAR-10 with 
horizontal flip and image cropping TTA (details in Appendix 
Sec. D), and also combine TTA with accelerated ensembling. 
Simple flip TTA reduces variability across all metrics (21% </p>
<p>Table 5 .
5Comparison of single and ensemble model variability on CIFAR-10 with proposed methods for reducing the effects of nondeterminism. For standard ensembles, N denotes the number of constituent models, "Acc. Ens." uses accelerated ensembling, and [Single|Acc. Ens.]/[Flip|CropX|Flip-CropX]-TTA use either horizontal flips, crops (with X crops), or flips and crops for test-time augmentation on top of either regular single models or an accelerated ensemble. Also shown is the training time and average relative reduction in variability across metrics compared to the baseline 'Single Model". All results are based on 100 runs of model training.Model 
Training 
Accuracy 
Cross-Entropy 
Pairwise 
Pairwise Ensemble Variability 
Cost 
SD (%) 
SD 
Disagree (%) 
Corr. 
∆ (%) 
Reduction </p>
<p>Single Model 
1× 
0.26 ± 0.02 0.0072 ± 0.0005 
10.7 
0.871 
1.82 
n/a 
Ensemble (N = 2) 
2× 
0.19 ± 0.02 0.0044 ± 0.0004 
6.9 
0.929 
0.89 
39% 
Ensemble (N = 3) 
3× 
0.15 ± 0.02 0.0033 ± 0.0005 
5.5 
0.951 
0.59 
55% 
Ensemble (N = 4) 
4× 
0.17 ± 0.02 0.0030 ± 0.0004 
4.6 
0.963 
0.43 
60% 
Ensemble (N = 5) 
5× 
0.12 ± 0.02 0.0028 ± 0.0004 
4.1 
0.970 
0.34 
67% 
Ensemble (N = 10) 
10× 
0.11 ± 0.02 0.0022 ± 0.0004 
2.9 
0.985 
0.20 
76% 
Ensemble (N = 20) 
20× 
0.11 ± 0.04 0.0018 ± 0.0005 
2.0 
0.992 
0.08 
81% 
Acc. Ens. 
1× 
0.19 ± 0.02 0.0044 ± 0.0003 
6.1 
0.957 
0.63 
48% 
Single/Flip-TTA 
1× 
0.24 ± 0.02 0.0061 ± 0.0005 
8.2 
0.905 
1.20 
21% 
Single/Crop25-TTA 
1× 
0.23 ± 0.02 0.0059 ± 0.0004 
9.2 
0.893 
1.49 
16% 
Single/Crop81-TTA 
1× 
0.21 ± 0.01 0.0055 ± 0.0004 
8.8 
0.898 
1.39 
21% 
Single/Flip-Crop25-TTA 
1× 
0.21 ± 0.02 0.0051 ± 0.0004 
7.2 
0.920 
0.99 
33% 
Single/Flip-Crop81-TTA 
1× 
0.19 ± 0.01 0.0049 ± 0.0004 
6.9 
0.922 
0.92 
37% 
Acc. Ens./Flip-TTA 
1× 
0.15 ± 0.01 0.0039 ± 0.0003 
5.0 
0.967 
0.45 
58% 
Acc. Ens./Flip-Crop81-TTA 
1× 
0.16 ± 0.01 0.0033 ± 0.0002 
4.6 
0.972 
0.38 
61% </p>
<p>Table 7. Linear, 2-layer, and ResNet-10 experiments on CIFAR-10. 125/Random Bit Change (N=30) 0.66 ± 0.10 0.0133 ± 0.0020Table 8. Experiments varying model width for ResNet-14 on CIFAR-10. In each row, the experimental setting is abbreviated by [width multiplier]/[sources of nondeterminism] (N=[number of models trained]).Nondeterminism Source 
Accuracy 
Cross-Entropy 
Pairwise 
Pairwise Ensemble 
SD (%) 
SD 
Disagree (%) 
Corr. 
∆ (%) </p>
<p>CIFAR-10: Linear model </p>
<p>Parameter Initialization 
0.03 ± 2e-3 
0.0002 ± 1e-5 
0.5 
0.997 
-4e-3 
All Nondeterminism Sources 
0.10 ± 0.01 
0.0007 ± 4e-5 
5.5 
0.996 
0.06 
Random Bit Change 
0.00 ± 0.00 
1e-7 ± 1e-8 
0.0 
1.000 
0.00 
Single/Flip-Crop-TTA 
0.05 ± 3e-3 
0.0001 ± 1e-5 
0.9 
0.998 
-3e-3 </p>
<p>CIFAR-10: One hidden layer (fully-connected) </p>
<p>Parameter Initialization 
0.31 ± 0.02 
0.0051 ± 0.0003 
24.2 
0.941 
1.38 
All Nondeterminism Sources 
0.30 ± 0.02 
0.0054 ± 0.0004 
24.9 
0.937 
1.49 
Random Bit Change 
0.28 ± 0.02 
0.0051 ± 0.0004 
23.4 
0.945 
1.32 
Single/Flip-Crop-TTA 
0.15 ± 0.01 
0.0017 ± 0.0001 
7.1 
0.993 
0.15 </p>
<p>CIFAR-10: One hidden layer (convolutional) </p>
<p>Parameter Initialization 
0.26 ± 0.02 
0.0040 ± 0.0003 
12.5 
0.974 
0.64 
All Nondeterminism Sources 
0.22 ± 0.01 
0.0042 ± 0.0003 
12.7 
0.973 
0.68 
Random Bit Change 
0.14 ± 0.01 
0.0022 ± 0.0002 
6.4 
0.993 
0.18 
Single/Flip-Crop-TTA 
0.19 ± 0.01 
0.0033 ± 0.0002 
7.5 
0.989 
0.24 </p>
<p>CIFAR-10: ResNet-10 </p>
<p>Parameter Initialization 
0.23 ± 0.01 
0.0060 ± 0.0003 
13.7 
0.912 
2.13 
All Nondeterminism Sources 
0.23 ± 0.01 
0.0065 ± 0.0004 
13.6 
0.911 
2.13 
Random Bit Change 
0.25 ± 0.02 
0.0065 ± 0.0005 
13.5 
0.913 
2.08 
Single/Flip-Crop-TTA 
0.24 ± 0.02 
0.0047 ± 0.0003 
9.5 
0.943 
1.22 
Acc. Ens. 
0.23 ± 0.01 
0.0047 ± 0.0003 
8.8 
0.962 
0.96 
Acc. Ens./Flip-Crop-TTA 
0.18 ± 0.01 
0.0035 ± 0.0002 
6.7 
0.973 
0.58 </p>
<p>Setting 
Accuracy 
Cross-Entropy 
Pairwise 
Pairwise Ensemble 
SD (%) 
SD 
Disagree (%) 
Corr. 
∆ (%) </p>
<p>0.125/All Sources (N=30) 
0.86 ± 0.10 0.0183 ± 0.0020 
40.8 
0.887 
2.04 
0.38.0 
0.943 
1.73 
0.25/All Sources (N=30) 
0.59 ± 0.06 0.0124 ± 0.0012 
25.9 
0.880 
2.37 
0.25/Random Bit Change (N=30) 
0.71 ± 0.08 0.0145 ± 0.0019 
25.2 
0.908 
2.15 
0.5/All Sources (N=30) 
0.33 ± 0.04 0.0077 ± 0.0010 
15.8 
0.890 
2.21 
0.5/Random Bit Change (N=30) 
0.30 ± 0.04 0.0076 ± 0.0008 
15.0 
0.917 
1.98 
1.0/All Sources (N=100) 
0.26 ± 0.02 0.0072 ± 0.0005 
10.7 
0.871 
1.82 
1.0/Random Bit Change (N=100) 
0.21 ± 0.01 0.0068 ± 0.0004 
10.6 
0.874 
1.82 
2.0/All Sources (N=30) 
0.13 ± 0.01 0.0066 ± 0.0008 
7.1 
0.790 
1.19 
2.0/Random Bit Change (N=30) 
0.18 ± 0.02 0.0054 ± 0.0012 
7.0 
0.798 
1.13 
4.0/All Sources (N=30) 
0.14 ± 0.02 0.0049 ± 0.0006 
5.0 
0.781 
0.75 
4.0/Random Bit Change (N=30) 
0.15 ± 0.02 0.0058 ± 0.0007 
4.9 
0.785 
0.74 
8.0/All Sources (N=30) 
0.13 ± 0.02 0.0038 ± 0.0005 
3.8 
0.807 
0.53 
8.0/Random Bit Change (N=30) 
0.11 ± 0.01 0.0039 ± 0.0004 
3.8 
0.809 
0.50 </p>
<p>0 
100 
200 
300 
400 
500 </p>
<p>Epochs trained </p>
<p>1.4 </p>
<p>1.6 </p>
<p>1.8 </p>
<p>2.0 </p>
<p>2.2 </p>
<p>Cross-Entropy </p>
<p>Linear 
One hidden layer (FC) </p>
<p>0 
100 
200 
300 
400 
500 </p>
<p>Epochs trained </p>
<p>10 −8 </p>
<p>10 −7 </p>
<p>10 −6 </p>
<p>10 −5 </p>
<p>10 −4 </p>
<p>10 −3 </p>
<p>10 −2 </p>
<p>10 −1 </p>
<p>Cross-Entropy SD </p>
<p>2 
4 
6 
8 
10 </p>
<p>Epochs trained </p>
<p>1.7 </p>
<p>1.8 </p>
<p>1.9 </p>
<p>2.0 </p>
<p>2.1 </p>
<p>2.2 </p>
<p>Cross-Entropy </p>
<p>Linear 
One hidden layer (FC) </p>
<p>2 
4 
6 
8 
10 </p>
<p>Epochs trained </p>
<p>10 −8 </p>
<p>10 −7 </p>
<p>10 −6 </p>
<p>10 −5 </p>
<p>10 −4 </p>
<p>10 −3 </p>
<p>10 −2 </p>
<p>Table 9 .
9Experiments varying the learning rate and number of epochs for ResNet-14 on CIFAR-10. In each row, the experimental setting is abbreviated by [sources of nondeterminism]/[maximum learning rate]/[number of epochs] (N=[number of models trained]), with the exception of the last row, which is a Snapshot ensemble but otherwise follows the same format.Setting 
PPL SD 
Pairwise Disagree (%) Ensemble PPL ∆ </p>
<p>All Nondeterminism Sources/500 
0.18 ± 0.01 
17.4 
-2.07 
Acc. Ens./All Sources/500 
0.21 ± 0.02 
13.7 
-1.33 
All Nondeterminism Sources/1000 0.17 ± 0.01 
17.6 
-2.08 
Acc. Ens./All Sources/1000 
0.16 ± 0.01 
14.1 
-1.34 </p>
<p>Table 10 .
10The effects of accelerated model ensembling on Penn Treebank; 100 runs per row. "Acc. Ens." indicates accelerated ensembling, and the trailing number in each setting name is the number of epochs models are trained for.</p>
<p>Critical learning periods in deep neural networks. A Achille, M Rovere, S Soatto, International Conference on Learning Representations. Achille, A., Rovere, M., and Soatto, S. Critical learning peri- ods in deep neural networks. In International Conference on Learning Representations, 2019.</p>
<p>Stochastic gradient descent tricks. L Bottou, Neural networks: Tricks of the trade. SpringerBottou, L. Stochastic gradient descent tricks. In Neural networks: Tricks of the trade, pp. 421-436. Springer, 2012.</p>
<p>Stability and generalization. O Bousquet, A Elisseeff, Journal of machine learning research. 2Bousquet, O. and Elisseeff, A. Stability and generalization. Journal of machine learning research, 2(Mar):499-526, 2002.</p>
<p>J Bradbury, S Merity, C Xiong, R Socher, arXiv:1611.01576Quasi-recurrent neural networks. arXiv preprintBradbury, J., Merity, S., Xiong, C., and Socher, R. Quasi-recurrent neural networks. arXiv preprint arXiv:1611.01576, 2016.</p>
<p>S Chetlur, C Woolley, P Vandermersch, J Cohen, J Tran, B Catanzaro, E Shelhamer, Cudnn, arXiv:1410.0759Efficient primitives for deep learning. arXiv preprintChetlur, S., Woolley, C., Vandermersch, P., Cohen, J., Tran, J., Catanzaro, B., and Shelhamer, E. cudnn: Efficient primitives for deep learning. arXiv preprint arXiv:1410.0759, 2014.</p>
<p>Why does unsupervised pretraining help deep learning. D Erhan, Y Bengio, A Courville, P.-A Manzagol, P Vincent, S Bengio, Journal of Machine Learning Research. 11Erhan, D., Bengio, Y., Courville, A., Manzagol, P.-A., Vin- cent, P., and Bengio, S. Why does unsupervised pre- training help deep learning? Journal of Machine Learn- ing Research, 11(Feb):625-660, 2010.</p>
<p>Deep ensembles: A loss landscape perspective. S Fort, H Hu, B Lakshminarayanan, arXiv:1912.02757arXiv preprintFort, S., Hu, H., and Lakshminarayanan, B. Deep en- sembles: A loss landscape perspective. arXiv preprint arXiv:1912.02757, 2019.</p>
<p>J Frankle, D J Schwab, A S Morcos, arXiv:2002.10365The early phase of neural network training. arXiv preprintFrankle, J., Schwab, D. J., and Morcos, A. S. The early phase of neural network training. arXiv preprint arXiv:2002.10365, 2020.</p>
<p>Loss surfaces, mode connectivity, and fast ensembling of dnns. T Garipov, P Izmailov, D Podoprikhin, D P Vetrov, A G Wilson, Advances in Neural Information Processing Systems. Garipov, T., Izmailov, P., Podoprikhin, D., Vetrov, D. P., and Wilson, A. G. Loss surfaces, mode connectivity, and fast ensembling of dnns. In Advances in Neural Information Processing Systems, pp. 8789-8798, 2018.</p>
<p>Understanding the difficulty of training deep feedforward neural networks. X Glorot, Y Bengio, Proceedings of the thirteenth international conference on artificial intelligence and statistics. the thirteenth international conference on artificial intelligence and statisticsGlorot, X. and Bengio, Y. Understanding the difficulty of training deep feedforward neural networks. In Pro- ceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249-256, 2010.</p>
<p>Stable architectures for deep neural networks. E Haber, L Ruthotto, Inverse Problems. 34114004Haber, E. and Ruthotto, L. Stable architectures for deep neural networks. Inverse Problems, 34(1):014004, 2017.</p>
<p>Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionHe, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE inter- national conference on computer vision, pp. 1026-1034, 2015.</p>
<p>Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.</p>
<p>Deep reinforcement learning that matters. P Henderson, R Islam, P Bachman, J Pineau, D Precup, D Meger, Thirty-Second AAAI Conference on Artificial Intelligence. Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., and Meger, D. Deep reinforcement learning that matters. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.</p>
<p>Accuracy and stability of numerical algorithms. N J Higham, SIAMHigham, N. J. Accuracy and stability of numerical algo- rithms. SIAM, 2002.</p>
<p>G Huang, Y Li, G Pleiss, Z Liu, J E Hopcroft, K Q Weinberger, arXiv:1704.00109Snapshot ensembles: Train 1, get m for free. Setting Accuracy Cross-EntropyarXiv preprintHuang, G., Li, Y., Pleiss, G., Liu, Z., Hopcroft, J. E., and Weinberger, K. Q. Snapshot ensembles: Train 1, get m for free. arXiv preprint arXiv:1704.00109, 2017. Setting Accuracy Cross-Entropy</p>
<p>. / Shuffle, Shuffle/.10/2000 (N=100)</p>
<p>. Param, /.10/2000 (N=100Param. Init/.10/2000 (N=100)</p>
<p>. Acc, Ens, /.40/200 (N=100) 0Acc. Ens./All Sources/.40/200 (N=100) 0</p>            </div>
        </div>

    </div>
</body>
</html>
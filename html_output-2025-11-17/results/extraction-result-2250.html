<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2250 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2250</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2250</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-62.html">extraction-schema-62</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <p><strong>Paper ID:</strong> paper-276095040</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.00379v5.pdf" target="_blank">Latent Action Learning Requires Supervision in the Presence of Distractors</a></p>
                <p><strong>Paper Abstract:</strong> Recently, latent action learning, pioneered by Latent Action Policies (LAPO), have shown remarkable pre-training efficiency on observation-only data, offering potential for leveraging vast amounts of video available on the web for embodied AI. However, prior work has focused on distractor-free data, where changes between observations are primarily explained by ground-truth actions. Unfortunately, real-world videos contain action-correlated distractors that may hinder latent action learning. Using Distracting Control Suite (DCS) we empirically investigate the effect of distractors on latent action learning and demonstrate that LAPO struggle in such scenario. We propose LAOM, a simple LAPO modification that improves the quality of latent actions by 8x, as measured by linear probing. Importantly, we show that providing supervision with ground-truth actions, as few as 2.5% of the full dataset, during latent action learning improves downstream performance by 4.2x on average. Our findings suggest that integrating supervision during Latent Action Models (LAM) training is critical in the presence of distractors, challenging the conventional pipeline of first learning LAM and only then decoding from latent to ground-truth actions.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2250.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2250.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LAOM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Latent Action Observation Model (LAOM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper's proposed modification of LAPO that removes quantization, replaces image reconstruction with latent temporal consistency, adds multi-step inverse dynamics, larger latent-action dimensionality, and augmentations; can be supervised by a small number of ground-truth actions during pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LAOM</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>latent-action space operating on compact latent observation embeddings (no pixel-level reconstruction; latent temporal consistency)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>implicit via multi-step IDM and optional linear supervision from ground-truth actions (no explicit masking); augmentations and temporal consistency encourage focusing on control-related features</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>continuous-control tasks from Distracting Control Suite (cheetah-run, walker-run, hopper-hop, humanoid-walk)</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>explicitly evaluated with visual distractors: dynamic background videos, camera shake, and agent color changes (action-correlated exogenous noise)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>LAOM (unsupervised) doubles downstream performance vs LAPO on average; improves latent action quality by ~8x (linear-probe measure); LAOM+supervision achieves average normalized score 0.44 using 2.5% labeled transitions and improves downstream performance by ~4.2–4.3x vs unsupervised LAOM in presence of distractors; specific BC returns used for normalization (with distractors) cheetah 823, walker 749, hopper 253, humanoid 428</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Parameter counts: LAOM ~192,307,136 params; LAOM+supervision ~192,479,189 params (Table 4); training time: LAOM ~6h43m, LAOM+supervision ~7h6m (Table 2) on H100 single GPU, bf16 with AMP; FLOPs and memory not reported</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Compared to LAPO: LAOM improves latent-action probe quality ~8x and downstream returns up to 2x in some settings; compared to IDM and BC baselines, LAOM (unsupervised) still underperforms BC/IDM in presence of distractors, but LAOM+supervision outperforms IDM and LAPO across label budgets and outperforms other baselines on average (normalized score 0.44 with 2.5% labels)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Cross-embodied (leave-one-out) pretraining: supervision during LAOM pretraining yields large improvement vs unsupervised LAOM, but final performance is not better than BC trained only on the provided labels from the target embodiment; overall limited positive transfer — supervision helps but does not exceed labeled-only BC</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Evaluated across four different environments (cheetah, walker, hopper, humanoid); LAOM shares representations (shared encoder) and is pre-trained on combined datasets in cross-embodied experiments; LAOM+supervision scales better across tasks than IDM when novel distractors are present (Figure 9a), but does not surpass BC trained on full labels</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Without supervision LAOM tends to encode action-correlated distractors into latent actions (non-minimal representations), large latent-action dimensionality (e.g., 8192) necessary to contain true actions but sacrifices minimality and harms imitation; quantization hinders performance; performance gap remains large with distractors unless supervision is added</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Ablations include: removal of quantization (improves probe quality, Fig.5), increasing latent-action dim from 128 to 8192 (gives ~2.5x probe improvement), adding multi-step IDM (doubles latent-action quality), replacing reconstruction with latent temporal consistency, adding augmentations (stabilizes and improves); combined modifications yield ~8x probe improvement (Fig.6); supervision ablations show LAOM+supervision loses only 16% performance reducing latent dim 8192->64 vs 63% loss for LAOM (Fig.9b/c)</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>LAOM+supervision recovers ~0.44 normalized score using only ~2.5% of dataset labels (e.g., 128 labelled trajectories were used in some experiments); unsupervised LAOM requires full unlabeled dataset but yields lower downstream sample efficiency (no explicit sample-efficiency curves beyond label budgets reported)</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>LAOM+supervision generalizes better than IDM to novel distractor background videos (evaluation on never-seen distractor set, Fig.9a); unsupervised LAOM and LAPO generalize poorly when distractors differ between labeled and unlabeled sets</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>LAOM removes pixel-level reconstruction; reconstruction quality metrics (MSE/SSIM/PSNR) not reported because model avoids image-space reconstruction in FDM, instead uses latent temporal consistency (MSE used for that loss per Appendix D)</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Paper analyzes task-relevant vs irrelevant features by showing latent actions encode a lot of background (control-irrelevant) information without supervision (visualization in Fig.11) and via linear probes predicting ground-truth actions from latent actions and observation embeddings; supervision grounds latents to control-relevant features</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>LAOM does not dynamically alter abstraction at inference time; abstraction changes are architectural (no reconstruction vs reconstruction) and via latent dimensionality; supervision allows compact latents (down to 64) with limited degradation, i.e., supports lower abstraction dimensionality when supervised</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not explicitly analyzed; paper focuses on offline pretraining and downstream imitation evaluation, not on online exploration/exploitation trade-offs</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>No formal information-theoretic measures (mutual information, rate-distortion) are reported; qualitative discussion of information bottleneck effects (quantization encourages encoding of distractors) is provided</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>Paper argues pixel-level reconstruction is detrimental in presence of distractors because it forces modelling of irrelevant pixel changes; empirical evidence: removing reconstruction and using latent temporal consistency yields better latent-action quality and downstream performance in distractor settings</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2250.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2250.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LAPO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Latent Action Policies (LAPO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior latent action learning method that infers discrete/quantized latent actions via IDM and FDM with a VQ-style quantizer and pixel-space next-observation reconstruction; used as the main baseline that the paper adapts and ablates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning to act without actions.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LAPO</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>original LAPO uses latent actions with quantization and pixel-level next-observation reconstruction (image-space decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>information bottleneck via quantization (VQ-VAE) intended to force IDM to represent action-related differences; no explicit mechanism to filter distractors</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>previously evaluated on ProcGen and in this work on Distracting Control Suite tasks</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>Designed/tested originally on distractor-free data; in this paper LAPO struggles in presence of action-correlated distractors (dynamic backgrounds etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In this paper LAPO's latent-action quality and downstream returns are low with distractors; quantization significantly reduces quality (Fig.5); exact numerical normalized returns vary by task and label budget (see Fig.8/7)</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Parameter count reported: LAPO ~211,847,849 params (Table 4); training time ~7h38m (Table 2); other FLOPs not reported</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>LAPO is outperformed by LAOM (their modifications) and by simple BC/IDM baselines in the presence of distractors; quantization is harmful compared to removing it</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Not evaluated positively in this paper for cross-embodied pretraining; LAPO pretrained without supervision fails on distractor-rich transfer scenarios</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Not explicitly shown to scale across tasks with distractors; underperforms on the four DCS tasks compared to LAOM+supervision</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Susceptible to codebook collapse in VQ-VAE, heavily harmed by action-correlated distractors, quantization incentivizes encoding distractors when they better explain dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Paper ablates quantization (removal improves performance), reconstruction removal, latent dimension changes — these ablations motivated LAOM changes</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>No clear sample-efficiency advantage in presence of distractors; requires full unlabeled dataset to pretrain but yields poor downstream performance without supervision</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>LAPO generalizes poorly to novel distractor backgrounds compared to LAOM+supervision and IDM in the experiments</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>LAPO reconstructs images in FDM; paper reports reconstruction objective forces latents to include pixel-level changes (including distractors) but does not provide numerical reconstruction metrics</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Paper demonstrates LAPO latents often capture distractor dynamics rather than control-related features when distractors present (via probes and downstream performance)</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>LAPO's quantized latent bottleneck is a fixed architectural abstraction (does not adapt dynamically); authors find it counterproductive with distractors</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not discussed</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>Discussion about information bottleneck and simplicity bias qualitatively; no formal MI measurements</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>Paper argues pixel-reconstruction in LAPO hurts in distractor settings; thus pixel-fidelity provides no benefit here</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2250.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2250.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LAM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Latent Action Model (general)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of models that infer latent actions between consecutive observations to enable pretraining from observation-only data and later decoding to ground-truth actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Latent Action Model (LAM)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>operates at latent-action level built on observation encoders; can be trained with pixel reconstruction or latent-only objectives</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>varies by instantiation: quantization (VQ), multi-step IDM, temporal consistency, or supervision; no single explicit universal mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>robotic continuous control / imitation from videos (Distracting Control Suite in this work)</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>Paper studies LAM behavior both with and without action-correlated visual distractors; shows distractors degrade unsupervised LAM</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>LAMs pretrained without supervision often fail to yield useful latent actions in distractor settings; supervised LAM variants (LAOM+supervision) substantially improve downstream imitation normalized score to 0.44 with 2.5% labels</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Parameter counts depend on instantiation (see LAPO/LAOM entries); no general FLOP estimate</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Compared to IDM and BC: unsupervised LAMs underperform when distractors are present; supervised LAMs can outperform IDM and LAPO under same label budgets</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Pretraining LAM across embodiments shows limited transfer benefit; supervision helps but does not exceed labeled-only BC in cross-embodied experiments</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>LAMs trained on combined datasets are evaluated in leave-one-out cross-embodied experiments; shared representations used but final gains limited</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Tendency to model exogenous distractors into latents, not learn control-endogenous minimal state without supervision</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>This paper contains ablations on quantization, latent dim, reconstruction vs latent loss, augmentations, and multi-step IDM for LAM variants</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Unsupervised LAMs offer pretraining benefits only on distractor-free data; with distractors supervision greatly improves sample efficiency for decoding actions (recovering ~44% with 2.5% labels)</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Supervised LAMs generalize better to novel distractors than IDM; unsupervised LAMs generalize poorly</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Varies by LAM; authors show reconstruction-based LAMs force encoding irrelevant pixel changes</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Paper explicitly studies task-relevance by probing latents for ground-truth actions and visualizing reconstruction of observations from representations (Fig.11), concluding unsupervised LAMs do not learn minimal control-related state</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>Not dynamically adjusted; architecture choices determine abstraction</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not addressed</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>Qualitative discussion of bottleneck effects; no formal bounds reported</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>Authors find pixel-fidelity harms LAMs in distractor settings and recommend latent temporal consistency instead</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2250.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2250.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IDM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inverse Dynamics Model (IDM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Predicts actions given consecutive observations (o_t, o_{t+1}); used both as a baseline for relabeling datasets and as a component of LAPO/LAOM (latent IDM).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Inverse Dynamics Model (IDM)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>operates on observation embeddings or pixels to predict action vectors (action-level prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>learned mapping from observation differences to actions; no explicit distractor filtering unless trained with supervision and augmentations</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>offline imitation/relabeling for continuous-control tasks in DCS</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>IDM performs well without distractors but generalizes poorly when distractors differ between labeled and unlabeled data (Fig.9a)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>IDM baseline performs competitively in distractor-free settings; with distractors IDM generalizes worse than LAOM+supervision (Fig.9a); specific normalized returns vary by environment and label budget (see Fig.8/13)</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>IDM parameter count reported ~192,258,965 (Table 4); training time ~5h30m (Table 2); FLOPs not reported</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>IDM relabeling is a strong baseline without distractors and with matched label/distractor distributions; underperforms LAOM+supervision on generalization to novel distractors</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>IDM trained on small labeled set then used to relabel full dataset generalizes poorly to unseen distractors or out-of-distribution backgrounds compared to LAOM+supervision</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Not specifically demonstrated to scale; IDM relabeling approach limited by labeled set coverage across distractors and tasks</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Fails when labeled dataset lacks distractor diversity or when distractors are action-correlated and differ between labeled and unlabeled data; limited generalization cap</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Used as baseline and compared against in many plots; no internal IDM ablations beyond standard hyperparameter tuning described in Appendix</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Has been shown in prior work to need as little as 10% labels to match fully labeled dataset in some settings (cited), but in distractor settings this sample efficiency breaks down</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Empirical evaluation shows IDM generalizes worse than LAOM+supervision to novel distractor backgrounds (Fig.9a)</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Not applicable (IDM predicts actions, not reconstruct images)</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>IDM implicitly focuses on features predictive of actions from labeled pairs, but when labels don't cover distractor diversity it will learn spurious correlations</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>No</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2250.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2250.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FDM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Forward Dynamics Model (FDM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Predicts next observation (or next observation embedding) conditioned on current observation and latent action; used in LAPO and LAOM as the predictive component.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Forward Dynamics Model (FDM)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>pixel-space prediction in LAPO (image decoder) or latent-space prediction in LAOM (MLP predicting embedding using latent temporal consistency)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>depends on objective: reconstruction forces modelling of all pixels; latent temporal consistency encourages compact, task-focused embeddings; no explicit attention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>DCS continuous-control tasks</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>When operating in pixel-space, FDM tends to model distractors; latent-space FDM (LAOM) reduces but can still include distractors unless supervised</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Latent-space FDM in LAOM contributes to improved probe MSE and downstream returns vs pixel-space FDM in LAPO; quantitative numbers are tied to aggregate LAOM improvements (8x probe improvement when combining ablations)</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>LAOM uses MLP-based FDM in latent space which reduces decoder costs compared to image decoders (paper reports reduced model size and faster training; exact FLOPs not reported)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Latent-space FDM (LAOM) outperforms image-space FDM (LAPO) in distractor settings; removal of reconstruction improves performance</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Latent FDM generalizes better under supervision; unsupervised latent FDM still captures distractor dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Used within shared-encoder LAMs across tasks; performance varies by supervision and architecture</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Pixel reconstruction FDM forces encoding irrelevant features; latent FDM still susceptible to encoding distractors unless grounded by supervision</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Ablation removing observation reconstruction in favor of latent temporal consistency yields better latent-action quality and speed (Section 4)</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Latent-space FDM reduces compute and can be trained faster; exact sample-efficiency gains not numerically specified</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Latent-space FDM with supervision generalizes better to novel distractors than pixel-space FDM</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Pixel-space FDM in LAPO performs image reconstruction, but quality metrics not reported; LAOM avoids image reconstruction</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Paper shows image reconstruction forces modelling task-irrelevant background; latent FDM mitigates but does not fully solve without supervision</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>No</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2250.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2250.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VQ-VAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vector Quantized Variational Autoencoder (VQ-VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A quantization mechanism used by prior LAPO to discretize latent actions (information bottleneck); found here to be susceptible to codebook collapse and harmful in distractor settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural discrete representation learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VQ-VAE quantization (as used in LAPO)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>discrete/quantized latent-action space intended to reduce capacity</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>implicit information bottleneck via quantization (limited codebook capacity) to encourage encoding of salient action-related signals</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>latent action learning for continuous-control tasks</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>Quantization was harmful even in distractor-free data in this study and exacerbated problems in presence of distractors (codebook collapse, need for heavy tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Removing quantization improved latent-action probe quality substantially (Fig.5); quantization reduced downstream performance compared to unquantized variants</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>VQ-VAE requires codebook maintenance and tuning; specific compute overhead not reported</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>FSQ (finite scalar quantization) attempted but still problematic; removal of quantization yielded better results in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Not positive in this study — quantization did not help transfer in distractor scenarios</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Not specifically evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Codebook collapse; encourages encoding distractor dynamics that better explain observations under action-correlated noise</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Explicit ablation: compare LAPO with and without quantization (removal improves both probe MSE and downstream performance)</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Quantization intended to improve sample efficiency via bottleneck but empirically harmed performance here</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Quantization reduced generalization in presence of distractors</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>N/A (quantizer component)</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Authors argue quantization incentivizes IDM to encode noise/distractors when those explain dynamics more easily</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>Qualitative discussion of information bottleneck; no quantitative MI</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2250.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2250.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FSQ</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Finite Scalar Quantization (FSQ)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modern, simpler quantization alternative to VQ-VAE tested by the authors; did not resolve quantization-related issues in this domain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Finite scalar quantization: Vq-vae made simple</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FSQ quantization</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>quantized latent-action space (scalar quantization)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>information bottleneck via scalar quantization</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>latent action learning for continuous-control tasks</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>FSQ still failed to significantly improve results in distractor settings after tuning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>FSQ did not meaningfully improve latent-action quality over VQ for the authors' settings; removing quantization entirely yielded best results</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Not reported beyond being a lighter-weight quantization than VQ-VAE</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>FSQ better than naive VQ-VAE in some RL contexts per literature but here still not sufficient; unquantized LAOM outperformed FSQ-quantized variants</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Still susceptible to encoding distractors and not solving codebook collapse issues fully in this setup</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Tried FSQ vs VQ and vs no quantization; results favored no quantization</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Not promising in distractor situations</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Same critique as VQ: information bottleneck can push encoding of distractors</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>No</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2250.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2250.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Latent temporal consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Latent Temporal Consistency Loss (temporal consistency in latent space)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A loss that enforces consistency of representation across time without reconstructing pixels, replacing image-space reconstruction and encouraging compact latent observation representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Latent temporal consistency objective</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>latent-space (semantic/compact observation embeddings) rather than pixel-level reconstruction</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>encourages representations that are predictive and stable over time, reducing incentive to model high-frequency pixel noise/distractors</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>latent action learning for DCS tasks; used in LAOM</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>Designed to mitigate modeling of distractors by not forcing pixel reconstruction; improves stability and latent-action quality in distractor settings</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Removal of reconstruction in favor of latent temporal consistency contributed to improved latent-action probe scores and downstream performance (combined with other changes yielded 8x probe improvement)</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Enables removal of expensive image decoder; decreases model size and training time compared to pixel reconstruction (exact savings not numerically specified)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Outperforms pixel-reconstruction objectives in presence of distractors; part of LAOM improvements vs LAPO</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Latent temporal consistency with supervision aids transfer/generalization to novel distractors compared to reconstruction-based training</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Used with shared encoder across tasks; helps stabilize multi-environment pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Alone can be unstable (authors note small probe MSE instability fixed by augmentations and architecture tweaks)</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Ablated against reconstruction-based FDM — showed benefits when combined with other LAOM changes</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Helps avoid aimless pixel modelling, improving training efficiency; no explicit numeric sample-efficiency provided</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Improves generalization under distractors especially when combined with augmentations and supervision</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>N/A (reconstruction removed)</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Encourages focusing on dynamics relevant for next-state prediction in latent space rather than pixel fidelity</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>No</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2250.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2250.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-step IDM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-step Inverse Dynamics Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>IDM variant that predicts latent actions z_t from o_t and o_{t+k} for k sampled from {1..K} (K=10), promoting discovery of control-endogenous information and improving latent-action quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Guaranteed discovery of control-endogenous latent states with multi-step inverse models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multi-step IDM</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>operates on observation embeddings and multi-step temporal pairs (longer temporal abstraction)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>multi-step prediction biases representation toward control-related signals that persist across multiple timesteps (reduces sensitivity to short-term distractors)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>latent action learning for DCS tasks</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>Helps in presence of action-correlated distractors by encouraging encoding of control-endogenous features</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Adding multi-step IDM alone doubled latent-action quality (probe-based) according to the paper (Section 4)</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Increases IDM training complexity (samples from multiple k), no FLOP counts reported</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Multi-step IDM improves over single-step IDM used in vanilla LAPO and over IDM-only baselines in distractor settings</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Contributes to better generalization when combined with supervision, but alone insufficient to fully recover downstream performance</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Helps shared representation focus on control-relevant dynamics across tasks; used in LAOM shared encoder setup</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Does not by itself guarantee minimality; still may include distractor info absent explicit supervision</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Ablation showing multi-step IDM provides ~2x improvement in latent-action quality (Section 4)</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Improves representation quality at same unlabeled-data scale; exact sample-efficiency not quantified</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Improves robustness to exogenous noise compared to single-step IDM</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Not applicable</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Justified by literature (Lamb et al., 2022) and empirically shown to focus representations on control-endogenous information</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>Imposes multi-step temporal abstraction but not dynamic switching</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>No</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2250.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2250.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Behavioral Cloning (BC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Supervised imitation learning baseline trained to map observations to ground-truth actions; used both as normalization oracle (BC on full labels) and as limited-label baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Behavioral Cloning (BC)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>direct observation->action mapping (no latent-action abstraction)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>learned end-to-end from labeled (observation,action) pairs; no explicit distractor filtering except via augmentations if applied</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>DCS continuous-control tasks; used to normalize performance</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>BC trained on full labeled dataset with distractors is used as normalization; BC can recover most expert performance when trained on full dataset in these experiments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>BC returns used for normalization (with distractors): cheetah 823, walker 749, hopper 253, humanoid 428 (Table 3); BC training time ~1h10m; BC param count ~107,541,504</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Parameters ~107.5M; training time ~1h10m (Table 2); single GPU H100 bf16</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>BC on full labels is the upper-bound used for normalization; LAOM+supervision recovers ~44% of BC full-data performance with only 2.5% labels</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>BC trained only on labeled subset sometimes outperforms LAM pretraining in cross-embodied experiments (paper notes LAOM supervision did not beat BC trained on provided labels)</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>BC is per-task; no shared multi-task pretraining here</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Requires sufficient labeled coverage of distractor diversity; limited when labels scarce</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Used across label budgets (2..128 trajectories) as baseline; no internal BC ablations beyond hyperparameter tuning reported</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>BC on small labeled sets (2..128 trajectories) used to study scaling; LAOM+supervision improves over BC trained from same small labeled set in many cases</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>BC generalization tied to labeled data coverage; in cross-embodied experiments BC trained on labels can match or beat pretrained LAM variants</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>BC learns whatever features present in labeled data; does not separate task-relevant vs irrelevant explicitly</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>No</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning to act without actions. <em>(Rating: 2)</em></li>
                <li>Guaranteed discovery of control-endogenous latent states with multi-step inverse models <em>(Rating: 2)</em></li>
                <li>Dynamo: In-domain dynamics pretraining for visuo-motor control <em>(Rating: 2)</em></li>
                <li>Learning predictive world models without reconstruction <em>(Rating: 2)</em></li>
                <li>Ad3: Implicit action is the key for world models to distinguish the diverse visual distractors <em>(Rating: 1)</em></li>
                <li>Learning world models with identifiable factorization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2250",
    "paper_id": "paper-276095040",
    "extraction_schema_id": "extraction-schema-62",
    "extracted_data": [
        {
            "name_short": "LAOM",
            "name_full": "Latent Action Observation Model (LAOM)",
            "brief_description": "This paper's proposed modification of LAPO that removes quantization, replaces image reconstruction with latent temporal consistency, adds multi-step inverse dynamics, larger latent-action dimensionality, and augmentations; can be supervised by a small number of ground-truth actions during pretraining.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LAOM",
            "abstraction_level": "latent-action space operating on compact latent observation embeddings (no pixel-level reconstruction; latent temporal consistency)",
            "feature_selection_mechanism": "implicit via multi-step IDM and optional linear supervision from ground-truth actions (no explicit masking); augmentations and temporal consistency encourage focusing on control-related features",
            "task_domain": "continuous-control tasks from Distracting Control Suite (cheetah-run, walker-run, hopper-hop, humanoid-walk)",
            "distractor_presence": "explicitly evaluated with visual distractors: dynamic background videos, camera shake, and agent color changes (action-correlated exogenous noise)",
            "performance_metrics": "LAOM (unsupervised) doubles downstream performance vs LAPO on average; improves latent action quality by ~8x (linear-probe measure); LAOM+supervision achieves average normalized score 0.44 using 2.5% labeled transitions and improves downstream performance by ~4.2–4.3x vs unsupervised LAOM in presence of distractors; specific BC returns used for normalization (with distractors) cheetah 823, walker 749, hopper 253, humanoid 428",
            "computational_cost_details": "Parameter counts: LAOM ~192,307,136 params; LAOM+supervision ~192,479,189 params (Table 4); training time: LAOM ~6h43m, LAOM+supervision ~7h6m (Table 2) on H100 single GPU, bf16 with AMP; FLOPs and memory not reported",
            "comparison_to_baselines": "Compared to LAPO: LAOM improves latent-action probe quality ~8x and downstream returns up to 2x in some settings; compared to IDM and BC baselines, LAOM (unsupervised) still underperforms BC/IDM in presence of distractors, but LAOM+supervision outperforms IDM and LAPO across label budgets and outperforms other baselines on average (normalized score 0.44 with 2.5% labels)",
            "transfer_learning_results": "Cross-embodied (leave-one-out) pretraining: supervision during LAOM pretraining yields large improvement vs unsupervised LAOM, but final performance is not better than BC trained only on the provided labels from the target embodiment; overall limited positive transfer — supervision helps but does not exceed labeled-only BC",
            "multi_task_performance": "Evaluated across four different environments (cheetah, walker, hopper, humanoid); LAOM shares representations (shared encoder) and is pre-trained on combined datasets in cross-embodied experiments; LAOM+supervision scales better across tasks than IDM when novel distractors are present (Figure 9a), but does not surpass BC trained on full labels",
            "failure_modes": "Without supervision LAOM tends to encode action-correlated distractors into latent actions (non-minimal representations), large latent-action dimensionality (e.g., 8192) necessary to contain true actions but sacrifices minimality and harms imitation; quantization hinders performance; performance gap remains large with distractors unless supervision is added",
            "ablation_studies": "Ablations include: removal of quantization (improves probe quality, Fig.5), increasing latent-action dim from 128 to 8192 (gives ~2.5x probe improvement), adding multi-step IDM (doubles latent-action quality), replacing reconstruction with latent temporal consistency, adding augmentations (stabilizes and improves); combined modifications yield ~8x probe improvement (Fig.6); supervision ablations show LAOM+supervision loses only 16% performance reducing latent dim 8192-&gt;64 vs 63% loss for LAOM (Fig.9b/c)",
            "sample_efficiency": "LAOM+supervision recovers ~0.44 normalized score using only ~2.5% of dataset labels (e.g., 128 labelled trajectories were used in some experiments); unsupervised LAOM requires full unlabeled dataset but yields lower downstream sample efficiency (no explicit sample-efficiency curves beyond label budgets reported)",
            "generalization_analysis": "LAOM+supervision generalizes better than IDM to novel distractor background videos (evaluation on never-seen distractor set, Fig.9a); unsupervised LAOM and LAPO generalize poorly when distractors differ between labeled and unlabeled sets",
            "reconstruction_quality": "LAOM removes pixel-level reconstruction; reconstruction quality metrics (MSE/SSIM/PSNR) not reported because model avoids image-space reconstruction in FDM, instead uses latent temporal consistency (MSE used for that loss per Appendix D)",
            "task_relevance_analysis": "Paper analyzes task-relevant vs irrelevant features by showing latent actions encode a lot of background (control-irrelevant) information without supervision (visualization in Fig.11) and via linear probes predicting ground-truth actions from latent actions and observation embeddings; supervision grounds latents to control-relevant features",
            "dynamic_abstraction": "LAOM does not dynamically alter abstraction at inference time; abstraction changes are architectural (no reconstruction vs reconstruction) and via latent dimensionality; supervision allows compact latents (down to 64) with limited degradation, i.e., supports lower abstraction dimensionality when supervised",
            "exploration_vs_exploitation": "Not explicitly analyzed; paper focuses on offline pretraining and downstream imitation evaluation, not on online exploration/exploitation trade-offs",
            "information_theoretic_analysis": "No formal information-theoretic measures (mutual information, rate-distortion) are reported; qualitative discussion of information bottleneck effects (quantization encourages encoding of distractors) is provided",
            "pixel_fidelity_benefits": "Paper argues pixel-level reconstruction is detrimental in presence of distractors because it forces modelling of irrelevant pixel changes; empirical evidence: removing reconstruction and using latent temporal consistency yields better latent-action quality and downstream performance in distractor settings",
            "uuid": "e2250.0"
        },
        {
            "name_short": "LAPO",
            "name_full": "Latent Action Policies (LAPO)",
            "brief_description": "A prior latent action learning method that infers discrete/quantized latent actions via IDM and FDM with a VQ-style quantizer and pixel-space next-observation reconstruction; used as the main baseline that the paper adapts and ablates.",
            "citation_title": "Learning to act without actions.",
            "mention_or_use": "use",
            "model_name": "LAPO",
            "abstraction_level": "original LAPO uses latent actions with quantization and pixel-level next-observation reconstruction (image-space decoder)",
            "feature_selection_mechanism": "information bottleneck via quantization (VQ-VAE) intended to force IDM to represent action-related differences; no explicit mechanism to filter distractors",
            "task_domain": "previously evaluated on ProcGen and in this work on Distracting Control Suite tasks",
            "distractor_presence": "Designed/tested originally on distractor-free data; in this paper LAPO struggles in presence of action-correlated distractors (dynamic backgrounds etc.)",
            "performance_metrics": "In this paper LAPO's latent-action quality and downstream returns are low with distractors; quantization significantly reduces quality (Fig.5); exact numerical normalized returns vary by task and label budget (see Fig.8/7)",
            "computational_cost_details": "Parameter count reported: LAPO ~211,847,849 params (Table 4); training time ~7h38m (Table 2); other FLOPs not reported",
            "comparison_to_baselines": "LAPO is outperformed by LAOM (their modifications) and by simple BC/IDM baselines in the presence of distractors; quantization is harmful compared to removing it",
            "transfer_learning_results": "Not evaluated positively in this paper for cross-embodied pretraining; LAPO pretrained without supervision fails on distractor-rich transfer scenarios",
            "multi_task_performance": "Not explicitly shown to scale across tasks with distractors; underperforms on the four DCS tasks compared to LAOM+supervision",
            "failure_modes": "Susceptible to codebook collapse in VQ-VAE, heavily harmed by action-correlated distractors, quantization incentivizes encoding distractors when they better explain dynamics",
            "ablation_studies": "Paper ablates quantization (removal improves performance), reconstruction removal, latent dimension changes — these ablations motivated LAOM changes",
            "sample_efficiency": "No clear sample-efficiency advantage in presence of distractors; requires full unlabeled dataset to pretrain but yields poor downstream performance without supervision",
            "generalization_analysis": "LAPO generalizes poorly to novel distractor backgrounds compared to LAOM+supervision and IDM in the experiments",
            "reconstruction_quality": "LAPO reconstructs images in FDM; paper reports reconstruction objective forces latents to include pixel-level changes (including distractors) but does not provide numerical reconstruction metrics",
            "task_relevance_analysis": "Paper demonstrates LAPO latents often capture distractor dynamics rather than control-related features when distractors present (via probes and downstream performance)",
            "dynamic_abstraction": "LAPO's quantized latent bottleneck is a fixed architectural abstraction (does not adapt dynamically); authors find it counterproductive with distractors",
            "exploration_vs_exploitation": "Not discussed",
            "information_theoretic_analysis": "Discussion about information bottleneck and simplicity bias qualitatively; no formal MI measurements",
            "pixel_fidelity_benefits": "Paper argues pixel-reconstruction in LAPO hurts in distractor settings; thus pixel-fidelity provides no benefit here",
            "uuid": "e2250.1"
        },
        {
            "name_short": "LAM",
            "name_full": "Latent Action Model (general)",
            "brief_description": "A class of models that infer latent actions between consecutive observations to enable pretraining from observation-only data and later decoding to ground-truth actions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Latent Action Model (LAM)",
            "abstraction_level": "operates at latent-action level built on observation encoders; can be trained with pixel reconstruction or latent-only objectives",
            "feature_selection_mechanism": "varies by instantiation: quantization (VQ), multi-step IDM, temporal consistency, or supervision; no single explicit universal mechanism",
            "task_domain": "robotic continuous control / imitation from videos (Distracting Control Suite in this work)",
            "distractor_presence": "Paper studies LAM behavior both with and without action-correlated visual distractors; shows distractors degrade unsupervised LAM",
            "performance_metrics": "LAMs pretrained without supervision often fail to yield useful latent actions in distractor settings; supervised LAM variants (LAOM+supervision) substantially improve downstream imitation normalized score to 0.44 with 2.5% labels",
            "computational_cost_details": "Parameter counts depend on instantiation (see LAPO/LAOM entries); no general FLOP estimate",
            "comparison_to_baselines": "Compared to IDM and BC: unsupervised LAMs underperform when distractors are present; supervised LAMs can outperform IDM and LAPO under same label budgets",
            "transfer_learning_results": "Pretraining LAM across embodiments shows limited transfer benefit; supervision helps but does not exceed labeled-only BC in cross-embodied experiments",
            "multi_task_performance": "LAMs trained on combined datasets are evaluated in leave-one-out cross-embodied experiments; shared representations used but final gains limited",
            "failure_modes": "Tendency to model exogenous distractors into latents, not learn control-endogenous minimal state without supervision",
            "ablation_studies": "This paper contains ablations on quantization, latent dim, reconstruction vs latent loss, augmentations, and multi-step IDM for LAM variants",
            "sample_efficiency": "Unsupervised LAMs offer pretraining benefits only on distractor-free data; with distractors supervision greatly improves sample efficiency for decoding actions (recovering ~44% with 2.5% labels)",
            "generalization_analysis": "Supervised LAMs generalize better to novel distractors than IDM; unsupervised LAMs generalize poorly",
            "reconstruction_quality": "Varies by LAM; authors show reconstruction-based LAMs force encoding irrelevant pixel changes",
            "task_relevance_analysis": "Paper explicitly studies task-relevance by probing latents for ground-truth actions and visualizing reconstruction of observations from representations (Fig.11), concluding unsupervised LAMs do not learn minimal control-related state",
            "dynamic_abstraction": "Not dynamically adjusted; architecture choices determine abstraction",
            "exploration_vs_exploitation": "Not addressed",
            "information_theoretic_analysis": "Qualitative discussion of bottleneck effects; no formal bounds reported",
            "pixel_fidelity_benefits": "Authors find pixel-fidelity harms LAMs in distractor settings and recommend latent temporal consistency instead",
            "uuid": "e2250.2"
        },
        {
            "name_short": "IDM",
            "name_full": "Inverse Dynamics Model (IDM)",
            "brief_description": "Predicts actions given consecutive observations (o_t, o_{t+1}); used both as a baseline for relabeling datasets and as a component of LAPO/LAOM (latent IDM).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Inverse Dynamics Model (IDM)",
            "abstraction_level": "operates on observation embeddings or pixels to predict action vectors (action-level prediction)",
            "feature_selection_mechanism": "learned mapping from observation differences to actions; no explicit distractor filtering unless trained with supervision and augmentations",
            "task_domain": "offline imitation/relabeling for continuous-control tasks in DCS",
            "distractor_presence": "IDM performs well without distractors but generalizes poorly when distractors differ between labeled and unlabeled data (Fig.9a)",
            "performance_metrics": "IDM baseline performs competitively in distractor-free settings; with distractors IDM generalizes worse than LAOM+supervision (Fig.9a); specific normalized returns vary by environment and label budget (see Fig.8/13)",
            "computational_cost_details": "IDM parameter count reported ~192,258,965 (Table 4); training time ~5h30m (Table 2); FLOPs not reported",
            "comparison_to_baselines": "IDM relabeling is a strong baseline without distractors and with matched label/distractor distributions; underperforms LAOM+supervision on generalization to novel distractors",
            "transfer_learning_results": "IDM trained on small labeled set then used to relabel full dataset generalizes poorly to unseen distractors or out-of-distribution backgrounds compared to LAOM+supervision",
            "multi_task_performance": "Not specifically demonstrated to scale; IDM relabeling approach limited by labeled set coverage across distractors and tasks",
            "failure_modes": "Fails when labeled dataset lacks distractor diversity or when distractors are action-correlated and differ between labeled and unlabeled data; limited generalization cap",
            "ablation_studies": "Used as baseline and compared against in many plots; no internal IDM ablations beyond standard hyperparameter tuning described in Appendix",
            "sample_efficiency": "Has been shown in prior work to need as little as 10% labels to match fully labeled dataset in some settings (cited), but in distractor settings this sample efficiency breaks down",
            "generalization_analysis": "Empirical evaluation shows IDM generalizes worse than LAOM+supervision to novel distractor backgrounds (Fig.9a)",
            "reconstruction_quality": "Not applicable (IDM predicts actions, not reconstruct images)",
            "task_relevance_analysis": "IDM implicitly focuses on features predictive of actions from labeled pairs, but when labels don't cover distractor diversity it will learn spurious correlations",
            "dynamic_abstraction": "No",
            "exploration_vs_exploitation": "No",
            "information_theoretic_analysis": "No",
            "uuid": "e2250.3"
        },
        {
            "name_short": "FDM",
            "name_full": "Forward Dynamics Model (FDM)",
            "brief_description": "Predicts next observation (or next observation embedding) conditioned on current observation and latent action; used in LAPO and LAOM as the predictive component.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Forward Dynamics Model (FDM)",
            "abstraction_level": "pixel-space prediction in LAPO (image decoder) or latent-space prediction in LAOM (MLP predicting embedding using latent temporal consistency)",
            "feature_selection_mechanism": "depends on objective: reconstruction forces modelling of all pixels; latent temporal consistency encourages compact, task-focused embeddings; no explicit attention",
            "task_domain": "DCS continuous-control tasks",
            "distractor_presence": "When operating in pixel-space, FDM tends to model distractors; latent-space FDM (LAOM) reduces but can still include distractors unless supervised",
            "performance_metrics": "Latent-space FDM in LAOM contributes to improved probe MSE and downstream returns vs pixel-space FDM in LAPO; quantitative numbers are tied to aggregate LAOM improvements (8x probe improvement when combining ablations)",
            "computational_cost_details": "LAOM uses MLP-based FDM in latent space which reduces decoder costs compared to image decoders (paper reports reduced model size and faster training; exact FLOPs not reported)",
            "comparison_to_baselines": "Latent-space FDM (LAOM) outperforms image-space FDM (LAPO) in distractor settings; removal of reconstruction improves performance",
            "transfer_learning_results": "Latent FDM generalizes better under supervision; unsupervised latent FDM still captures distractor dynamics",
            "multi_task_performance": "Used within shared-encoder LAMs across tasks; performance varies by supervision and architecture",
            "failure_modes": "Pixel reconstruction FDM forces encoding irrelevant features; latent FDM still susceptible to encoding distractors unless grounded by supervision",
            "ablation_studies": "Ablation removing observation reconstruction in favor of latent temporal consistency yields better latent-action quality and speed (Section 4)",
            "sample_efficiency": "Latent-space FDM reduces compute and can be trained faster; exact sample-efficiency gains not numerically specified",
            "generalization_analysis": "Latent-space FDM with supervision generalizes better to novel distractors than pixel-space FDM",
            "reconstruction_quality": "Pixel-space FDM in LAPO performs image reconstruction, but quality metrics not reported; LAOM avoids image reconstruction",
            "task_relevance_analysis": "Paper shows image reconstruction forces modelling task-irrelevant background; latent FDM mitigates but does not fully solve without supervision",
            "dynamic_abstraction": "No",
            "exploration_vs_exploitation": "No",
            "information_theoretic_analysis": "No",
            "uuid": "e2250.4"
        },
        {
            "name_short": "VQ-VAE",
            "name_full": "Vector Quantized Variational Autoencoder (VQ-VAE)",
            "brief_description": "A quantization mechanism used by prior LAPO to discretize latent actions (information bottleneck); found here to be susceptible to codebook collapse and harmful in distractor settings.",
            "citation_title": "Neural discrete representation learning",
            "mention_or_use": "use",
            "model_name": "VQ-VAE quantization (as used in LAPO)",
            "abstraction_level": "discrete/quantized latent-action space intended to reduce capacity",
            "feature_selection_mechanism": "implicit information bottleneck via quantization (limited codebook capacity) to encourage encoding of salient action-related signals",
            "task_domain": "latent action learning for continuous-control tasks",
            "distractor_presence": "Quantization was harmful even in distractor-free data in this study and exacerbated problems in presence of distractors (codebook collapse, need for heavy tuning)",
            "performance_metrics": "Removing quantization improved latent-action probe quality substantially (Fig.5); quantization reduced downstream performance compared to unquantized variants",
            "computational_cost_details": "VQ-VAE requires codebook maintenance and tuning; specific compute overhead not reported",
            "comparison_to_baselines": "FSQ (finite scalar quantization) attempted but still problematic; removal of quantization yielded better results in this paper",
            "transfer_learning_results": "Not positive in this study — quantization did not help transfer in distractor scenarios",
            "multi_task_performance": "Not specifically evaluated",
            "failure_modes": "Codebook collapse; encourages encoding distractor dynamics that better explain observations under action-correlated noise",
            "ablation_studies": "Explicit ablation: compare LAPO with and without quantization (removal improves both probe MSE and downstream performance)",
            "sample_efficiency": "Quantization intended to improve sample efficiency via bottleneck but empirically harmed performance here",
            "generalization_analysis": "Quantization reduced generalization in presence of distractors",
            "reconstruction_quality": "N/A (quantizer component)",
            "task_relevance_analysis": "Authors argue quantization incentivizes IDM to encode noise/distractors when those explain dynamics more easily",
            "dynamic_abstraction": "No",
            "exploration_vs_exploitation": "No",
            "information_theoretic_analysis": "Qualitative discussion of information bottleneck; no quantitative MI",
            "uuid": "e2250.5"
        },
        {
            "name_short": "FSQ",
            "name_full": "Finite Scalar Quantization (FSQ)",
            "brief_description": "A modern, simpler quantization alternative to VQ-VAE tested by the authors; did not resolve quantization-related issues in this domain.",
            "citation_title": "Finite scalar quantization: Vq-vae made simple",
            "mention_or_use": "use",
            "model_name": "FSQ quantization",
            "abstraction_level": "quantized latent-action space (scalar quantization)",
            "feature_selection_mechanism": "information bottleneck via scalar quantization",
            "task_domain": "latent action learning for continuous-control tasks",
            "distractor_presence": "FSQ still failed to significantly improve results in distractor settings after tuning",
            "performance_metrics": "FSQ did not meaningfully improve latent-action quality over VQ for the authors' settings; removing quantization entirely yielded best results",
            "computational_cost_details": "Not reported beyond being a lighter-weight quantization than VQ-VAE",
            "comparison_to_baselines": "FSQ better than naive VQ-VAE in some RL contexts per literature but here still not sufficient; unquantized LAOM outperformed FSQ-quantized variants",
            "transfer_learning_results": "Not reported",
            "multi_task_performance": "Not reported",
            "failure_modes": "Still susceptible to encoding distractors and not solving codebook collapse issues fully in this setup",
            "ablation_studies": "Tried FSQ vs VQ and vs no quantization; results favored no quantization",
            "sample_efficiency": "Not reported",
            "generalization_analysis": "Not promising in distractor situations",
            "reconstruction_quality": "N/A",
            "task_relevance_analysis": "Same critique as VQ: information bottleneck can push encoding of distractors",
            "dynamic_abstraction": "No",
            "exploration_vs_exploitation": "No",
            "information_theoretic_analysis": "No",
            "uuid": "e2250.6"
        },
        {
            "name_short": "Latent temporal consistency",
            "name_full": "Latent Temporal Consistency Loss (temporal consistency in latent space)",
            "brief_description": "A loss that enforces consistency of representation across time without reconstructing pixels, replacing image-space reconstruction and encouraging compact latent observation representations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Latent temporal consistency objective",
            "abstraction_level": "latent-space (semantic/compact observation embeddings) rather than pixel-level reconstruction",
            "feature_selection_mechanism": "encourages representations that are predictive and stable over time, reducing incentive to model high-frequency pixel noise/distractors",
            "task_domain": "latent action learning for DCS tasks; used in LAOM",
            "distractor_presence": "Designed to mitigate modeling of distractors by not forcing pixel reconstruction; improves stability and latent-action quality in distractor settings",
            "performance_metrics": "Removal of reconstruction in favor of latent temporal consistency contributed to improved latent-action probe scores and downstream performance (combined with other changes yielded 8x probe improvement)",
            "computational_cost_details": "Enables removal of expensive image decoder; decreases model size and training time compared to pixel reconstruction (exact savings not numerically specified)",
            "comparison_to_baselines": "Outperforms pixel-reconstruction objectives in presence of distractors; part of LAOM improvements vs LAPO",
            "transfer_learning_results": "Latent temporal consistency with supervision aids transfer/generalization to novel distractors compared to reconstruction-based training",
            "multi_task_performance": "Used with shared encoder across tasks; helps stabilize multi-environment pretraining",
            "failure_modes": "Alone can be unstable (authors note small probe MSE instability fixed by augmentations and architecture tweaks)",
            "ablation_studies": "Ablated against reconstruction-based FDM — showed benefits when combined with other LAOM changes",
            "sample_efficiency": "Helps avoid aimless pixel modelling, improving training efficiency; no explicit numeric sample-efficiency provided",
            "generalization_analysis": "Improves generalization under distractors especially when combined with augmentations and supervision",
            "reconstruction_quality": "N/A (reconstruction removed)",
            "task_relevance_analysis": "Encourages focusing on dynamics relevant for next-state prediction in latent space rather than pixel fidelity",
            "dynamic_abstraction": "No",
            "exploration_vs_exploitation": "No",
            "information_theoretic_analysis": "No",
            "uuid": "e2250.7"
        },
        {
            "name_short": "Multi-step IDM",
            "name_full": "Multi-step Inverse Dynamics Model",
            "brief_description": "IDM variant that predicts latent actions z_t from o_t and o_{t+k} for k sampled from {1..K} (K=10), promoting discovery of control-endogenous information and improving latent-action quality.",
            "citation_title": "Guaranteed discovery of control-endogenous latent states with multi-step inverse models",
            "mention_or_use": "use",
            "model_name": "Multi-step IDM",
            "abstraction_level": "operates on observation embeddings and multi-step temporal pairs (longer temporal abstraction)",
            "feature_selection_mechanism": "multi-step prediction biases representation toward control-related signals that persist across multiple timesteps (reduces sensitivity to short-term distractors)",
            "task_domain": "latent action learning for DCS tasks",
            "distractor_presence": "Helps in presence of action-correlated distractors by encouraging encoding of control-endogenous features",
            "performance_metrics": "Adding multi-step IDM alone doubled latent-action quality (probe-based) according to the paper (Section 4)",
            "computational_cost_details": "Increases IDM training complexity (samples from multiple k), no FLOP counts reported",
            "comparison_to_baselines": "Multi-step IDM improves over single-step IDM used in vanilla LAPO and over IDM-only baselines in distractor settings",
            "transfer_learning_results": "Contributes to better generalization when combined with supervision, but alone insufficient to fully recover downstream performance",
            "multi_task_performance": "Helps shared representation focus on control-relevant dynamics across tasks; used in LAOM shared encoder setup",
            "failure_modes": "Does not by itself guarantee minimality; still may include distractor info absent explicit supervision",
            "ablation_studies": "Ablation showing multi-step IDM provides ~2x improvement in latent-action quality (Section 4)",
            "sample_efficiency": "Improves representation quality at same unlabeled-data scale; exact sample-efficiency not quantified",
            "generalization_analysis": "Improves robustness to exogenous noise compared to single-step IDM",
            "reconstruction_quality": "Not applicable",
            "task_relevance_analysis": "Justified by literature (Lamb et al., 2022) and empirically shown to focus representations on control-endogenous information",
            "dynamic_abstraction": "Imposes multi-step temporal abstraction but not dynamic switching",
            "exploration_vs_exploitation": "No",
            "information_theoretic_analysis": "No",
            "uuid": "e2250.8"
        },
        {
            "name_short": "BC",
            "name_full": "Behavioral Cloning (BC)",
            "brief_description": "Supervised imitation learning baseline trained to map observations to ground-truth actions; used both as normalization oracle (BC on full labels) and as limited-label baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Behavioral Cloning (BC)",
            "abstraction_level": "direct observation-&gt;action mapping (no latent-action abstraction)",
            "feature_selection_mechanism": "learned end-to-end from labeled (observation,action) pairs; no explicit distractor filtering except via augmentations if applied",
            "task_domain": "DCS continuous-control tasks; used to normalize performance",
            "distractor_presence": "BC trained on full labeled dataset with distractors is used as normalization; BC can recover most expert performance when trained on full dataset in these experiments",
            "performance_metrics": "BC returns used for normalization (with distractors): cheetah 823, walker 749, hopper 253, humanoid 428 (Table 3); BC training time ~1h10m; BC param count ~107,541,504",
            "computational_cost_details": "Parameters ~107.5M; training time ~1h10m (Table 2); single GPU H100 bf16",
            "comparison_to_baselines": "BC on full labels is the upper-bound used for normalization; LAOM+supervision recovers ~44% of BC full-data performance with only 2.5% labels",
            "transfer_learning_results": "BC trained only on labeled subset sometimes outperforms LAM pretraining in cross-embodied experiments (paper notes LAOM supervision did not beat BC trained on provided labels)",
            "multi_task_performance": "BC is per-task; no shared multi-task pretraining here",
            "failure_modes": "Requires sufficient labeled coverage of distractor diversity; limited when labels scarce",
            "ablation_studies": "Used across label budgets (2..128 trajectories) as baseline; no internal BC ablations beyond hyperparameter tuning reported",
            "sample_efficiency": "BC on small labeled sets (2..128 trajectories) used to study scaling; LAOM+supervision improves over BC trained from same small labeled set in many cases",
            "generalization_analysis": "BC generalization tied to labeled data coverage; in cross-embodied experiments BC trained on labels can match or beat pretrained LAM variants",
            "reconstruction_quality": "N/A",
            "task_relevance_analysis": "BC learns whatever features present in labeled data; does not separate task-relevant vs irrelevant explicitly",
            "dynamic_abstraction": "No",
            "exploration_vs_exploitation": "No",
            "information_theoretic_analysis": "No",
            "uuid": "e2250.9"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning to act without actions.",
            "rating": 2
        },
        {
            "paper_title": "Guaranteed discovery of control-endogenous latent states with multi-step inverse models",
            "rating": 2
        },
        {
            "paper_title": "Dynamo: In-domain dynamics pretraining for visuo-motor control",
            "rating": 2
        },
        {
            "paper_title": "Learning predictive world models without reconstruction",
            "rating": 2
        },
        {
            "paper_title": "Ad3: Implicit action is the key for world models to distinguish the diverse visual distractors",
            "rating": 1
        },
        {
            "paper_title": "Learning world models with identifiable factorization",
            "rating": 1
        }
    ],
    "cost": 0.0215445,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Latent Action Learning Requires Supervision in the Presence of Distractors
12 Jun 2025</p>
<p>Alexander Nikulin 
Ilya Zisman 
Denis Tarasov 
Nikita Lyubaykin 
Andrei Polubarov 
Igor Kiselev 
Vladislav Kurenkov 
Latent Action Learning Requires Supervision in the Presence of Distractors
12 Jun 202586BF18F750C18AAB86E744F8CD073711arXiv:2502.00379v5[cs.CV]
Recently, latent action learning, pioneered by Latent Action Policies (LAPO), have shown remarkable pre-training efficiency on observationonly data, offering potential for leveraging vast amounts of video available on the web for embodied AI.However, prior work has focused on distractor-free data, where changes between observations are primarily explained by ground-truth actions.Unfortunately, real-world videos contain action-correlated distractors that may hinder latent action learning.Using Distracting Control Suite (DCS) we empirically investigate the effect of distractors on latent action learning and demonstrate that LAPO struggle in such scenario.We propose LAOM, a simple LAPO modification that improves the quality of latent actions by 8x, as measured by linear probing.Importantly, we show that providing supervision with ground-truth actions, as few as 2.5% of the full dataset, during latent action learning improves downstream performance by 4.2x on average.Our findings suggest that integrating supervision during Latent Action Models (LAM) training is critical in the presence of distractors, challenging the conventional pipeline of first learning LAM and only then decoding from latent to ground-truth actions.</p>
<p>Introduction</p>
<p>Recently, a new wave of approaches based on latent action learning has emerged (Edwards et al., 2019), demonstrating superior pre-training efficiency on datasets without action labels in large-scale robotics (Ye et al., 2024;Chen et al., 2024b;a;Cui et al., 2024;Bruce et al., 2024) and reinforcement learning (Schmidt &amp; Jiang, 2023).Latent Action Mod-Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025.Copyright 2025 by the author(s). .We show that in the presence of distractors, LAPO struggles to learn latent actions useful for pre-training and that simple BC or IDM are more effective.We propose LAOM, a simple modification that doubles the performance but still underperforms.Thus, we propose to reuse available ground-truth action labels to supervise latent action learning, which significantly improves the performance, achieving normalized score of 0.44.It recovers almost half the performance of BC with access to the full actionlabeled dataset, while having access to only 2.5%.Results are averaged over four environments from Distracting Control Suite, three random seeds each.We provide per-environment plots on Figure 8. See Section 3 for the evaluation protocol, Sections 4 and 5 for method details. els (LAM) infer latent actions between successive observations, effectively compressing observed changes.Under certain conditions, latent actions can even rediscover the ground truth action space (Schmidt &amp; Jiang, 2023;Bruce et al., 2024).After training, LAM can be utilized for imitation learning on latent actions to obtain useful behavioral priors.For example, LAPA (Ye et al., 2024) showed that latent action learning can be used to pre-train large model on only human manipulation videos, and despite the huge cross-embodiment gap, still outperform OpenVLA (Kim et al., 2024b), which was pre-trained on expert in-domain data with available action labels.</p>
<p>Despite the initial success and the promise of unlocking vast amounts of video available on the web (Schmidt &amp; Jiang, 2023;Ye et al., 2024), there is a critical shortcoming of previous work -it uses distractor-free data, where all changes between observations are mainly and most efficiently explained by ground truth actions only, such as robot manipulation on a static background (Khazatsky et al., 2024).Unfortunately, this is not true for real-world webscale data, as it contains a lot of action-correlated noise (Misra et al., 2024), e.g.people moving in the background.Such noise may better explain video dynamics and thus lead to latent actions unrelated to real actions.The phenomenon of overfitting to task-irrelevant information is not new and has been studied in model-based (Wang et al., 2024) and representation learning (Lamb et al., 2022;Zhang et al., 2020;Zhou et al., 2023).However, the effect of distractors on latent action learning, which we aim to address in this work, has not been similarly investigated.</p>
<p>In this work we empirically investigate the effect of actioncorrelated distractors on latent action learning using Distracting Control Suite (Stone et al., 2021).We demonstrate that naive latent action learning based on quantization and reconstruction objectives, such as LAPO (Schmidt &amp; Jiang, 2023), struggle in the precense of distractors (see Section 4).</p>
<p>We propose LAOM, a simple LAPO modification that improve the quality of latent actions by 8x, as measured by linear probing, and double the downstream performance (see Figure 8).However, even after this, the resulting performance is only slightly better than simple Behavioral Cloning on available ground-truth actions.Thus, as our core contribution, we show that providing supervision with a small number, as little as 2.5% of the complete dataset, of action labels during LAOM training improves the downstream performance by 4.3x on average (see Section 5), outperforming all baselines (see Figure 8).Our findings suggest that the pipeline used in most current work (Ye et al., 2024;Cui et al., 2024;Chen et al., 2024b) to first learn LAM and only then decode to ground-truth actions is suboptimal when distractors are present, as with supervision better result can be achieved using the same budget of actions labels.In addi-
BC ❆ IDM ❆ Latent Action IDM FDM BC</p>
<p>Latent Action Model Latent Action Pretraining Action Finetuning</p>
<p>Figure 3. Overview of the latent action learning pipeline.In the first stage, the Latent Action Model (LAM) is pre-trained to infer latent actions between consecutive observations.In the second stage, the LAM is used to relabel the entire dataset with latent actions, which are then used for behavioral cloning.Finally, a decoder is trained to map from latent to true actions using a small number of labelled trajectories.In our work, we do not modify this pipeline in any way; we only examine the LAM architecture itself (see Figure 4).</p>
<p>tion, we show that latent action learning with supervision generalizes better in contrast to approaches based on inverse dynamics models (Baker et al., 2022;Zhang et al., 2022a;Zheng et al., 2023) but does not learn control-endogenous minimal state (Lamb et al., 2022).</p>
<p>Preliminaries</p>
<p>Learning from observations.Most methods in reinforcement learning require access to the dataset D := {τ n } N n=1 of N trajectories, where each τ n := {(o n i , a n i , r n i )} τ i=1 contains observations, actions and rewards.Similarly, imitation learning requires access to trajectories
τ n := {(o n i , a n i )} τ i=1
that contain actions.Unfortunately, most expert demonstrations in the real world, such as YouTube videos of some human activity (Aytar et al., 2018;Baker et al., 2022;Zhang et al., 2022a;Ghosh et al., 2023), do not include rewards or action labels.Thus, researchers are actively exploring how to most effectively use the data τ n := {(o n i )} τ i=1 without action labels to accelerate the learning of embodied agents at scale (Torabi et al., 2019).Still, we can often assume that a very small number of action labels are available.For example, previous work has explored ratios of up to 10% (Zheng et al., 2023), whereas in our work we allow a maximum of ∼ 2.5% of labeled transitions.</p>
<p>Latent action learning.Latent action learning approaches (Edwards et al., 2019;Schmidt &amp; Jiang, 2023;Chen et al., 2024b;Cui et al., 2024;Ye et al., 2024)  useful behavioral priors.Finally, small decoder heads can be learned from latent to real actions of domain of interest.</p>
<p>We base our work on LAPO (Schmidt &amp; Jiang, 2023), which is used in recent work (Chen et al., 2024b;Cui et al., 2024;Ye et al., 2024).LAPO uses two models in combination to infer latent actions.4.</p>
<p>Given the information bottleneck on latent actions, e.g.quantization via the VQ-VAE (Van Den Oord et al., 2017), IDM cannot simply copy the next observation into the FDM as is, so it will be forced to compress and encode the difference between observations to be most predictive of the next observation.Without the distractors, through simplicity bias (Shah et al., 2020), the latent actions will recover the ground truth actions as they are most predictive of the dynamics.However, due to the presence of distractors, it may be not true for real-world data.In this work, we empirically examine how well current LAMs can recover true actions in such circumstances.</p>
<p>Control-endogenous minimal state.However, as showed by Misra et al. (2024), in the presence of exogenous noise, i.e. non-iid noise that is temporally action-correlated, the sample complexity of learning control-endogenous minimal state from video data can be exponentially worse than from action-labeled data.They hypothesized that this is true for latent action learning as well but did not provide any analysis regarding the quality of latent actions, which we tried to empirically address in this work.</p>
<p>Experimental Setup</p>
<p>Environments and datasets.To decouple the effects of latent action quality and exploration on performance, we work in an offline setting.For our purposes, it is essential that the Behavior Cloning (BC) agent should recover most of the expert performance when trained on the full dataset with ground-truth actions revealed, otherwise it would be difficult to understand the effect of latent action quality on pre-training.</p>
<p>As currently existing benchmarks with distractors (Stone et al., 2021;Ortiz et al., 2024) are not yet solved, we collect new datasets with custom difficulty, based on Distracting Control Suite (DCS) (Stone et al., 2021).DCS uses dynamic background videos, camera shaking and agent color change as distractors (see Figure 2 for visualization).The complexity is determined by the number of videos as well as the scale for the magnitude of the camera and the color change.We empirically found that using 60 videos and a scale of 0.1 is the hardest setting when BC can still recover expert performance.We collect datasets with five thousand trajectories for four tasks: cheetah-run, walker-run, hopperhop and humanoid-walk, listed in the order of increasing difficulty.See Appendix C for additional details.</p>
<p>Evaluation.To access the quality of the latent actions, we use two methods.First, we follow the approach of Zhang et al. (2022b) and use linear probing (Alain, 2016), which is a common technique used to evaluate the quality of learned representations by training a simple linear classifier or regressor on top the representations.Since we include ground truth actions in our datasets for debugging purposes, we train linear probes to predict them from latent actions simultaneously with the main method, e.g.LAPO (Schmidt &amp; Jiang, 2023).We do not pass the gradient through the latent actions, so this does not affect the training.Second, following the most commonly used three-stage pipeline (Schmidt &amp; Jiang, 2023;Chen et al., 2024b;Ye et al., 2024), we first pre-train LAM, then train BC model to predict latent actions on the full dataset, and finally, we reveal a small number of labeled trajectories to train a small two-layer MLP decoder from latent to real actions (see Figure 3).Using this decoder, we then evaluate the resulting agent in the environment for 25 episodes.To access scaling properties with different budgets of real actions, similar to Schmidt &amp; Jiang (2023), we repeat this process for a variable number of labeled trajectories, from 2 to 128.All experiments are averaged over three random seeds.</p>
<p>Baselines.We use BC on true actions as our main baseline, since the main goal of latent action learning is to pre-train useful behavioral policies (Edwards et al., 2019;Schmidt &amp; Jiang, 2023), which can be achieved by recovering true actions as accurately as possible.We use it in two ways.First, we try to get the best performance for each full dataset with true actions to use the final return for normalization.With such normalization, we can quantify how much performance we have recovered compared to if we had access to a fully action-labeled dataset.Second, we train BC from scratch on the same number of labels available to LAM, to evaluate the benefit of pre-training on large unlabeled data.Our last baseline is IDM, as it remains one of the most successful and simplest approaches to learn from action-free data at scale (Baker et al., 2022;Zhang et al., 2022a;Zheng et al., 2023).For additional details, see Appendix E.</p>
<p>We do not consider other possible types of unsupervised pre-training, as it was already extensively explored by other researchers (Tomar et al., 2021;Zhang et al., 2022b;Kim et al., 2024a), even with distractors (Misra et al., 2024;Ortiz et al., 2024).Our aim is not to compare latent action learning with existing approaches, but to investigate whether it works at all in the presence of action-correlated distractors.</p>
<p>On hyperparameters tuning.We tune the hyperparameters based on online performance for BC, on MSE to real actions on the full dataset for IDM, and on final linear probe MSE to real actions for latent action learning.In more practical tasks, we usually do not have this luxury, but since we are interested in estimating the upper bound performance of each method in a controlled setting, we believe that it is appropriate.For exact hyperparameters see Appendix F.</p>
<p>Latent Action Learning Struggle in the Presence of Distractors</p>
<p>To access the effect of distractors on latent action learning we start by carefully reproducing and adapting LAPO (Schmidt &amp; Jiang, 2023) for our domain.We use similar architecture (see Figure 4) with ResNet (He et al., 2016) as observation encoders, borrowed from the open-source official LAPO implementation.Similar to Schmidt &amp; Jiang (2023) we resize observations to 64 height and width, stacking 3 consecutive frames.</p>
<p>Quantization hinders latent action learning.To validate our implementation, we first measured performance on distractor-free datasets, which should not cause any difficulty.Contrary to previous research (Schmidt &amp; Jiang, 2023;Chen et al., 2024b;Ye et al., 2024;Bruce et al., 2024), we found that commonly used latent action quantization during training significantly hindered the resulting latent action quality.We initially hypothesized that the problem might be with the VQ-VAE used for quantizing.In conversation with Schmidt &amp; Jiang (2023) we confirmed that VQ-VAE is indeed susceptible to codebook collapse and requires extensive tuning.We tried the more modern FSQ (Mentzer et al., 2023), which has already been used successfully in RL (Scannell et al., 2024) and does not suffer from codebook collapse.Unfortunately, even after tuning, we were unable to improve the results significantly, so we simply removed it.To our surprise, this resulted in a large positive improvement (see Figure 5), but only for datasets without distractors, while with distractors the action quality remained at almost the same level.</p>
<p>One explanation for the result on Figure 5 may be that we are working with continuous actions, unlike the Schmidt &amp; Jiang (2023) which used discrete actions.However, we believe that there are more general reasons.The main motivation for quantizing latent actions was to prevent shortcut learning, i.e.IDM copying o t+1 to FDM as is, and to incentivize IDM to learn simpler latents that capture only action-related changes.We observed no evidence for shortcut learning, suggesting that it is unlikely to occur with high-dimensional observations, similar to the unlikelihood of collapse in Siamese networks (Chen &amp; He, 2021).More importantly, in the presence of action-correlated distractors, the information bottleneck may have the opposite effect, incentivising the IDM to encode noise into latent actions.This noise can explain the dynamics more easily, so without guidance, the IDM has no way of distinguishing it from real actions.Therefore, we advise against the use of quantization for LAM training on real-world data.</p>
<p>Latent action quality can be significantly improved.As Figure 5 shows, naive LAPO may not be able to learn good latent actions in the presence of distractors and further improvements are needed.Thus, we propose simple modifications to the LAPO architecture, which in combination improve latent action quality by 8x, almost closing the gap with distractor-free setting (see Figure 6).Interestingly, on distractor-free data improvements are marginal, further Increasing latent actions capacity.So far we have used latent actions with 128 dimensions, as in the original LAPO.However, for reasons similar to quantization removal, we significantly increased it to 8192, as it allows better nextobservation prediction.Since IDM cannot distinguish control-related features from noise, the best we can hope for in general is to learn the full dynamics of the environment as accurately as possible.In such a case, latent actions will by definition contain true actions and we will be able to extract them via the probe.This change gives an additional 2.5x improvement.</p>
<p>Removing observation reconstruction.The need to fully reconstruct the next observation forces latent actions to encapsulate changes in each pixel, which is not always related to true actions, e.g.video in the background.Thus, we use the latent temporal consistency loss (Schwarzer et al., 2020;Hansen et al., 2022;Zhao et al., 2023)  As can be seen, LAPO struggles in the presence of distractors, being outperformed by simpler baselines.LAOM, our modification of LAPO, performs better, but not significantly.However, when we reuse the same labels used for decoding from latent to true actions to provide supervision during LAOM training (see Section 5), we significantly improve downstream performance, outperforming baselines in all environments.Importantly, all methods were pre-trained on the same unlabeled datasets and had access to exactly the same action labels, differing only in their use.Results are averaged over three random seeds.For a detailed description of the evaluation pipeline, see Section 3. observation in compact latent space without reconstruction.IDM and FMD now operate on latent representation and consist of MLPs instead of ResNets (see Figure 4).This brings additional benefits, as with such architecture we can get rid of expensive decoder, reducing model size and increasing training speed.For target next observation we use simple stop-grad as in Chen &amp; He (2021) or EMA encoder (Schwarzer et al., 2020).These change alone slightly increases probe MSE due to the instabilities.We fix them with the next change.</p>
<p>Adding augmentations.Augmentations are commonly used in conjunction with self-supervised objectives to stabilize training and avoid collapse (Schwarzer et al., 2020;Hansen et al., 2022;Zhao et al., 2023).Similarly, we found that augmentations help with stability and improve performance to even smaller probe MSE.We use the subset of augmentations from Almuzairee et al. (2024), which consists of random shifts, rotations and changes of perspective.We apply then only during latent actions training and do not use in later stages.</p>
<p>The large gap in downstream performance remains.As Figure 7 shows, our improvements partially transfer to downstream performance, as LAOM outperforms vanilla LAPO on all label budgets, improving performance by up to 2x.LAOM also outperforms LAPO on data without distractors, but not significantly.However, there remains a large gap in final performance with and without distractors.We should emphasize that this gap is not due to the fact that setting with distractors is more difficult for BC, for example.We normalize performance by the return achieved by BC trained on each full dataset with ground-truth actions.Thus, the difference in performance is relative to BC and is explained by a difference in the quality of the latent actions.</p>
<p>Unfortunately, linear probing has a major limitation -it can only tell us whether real actions are contained in latent actions or not.For example, by increasing the dimensionality of latent actions in LAOM, we have improved the quality according to the probe, but sacrificed their minimality, i.e. they additionally describe full dynamics, that is mostly unrelated to real actions.This can be detrimental as, during the BC stage, not only do we waste capacity predicting actions with higher dimensionality, but we also risk learning spurious correlations.This is probably the main reason for the poor performance, but it is the best we can do, otherwise latent actions will not contain true actions at all.</p>
<p>Latent Action Learning Requires Supervision</p>
<p>In previous sections, we proposed LAOM, an improved version of LAPO which almost doubled the downstream performance in the presence of distractors for all budgets of true action labels considered.However, overall performance remained quite low.Similar to unlikelihood of recovering the control-endogenous minimal state in the presence of distractors (Misra et al., 2024), our results suggest that without any supervision latent action learning may not be able to learn actions useful for efficient pre-training.What if we can provide supervision?Even the smallest number of true actions may ground latent action learning to focus on control-related features.We explore this in the following experiments.</p>
<p>Supervision significantly increases downstream performance.Despite the fact that existing approaches (Schmidt &amp; Jiang, 2023;Ye et al., 2024;Chen et al., 2024b) pre-train LAM without true actions, in practice we still need to have some number of labels to learn the action decoder as last stage.We reuse these labels to provide supervision by linearly predicting them from latent actions during LAOM training (see Figure 4 for the final architecture).We plot the resulting downstream performance for each environment in Figure 8 and summarize in Figure 1.As can be seen, LAOM+supervision outperforms all baselines and scales better with a larger budget of real actions.It achieves an average normalized score of 0.44, i.e. it recovers almost half the performance of BC with access to the full dataset of true actions, while using only 2.5% of the action labels.</p>
<p>Importantly, all methods have access to exactly the same number of action labels, differing only in how they use them.</p>
<p>We provide results for distractor-free data in Appendix A.</p>
<p>Latent action learning with supervision generalizes better that IDM.Learning to predict true actions with IDM with a small number of labels and then relabeling larger datasets has recently been a quite successful approach (Baker et al., 2022;Zheng et al., 2023).Unfortunately, IDM is greatly limited in its generalization capabilites as dataset with labels may not contain some distractors or cover all actions.LAOM+supervision on other hand pre-trains on full combined dataset and can adapt better to larger variety of distractors and actions.We confirm this intuition in Fig- ure 9a measuring action prediction accuracy on evaluation dataset with never seen distractor background videos.IDM indeed generalizes worse than LAOM+supervision.</p>
<p>Supervision enables compact latent actions without large performance degradation.As we mentioned earlier very high dimensional latent actions are not optimal, as they may not be minimal, i.e. contain control-unrelated information and require larger BC models to imitate accurately.Similarly, LAPA (Ye et al., 2024) also reported that more compact latent action space increases pre-training efficiency.Unfortunately, the effectiveness of LAPO and even LAOM degrades dramatically when the dimensionality of latent actions is reduced.In Figure 9b and Figure 9c we show that supervision can partially mitigate this effect.LAOM+supervision loses only 16% of performance when reducing latent actions dimensionality from 8192 to 64, compared to 63% loss for LAOM.We used 128 labeled trajectories for this experiment.</p>
<p>Supervision improves cross-embodied pre-training.So far we have used homogeneous datasets, which contain data from only one environment.However, in practice our hope is to pre-train LAM on large and diverse dataset from different embodiments, including humans (McCarthy et al., 2024;Ye et al., 2024).To access performance in such a scenario, we assemble cross-embodied datasets in a leave-one-out fashion, e.g. for the cheetah-run, we sample 1666 trajectories (to get ∼ 5k) from other environments and combine them into a single dataset.We pre-train LAM and BC on them as usual and use the labeled data from the excluded environment for action decoding or supervision during LAOM training.As Figure 10 shows supervision during LAM pretraining yields a large performance improvement.However, the final performance is no better than training BC only on the provided labels from scratch.This is slightly concerning and further emphasizes the limitations of LAM methods in the presence of distractors.</p>
<p>In contrast to IDM, latent action learning does not learn minimal state.DynaMo (Cui et al., 2024) 2024), fully ignoring control-unrelated information, such as background videos or agent color.This result appears to provide compelling evidence that using LAM exclusively as a way to obtain visual representations is not a viable approach in the presence of distractors.</p>
<p>Related Work</p>
<p>Action relabeling with inverse dynamics models.Simplest approach to utilize unlabeled data it to pretrain IDM on small number of action labels to further re-label a much large dataset (Torabi et al., 2018).Baker et al. (2022) showed that this approach can work on a scale, achieving great success in Minecraft (Kanervisto et al., 2022).Zhang et al. (2022a) used similar pipeline, unlocking hours of in-the-wild driving videos for pretraining.Schmeckpeper et al. (2020) used unlabeled human manipulation videos within online RL loop, which supplied labels to IDM for re-labeling.Zheng et al. (2023) conducted large scale analysis of IDM re-labeling in offline RL setup, showing that only 10% of suboptimal trajectories with labels is enough to match performance on fully labeled dataset.</p>
<p>In contrast to previous work (Schmeckpeper et al., 2020;Baker et al., 2022;Zheng et al., 2023), we show that while IDM is a strong baseline in setups without distractors (see Figure 15 in Appendix A), it generalizes poorly when distractors are present.Our results show that when a small number of action labels are available, it is much better to combine IDM and latent action learning to achieve much stronger performance and generalization (see Figure 8), suggesting that for web-scale data (Baker et al., 2022;Zhang et al., 2022a) our approach may be better than simple IDM re-labeling.</p>
<p>Latent action learning.To our knowledge, Edwards et al. (2019) was the first to propose the task of recovering latent actions and imitating latent policies from observation, with limited success on simple problems.However, the original objective had scalability issues (Struckmeier &amp; Kyrki, 2023).LAPO (Schmidt &amp; Jiang, 2023) greatly simplified the approach, removed scalability barriers, and for the first time achieved high success on the hard, procedurally generated ProcGen benchmark (Cobbe et al., 2020) In contrast to our work, all the mentioned approaches (Schmidt &amp; Jiang, 2023;Ye et al., 2024;Cui et al., 2024;Chen et al., 2024b;a) use data without distractors, where all changes in dynamics are mainly explained by ground truth actions only.As we show in our work (see Section 4), naive latent action learning does not work in the presence of distractors.Although we propose improvements that double the performance, it is not enough (see Figure 7).Providing supervision with a small number of action labels during LAM training significantly improves performance (see Figure 1), suggesting that the pipeline used in most current work (Ye et al., 2024;Cui et al., 2024;Chen et al., 2024b;a) to first learn LAM and only then decode to ground-truth actions is suboptimal.</p>
<p>The most closely related to us is the work of Cui et al. (2024), which also removes latent action quantization, the reconstruction objective in favor of latent temporal con-sistency (Schwarzer et al., 2020;Zhao et al., 2023), and provides ablation with ground-truth actions supervision during LAM training.However, they train LAM only as a way to pre-train visual representations and do not provide any analysis regarding the effect of their proposed changes on the quality of the resulting latent actions.This also explains why they report that supervision with true actions gives no improvement, while we show that it gives significant gains (see Figure 1).Moreover, visually reconstructing representations, we show that latent action learning methods do not produce control-endogenous state (see Figure 11), and thus are probably not suitable as a method of visual representation learning in the presence of distractors.</p>
<p>Limitations</p>
<p>There are several notable limitations to our work.First, although we used the Distracting Control Suite (Stone et al., 2021), which allows us to precisely control the difficulty of distractors in a convenient way and clearly access generalization to new distractors, the overall distribution and noise patterns may be quite different compared to real-world videos on the web.Thus, our conclusions may not be fully applicable, e.g. it is possible that supervision is not as important for relevant to embodied AI data, or vice versa, it may turn out to be much more necessary for good results than we have used.Nevertheless, we believe that the overall conclusion about the need for some form of supervision is quite general.</p>
<p>Second, the need for supervision for latent action learning is a serious limitation, as compared to our setup, which is more reminiscent of Minecraft (Kanervisto et al., 2022) or Nethack (Hambro et al., 2022), where both labeled and unlabeled data are available, we have no chance to get real labels for already existing videos on the web or to fully cover their diversity with hand-crafted labels.Therefore, further research is needed to find out whether pre-training LAM on web data combined with supervision on robot data will achieve a similar effect, although our preliminary experiment on cross-embodied pre-training is pessimistic.It is quite possible that supervision can come in other forms than ground-truth actions, as we simply need a way to ground latent actions on control-related features of the observations.For example, for egocentric videos (Grauman et al., 2022) we can use hand tracking as a proxy action to supervise latent action learning.</p>
<p>Finally, similar to offline RL (Levine et al., 2020), the problem of hyperparameter tuning remains, since without action labels there is currently no way to access the quality of latent actions.</p>
<p>Conclusion</p>
<p>In this work, we empirically investigated the effect of actioncorrelated distractors on latent action learning.We showed that LAPO struggles to learn latent actions useful for pretraining.Although we proposed LAOM, a simple modification of LAPO, which doubled performance, it did not fully close the gap with the distractor-free setting.Crucially, we found that even minimal supervision -reusing as little as 2.5% of the dataset's ground-truth action labels during latent action learning significantly improved downstream performance, challenging the conventional pipeline of first pre-training LAM and only then decoding from latent to real actions.Our findings suggest that integrating supervision is essential for robust latent action learning in real-world scenarios, paving the way for unlocking the vast amounts of video data available on the web for embodied AI.We discuss the limitations of our work in the Section 7. We took the observation embedding from the LAOM visual encoder and trained the linear probe to predict real actions, similar to probing from latent actions.(a) As can be seen, for LAOM the probe from observation embedding is better for smaller latent action dimensionality.This can be explained by the fact that the information bottleneck induces the IDM to mainly encode noise in latent actions, as it can better explain the dynamics (deterministic distractors in the background), while observation embedding mostly preserves the information.At higher latent action dimensions, they are expected to equalize, as latent actions without bottleneck can encode the full dynamics, including noise and real actions.This is exactly the effect we described in Section 4 which motivated us to add supervision.(b) However, we see a different effect with LAOM+supervision, where the probe from the embedding observation is generally worse than from the latent actions, as with supervision we can ground the latent actions to focus on features relevant for control even with small dimensions, filtering out the noise.</p>
<p>A. Additional Figures</p>
<p>(c) We re-evaluate the effect of the quantization during LAM training, given all other LAOM improvements from Section 4 and measuring actual performance in the environment instead of probing.As can be seen, quantization is indeed harmful for performance.13, analogously to our main result in Figure 1.As can be seen, supervision help even without distractors, although all methods work good in this setting.</p>
<p>B. Additional Related Work</p>
<p>Learning with distractors.Distractors in various forms are commonly used in many sub-fields of reinforcement learning, such as: visual model-based learning, model-free learning, and representation learning.</p>
<p>In model-based learning, researchers explore ways to efficiently train world models that do not waste their capacity to model task-irrelevant details, either via decomposing world models to predict relevant and irrelevant parts separately (Fu et al., 2021;Wang et al., 2022;Wan et al., 2023;Wang et al., 2024) or by avoiding reconstructing observations (Okada &amp; Taniguchi, 2021;Deng et al., 2022;Zhu et al., 2023;Liu et al., 2023b;Burchi &amp; Timofte, 2024).In our work, we have a similar need to not model action irrelevant details, as this will result in latent actions that describe changes in exogenous noise, not changes cased by ground truth actions.Thus, we use the commonly occurring latent temporal consistency loss (Schwarzer et al., 2020;Hansen et al., 2022;Zhao et al., 2023).</p>
<p>In model-free learning, researchers explore various techniques to improve generalization to new distractors and domain shifts (Hansen &amp; Wang, 2021;Hansen et al., 2021;Bertoin et al., 2022;Huang et al., 2022b;Batra &amp; Sukhatme, 2024;Almuzairee et al., 2024), which often revolves around the use of augmentations (Ma et al., 2022).In our work we also use augmentations, specifically a subset of ones proposed by Almuzairee et al. (2024), to stabilize LAM training with latent temporal consistency loss (Schwarzer et al., 2020).</p>
<p>In representation learning, researchers search for ways to obtain minimal representations that contain only task- (Yamada et al., 2022), reward-(Zhou et al., 2023) or control-related information (Zhang et al., 2020;Lamb et al., 2022;Liu et al., 2023a;Ni et al., 2024;Levine et al., 2024), as this can greatly increase sample efficiency and generalization (Kim et al., 2024a).In our work, inspired by Lamb et al. (2022), we incorporate the multi-step IDM into LAM and show that it can help learn better latent actions in the presence of exogenous noise.Moreover, when small number of ground truth actions is available for pre-training (see Figure 1), our model on them conceptually reduces to one proposed by Levine et al. (2024), for which it has been theoretically shown that it can recover control-endogenous minimal state.This may explain why incorporating labels during LAM pre-training, rather than during final fine-tuning, brings so much benefit, since discovering true actions is trivial given a minimal state.We however, found a contradicting evidence, as Figure 11 shows that our proposed methods do not learn minimal state in practice.</p>
<p>Overall, although we were inspired by existing approaches, they have not previously been used to improve latent action learning, especially in combination, which, as we show (see Figure 6) is essential for good performance in the presence of distractors.</p>
<p>C. Data Collection</p>
<p>We used environments from the Distracting Control Suite (DCS), wrapped with Shimmy wrappers for compatibility with the Gymnasium API.For cheetah-run, walker-run and hopper-hop we used PPO (Schulman et al., 2017), adapted from the CleanRL (Huang et al., 2022a) library.For humanoid-walk, we used SAC (Haarnoja et al., 2018) from the stable-baselines3 (Raffin et al., 2021) library, as PPO from CleanRL was not able to solve it at the expert level.We used default hyperparameters and trained on 1M transitions in each environment, except for humanoid-walk, where we trained on 100k transitions.Importantly, for speed, all experts were trained with proprioceptive states and no distractors, we later rendered proprioceptive states to 64px images with or without distractors during data collection.For each environment, we collected 5k trajectories, with an additional 50 trajectories for evaluation with novel distractor videos (from the evaluation set in the DCS).As each trajectory consists of 1000 steps, the datasets contain 5M transitions.We include ground truth actions and states for debugging purposes.The datasets will be released together with the main code repository.</p>
<p>D. Implementation Details</p>
<p>All experiments were run on H100 GPUs, in single-gpu mode and PyTorch bf16 precision with AMP.For the visual encoder, we used ResNets from the open-source LAPO (Schmidt &amp; Jiang, 2023) codebase, which also borrowed from baselines originally provided as part of the ProcGen 2020 competition.For the action decoder, we used a two-layer MLP with 256 hidden dimensions and ReLU activations.</p>
<p>In contrast to the commonly used cosine similarity, we used MSE for temporal consistency loss.We also found that projection heads degraded performance, so we did not use them.We use slightly non-standard MLP for latent IDM and FDM: we compose it from multiple MLP blocks inspired by Transformer architecture (Vaswani, 2017) and condition on latent action and observation representation on all layers instead of just the first.We have found that this greatly improves prediction, especially for latent actions.We also use ReLU6 activations instead of GELU, as it naturally bounds the activations, which helps with stability during training, similar to target networks in RL (Bhatt et al., 2019).Without supervision, we use the EMA target encoder.With supervision, we find that a simple stop-grad is sufficient to prevent any signs of collapse, a finding also reported by Schwarzer et al. (2020).</p>
<p>For all experiments we use the cosine learning late schedule with warmup.For hyperparameters see Appendix F. We open-source the code at https://github.com/dunnolab/laom.</p>
<p>E. Evaluation Details</p>
<p>We outline the evaluation procedures used in our experiments for each method.First, we review the general setup.For each environment, we have a large dataset without action labels, with and without distractors.To decode the learned latent actions to ground truth for evaluation, we allow a small amount of action labeled data, in line with previous work (Schmidt &amp; Jiang, 2023;Ye et al., 2024).We sample it once from the existing dataset, revealing true actions, to ensure that all methods are on equal conditions.We use identical backbones where possible, and try our best to make all methods equal in the number of trainable weights.For hyperparameters, see Appendix F. We report the scores achieved by BC trained on datasets with all actions revealed in Table 4.We use these for normalization in all our experiments.</p>
<p>BC.We trained BC from scratch to predict ground-truth actions on available labels, i.e. on 2 or 128 trajectories.</p>
<p>IDM.We used two-staged pipeline.First, we trained IDM to predict actions on available labels, i.e. on 2 trajectories.Then, we trained BC on full unlabeled dataset, providing labels via pre-trained IDM.We report BC final return.</p>
<p>LAPO and LAOM.We used three-stage pipline.First, we pre-train latent actions on full unlabeled datasets.Then, we trained BC, providing latent action labels via pre-trained LAM.Finally, we trained action decoder on small amount of labels, while freezing the rest of the policy weights.</p>
<p>LAOM+supervision.Almost like LAOM, with the difference being that we exactly aligned stages in terms of action labels used.While in LAOM we can pre-train it once and then re-use for later stages regardless of the number of action labels, in LAOM+supervision we trained separate LAM for each budget of labels.Thus, for LAOM+supervision trained with supervision from 32 trajectories of labels, on final stage the decoder was trained only on the same 32 trajectories.We repeat this process for all cases, from 2 to 128 trajectories.</p>
<p>Figure1.We show that in the presence of distractors, LAPO struggles to learn latent actions useful for pre-training and that simple BC or IDM are more effective.We propose LAOM, a simple modification that doubles the performance but still underperforms.Thus, we propose to reuse available ground-truth action labels to supervise latent action learning, which significantly improves the performance, achieving normalized score of 0.44.It recovers almost half the performance of BC with access to the full actionlabeled dataset, while having access to only 2.5%.Results are averaged over four environments from Distracting Control Suite, three random seeds each.We provide per-environment plots on Figure8.See Section 3 for the evaluation protocol, Sections 4 and 5 for method details.</p>
<p>Figure 2 .
2
Figure 2. Visualization of the environments from the Distracting Control Suite (DCS) used in our work.Top row: without any distractors, identical to the original DeepMind Control Suite.Bottom row: with distractors, which consists of dynamic background videos, agent color change and camera shaking.See Section 3 for additional details.</p>
<p>Figure 5 .
5
Figure5.Quality of latent actions learned by LAPO.We show that quantization of latent actions significantly reduces the quality of actions, even on data without distractors, where LAPO should work without problems.Removing the quantization recovers the latent action quality, but additional modifications are needed to improve LAPO performance with distractors.Results are averaged across all four environments, each with three random seeds.</p>
<p>Figure 6 .
6
Figure 6.The individual effect of each proposed change in LAOM, our modification of LAPO, which overall improves latent action quality in the presence of distractors by a factor of 8. We describe the proposed changes in detail in Section 4 and visualize the final architecture in Figure 4. Results are averaged across all four environments, each with three random seeds.</p>
<p>Figure 9 .
9
Figure9.(a) We show that latent action learning with supervision generalizes better than IDM to novel distractors for all considered budgets of ground-truth action labels available for pre-training.(b)-(c) Supervision with a small number of ground-truth actions during latent action learning allows for smaller action dimensionality without major performance degradation.Without supervision, the quality of latent actions, as well as performance, quickly degrades.</p>
<p>Figure 11 .
11
Figure11.In contrast to IDM, latent action learning encode a lot of control-unrelated information, such as background videos, into the observation representations.This finding suggest that using latent action learning exclusively as a way to pre-train visual representations is not viable in the presence of distractors.We visualize the representations by training a separate decoder to reconstruct original observations.</p>
<p>. Latent action learning was further scaled by Bruce et al. (2024); Cui et al. (2024); Ye et al. (2024); Chen et al. (2024b;a) to larger models, data, and harder, more diverse robotics domains.</p>
<p>Figure 12 .
12
Figure12.We provide additional ablations on the walker environment with three random seeds.(a)-(b) We took the observation embedding from the LAOM visual encoder and trained the linear probe to predict real actions, similar to probing from latent actions.(a) As can be seen, for LAOM the probe from observation embedding is better for smaller latent action dimensionality.This can be explained by the fact that the information bottleneck induces the IDM to mainly encode noise in latent actions, as it can better explain the dynamics (deterministic distractors in the background), while observation embedding mostly preserves the information.At higher latent action dimensions, they are expected to equalize, as latent actions without bottleneck can encode the full dynamics, including noise and real actions.This is exactly the effect we described in Section 4 which motivated us to add supervision.(b) However, we see a different effect with LAOM+supervision, where the probe from the embedding observation is generally worse than from the latent actions, as with supervision we can ground the latent actions to focus on features relevant for control even with small dimensions, filtering out the noise.(c) We re-evaluate the effect of the quantization during LAM training, given all other LAOM improvements from Section 4 and measuring actual performance in the environment instead of probing.As can be seen, quantization is indeed harmful for performance.</p>
<p>aim to infer latent actions z t such that they are maximally informative about each observed transition (o t , o t+1 ) while being minimal.After the latent action model (LAM) is pre-trained, we can train policies to imitate latent actions on full data to obtain
EncoderQuantizer EncoderDecoderEncoderLatent IDMLatent FDMLAPOLAOM
Linear IDM FDM Figure 4. Simplified architecture visualization of LAPO, and LAOM -our proposed modification.LAPO consists of IDM and FMD, both with separate encoders, uses latent action quantization and predict next observation in image space via the decoder in FDM.LAOM incorporates multi-step IDM, removes quantization and does not reconstruct images, relying on latent temporal consistency loss.Images are encoded by shared encoder, while IDM and FDM operate in compact latent space.When small number of ground-truth action labels is available, we use them for supervision, linearly predicting from latent actions.For detailed description see Section 4.</p>
<p>Performance evaluation of the LAPO and the proposed LAOM with and without distractors.As can be seen, large gap in performance remains in the presence of distractors.Results are averaged across all four environments, each with three random seeds.
Normalized Return0.1 0.2 0.3 0.4 0.5LAPO, without distractors LAOM (ours), without distractors LAPO, with distractors LAOM (ours), with distractors0.0248 Labeled trajectories 16 3264128Figure 7.
(Lamb et al., 2022;Levine et al., 2024)posed changes to specifically help latent action learning in the presence of distractors.We visualize the resulting architecture, which we called Latent Action Observation Model (LAOM) in Figure4and describe changes in detail next:Multi-step IDM.Inspired by research on control-endogenous minimal state discovery(Lamb et al., 2022;Levine et al., 2024)via multi-step IDM, we slightly modify our IDM objective to estimate latent action z t from o t and o t+k , where k ∈ {1, 2, 3, . . ., K}, instead of just consecutive observations.During training, we sampled k uniformly for each sample and found that K := 10 worked best.Multi-step IDM helps to learn representation which encodes controlendogenous information with respect to current latent actions, which in turn helps learn better latent actions.This simple change alone doubled the latent action quality.</p>
<p>Performance evaluation of latent action learning approaches and baselines across different budgets of ground-truth action labels.
LAOM+supervision (ours)LAOM (ours)LAPO (original)IDMBCNormalized Return0.1 0.2 0.3 0.4 0.5 0.6 0.72 4 8 16 32 64 128 Labeled trajectories cheetah-run Normalized Return0.1 0.2 0.3 0.4 0.52 4 8 16 32 64 128 Labeled trajectories Normalized Return walker-run0.0 0.1 0.2 0.32 4 8 16 32 64 128 Labeled trajectories Normalized Return hopper-hop0.0 0.1 0.2Labeled trajectories 2 4 8 16 32 64 128 humanoid-walkFigure 8.to predict next</p>
<p>Main results without distractors, analogously to our main result in Figure8.As can be seen, supervision help even without distractors, although all methods work good in this setting.Notably, IDM is a strong baseline.Mixed-embodied pre-training experiment results for each environment.For details see Figure14.
Normalized Return0.2 0.3 0.4 0.5 0.6 0.7LAOM+supervision (ours) LAOM (ours) LAPO (original) IDM BC0.12 4 8 16 32 64 128 Labeled trajectories cheetah-run Normalized Return LAOM+supervision (ours) 2 4 8 16 32 64 128 Labeled trajectories 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 walker-run Normalized Return LAOM (ours) Figure 13. 2 4 8 16 32 64 128 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Normalized Return Labeled trajectories 0.1 0.2 0.3 0.4 Normalized Return cheetah-run 2 4 8 16 32 64 128 Labeled trajectories 0.1 0.2 0.3 Normalized Return walker-run Normalized Return LAOM+supervision (ours) LAOM (ours) 2 4 8 16 32 64 128 Labeled trajectories 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 hopper-hop Normalized Return LAPO (original) IDM 0.0 0.1 0.2 0.3 0.4 2 4 8 16 32 64 128 Labeled trajectories 0.0 0.1 0.2 hopper-hop 0.0 0.1 Normalized Return LAPO (original) 4 8 16 32 64 128 Labeled trajectories Figure 14. 2 Figure 15. Figure summarizing the results from Figure2 4 8 16 32 64 128 Labeled trajectories humanoid-walk BC 2 4 8 16 32 64 128 Labeled trajectories humanoid-walk BC</p>
<p>Table 1 .
1
Datasets statistics.
DatasetAverage Return Size (GB)cheetah-run837.7057.7walker-run739.7957.8hopper-hop306.6357.6humanoid-walk617.2258.9</p>
<p>Table 2 .
2
Methods training time summed from all stages (including online evaluation) for each method.
MethodTraining TimeLAPO∼ 7h 38mLAOM∼ 6h 43mLAOM+supervision∼ 7h 6mBC∼ 1h 10mIDM∼ 5h 30m</p>
<p>Table 3 .
3
Evaluation returns of BC trained on full datasets with ground-truth actions revealed.We use them for normalization.
DatasetWith distractors Without distractorscheetah-run823840walker-run749735hopper-hop253300humanoid-walk428601</p>
<p>Table 4 .
4
Total parameters for each method according to the hyperparameters used in Appendix F.
DatasetTotal ParametersLAPO211847849LAOM192307136LAOM+supervision192479189BC (on all stages)107541504IDM192258965</p>
<p>Table 5 .
5
LAPO hyperparameters.We use the same hyperparameters for all experiments and explicitly mention any exceptions.Names are exactly follow the configuration files used in code.
StageParameterValuegrad normNonebatch size512num epochs10frame stack3encoder deepFalseLatent actions learningweight decay encoder scaleNone 6learning rate0.0001warmup epochs3future obs offset10latent action dim8192encoder num res blocks2dropout0.0use augFalsebatch size512num epochs10frame stack3Latent behavior cloningencoder deepFalseweight decayNoneencoder scale32learning rate0.0001warmup epochs0encoder num res blocks2use augFalsebatch size512hidden dim256Latent actions decodingweight decay eval episodesNone 25learning rate0.0003total updates2500warmup epochs0.0</p>
<p>Table 6 .
6
LAOM hyperparameters.We use the same hyperparameters for all experiments and explicitly mention any exceptions.Names are exactly follow the configuration files used in code.
StageParameterValueuse augTruegrad normNonebatch size512num epochs10target tau0.001frame stack3act head dim1024encoder deepFalseobs head dim1024weight decayNoneLatent actions learningencoder scale6learning rate0.0001warmup epochs3encoder dropout0.0act head dropout0.0encoder norm outFalseobs head dropout0.0future obs offset10latent action dim8192target update every1encoder num res blocks2dropout0.0use augFalsebatch size512num epochs10frame stack3Latent behavior cloningencoder deepFalseweight decayNoneencoder scale32learning rate0.0001warmup epochs0.0encoder num res blocks2use augFalsebatch size512hidden dim256Latent actions decodingweight decay eval episodesNone 25learning rate0.0003total updates2500warmup epochs0</p>
<p>Table 7 .
7
LAOM+supervision hyperparameters.We use the same hyperparameters for all experiments and explicitly mention any exceptions.Names are exactly follow the configuration files used in code.
StageParameterValueuse augTruegrad normNonebatch size512num epochs10target tau0.001frame stack3act head dim1024encoder deepFalseobs head dim1024weight decay0.0encoder scale6Latent actions learninglearning rate0.0001warmup epochs3encoder dropout0.0act head dropout0.0encoder norm outFalseobs head dropout0.0future obs offset10labeled loss coef0.01 (0.001, cheetah-run)latent action dim8192labeled batch size128target update every1encoder num res blocks2dropout0.0use augFalsebatch size512num epochs10frame stack3Latent behavior cloningencoder deepFalseweight decayNoneencoder scale32learning rate0.0001warmup epochs0encoder num res blocks2use augFalsebatch size512hidden dim256Latent actions decodingweight decay eval episodes0 25learning rate0.0003total updates2500warmup epochs0</p>
<p>Table 8 .
8
IDM hyperparameters.We use the same hyperparameters for all experiments and explicitly mention any exceptions.Names are exactly follow the configuration files used in code.
StageParameterValueuse augFalsegrad normNonebatch size512frame stack3act head dim1024encoder deepFalseweight decayNoneIDM learningencoder scale12learning rate0.0001total updates10000warmup epochs3encoder dropout0.0act head dropout0.0future obs offset1encoder num res blocks2dropout0.0use augFalsebatch size512num epochs10frame stack3Behavior cloning on IDM actionsencoder deepFalseweight decayNoneencoder scale32eval episodes25learning rate0.0001warmup epochs0encoder num res blocks2</p>
<p>Table 9 .
9
BC as baseline hyperparameters.We use the same hyperparameters for all experiments and explicitly mention any exceptions.Names are exactly follow the configuration files used in code.
ParameterValuedropout0.0use augfalsebatch size512frame stack3encoder deepfalseweight decay0encoder scale32eval episodes25learning rate0.0001total updates10000warmup epochs0cooldown ratio0encoder num res blocks2</p>
<p>Table 10 .
10
BC for normalization hyperparameters.We use the same hyperparameters for all experiments and explicitly mention any exceptions.Names are exactly follow the configuration files used in code.
ParameterValuedropout0.0use augfalsebatch size512frame stack3encoder deepfalseweight decay0encoder scale32eval episodes25learning rate0.0001num epochs10warmup epochs0cooldown ratio0encoder num res blocks2
AIRI
MIPT
Skoltech
Research Center for Trusted Artificial Intelligence, ISP RAS
Innopolis University
Accenture. Correspondence to: Alexander Nikulin <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#110;&#105;&#107;&#117;&#108;&#105;&#110;&#64;&#97;&#105;&#114;&#105;&#46;&#110;&#101;&#116;">&#110;&#105;&#107;&#117;&#108;&#105;&#110;&#64;&#97;&#105;&#114;&#105;&#46;&#110;&#101;&#116;</a>. Work done by dunnolab.ai.
AcknowledgementsThis work was supported by the Ministry of Economic Development of the RF (code 25-139-66879-1-0003).Impact StatementThis paper presents work whose goal is to advance the field of Machine Learning.There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.
Understanding intermediate layers using linear classifier probes. G Alain, arXiv:1610.016442016arXiv preprint</p>
<p>A recipe for unbounded data augmentation in visual reinforcement learning. A Almuzairee, N Hansen, H I Christensen, arXiv:2405.174162024arXiv preprint</p>
<p>Playing hard exploration games by watching youtube. Y Aytar, T Pfaff, D Budden, T Paine, Z Wang, De Freitas, N , Advances in neural information processing systems. 201831</p>
<p>Video pretraining (vpt): Learning to act by watching unlabeled online videos. B Baker, I Akkaya, P Zhokov, J Huizinga, J Tang, A Ecoffet, B Houghton, R Sampedro, J Clune, Advances in Neural Information Processing Systems. 202235</p>
<p>Zero-shot generalization of vision-based rl without data augmentation. S Batra, G S Sukhatme, arXiv:2410.074412024arXiv preprint</p>
<p>Look where you look! saliency-guided q-networks for generalization in visual reinforcement learning. D Bertoin, A Zouitine, M Zouitine, E Rachelson, Advances in Neural Information Processing Systems. 202235</p>
<p>A Bhatt, D Palenicek, B Belousov, M Argus, A Amiranashvili, T Brox, J Peters, Crossq, arXiv:1902.05605Batch normalization in deep reinforcement learning for greater sample efficiency and simplicity. 2019arXiv preprint</p>
<p>J Bruce, M D Dennis, A Edwards, J Parker-Holder, Y Shi, E Hughes, M Lai, A Mavalankar, R Steigerwald, C Apps, Forty-first International Conference on Machine Learning. 2024Generative interactive environments</p>
<p>Learning predictive world models without reconstruction. M Burchi, R Timofte, Mudreamer, arXiv:2405.150832024arXiv preprint</p>
<p>Exploring simple siamese representation learning. X Chen, K He, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2021</p>
<p>Image-goal representations are the atomic control units for foundation models in embodied ai. X Chen, J Guo, T He, C Zhang, P Zhang, D C Yang, L Zhao, J Bian, Igor, arXiv:2411.007852024aarXiv preprint</p>
<p>Y Chen, Y Ge, Y Li, Y Ge, M Ding, Y Shan, X Liu, Moto, arXiv:2412.04445Latent motion token as the bridging language for robot manipulation. 2024barXiv preprint</p>
<p>Leveraging procedural generation to benchmark reinforcement learning. K Cobbe, C Hesse, J Hilton, J Schulman, International conference on machine learning. PMLR2020</p>
<p>Dynamo: In-domain dynamics pretraining for visuo-motor control. Z J Cui, H Pan, A Iyer, S Haldar, L Pinto, arXiv:2409.121922024arXiv preprint</p>
<p>Reconstructionfree model-based reinforcement learning with prototypical representations. F Deng, I Jang, S Ahn, Dreamerpro, International conference on machine learning. PMLR2022</p>
<p>Imitating latent policies from observation. A Edwards, H Sahni, Y Schroecker, C Isbell, International conference on machine learning. PMLR2019</p>
<p>Reinforcement learning from passive data via latent intentions. X Fu, G Yang, P Agrawal, T Jaakkola, D Ghosh, C A Bhateja, S Levine, International Conference on Machine Learning. PMLR2021. 2023International Conference on Machine Learning</p>
<p>Ego4d: Around the world in 3,000 hours of egocentric video. K Grauman, A Westbury, E Byrne, Z Chavis, A Furnari, R Girdhar, J Hamburger, H Jiang, M Liu, X Liu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. T Haarnoja, A Zhou, P Abbeel, S Levine, International conference on machine learning. PMLR2018</p>
<p>Dungeons and data: A large-scale nethack dataset. E Hambro, R Raileanu, D Rothermel, V Mella, T Rocktäschel, H Küttler, N Murray, Advances in Neural Information Processing Systems. 202235</p>
<p>Generalization in reinforcement learning by soft data augmentation. N Hansen, X Wang, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE2021</p>
<p>Stabilizing deep qlearning with convnets and vision transformers under data augmentation. Advances in neural information processing systems. N Hansen, H Su, X Wang, 202134</p>
<p>N Hansen, X Wang, H Su, arXiv:2203.04955Temporal difference learning for model predictive control. 2022arXiv preprint</p>
<p>Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>Cleanrl: High-quality single-file implementations of deep reinforcement learning algorithms. S Huang, R F J Dossa, C Ye, J Braga, D Chakraborty, K Mehta, Ara Ãšjo, J G , Journal of Machine Learning Research. 232742022a</p>
<p>Spectrum random masking for generalization in image-based reinforcement learning. Y Huang, P Peng, Y Zhao, G Chen, Y Tian, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc2022b35</p>
<p>Agent-controller representations: Principled offline rl with rich exogenous information. R Islam, M Tomar, A Lamb, Y Efroni, H Zang, A Didolkar, D Misra, X Li, H Van Seijen, R T D Combes, arXiv:2211.001642022arXiv preprint</p>
<p>Minerl diamond 2021 competition: Overview, results, and lessons learned. A Kanervisto, S Milani, K Ramanauskas, N Topin, Z Lin, J Li, J Shi, D Ye, Q Fu, W Yang, W Hong, Z Huang, H Chen, G Zeng, Y Lin, V Micheli, E Alonso, F Fleuret, A Nikulin, Y Belousov, O Svidchenko, A Shpilman, Proceedings of the NeurIPS 2021 Competitions and Demonstrations Track. Proceedings of Machine Learning Research. D Kiela, M Ciccone, B Caputo, the NeurIPS 2021 Competitions and Demonstrations TrackPMLRDec 2022176</p>
<p>A Khazatsky, K Pertsch, S Nair, A Balakrishna, S Dasari, S Karamcheti, S Nasiriany, M K Srirama, L Y Chen, K Ellis, arXiv:2403.12945A large-scale in-the-wild robot manipulation dataset. 2024arXiv preprint</p>
<p>Investigating pre-training objectives for generalization in vision-based reinforcement learning. D Kim, H Lee, K Lee, D Hwang, J Choo, arXiv:2406.060372024aarXiv preprint</p>
<p>M J Kim, K Pertsch, S Karamcheti, T Xiao, A Balakrishna, S Nair, R Rafailov, E Foster, G Lam, P Sanketi, arXiv:2406.09246An open-source vision-languageaction model. 2024barXiv preprint</p>
<p>A Lamb, R Islam, Y Efroni, A Didolkar, D Misra, D Foster, L Molu, R Chari, A Krishnamurthy, J Langford, arXiv:2207.08229Guaranteed discovery of control-endogenous latent states with multi-step inverse models. 2022arXiv preprint</p>
<p>A Levine, P Stone, A Zhang, arXiv:2403.11940Multistep inverse is not all you need. 2024arXiv preprint</p>
<p>Offline reinforcement learning: Tutorial, review, and perspectives on open problems. S Levine, A Kumar, G Tucker, J Fu, arXiv:2005.016432020arXiv preprint</p>
<p>Robust representation learning by clustering with bisimulation metrics for visual reinforcement learning with distractions. Q Liu, Q Zhou, R Yang, J Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2023a37</p>
<p>Learning world models with identifiable factorization. Y Liu, B Huang, Z Zhu, H Tian, M Gong, Y Yu, K Zhang, Advances in Neural Information Processing Systems. 2023b36</p>
<p>A comprehensive survey of data augmentation in visual reinforcement learning. G Ma, Z Wang, Z Yuan, X Wang, B Yuan, D Tao, arXiv:2210.045612022arXiv preprint</p>
<p>R Mccarthy, D C Tan, D Schmidt, F Acero, N Herr, Y Du, T G Thuruthel, Z Li, arXiv:2404.19664Towards generalist robot learning from internet video: A survey. 2024arXiv preprint</p>
<p>F Mentzer, D Minnen, E Agustsson, M Tschannen, arXiv:2309.15505Finite scalar quantization: Vq-vae made simple. 2023arXiv preprint</p>
<p>Towards principled representation learning from videos for reinforcement learning. D Misra, A Saran, T Xie, A Lamb, J Langford, arXiv:2403.137652024arXiv preprint</p>
<p>T Ni, B Eysenbach, E Seyedsalehi, M Ma, C Gehring, A Mahajan, P.-L Bacon, arXiv:2401.08898Bridging state and history representations: Understanding self-predictive rl. 2024arXiv preprint</p>
<p>Dreaming: Model-based reinforcement learning by latent imagination without reconstruction. M Okada, T Taniguchi, 2021 ieee international conference on robotics and automation (icra). IEEE2021</p>
<p>Dmc-vb: A benchmark for representation learning for control with visual distractors. J Ortiz, A Dedieu, W Lehrach, S Guntupalli, C Wendelken, A Humayun, G Zhou, S Swaminathan, M Lázaro-Gredilla, K Murphy, arXiv:2409.183302024arXiv preprint</p>
<p>Stable-baselines3: Reliable reinforcement learning implementations. A Raffin, A Hill, A Gleave, A Kanervisto, M Ernestus, N Dormann, Journal of Machine Learning Research. 222682021</p>
<p>A Scannell, K Kujanpää, Y Zhao, M Nakhaei, A Solin, J Pajarinen, arXiv:2406.02696iqrl-implicitly quantized representations for sample-efficient reinforcement learning. 2024arXiv preprint</p>
<p>Reinforcement learning with videos: Combining offline observations with interaction. K Schmeckpeper, O Rybkin, K Daniilidis, S Levine, C Finn, arXiv:2011.065072020arXiv preprint</p>
<p>Learning to act without actions. D Schmidt, M Jiang, arXiv:2312.108122023arXiv preprint</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Data-efficient reinforcement learning with self-predictive representations. M Schwarzer, A Anand, R Goel, R D Hjelm, A Courville, P Bachman, arXiv:2007.059292020arXiv preprint</p>
<p>The pitfalls of simplicity bias in neural networks. H Shah, K Tamuly, A Raghunathan, P Jain, P Netrapalli, Advances in Neural Information Processing Systems. 202033</p>
<p>The distracting control suite-a challenging benchmark for reinforcement learning from pixels. A Stone, O Ramirez, K Konolige, R Jonschkowski, arXiv:2101.027222021arXiv preprint</p>
<p>Preventing mode collapse when imitating latent policies from observations. O Struckmeier, V Kyrki, 2023</p>
<p>M Tomar, U A Mishra, A Zhang, M E Taylor, arXiv:2111.07775Learning representations for pixel-based control: What matters and why?. 2021arXiv preprint</p>
<p>F Torabi, G Warnell, P Stone, arXiv:1805.01954Behavioral cloning from observation. 2018arXiv preprint</p>
<p>. F Torabi, G Warnell, P Stone, arXiv:1905.135662019arXiv preprintRecent advances in imitation learning from observation</p>
<p>Neural discrete representation learning. Advances in neural information processing systems. A Van Den Oord, O Vinyals, 201730</p>
<p>Attention is all you need. A Vaswani, Advances in Neural Information Processing Systems. 2017</p>
<p>Semail: eliminating distractors in visual imitation via separated models. S Wan, Y Wang, M Shao, R Chen, D.-C Zhan, International Conference on Machine Learning. PMLR2023</p>
<p>T Wang, S S Du, A Torralba, P Isola, A Zhang, Y Tian, arXiv:2206.15477Denoised mdps: Learning world models better than the world itself. 2022arXiv preprint</p>
<p>Ad3: Implicit action is the key for world models to distinguish the diverse visual distractors. Y Wang, S Wan, L Gan, S Feng, D.-C Zhan, arXiv:2403.099762024arXiv preprint</p>
<p>J Yamada, K Pertsch, A Gunjal, J J Lim, arXiv:2204.11827Task-induced representation learning. 2022arXiv preprint</p>
<p>S Ye, J Jang, B Jeon, S Joo, J Yang, B Peng, A Mandlekar, R Tan, Y.-W Chao, B Y Lin, arXiv:2410.11758Latent action pretraining from videos. 2024arXiv preprint</p>
<p>Learning invariant representations for reinforcement learning without reconstruction. A Zhang, R Mcallister, R Calandra, Y Gal, S Levine, arXiv:2006.107422020arXiv preprint</p>
<p>Learning to drive by watching youtube videos: Action-conditioned contrastive policy pretraining. Q Zhang, Z Peng, B Zhou, European Conference on Computer Vision. Springer2022a</p>
<p>and Carion, N. Light-weight probing of unsupervised representations for reinforcement learning. W Zhang, A Gx-Chen, V Sobal, Y Lecun, arXiv:2208.123452022barXiv preprint</p>
<p>Simplified temporal consistency reinforcement learning. Y Zhao, W Zhao, R Boney, J Kannala, J Pajarinen, International Conference on Machine Learning. PMLR2023</p>
<p>Semisupervised offline reinforcement learning with actionfree trajectories. Q Zheng, M Henaff, B Amos, A Grover, International conference on machine learning. PMLR2023</p>
<p>Learning robust representation for reinforcement learning with distractions by reward sequence prediction. Q Zhou, J Wang, Q Liu, Y Kuang, W Zhou, H Li, Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence. R J Evans, I Shpitser, the Thirty-Ninth Conference on Uncertainty in Artificial IntelligencePMLR31 Jul-04 Aug 2023216of Proceedings of Machine Learning Research</p>
<p>Resilient model-based reinforcement learning by regularizing posterior predictability. C Zhu, M Simchowitz, S Gadipudi, A Gupta, Repo, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>            </div>
        </div>

    </div>
</body>
</html>
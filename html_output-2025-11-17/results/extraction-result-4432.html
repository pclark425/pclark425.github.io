<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4432 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4432</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4432</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-274822886</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2412.13645v1.pdf" target="_blank">On the Role of Model Prior in Real-World Inductive Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) show impressive inductive reasoning capabilities, enabling them to generate hypotheses that could generalize effectively to new instances when guided by in-context demonstrations. However, in real-world applications, LLMs' hypothesis generation is not solely determined by these demonstrations but is significantly shaped by task-specific model priors. Despite their critical influence, the distinct contributions of model priors versus demonstrations to hypothesis generation have been underexplored. This study bridges this gap by systematically evaluating three inductive reasoning strategies across five real-world tasks with three LLMs. Our empirical findings reveal that, hypothesis generation is primarily driven by the model's inherent priors; removing demonstrations results in minimal loss of hypothesis quality and downstream usage. Further analysis shows the result is consistent across various label formats with different label configurations, and prior is hard to override, even under flipped labeling. These insights advance our understanding of the dynamics of hypothesis generation in LLMs and highlight the potential for better utilizing model priors in real-world inductive reasoning tasks.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4432.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4432.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hypothesis-based inference</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hypothesis-based classification inference</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Quantitative evaluation that applies generated natural-language hypotheses as explicit labeling rules/patterns to a held-out test set and measures downstream predictive performance (accuracy) of those hypotheses as classifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Hypothesis-based classification (predictive performance)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Each generated natural-language hypothesis is converted into a deterministic pattern check (single hypothesis or a small hypothesis bank) and applied to test examples; if an example satisfies the pattern it is assigned the corresponding class. Performance is measured on held-out test sets (single-hypothesis and multiple-hypothesis modes). The paper also reports a controlled variant that removes task-specific knowledge from the inference prompt to reduce prior influence.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Predictive accuracy on test set (mean ± std across random seeds), comparative accuracy difference between with/without demonstrations, single-hypothesis vs multi-hypothesis performance, robustness across label configurations (correct, flipped, random, only-positive, only-negative).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o; experiments also run with Qwen2-VL-72B and gemini-1.5-pro-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B (Qwen2-VL); other model sizes not specified</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Cross-domain real-world classification tasks (vision/vision-question hallucination, medical imaging (pneumonia), social-media text: unhealthy comments, funny Reddit posts, deceptive hotel reviews)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Empirical/predictive classification hypotheses — natural-language rules describing positive-class patterns</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Across five datasets and three baselines, removing in-context demonstrations produced minimal degradation in hypothesis-based accuracy (most differences < ~3%); an exception was the truthful hotel reviews where flipped labels produced ~4.5% degradation. Iterative-refinement yielded the best downstream accuracy among baselines, indicating that validation-driven selection/refinement improves use of data for hypothesis selection even when demonstrations add little to generation.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated: standard predictive metrics (accuracy) computed on held-out test sets; reported as mean ± standard deviation over seeds and across single/multiple hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Robustness checks with three random seeds, use of a validation set to rank/select hypotheses (validation-accuracy), and an alternate inference prompt that removes task-specific knowledge (Appendix C.1) to isolate hypothesis quality from model priors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Evaluation conflates model prior and hypothesis quality because LLM priors heavily influence generated hypotheses; method limited to classification tasks; mapping natural-language hypotheses to deterministic checks can be brittle and depends on prompt engineering for evaluation; priors can overshadow demonstration effects making it hard to assess demonstration usefulness.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Five real-world datasets used as benchmarks: hallucination pattern induction (vision+VQA), Unhealthy Comments (conversation toxic content), Funny Reddit posts (r/Jokes), PneumoniaMNIST (chest X-ray), and Truthful Hotel Reviews (deception detection).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Role of Model Prior in Real-World Inductive Reasoning', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4432.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4432.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-based evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based scoring and pairwise comparison (helpfulness & novelty)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automated evaluation where LLMs rate or compare generated hypotheses on axes such as helpfulness and novelty using scoring (1–5 scale) and pairwise forced-choice comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM-based scoring and pairwise comparison (helpfulness & novelty)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each hypothesis the LLM assigns a 1–5 score on predefined dimensions (helpfulness = how well the hypothesis captures underlying data patterns/generalizes; novelty = whether the hypothesis provides new insights). Additionally, hypotheses generated with and without demonstrations are randomly paired and the LLM is prompted to choose which hypothesis in each pair is better on a given dimension (pairwise win-rate). Results are aggregated across 25 hypotheses per baseline/setting.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Helpfulness (1–5), Novelty (1–5), pairwise win-rate (%) between conditions (w/ demos vs w/o demos), aggregated averages and standard deviations across hypotheses and datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (used for evaluation prompts); other evaluated LLMs include Qwen2-VL-72B and gemini-1.5-pro-002 for generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B (Qwen2-VL); GPT-4o/gemini sizes not specified</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Cross-domain (same datasets as hypothesis-based inference); evaluation method is domain-general</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Evaluative framework for natural-language hypotheses (assesses explanatory/predictive hypotheses produced by LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Overall LLM-based averages reported: Helpfulness overall average 4.01 (w/o demos) vs 3.95 (w/ demos); Novelty overall average 2.67 (w/o demos) vs 2.45 (w/ demos). For IO-prompting and iterative-refinement, hypotheses generated without demonstrations had higher helpfulness; novelty was higher without demonstrations for IO-prompting and HypoGeniC, while iterative-refinement tied. Pairwise comparisons showed that for helpfulness IO-prompting and iterative-refinement often preferred w/o demos, while HypoGeniC favored w/ demos.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (LLM-based) scoring + pairwise comparisons, subsequently validated against human pairwise judgments (hybrid validation).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>LLM-based evaluations were validated by running human pairwise comparisons for the same hypothesis pairs and checking consistency (paper reports using human evaluation to validate LLM-based metrics; no numeric correlation coefficient reported).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>LLM evaluators share priors with LLM generators, risking bias (evaluator and generator are not independent); novelty scores were low overall (hard to quantify novelty); LLM scoring can reflect generation priors rather than true external novelty/usefulness; sample of hypotheses is limited (25 per condition).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Same five datasets; 25 hypotheses per baseline/setting used for scoring and pairwise joins.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Role of Model Prior in Real-World Inductive Reasoning', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4432.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4432.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human pairwise evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human pairwise preference evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human evaluators perform pairwise comparisons of hypotheses (w/ vs w/o demonstrations) and select which hypothesis is higher quality or indicate no clear difference; used to validate LLM-based assessments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Human pairwise comparison (expert/non-expert raters)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For selected datasets accessible to non-experts (Unhealthy Comments, Truthful Reviews, Funny Reddit), generated hypotheses are randomly paired (with/without demonstrations) and nine human participants choose which hypothesis in each pair is higher-quality or mark indistinguishable. The interface provides context, paired hypotheses, and illustrative examples.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Pairwise preference percentages (which condition is preferred), qualitative judgments about clarity/usefulness; human judgments used to validate LLM-based metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Social-media text and deception detection domains that are accessible to lay evaluators (text datasets listed above)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Human judgment of natural-language hypotheses (evaluative rather than generative)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Across the three evaluated datasets and three baselines, hypotheses generated without demonstrations received the highest percentage of human preference overall, indicating a modest human preference for hypotheses produced from model prior alone. The magnitude varied by dataset. Nine participants were used.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Human-based; used specifically to validate LLM-based automated evaluations (hybrid approach overall).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Direct comparison of human pairwise outcomes with LLM-based pairwise/scoring outcomes (paper states human evaluation validates LLM-based evaluation but does not report exact statistical agreement metric).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Small evaluator pool (n=9), limited to datasets that are accessible to non-experts, potential lack of domain expertise for some tasks, potential subjectivity and limited statistical power.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Role of Model Prior in Real-World Inductive Reasoning', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4432.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4432.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ACR/BCR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adverse Correction Rate (ACR) and Beneficial Correction Rate (BCR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two metrics defined in the paper to quantify how flipping demonstration labels (correct vs flipped) changes downstream predictions: ACR measures fraction of examples where correct-label hypothesis predicted correctly but flipped-label hypothesis mispredicted; BCR measures the converse.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Adverse Correction Rate (ACR) and Beneficial Correction Rate (BCR)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>ACR = (# examples where hypothesis from correct labels was correct and hypothesis from flipped-label demos was incorrect) / (# examples where hypothesis from correct labels was correct). BCR = (# examples where hypothesis from correct labels was incorrect and hypothesis from flipped-label demos was correct) / (# examples where hypothesis from correct labels was incorrect). These ratios quantify how label-flipping in demonstrations adversely or beneficially changes downstream predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Fractional change in predictions attributable to flipping demonstration labels, reported per dataset and averaged across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (primary), other models used for generation in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B (Qwen2-VL); other sizes not specified</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Cross-domain classification tasks</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Diagnostic metric for robustness of hypothesis generation to demonstration label perturbations</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Flipping labels produced only minor changes in downstream predictions for four of five datasets (ACR/BCR small), but caused substantial shifts in the Truthful Hotel Reviews dataset (nearly half of predictions affected), indicating dataset-dependent sensitivity to demonstration label flips.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated metric computed over test-set predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Computed directly from test-set prediction outcomes comparing hypotheses generated with correct vs flipped demonstration labels; results presented in Table 5.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Does not explain why changes occur (mechanism); interpretation depends on test-set composition; aggregated ratios can hide per-class asymmetries; requires paired hypothesis generation runs with different demo labelings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Role of Model Prior in Real-World Inductive Reasoning', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4432.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4432.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Iterative refinement / HypoGeniC evaluation loop</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative refinement with ranking and HypoGeniC update-from-mistakes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Refinement frameworks that iteratively improve a bank of hypotheses by ranking with validation performance (iterative refinement) or by generating new hypotheses from misclassified examples (HypoGeniC), using reward scores and ranking feedback to guide subsequent generations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Iterative refinement with ranking; HypoGeniC (update-from-mistakes)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Iterative refinement: initialize a hypothesis bank via IO-prompting, evaluate hypotheses on a validation set, rank by accuracy, feed top-m hypotheses back into the LLM (plus demonstrations when available) to generate improved hypotheses; repeat for several iterations (refinement iteration=3, bank size 5). HypoGeniC: maintain a hypothesis bank and iteratively generate candidate hypotheses focused on groups of mistaken examples (when wrong-example counts reach thresholds), select hypotheses by reward scores (reward-efficient α=0.5) and update bank; when demonstrations absent use reward rankings as feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Validation set accuracy for ranking/selection, reward scores (used as ranking signal), downstream test accuracy after refinement, number of refinement iterations and bank size impact.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o primarily (used to generate and refine hypotheses), Qwen2-VL-72B and gemini-1.5-pro-002 also tested</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>72B (Qwen2-VL); other model sizes not specified</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Cross-domain classification problems</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Iteratively refined empirical hypotheses / hypothesis bank used for predictive classification</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Iterative refinement led to the best downstream classification performance among the three baselines, indicating that validation-driven selection/refinement helps pick higher-performing hypotheses even when demonstrations add little to hypothesis generation. HypoGeniC improved hypotheses by targeting incorrect-example groups but its benefits depended on the presence/absence of demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated: uses validation/test accuracy and reward scores for ranking; selection is algorithmic rather than human-in-the-loop.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validation-set ranking and multiple random seeds (averaging across 3 seeds); hyperparameters reported (refinement iterations=3, bank size=5, reward α=0.5, num_init=10, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Refinement helps selection but does not change the strong influence of model prior on initial hypothesis content; requires held-out validation data and careful reward/accuracy calibration; computational overhead due to iterative LLM calls.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On the Role of Model Prior in Real-World Inductive Reasoning', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Hypothesis generation with large language models <em>(Rating: 2)</em></li>
                <li>Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement <em>(Rating: 2)</em></li>
                <li>HypoGeniC <em>(Rating: 2)</em></li>
                <li>Data-driven discovery with large generative models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4432",
    "paper_id": "paper-274822886",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "Hypothesis-based inference",
            "name_full": "Hypothesis-based classification inference",
            "brief_description": "Quantitative evaluation that applies generated natural-language hypotheses as explicit labeling rules/patterns to a held-out test set and measures downstream predictive performance (accuracy) of those hypotheses as classifiers.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Hypothesis-based classification (predictive performance)",
            "evaluation_method_description": "Each generated natural-language hypothesis is converted into a deterministic pattern check (single hypothesis or a small hypothesis bank) and applied to test examples; if an example satisfies the pattern it is assigned the corresponding class. Performance is measured on held-out test sets (single-hypothesis and multiple-hypothesis modes). The paper also reports a controlled variant that removes task-specific knowledge from the inference prompt to reduce prior influence.",
            "evaluation_criteria": "Predictive accuracy on test set (mean ± std across random seeds), comparative accuracy difference between with/without demonstrations, single-hypothesis vs multi-hypothesis performance, robustness across label configurations (correct, flipped, random, only-positive, only-negative).",
            "model_name": "GPT-4o; experiments also run with Qwen2-VL-72B and gemini-1.5-pro-002",
            "model_size": "72B (Qwen2-VL); other model sizes not specified",
            "scientific_domain": "Cross-domain real-world classification tasks (vision/vision-question hallucination, medical imaging (pneumonia), social-media text: unhealthy comments, funny Reddit posts, deceptive hotel reviews)",
            "theory_type": "Empirical/predictive classification hypotheses — natural-language rules describing positive-class patterns",
            "human_comparison": false,
            "evaluation_results": "Across five datasets and three baselines, removing in-context demonstrations produced minimal degradation in hypothesis-based accuracy (most differences &lt; ~3%); an exception was the truthful hotel reviews where flipped labels produced ~4.5% degradation. Iterative-refinement yielded the best downstream accuracy among baselines, indicating that validation-driven selection/refinement improves use of data for hypothesis selection even when demonstrations add little to generation.",
            "automated_vs_human_evaluation": "Automated: standard predictive metrics (accuracy) computed on held-out test sets; reported as mean ± standard deviation over seeds and across single/multiple hypotheses.",
            "validation_method": "Robustness checks with three random seeds, use of a validation set to rank/select hypotheses (validation-accuracy), and an alternate inference prompt that removes task-specific knowledge (Appendix C.1) to isolate hypothesis quality from model priors.",
            "limitations_challenges": "Evaluation conflates model prior and hypothesis quality because LLM priors heavily influence generated hypotheses; method limited to classification tasks; mapping natural-language hypotheses to deterministic checks can be brittle and depends on prompt engineering for evaluation; priors can overshadow demonstration effects making it hard to assess demonstration usefulness.",
            "benchmark_dataset": "Five real-world datasets used as benchmarks: hallucination pattern induction (vision+VQA), Unhealthy Comments (conversation toxic content), Funny Reddit posts (r/Jokes), PneumoniaMNIST (chest X-ray), and Truthful Hotel Reviews (deception detection).",
            "uuid": "e4432.0",
            "source_info": {
                "paper_title": "On the Role of Model Prior in Real-World Inductive Reasoning",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "LLM-based evaluation",
            "name_full": "LLM-based scoring and pairwise comparison (helpfulness & novelty)",
            "brief_description": "Automated evaluation where LLMs rate or compare generated hypotheses on axes such as helpfulness and novelty using scoring (1–5 scale) and pairwise forced-choice comparisons.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "LLM-based scoring and pairwise comparison (helpfulness & novelty)",
            "evaluation_method_description": "For each hypothesis the LLM assigns a 1–5 score on predefined dimensions (helpfulness = how well the hypothesis captures underlying data patterns/generalizes; novelty = whether the hypothesis provides new insights). Additionally, hypotheses generated with and without demonstrations are randomly paired and the LLM is prompted to choose which hypothesis in each pair is better on a given dimension (pairwise win-rate). Results are aggregated across 25 hypotheses per baseline/setting.",
            "evaluation_criteria": "Helpfulness (1–5), Novelty (1–5), pairwise win-rate (%) between conditions (w/ demos vs w/o demos), aggregated averages and standard deviations across hypotheses and datasets.",
            "model_name": "GPT-4o (used for evaluation prompts); other evaluated LLMs include Qwen2-VL-72B and gemini-1.5-pro-002 for generation",
            "model_size": "72B (Qwen2-VL); GPT-4o/gemini sizes not specified",
            "scientific_domain": "Cross-domain (same datasets as hypothesis-based inference); evaluation method is domain-general",
            "theory_type": "Evaluative framework for natural-language hypotheses (assesses explanatory/predictive hypotheses produced by LLMs)",
            "human_comparison": true,
            "evaluation_results": "Overall LLM-based averages reported: Helpfulness overall average 4.01 (w/o demos) vs 3.95 (w/ demos); Novelty overall average 2.67 (w/o demos) vs 2.45 (w/ demos). For IO-prompting and iterative-refinement, hypotheses generated without demonstrations had higher helpfulness; novelty was higher without demonstrations for IO-prompting and HypoGeniC, while iterative-refinement tied. Pairwise comparisons showed that for helpfulness IO-prompting and iterative-refinement often preferred w/o demos, while HypoGeniC favored w/ demos.",
            "automated_vs_human_evaluation": "Automated (LLM-based) scoring + pairwise comparisons, subsequently validated against human pairwise judgments (hybrid validation).",
            "validation_method": "LLM-based evaluations were validated by running human pairwise comparisons for the same hypothesis pairs and checking consistency (paper reports using human evaluation to validate LLM-based metrics; no numeric correlation coefficient reported).",
            "limitations_challenges": "LLM evaluators share priors with LLM generators, risking bias (evaluator and generator are not independent); novelty scores were low overall (hard to quantify novelty); LLM scoring can reflect generation priors rather than true external novelty/usefulness; sample of hypotheses is limited (25 per condition).",
            "benchmark_dataset": "Same five datasets; 25 hypotheses per baseline/setting used for scoring and pairwise joins.",
            "uuid": "e4432.1",
            "source_info": {
                "paper_title": "On the Role of Model Prior in Real-World Inductive Reasoning",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Human pairwise evaluation",
            "name_full": "Human pairwise preference evaluation",
            "brief_description": "Human evaluators perform pairwise comparisons of hypotheses (w/ vs w/o demonstrations) and select which hypothesis is higher quality or indicate no clear difference; used to validate LLM-based assessments.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Human pairwise comparison (expert/non-expert raters)",
            "evaluation_method_description": "For selected datasets accessible to non-experts (Unhealthy Comments, Truthful Reviews, Funny Reddit), generated hypotheses are randomly paired (with/without demonstrations) and nine human participants choose which hypothesis in each pair is higher-quality or mark indistinguishable. The interface provides context, paired hypotheses, and illustrative examples.",
            "evaluation_criteria": "Pairwise preference percentages (which condition is preferred), qualitative judgments about clarity/usefulness; human judgments used to validate LLM-based metrics.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Social-media text and deception detection domains that are accessible to lay evaluators (text datasets listed above)",
            "theory_type": "Human judgment of natural-language hypotheses (evaluative rather than generative)",
            "human_comparison": false,
            "evaluation_results": "Across the three evaluated datasets and three baselines, hypotheses generated without demonstrations received the highest percentage of human preference overall, indicating a modest human preference for hypotheses produced from model prior alone. The magnitude varied by dataset. Nine participants were used.",
            "automated_vs_human_evaluation": "Human-based; used specifically to validate LLM-based automated evaluations (hybrid approach overall).",
            "validation_method": "Direct comparison of human pairwise outcomes with LLM-based pairwise/scoring outcomes (paper states human evaluation validates LLM-based evaluation but does not report exact statistical agreement metric).",
            "limitations_challenges": "Small evaluator pool (n=9), limited to datasets that are accessible to non-experts, potential lack of domain expertise for some tasks, potential subjectivity and limited statistical power.",
            "uuid": "e4432.2",
            "source_info": {
                "paper_title": "On the Role of Model Prior in Real-World Inductive Reasoning",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "ACR/BCR",
            "name_full": "Adverse Correction Rate (ACR) and Beneficial Correction Rate (BCR)",
            "brief_description": "Two metrics defined in the paper to quantify how flipping demonstration labels (correct vs flipped) changes downstream predictions: ACR measures fraction of examples where correct-label hypothesis predicted correctly but flipped-label hypothesis mispredicted; BCR measures the converse.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Adverse Correction Rate (ACR) and Beneficial Correction Rate (BCR)",
            "evaluation_method_description": "ACR = (# examples where hypothesis from correct labels was correct and hypothesis from flipped-label demos was incorrect) / (# examples where hypothesis from correct labels was correct). BCR = (# examples where hypothesis from correct labels was incorrect and hypothesis from flipped-label demos was correct) / (# examples where hypothesis from correct labels was incorrect). These ratios quantify how label-flipping in demonstrations adversely or beneficially changes downstream predictions.",
            "evaluation_criteria": "Fractional change in predictions attributable to flipping demonstration labels, reported per dataset and averaged across datasets.",
            "model_name": "GPT-4o (primary), other models used for generation in experiments",
            "model_size": "72B (Qwen2-VL); other sizes not specified",
            "scientific_domain": "Cross-domain classification tasks",
            "theory_type": "Diagnostic metric for robustness of hypothesis generation to demonstration label perturbations",
            "human_comparison": false,
            "evaluation_results": "Flipping labels produced only minor changes in downstream predictions for four of five datasets (ACR/BCR small), but caused substantial shifts in the Truthful Hotel Reviews dataset (nearly half of predictions affected), indicating dataset-dependent sensitivity to demonstration label flips.",
            "automated_vs_human_evaluation": "Automated metric computed over test-set predictions.",
            "validation_method": "Computed directly from test-set prediction outcomes comparing hypotheses generated with correct vs flipped demonstration labels; results presented in Table 5.",
            "limitations_challenges": "Does not explain why changes occur (mechanism); interpretation depends on test-set composition; aggregated ratios can hide per-class asymmetries; requires paired hypothesis generation runs with different demo labelings.",
            "uuid": "e4432.3",
            "source_info": {
                "paper_title": "On the Role of Model Prior in Real-World Inductive Reasoning",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Iterative refinement / HypoGeniC evaluation loop",
            "name_full": "Iterative refinement with ranking and HypoGeniC update-from-mistakes",
            "brief_description": "Refinement frameworks that iteratively improve a bank of hypotheses by ranking with validation performance (iterative refinement) or by generating new hypotheses from misclassified examples (HypoGeniC), using reward scores and ranking feedback to guide subsequent generations.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_method_name": "Iterative refinement with ranking; HypoGeniC (update-from-mistakes)",
            "evaluation_method_description": "Iterative refinement: initialize a hypothesis bank via IO-prompting, evaluate hypotheses on a validation set, rank by accuracy, feed top-m hypotheses back into the LLM (plus demonstrations when available) to generate improved hypotheses; repeat for several iterations (refinement iteration=3, bank size 5). HypoGeniC: maintain a hypothesis bank and iteratively generate candidate hypotheses focused on groups of mistaken examples (when wrong-example counts reach thresholds), select hypotheses by reward scores (reward-efficient α=0.5) and update bank; when demonstrations absent use reward rankings as feedback.",
            "evaluation_criteria": "Validation set accuracy for ranking/selection, reward scores (used as ranking signal), downstream test accuracy after refinement, number of refinement iterations and bank size impact.",
            "model_name": "GPT-4o primarily (used to generate and refine hypotheses), Qwen2-VL-72B and gemini-1.5-pro-002 also tested",
            "model_size": "72B (Qwen2-VL); other model sizes not specified",
            "scientific_domain": "Cross-domain classification problems",
            "theory_type": "Iteratively refined empirical hypotheses / hypothesis bank used for predictive classification",
            "human_comparison": false,
            "evaluation_results": "Iterative refinement led to the best downstream classification performance among the three baselines, indicating that validation-driven selection/refinement helps pick higher-performing hypotheses even when demonstrations add little to hypothesis generation. HypoGeniC improved hypotheses by targeting incorrect-example groups but its benefits depended on the presence/absence of demonstrations.",
            "automated_vs_human_evaluation": "Automated: uses validation/test accuracy and reward scores for ranking; selection is algorithmic rather than human-in-the-loop.",
            "validation_method": "Validation-set ranking and multiple random seeds (averaging across 3 seeds); hyperparameters reported (refinement iterations=3, bank size=5, reward α=0.5, num_init=10, etc.).",
            "limitations_challenges": "Refinement helps selection but does not change the strong influence of model prior on initial hypothesis content; requires held-out validation data and careful reward/accuracy calibration; computational overhead due to iterative LLM calls.",
            "uuid": "e4432.4",
            "source_info": {
                "paper_title": "On the Role of Model Prior in Real-World Inductive Reasoning",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Hypothesis generation with large language models",
            "rating": 2,
            "sanitized_title": "hypothesis_generation_with_large_language_models"
        },
        {
            "paper_title": "Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement",
            "rating": 2,
            "sanitized_title": "phenomenal_yet_puzzling_testing_inductive_reasoning_capabilities_of_language_models_with_hypothesis_refinement"
        },
        {
            "paper_title": "HypoGeniC",
            "rating": 2
        },
        {
            "paper_title": "Data-driven discovery with large generative models",
            "rating": 1,
            "sanitized_title": "datadriven_discovery_with_large_generative_models"
        }
    ],
    "cost": 0.01562125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>On the Role of Model Prior in Real-World Inductive Reasoning
18 Dec 2024</p>
<p>Zhuo Liu zhuo.liu@rochester.edu 
University of Rochester</p>
<p>Ding Yu ding.yu@rochester.edu 
University of Rochester</p>
<p>Hangfeng He hangfeng.he@rochester.edu 
University of Rochester</p>
<p>On the Role of Model Prior in Real-World Inductive Reasoning
18 Dec 2024F0170C905C62561F10E7434C4332E644arXiv:2412.13645v1[cs.AI]
Large Language Models (LLMs) show impressive inductive reasoning capabilities, enabling them to generate hypotheses that could generalize effectively to new instances when guided by in-context demonstrations.However, in realworld applications, LLMs' hypothesis generation is not solely determined by these demonstrations but is significantly shaped by taskspecific model priors.Despite their critical influence, the distinct contributions of model priors versus demonstrations to hypothesis generation have been underexplored.This study bridges this gap by systematically evaluating three inductive reasoning strategies across five real-world tasks with three LLMs.Our empirical findings reveal that, hypothesis generation is primarily driven by the model's inherent priors; removing demonstrations results in minimal loss of hypothesis quality and downstream usage.Further analysis shows the result is consistent across various label formats with different label configurations, and prior is hard to override, even under flipped labeling.These insights advance our understanding of the dynamics of hypothesis generation in LLMs and highlight the potential for better utilizing model priors in real-world inductive reasoning tasks.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have drawn significant interests due to their performance on a diverse range of reasoning tasks (Kojima et al., 2022), such as mathematical reasoning, commonsense reasoning and symbolic reasoning.Inductive reasoningan important component of reasoning (Yang et al., 2022;Heit, 2000), as a way to derive abstract hypothesis from limited specific observations, is widely regarded as a core aspect of human intelligence.</p>
<p>Existing studies primarily assess the inductive reasoning capabilities of LLMs (Wang et al., 2023;Qiu et al., 2023;Cheng et al., 2024) by evaluating their ability to generate textual hypotheses based on in-context input-output pairs and subsequently test these hypotheses on unseen examples, thereby evaluating their generalization abilities.These studies demonstrated that LLMs can propose high-quality hypotheses, establishing them as exceptional hypothesis generators (Qiu et al., 2023;Cheng et al., 2024;Li et al., 2024).</p>
<p>LLMs employ various approaches to generate hypotheses depending on the nature of the task.For symbolic tasks, such as mathematical function discovery (Shojaee et al., 2024), LLMs rely primarily on input-output mappings in demonstrations, often with minimal prior knowledge about the mathematical functions.In contrast, research by Qi et al. (2023) demonstrated that LLMs can formulate hypotheses solely from provided background information, leveraging the extensive and diverse knowledge gained during pre-training.In real-world applications, hypothesis generation tends to be datadriven , such as generating hypotheses for trending Twitter headline patterns (Zhou et al., 2024), where both prior knowledge and demonstrations are utilized.In these cases, the interaction between the model's task-specific priors and provided examples is mixed.</p>
<p>In empirical science, data-driven hypothesis generation serves as the foundational step toward scientific discovery (Majumder et al., 2024a,b).When employing LLMs for hypothesis generation, the goal is to uncover novel hypotheses that contribute fresh insights and ideas to the existing literature (Zhou et al., 2024).However, due to the combined influence of the model's prior knowledge and the provided examples, the origin of generated hypotheses often remains unclear.For certain tasks, where LLMs are pre-trained on extensive knowledge bases, a strong model prior may even overshadow the potential for generating genuinely novel insights from the provided examples.This raises a critical question: What is the role of model prior in real-world inductive reasoning?</p>
<p>To address this issue, this paper presents a systematic empirical study on real-world inductive reasoning problems, focusing on classification tasks, where hypotheses are generated to capture patterns specific to the positive class.We evaluate three representative baselines: direct input-output prompting (Qiu et al., 2023), iterative refinement with ranking (Qiu et al., 2023;Shojaee et al., 2024), and HypoGeniC (Zhou et al., 2024;Liu et al., 2024), across five diverse real-world tasks covering text, image, and image-text modalities.For each baseline, we conduct experiments where LLMs generate hypotheses both with and without demonstrations.The quality of the generated hypotheses is then evaluated from three perspectives: hypothesisbased classification performance, LLM-based assessments, and human evaluation.</p>
<p>Our experimental results reveal that, for realworld tasks where LLMs have been trained on substantial amounts of relevant data, task-specific model prior plays a dominant role in hypothesis generation.Notably, removing in-context demonstrations has minimal impact on the quality of the hypotheses.This trend holds consistently across three baselines with three LLMs: GPT-4o, Qwen2-VL and Gemini-pro, strongly suggesting that, counterintuitively, LLMs depend more on task-specific prior knowledge than on in-context demonstrations for generating hypotheses.Further analysis across various label configurations and formats supports this conclusion, indicating that model prior is often so robust that it is minimally affected by the provided examples.</p>
<p>Related Work</p>
<p>Inductive Reasoning with LLMs.Primary studies on inductive reasoning mainly focus on evaluating their inductive reasoning capabilities.Qiu et al. (2023) evaluate LLMs by inducting rules from examples, demonstrated that LLMs are good hypothesis proposers.Wang et al. (2023) uses Python programs to select better hypothesis, thus improving the inductive reasoning performance.Besides these evaluations on symbolic tasks, Yang et al. (2022) propose to induce natural language rules from natural language facts while Hypotheses-to-Theories (Zhu et al., 2023) learns rules from deduction.Similarly, Honovich et al. (2022) also show LLMs are able to infer a natural task description by provided demonstrations.Recently, some works employ LLMs to generate hypothesis that can describe the difference or shift between two distributions in different modalities, such as text (Zhong et al., 2022(Zhong et al., , 2023;;Singh et al., 2022), and image (Dunlap et al., 2024;Kim et al., 2024).Distinct from these studies, our work delves into understanding how LLMs perform inductive reasoning for real-world tasks, offering insights into their underlying mechanisms.</p>
<p>Hypothesis Generation with LLMs.Yang et al. (2023b) uses raw web corpus as observations to generate scientific hypothesis, and Pham et al. ( 2023) generates hypothesis to uncover latent topics in a text collection.In Qi et al. (2023), it shows LLMs are good hypothesis proposers with only background knowledge.Majumder et al. (2024a) provides initial evidence for LLMs to do datadriven discovery, where both search and verification of hypotheses may be carried out using a dataset alone.HypoGeniC (Zhou et al., 2024) also uses LLMs to generate hypothesis from real-world labeled examples.Si et al. (2024) and Baek et al. (2024) further explore the potential to generate hypothesis in research with LLMs to provide insights and ideas for the literature.Additionally, Liu et al. (2024) combines theory-based generation and datadriven generation to get better hypothesis.However, these works do not clearly distinguish whether the hypotheses originate from hidden knowledge or provided examples-a distinction that is the central focus of our work.</p>
<p>Natural Language Hypothesis Generation</p>
<p>Let Z = D P ∪ D N represent the labeled data for a real-world classification task T , where D P and D N correspond to demonstrations of the positive (P ) and negative (N ) classes, respectively.Each sample in Z is a pair (x, y), where x denotes the example and y ∈ {P, N } represents the label.A valid natural language hypothesis h, as introduced by Zhong et al. (2022), is expressed as a natural language string.For any example x, h is capable of determining whether x belongs to the positive or negative class.Natural language hypothesis generation involves prompting LLMs to produce a set of valid hypotheses H = {h 1 , h 2 , . . ., h m } using in-context demonstrations tailored to task T .In this paper, we consider the setting where the input to LLMs can be divided into two parts, as shown in Figure 1 guage to describe the task and the requirements for the hypothesis.(2) Demonstrations: a set of exemplars from different groups structured in a specified way to show the patterns of each group.Ideally, we aim to prompt LLMs to generate a list of valid hypothesis to maximize the downstream task performance, by carefully selecting instructions and demonstrations.There are two factors contributing to the hypothesis generation: Task-Specific Model Prior: LLMs are pretrained on a diverse set of datasets, allowing them to accumulate extensive background knowledge across a wide range of domains.When provided with a task description, the model leverages its priors to infer relevant patterns, generating hypotheses based on this internalized knowledge.Input-Label Mappings in Demonstrations: The demonstrations provided serve as a specific guidance, offering cues about how to approach the task.The model may use these demonstrations to refine its hypothesis generation, aligning its output more closely with the intended task requirements.</p>
<p>Experimental Settings</p>
<p>Hypothesis Generation Baselines</p>
<p>In this paper, we evaluate three commonly-used hypothesis generation baselines.</p>
<p>Input-Output Prompting.Input-output prompting (IO-Prompting) represents the most common approach to prompting LLMs (Qiu et al., 2023).In this standard IO-Prompting framework, we directly provide the LLMs with a set of in-context demonstrations within the prompt context.The objective is to generate m hypotheses that effectively captures the patterns of positive class P .This approach is a single-step method, utilizing the incontext demonstrations once to guide the model's hypothesis generation.</p>
<p>Iterative Refinement with Ranking.Standard IO-prompting utilizes in-context demonstrations only once, potentially under utilizing their full capacity.To address this limitation, various methods have been proposed to iteratively refine hypotheses, thereby enhancing model performance (Wang et al., 2023;Qiu et al., 2023;Shojaee et al., 2024;Xiao et al., 2024).In our approach, we iteratively refine hypotheses using ranking information as a feedback signal.</p>
<p>The refinement process begins with an initial set of m hypotheses generated via IO-prompting.At each iteration, hypotheses in the bank are ranked based on their performance on a validation set.The top-ranked m hypotheses are then fed back to the model, along with in-context demonstrations, guiding it to generate hypotheses with improved performance.In cases where no demonstrations are available, only the ranked hypotheses with their accuracies are provided in the iterative refinement process.This approach thus augments data utilization by continuously leveraging feedback to generate higher-quality hypotheses.</p>
<p>Update from Mistakes: HypoGeniC.The previous methods leverage data within one single prompt to generate hypotheses, yet using all demonstrations in a single prompt may not be optimal for performance.Therefore, we also evaluate a strategy that updates hypotheses from mistakes made by current hypothesis.We largely follow an established approach, HypoGeniC (Zhou et al., 2024;Liu et al., 2024), which iteratively generate new hypotheses from incorrect prediction examples.</p>
<p>In our evaluation, we initialize the hypothesis bank using standard IO-prompting as well as the reward scores as in Zhou et al. (2024); Liu et al. (2024).During the update phase, if the number of incorrect examples for each group reaches a predefined number, these incorrect examples are employed to guide the generation of new hypotheses.In each update, m hypotheses with highest reward scores are kept in the hypothesis bank.This iterative updating approach enables the model to adapt hypotheses progressively, making better use of feedback from misclassifications.For a fair comparison, when demonstrations are absent, we update the hypothesis by iterative refinement, using reward scores for ranking.</p>
<p>All the implementation details are in the Appendix B.</p>
<p>Evaluation of Hypothesis</p>
<p>After generating a set of hypotheses H = {h 1 , h 2 , . . ., h m }, it is crucial to evaluate their quality to ensure that the generated hypotheses are both functional and interpretable.We perform this evaluation from three perspectives: hypothesisbased classification, LLM-based evaluation and human evaluation.These complementary methods allow for a robust assessment, combining quantitative performance metrics with qualitative assessments from domain experts.</p>
<p>Hypothesis-based Inference.In hypothesisbased inference (Liu et al., 2024;Zhou et al., 2024), the goal is to assess how well the generated hypotheses support downstream decisionmaking tasks.We measure the predictive performance of the hypothesis on a test dataset D test = {(x j , y j )} Ntest j=1 .The hypothesis is evaluated based on how accurately it assigns the correct label to each input x j .Predictions are made by comparing test examples x j with learned patterns, which can consist of a single hypothesis or multiple hypotheses.If a test example satisfies the pattern, it is assigned the corresponding class.Unless otherwise stated, the results reported in this work are based on patterns formed from single hypothesis.To re-move the influence of prior in the inference, we also do hypothesis-based inference without knowledge, which can be found in Appendix C.1.See Appendix F for evaluation prompts.</p>
<p>LLM-based Evaluation.In addition to assessing the effectiveness of hypotheses in downstream task usage, we also evaluate their helpfulness (Liu et al., 2024) and novelty (Liu et al., 2024;Si et al., 2024) through LLM-based metrics.Specifically:</p>
<p>(1) Helpfulness measures the extent to which a hypothesis accurately captures the underlying patterns of the data and generalizes effectively to unseen samples.(2) Novelty assesses whether the hypothesis introduces new insights or unique perspectives relevant to the task.</p>
<p>Our LLM-based evaluation incorporates both scoring and pairwise comparison assessments.For scoring, LLMs assign a rating on a 5-point scale to reflect each hypothesis's quality.For pairwise comparison, we randomly pair hypotheses generated with and without demonstrations, and prompt the LLMs to select the better hypothesis in each pair.This pairwise evaluation provides insights into relative performance, while scoring offers an absolute measure of quality.Human Evaluation.To validate the effectiveness of LLM-based evaluation, we also conduct a human evaluation to assess the quality of the generated hypotheses.Our goal is to examine the degree of alignment between LLM-based evaluation results and those obtained from human experts.</p>
<p>Given that scoring may be challenging for human evaluators, we employ a pairwise comparison format, allowing experts to select the higher-quality hypothesis or indicate if the difference is difficult to discern.A total of nine participants are recruited for this evaluation, ensuring diverse perspectives in assessing the hypotheses.</p>
<p>For further details in both LLM-based and human evaluations, refer to Appendix D.</p>
<p>Other Settings</p>
<p>Models.We conduct experiments with GPT-4o, Qwen2-VL-72B 1 and gemini-1.5-pro-002,leveraging both open-source models and API-accessible models to ensure diverse evaluation.Unless otherwise stated, we use GPT-4o 2 in experiments.</p>
<p>1 https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-AWQ</p>
<p>2 By default, we use GPT-4o-2024-08-06.However, if a request is rejected due to safety reasons, we will switch to GPT-4o-2024-05-13.Datasets.We conduct evaluations on five realworld inductive reasoning datasets: hallucination pattern induction (Li et al., 2023), unhealthy comments (Zhong et al., 2023), funny Reddit posts (Zhong et al., 2023), pneumoniaMNIST (Xiao et al., 2024), and truthful hotel reviews (Zhou et al., 2024).</p>
<p>Our selection of datasets is motivated by three key factors: (1) their coverage of three distinct modalities-text (unhealthy comments, funny Reddit posts, and truthful hotel reviews), image (pneu-moniaMNIST), and image-text (hallucination pattern induction), (2) diverse domains, including model behavior analysis (hallucination pattern induction), medical diagnosis (pneumoniaMNIST), and social media content (unhealthy comments, funny Reddit posts, and truthful hotel reviews), and (3) their status as widely studied problems in realworld inductive reasoning tasks.Further details and more references for these datasets are provided in Appendix 6.</p>
<p>Other Parameters.The number of in-context demonstrations is set to N = 30 for IO-prompting and iterative-refinement, and N = 50 for Hy-poGenic to encourage more updates.Examples are randomly sampled from the training set.For each dataset, we generate five candidate hypothe- ses.Main results are averaged over three random seeds to ensure robustness.More implementation details can be found in Appendix B.</p>
<p>5 Task-Specific Model Prior Dominates Hypothesis Generation</p>
<p>LLMs Are Zero-Shot Hypothesis Generators</p>
<p>To see the impact of the model prior in hypothesis generation, we compare the hypothesis generation in the following two settings.</p>
<p>Model Prior Only is a typical zero-shot hypothesis generation scenario without the use of demonstrations, relying primarily on prior for generation.Demos with Ground Truth Labels is used in a typical real-world inductive reasoning tasks, with demonstrations as a specific guidance.</p>
<p>Results for single hypothesis-based and multiple hypotheses-based classification are shown in Table 1 and Table 3. From the results, We find that removing in-context demonstrations cause little degradation for the downstream task performance.The trend is consistent across five different datasets on three baselines.In some cases, LLMs can even generate better hypothesis using only model prior.</p>
<p>Additionally, iterative refinement outperforms the other two baselines, showing that data still helps for hypothesis selection, but not as in-context demonstrations for hypothesis generation.</p>
<p>Resutls with Qwen2-VL and Gemini-1.5-pro.</p>
<p>The results for single hypothesis-based classification on Qwen2-VL and Gemini-1.5-pro-002,with IO-prompting, are provided in Table 2 and Appendix C.3.These results similarly show a negligible performance drop without demonstrations, underscoring the universality of our findings across different models.</p>
<p>These results indicates LLMs are good zero-shot hypothesis proposers under strong prior, and incontext demonstrations with ground truth labels are not necessary to achieve acceptable hypothesis.This is a counter-intuitive phenomenon, given that labeled data is very important in in-context learning (Brown, 2020), which can inform the model of corresponding data distribution (Min et al., 2022).</p>
<p>Input-Label Mappings in Demonstrations Cannot Override Strong Model Prior</p>
<p>To further explore the interaction between model prior and input-label mappings in demonstrations in hypothesis generation, we use in-context demonstrations with different label settings:</p>
<p>(1) Demos with ground truth (correct) labels.</p>
<p>(2) Demos with flipped labels.</p>
<p>(3) Demos with random labels.</p>
<p>(4) Only positive group demos.</p>
<p>(5) Only negative group demos.</p>
<p>Figure 2 illustrates the relative accuracy difference between various label settings and without demonstrations.From the result, there is quite limited difference (mostly smaller than 3%) of performance among different settings, with the flipped label setting in truthful review as an exception, which has a performance degradation about 4.5%.</p>
<p>These findings suggest that while demonstrations can provide some guidance, the models' hypothesis generation abilities are ultimately shaped more by its pre-trained priors than by any superficial label configurations.Furthermore, the prior is too strong to be overridden by the patterns in demonstrations, even with totally flipped labels.</p>
<p>LLM-based Evaluation Results</p>
<p>LLM-based Scoring.Table 4 summarizes the helpfulness and novelty scores for various approaches.Each score represents the average of 25 hypotheses generated across five datasets.For helpfulness, hypotheses generated without demonstrations achieve higher scores when using IOprompting and iterative-refinement. Regarding novelty, hypotheses generated without demonstrations score higher on IO-prompting and Hypogenic, while iterative-refinement yields a tie between the two settings.</p>
<p>LLM-based Pairwise Comparison. Figure 3 presents the pairwise comparison results for three baselines, evaluating hypotheses generated with and without demonstrations.The comparisons involve randomly paired hypotheses, with win rates aggregated across all datasets.For Helpfulness, IO prompting and iterative refinement perform better without demonstrations, while HypoGenic demonstrates improved performance with them.For Novelty, iterative refinement excels in the absence of demonstrations, whereas IO prompting and Hy-poGenic exhibit minimal differences between the two settings.These results highlight that LLMs can produce highly helpful and novel hypotheses even without in-context demonstrations.</p>
<p>Human Evaluation Results</p>
<p>We conduct a human evaluation on Funny Reddit, Truthful Reviews, and Unhealthy Comments To evaluate the consistency of results across different label formats, we compare two label formats: Label Format 1: Demonstrations are provided as examples for positive and negative classes as in Figure 1.Label Format 2: Demonstrations are presented in the format of (Example, Label).</p>
<p>The average accuracy across all datasets for the correct and flipped label settings is presented in Figure 5. (Results for each dataset of Label Fomat 2 can be found in Appendix C.2).With correct labels, the performance of the two label formats is very similar.However, in the flipped label settings, Label Format 2 shows almost no performance drop, which differs slightly from Label Format 1. Notably, neither label format outperforms the hypotheses generated without demonstrations.This finding highlights the dominant role of the strong model prior, regardless of the presentation style of the demonstrations.</p>
<p>What's the difference between correct label and flipped label settings?</p>
<p>To get an deep understanding for the impact of flipping labels and provide a more fine-grained evaluation, we adopt two additional metrics introduced by
ACR = n i=1 I (ycorrect(xi) = yi ∧ yflipped(xi) ̸ = yi) n i=1 I (ycorrect(xi) = yi) ,(1)BCR = n i=1 I (ycorrect(xi) ̸ = yi ∧ yflipped(xi) = yi) n i=1 I (ycorrect(xi) ̸ = yi) ,(2)
where y correct (x i ) and y flipped (x i ) represents the prediction results using the hypothesis generated with ground truth label and flipped label demonstrations, x i , y i are input and ground truth label, respectively.These metrics offer a comprehensive evaluation of how flipping labels of the demonstrations influence the prediction results in downstream tasks.</p>
<p>Results for multiple hypothesis-based classification prediction difference are shown in Table 5.The results indicate that flipping the labels of incontext demonstrations does lead to some shifts in prediction outcomes, particularly notable in the truthful hotel review dataset, where nearly half of the predictions are affected.In contrast, for the other four datasets, label flipping only minimally alters prediction results.This suggests that while the model leverages the input-label mappings in provided demonstrations to inform its hypothesis generation, the inherent task-specific knowledge remains predominant, preventing the provided patterns from overriding its established priors.</p>
<p>A Case Study: Hypothesis Generation for</p>
<p>Positive Sentiment Pattern</p>
<p>This case study highlights that large language models (LLMs) heavily rely on prior knowledge when generating hypotheses, often ignoring patterns introduced in demonstrations.As shown in Figure 6, we replace true positive demonstrations with flipped label demonstrations (negative examples) to test whether the model adjusts its hypothesis or adheres to its prior.</p>
<p>Using IO-prompting, we provide six demonstrations, varying the number of flipped label demos from 0 to 5, and prompt the model to generate a hypothesis and corresponding supporting demonstrations.Repeating the experiment across 50 random seeds, we track the distribution of true positive and negative examples within the model's supported demonstrations for its hypothesis.</p>
<p>The results, shown in Figure 7, reveal notable patterns.The distribution of positive examples in the supported demonstrations begins to shift when three flipped label demonstrations are introduced.When five flipped demonstrations are provided, the mean number of positive examples converges to one.However, the model consistently avoids using flipped label demonstrations in its hypothesis generation, even when five demonstrations are flipped.This indicates that the model's hypotheses are predominantly influenced by prior knowledge rather than the provided demonstrations.</p>
<p>Conclusion</p>
<p>In this paper, we explore the role of task-specific priors in a real-world inductive reasoning scenario-hypothesis generation from labeled data.Experiments reveal that LLMs rely heavily on strong priors, which are difficult to override with demonstrations, offering insights into hypothesis generation mechanisms and future research directions.</p>
<p>Limitations</p>
<p>Beyond Classification Problems.Our experiments are limited to classification problems.Extensions to multi-choice or other tasks requires better representation of the hypothesis.We leave extensions to non-classification tasks for future work.</p>
<p>Better Application of Generated Hypotheses.We think future can explore better application of generated hypotheses.For instance, this paper uses hypotheses to construct patterns for classification problems.Better application of hypotheses can improve downstream task performance, which we leave for future work.</p>
<p>A Dataset Details</p>
<p>In this paper, we include 5 real-world datasets: hallucination, unhealthy comments in conversation, truthful hotel review, pneumonia MNIST and funny reddit post.Hallucination Pattern.The dataset is first introduced in (Li et al., 2023).We use its adversarial sampling version, which can be found in https://github.com/RUCAIBox/POPE.To build our hallucination dataset, we prompt GPT-4o with each image-question pair once and see if the model hallucinates the object presence.As a result, we get 437 hallucinated image-question pairs and randomly sample another 437 image-question pairs as nonhallucination cases.</p>
<p>Unhealthy Comments.Expert-annotated unhealthy conversations are from (Price et al., 2020), and we use the version from (Zhong et al., 2023), which can be downloaded from https://github.com/ruiqi-zhong/D5.We sample longest 1000 samples for unhealthy and healthy comments from the dataset in our evaluation.</p>
<p>Truthful Hotel Reviews.Truthful review detection is an instance of deception.The dataset we use is from (Zhou et al., 2024).The dataset includes 800 genuine reviews and 800 fictitious reviews for 20 hotels in Chicago, which can be downloaded from https://github.com/ChicagoHAI/hypothesisgeneration.</p>
<p>Funny Reddit Posts.We collect jokes posted on the Reddit forum r/Jokes and cleaned by (Zhong et al., 2023).This dataset can be downloaded from https://github.com/ruiqi-zhong/D5.We also remove all the duplicate samples for better dataset quality.</p>
<p>Pneumonia MNIST.Pneumonia recognition via chest X-ray image is an important problem.The dataset is from (Yang et al., 2023a), and can be downloaded from https://medmnist.com/.</p>
<p>For each dataset, we have at least 200 samples for training, 100 samples for validation and 300 samples for test.For each dataset, we keep a balance between positive and negative class.Detailed statistics is shown in Table 6.</p>
<p>B Implementation Details</p>
<p>Model Parameters.For API usage, the temperature and top-p are set to a small number 1 × 10 −15 and 1 × 10 −10 , respectively.</p>
<p>Iterative Refinement.We initialize the hypothesis bank with 5 hypotheses generated using IOprompting.In refinement process, for each iteration, we select 5 hypotheses achieving highest accuracy on the validation set to LLMs for refinement and hope to get hypothesis with better quality.We evaluate 5 hypotheses with the best performance on validation dataset.We set refinement iteration to 3 in the paper.</p>
<p>HypoGeniC.We set the hypothesis bank size to 5. Throughout the experiment, we use the reward efficient α = 0.5, the number of initialized examples num_init = 10, and maximum number of wrong examples for each group to 2 for more updates.For each iteration, we select top 3 hypotheses to evaluate.For each update, we generate 1 new hypothesis with incorrect examples.When there are no demonstrations, we rank the hypotheses in the bank by reward scores and use this ranking as feedback to get better hypothesis.</p>
<p>C Additional Results</p>
<p>C.1 Hypothesis-based Inference without task-specific knowledge</p>
<p>To minimize the impact of prior knowledge in hypothesis-based inference, we eliminate taskspecific knowledge from the evaluation prompt and remove learned patterns from the hypothesis.Instead, we reformulate the task into its corresponding modalities, prompting large language models (LLMs) with: "Does the provided text/image/image-question align with the given text/image/image-question patterns?"This approach isolates the quality of the hypothesis, ensuring that inference is not influenced by prior knowledge.</p>
<p>The results are shown as Table 7.On average, there is limited difference between the hypotheses generated with and without demonstrations.The findings demonstrate again that LLMs are able to generate hypothesis with high quality only with task-specific prior.</p>
<p>C.2 Results of Different Datasets with Label</p>
<p>Format 2</p>
<p>We provide results on each dataset with Label Format 2. The results are shown as Table 8.From the results, we can see that the results vary by dataset.However, there is quite limited difference (smaller than 3%) between correct and flipped label settings, showing the prior is too strong to be overridden by provided demonstrations.</p>
<p>C.3 Results with Gemini Model</p>
<p>We test IO-prompting with and without demonstrations on model gemini-1.5-pro-002.We report the average over two random seeds.The results are shown as Table 9.On average, there is quite limited performance difference with and without demonstrations, demonstrating that with only prior, LLMs can generate good hypotheses.</p>
<p>D Evaluation Details</p>
<p>LLM-based Evaluation Details.We prompt large language models (LLMs) to generate five hypotheses for each dataset across three different baselines.This results in a total of 25 hypotheses per baseline for both settings: with and without demonstrations.</p>
<p>For LLM-based scoring, each hypothesis is evaluated by prompting the LLMs to assign a score on a 1-5 scale.Additionally, for pairwise comparisons, we randomly pair hypotheses generated with and without demonstrations, creating a total of 25 pairs for evaluation.</p>
<p>Human Evaluation Details.We randomly pair the hypotheses generated with and without demonstrations across three datasets and three baselines.We selected the datasets unhealthy comments, truthful reviews, and funny Reddit posts because their domain knowledge is accessible to non-experts.</p>
<p>Participants were provided with a questionnaire for evaluation.For each evaluation, we included the evaluation context, paired hypotheses, and illustrative examples to guide participants.An example of the evaluation interface is shown in Figure 8.</p>
<p>E Examples of Generated Hypothesis</p>
<p>We randomly select generated hypothesis with and without demonstrations for each dataset, shown as Table 10.</p>
<p>F Prompts</p>
<p>For prompt construction, we begin by manually crafting a prompt for hallucination pattern induction, following a format similar to that used in (Zhou et al., 2024).Subsequently, we leverage incontext learning to generate prompts for other tasks.Specifically, we provide the task name along with the manually constructed prompt to the language model, enabling it to generate prompts tailored to other tasks.Table 9: Accuraccy comparison of single hypothesis-based classification with gemini-1.5-pro-002:accuracy (mean ± standard deviation) for the best single hypothesis and the average across five hypotheses, with (w/) and without (w/o) demonstrations."-" means the response is prohibited due to satety reasons.</p>
<p>Dataset Hypothesis without Demos Hypothesis with Demos</p>
<p>Hallucination</p>
<p>Hallucinations are more likely to occur when the questioned object is partially occluded or located in a cluttered environment, making it difficult for the model to accurately identify its presence or absence.</p>
<p><strong>Complex Backgrounds Hypothesis</strong>: Images with complex or cluttered backgrounds may lead to hallucinations, as the model might misinterpret overlapping or densely packed objects as the queried item.</p>
<p>Unhealthy Comments</p>
<p>Comments containing personal attacks or insults are more likely to be unhealthy, as they often escalate conflicts and discourage constructive dialogue.</p>
<p>Comments that include personal attacks or derogatory language towards individuals are more likely to be unhealthy.</p>
<p>Funny Reddit Posts</p>
<p>Posts that incorporate unexpected punchlines or twists are more likely to be perceived as funny, as they play on the element of surprise and subvert reader expectations.</p>
<p>Posts that use wordplay or double entendres, where a phrase can be interpreted in multiple humorous ways, tend to be perceived as funny.</p>
<p>Pneumonia MNIST</p>
<p>The presence of pleural effusion, seen as blunting of the costophrenic angles or fluid layering in the pleural space, may indicate pneumonia.</p>
<p>Presence of air bronchograms within areas of increased opacity suggests pneumonia.</p>
<p>Truthful Hotel Reviews Truthful reviews often mention both positive and negative aspects of the stay, providing a balanced perspective rather than an overly positive or negative one.</p>
<p>Truthful reviews often mention both positive and negative aspects of the stay, providing a balanced perspective that suggests authenticity.</p>
<p>Prompt for hallucination with demonstrations</p>
<p>You're a professional vision-language model behavior analyst.Given a set of image-question pairs, we want to generate hypotheses that are useful for predicting whether a model will hallucinate the existence of an object in response to a given question.</p>
<p>In other words, we want to know whether the model will falsely claim the presence of an object in the image when answering the question.You are an expert in vision-language models, specializing in detecting and preventing hallucinations.</p>
<p>We want to generate hypotheses that are useful for predicting whether a vision-language model will hallucinate the existence of an object when responding to a question about an image.</p>
<p>In other words, we want to identify patterns that indicate when the model will incorrectly claim the presence of an object not present in the image, or the absence of an object that is present.Please propose {{num_hypotheses}} possible hypotheses.These hypotheses should identify specific patterns that occur across common hallucination cases and focus on the relationship between the image content and the questioned object.You are an expert in vision-language model behavior detection, and your job is to apply learned patterns to predict if the model will hallucinate for the given image and question.</p>
<p>Here are some previously learned hallucination patterns: {{patterns}} The image is shown and the question is: {{text}} Based on the learned patterns, will the model hallucinate?Let's think step by step.</p>
<p>Step 1: Look at the given image and question, and compare them with the provided hallucination patterns.</p>
<p>Step 2: If the image and question pair matches learned hallucination pattern, then it is likely that the model will hallucinate.If it does not match the pattern, then it is likely that the model will not hallucinate.</p>
<p>Step 3: Apply the pattern to the given image-question pair and predict whether the model will hallucinate based on the learned patterns.</p>
<p>Step 4: Give your final answer: yes or no.If you are unsure, respond with no.</p>
<p>Please give your answer strictly in the following format: """ Analysis: [your step-by-step analysis] Answer: [your answer] """ Give your answer in the above format.Don't talk about any other words.</p>
<p>Evaluation prompt for unhealthy comments</p>
<p>You will predict whether a comment is unhealthy based on the provided unhealthy comment patterns.</p>
<p>Here are some previously generated unhealthy comment patterns: {{patterns}} A comment is the following: {{text}} Is this comment unhealthy?Think step-by-step.</p>
<p>Step 1: Look at the new comment and compare it with the provided unhealthy comment patterns.</p>
<p>Step 2: If the comment matches the pattern, then it is likely unhealthy.If it does not match the pattern, then it is likely healthy.</p>
<p>Step 3: Apply the pattern to the new comment and predict whether the new comment is unhealthy.</p>
<p>Step 4: Give your final answer: yes or no.If you are unsure, respond with no.</p>
<p>Please give your answer strictly in the following format: """ Analysis: [your step-by-step analysis] Answer: [your answer] """</p>
<p>Evaluation prompt for truthful review You will predict whether a hotel review is truthful based on the given truthful review patterns.</p>
<p>Here are some previously generated truthful review patterns: {{patterns}} A hotel review is the following: {{text}} Is this hotel review truthful?Think step-by-step.</p>
<p>Step 1: Look at the new hotel review and compare it with the provided truthful review patterns.</p>
<p>Step 2: If the review matches the pattern, then it is likely truthful.If it does not match the pattern, then it is likely not truthful.</p>
<p>Step 3: Apply the pattern to the new hotel review and predict whether the new hotel review is truthful.</p>
<p>Step 4: Give your final answer: yes or no.If you are unsure, respond with no.</p>
<p>Please give your answer strictly in the following format: """ Analysis: [your step-by-step analysis] Answer: [your answer] """</p>
<p>Evaluation prompt for pneumoniaMNIST</p>
<p>You are an expert in pneumonia detection, and your job is to apply learned patterns to predict if a person has pneumonia.</p>
<p>Here are some previously generated pneumonia patterns: {{patterns}} A chest X-ray image is shown.</p>
<p>Based on the learned patterns and given image, is this person likely to have pneumonia based on the learned patterns?Think step-by-step.</p>
<p>Step 1: Look at the given chest X-ray image and compare it with the provided pneumonia patterns.</p>
<p>Step 2: If the image features match the pneumonia patterns, then the person is likely to have pneumonia.If the features do not match the patterns, then the person is likely not to have pneumonia.</p>
<p>Step 3: Apply the pattern to the new chest X-ray image and predict whether the person has pneumonia.</p>
<p>Step 4: Give your final answer: yes or no.If you are unsure, respond with no.</p>
<p>Please give your answer strictly in the following format: """ Analysis: [your step-by-step analysis] Answer: [your answer] """ Give your answer in the above format.Don't talk about any other words.</p>
<p>Evaluation prompt for funny reddit You will predict whether a Reddit post is funny based on the given funny Reddit post patterns.</p>
<p>Here are some previously generated funny Reddit post patterns: {{patterns}} A Reddit post is the following: {{text}} Is this Reddit post funny?Think step-by-step:</p>
<p>Step 1: Look at the new Reddit post and compare it with the provided funny post patterns.</p>
<p>Step 2: If the post matches the pattern, then it is likely funny.If it does not match the pattern, then it is likely not funny.</p>
<p>Step 3: Apply the pattern to the new Reddit post and predict whether the new post is funny.</p>
<p>Step 4: Give your final answer: yes or no.If you are unsure, respond with no.</p>
<p>Please give your answer strictly in the following format: """ Analysis: [your step-by-step analysis] Answer: [your answer] """</p>
<p>Figure 1 :
1
Figure 1: Prompt template for hypothesis generation.</p>
<p>Figure 2 :
2
Figure 2: Accuracy difference comparison of single hypothesis-based classification under different label settings: Accuracy difference (accuracy of different label settings -accuracy without demos) across five datasets with IO-Prompting.</p>
<p>Figure 3 :
3
Figure 3: LLM-based Pairwise Comparison: Pairwise win rate (%) of three baselines.The left plot shows the comparison of Helpfulness, while the right plot presents Novelty.The dashed line indicates a tie where "w/ demos" and "w/o demos" perform equally well.</p>
<p>Figure 6 :Figure 7 :
67
Figure 6: An illustration of the case study: positive sentiment hypothesis generation.The highlighted text with a green background represents flipped label demos.</p>
<p>Figure 8 :
8
Figure 8: Example interface of human evaluation.</p>
<p>Table 3 :
3
± 1.0 61.1 ± 0.3 60.1 ± 4.5 61.1 ± 1.3 58.6 ± 4.0 60.1 ± 0.5 Unhealthy Comments 71.5 ± 0.7 70.9 ± 0.5 71.0 ± 0.4 70.9 ± 0.3 70.9 ± 1.0 70.7 ± 2.3 Funny Reddit 58.3 ± 0.4 59.2 ± 0.3 63.9 ± 2.7 67.3 ± 1.2 58.8 ± 0.7 58.4 ± 0.5 Truthful Reviews 63.8 ± 1.4 65.3 ± 0.9 68.5 ± 0.3 69.1 ± 1.3 67.7 ± 1.5 62.1 ± 4.6 PneumoniaMNIST 75.8 ± 0.9 72.2 ± 1.2 76.0 ± 2.5 74.1 ± 1.7 74.9 ± 1.7 74.6 ± 1.0 Accuracy comparison of multiple hypotheses-based classification across five datasets of three baselines: accuracy (mean ± standard deviation) with (w/) and without (w/o) demonstrations.The better overall average between (w/) and (w/o) is highlighted in bold.
IO-PromptingIterative-RefinementHypoGeniCDatasetw/o demos w/ demos w/o demos w/ demos w/o demos w/ demosHallucination 62.2 Overall Average 66.3265.7467.9068.5066.1865.18CorrectFlippedRandomOnly PositiveOnly NegativeAccuracy Difference-5.0% -2.5% 0.0% 2.5% 5.0%Hallucination Unhealthy Comments Funny RedditTruthful Review PneumoniaMNISTAverage</p>
<p>Table 4 :
4
Helpfulness 4.00 ± 0.000 3.96 ± 0.195 4.00 ± 0.000 3.80 ± 0.400 4.04 ± 0.195 4.08 ± 0.271 4.01 3.95 Novelty 2.56 ± 0.571 2.40 ± 0.566 2.60 ± 0.693 2.60 ± 0.748 2.84 ± 0.674 2.36 ± 0.741 2.67 2.45 LLM-based Scoring: Comparison of Helpfulness and Novelty scores across three baselines, with and without demonstrations (w/ demos vs. w/o demos).The better overall average between (w/) and (w/o) is highlighted in bold.
IO-PromptingIterative-RefinementHypoGeniCOverall AverageCriteriaw/ow/w/ow/w/ow/w/ow/</p>
<p>Table 5 :
5
Accuracy comparison of different label formats in correct and flipped label settings with IO-prompting.Each number is the average over five datasets.
FormatCorrect LabelFlipped LabelBest Average Best AverageLabel Format1 68.5662.7265.1559.96Label Format2 67.8862.7867.4961.90w/o demosBest: 68.62Average: 62.120.1 0.2 0.3 0.4 0.5 Metric ValuesACR BCRHallucination Comments 0.0RedditReview PneumoniaFigure 5: Difference of predictions between correctlabel and flipped label demos: Adverse Correction Rate(ACR) and Beneficial Correction Rate (BCR) valuesunder multiple hypotheses-based classification.datasets, as the other datasets require more special-ized expertise. The results are illustrated in Figure4. Across the three datasets, hypotheses generatedwithout demonstrations received the highest per-centage of preference. These findings indicate aslight overall preference for hypotheses generatedusing only the model's prior, though the extent ofthis preference varies by dataset.6 Analysis6.1 Is the result consistent with differentin-context demonstration label formats?</p>
<p>Table 6 :
6
Dataset Split for Train, Validation, and Test Sets.
DatasetTrain Validation TestHallucination400100374Pneumonia MNIST800270468Unhealthy Conversation800400800Funny Reddit200100308Truthful Hotel Review800300500</p>
<p>Table 7 :
7
Demos Hallucination Unhealthy Comments Funny Reddit Truthful Review PneumoniaMNIST Overall Average Accuraccy comparison of single hypothesis-based classification without task-specific knowledge in inference: accuracy for the single hypothesis and the average across five hypotheses, with (w/) and without (w/o) demonstrations.
Bestw/o w/63.1 57.570.1 68.061.6 59.164.0 64.675.6 80.866.9 66.0Averagew/o w/54.4 53.660.3 63.354.1 54.856.7 51.869.8 73.159.1 59.3LabelHallucination Unhealthy Comments Funny Reddit Truthful Review PneumoniaMNIST AverageCorrect (Best)63.970.661.768.075.267.9Flipped (Best)61.271.562.068.873.967.5Correct (Avg)57.065.159.062.570.362.8Flipped (Avg)57.864.757.361.368.561.9</p>
<p>Table 8 :
8
Accuracy comparison across five datasets with correct and flipped label settings in the Label Format 2.
Demos Hallucination Unhealthy Comments Funny Reddit Truthful Review PneumoniaMNIST Overall AverageBestw/o w/--67.9 ± 0.2 67.8 ± 1.362.7 ± 0.3 65.9 ± 0.368.8 ± 2.0 66.9 ± 1.758.2 ± 1.8 55.7 ± 1.464.4 64.1Averagew/o w/--61.9 ± 0.3 62.4 ± 1.156.8 ± 0.0 58.0 ± 1.264.5 ± 2.3 63.4 ± 1.253.1 ± 0.2 53.0 ± 1.559.1 59.2</p>
<p>Table 10 :
10
Examples of Generated Hypotheses with and without In-Context Demonstrations.</p>
<p>Using the given examples, please propose {{num_hypotheses}} possible hypotheses that can identify specific patterns that occur across the provided image-question pairs.Each hypothesis should contain the following: a hypothesis about what image content features, object features, or contextual relationships make the model more likely to hallucinate.The hypotheses should analyze what kinds of image-question pairs are more likely to trigger hallucinations.Some examples of hallucination and non-hallucination cases are shown.Hallucination cases are from number 1 to {{num_1}}, and non-hallucination cases are from number {{num_2}} to {{num_3}}.Based on provided examples, please generate hypotheses that are useful for predicting whether the model will hallucinate the existence of an object in response to a given question.Propose {{num_hypotheses}} possible hypotheses for hallucination patterns.</p>
<p>Generate them in the format of 1. [hypothesis], 2. [hypothesis], ... {{num_hypotheses}}.[hypothesis].Proposed hypotheses: Prompt for hallucination without demonstrations</p>
<p>Each hypothesis should contain the following: a hypothesis about what image content features, object features, or contextual relationships make the model more likely to hallucinate.You're an expert comment analyst in online conversation.Given a set of comments, we want to generate hypotheses that are useful for predicting whether a comment is unhealthy.In other words, we want to know if the comment contributes to unhealthy conversations online.Using the given examples, please propose {{num_hypotheses}} possible hypotheses.These hypotheses should identify specific patterns that occur across the provided unhealthy comments.Each hypothesis should contain the following: A hypothesis about what makes comments more likely to be unhealthy.The hypotheses should analyze what kind of comments are likely to be unhealthy.Here are some examples of unhealthy and healthy comments: Given a set of hotel reviews, we want to generate hypotheses that are useful for predicting whether a review is truthful.In other words, we want to know whether the review is written by someone who actually lived in the hotel.Using the given examples, please propose {{num_hypotheses}} possible hypotheses.These hypotheses should identify specific patterns that occur across the provided reviews.Each hypothesis should contain the following: A hypothesis about what makes reviews more likely to be truthful.The hypotheses should analyze what kind of reviews are likely to be truthful.Here are some examples of truthful and deceptive reviews:You're a professional radiologist specializing in chest X-rays.Given a set of labeled chest X-ray images, we want to generate hypotheses that are useful for predicting whether a patient has pneumonia.In other words, we want to know whether the X-ray shows signs of pneumonia.Using the given examples, please propose {{num_hypotheses}} possible hypotheses.These hypotheses should identify specific patterns that occur across the provided X-ray images.Each hypothesis should contain the following: A hypothesis about what makes an X-ray more likely to indicate pneumonia.The hypotheses should analyze what kind of image patterns are likely to be indicative of pneumonia or not.You're a professional humor analyst for Reddit posts.Given a set of Reddit posts, we want to generate hypotheses that are useful for predicting whether a post is considered funny or not.In other words, we want to know whether a post contains humor patterns often associated with successful humorous posts.Using the provided examples, please propose {{num_hypotheses}} possible hypotheses.These hypotheses should identify specific patterns that occur across the provided posts.Each hypothesis should contain the following: A hypothesis about what makes posts more likely to be considered funny.The hypotheses should analyze what kind of posts are likely to be perceived as funny or not.Here are some examples of funny and unfunny posts:
Prompt for unhealthy comments with demonstrations Prompt for truthful reviews with demonstrations Prompt for PneumoniaMNIST with demonstrations Prompt for funny reddit with demonstrations Evaluation prompt for hallucinationYou're a professional hotel review analyst.Unhealthy comments: {{positive_examples}} Healthy comments: {{negative_examples}} Some examples of X-ray images labeled as pneumonia and non-pneumonia are shown. Truthful reviews: Pneumonia images are from number 1 to {{num_1}}, and non-pneumonia images are from Funny posts: {{positive_examples}} number {{num_2}} to {{num_3}}. {{positive_examples}} Deceptive reviews: Unfunny posts: {{negative_examples}} Based on provided examples, please generate hypotheses that are useful for predicting whether an {{negative_examples}}Based on the provided examples, please generate hypotheses that are useful for predicting whether a comment is unhealthy. Propose {{num_hypotheses}} possible hypotheses for unhealthy comment patterns. Generate them in the format of 1. [hypothesis], 2. [hypothesis], ... {{num_hypotheses}}. [hypoth-esis]. Don't include any other words. Proposed hypotheses: X-ray shows pneumonia or not. Based on provided examples, please generate hypotheses that are useful for predicting whether a Propose {{num_hypotheses}} possible hypotheses for pneumonia pattern recognition. Based on the provided examples, please generate hypotheses that are useful for predicting whether review is truthful. Generate them in the format of 1. [hypothesis], 2. [hypothesis], ... {{num_hypotheses}}. [hypoth-a post is funny or not. Propose {{num_hypotheses}} possible hypotheses for truthful review patterns. esis]. Propose {{num_hypotheses}} possible hypotheses for funny post patterns. Generate them in the format of 1. [hypothesis], 2. [hypothesis], ... {{num_hypotheses}}. [hypoth-Don't include any other information. Generate them in the format of 1. [hypothesis], 2. [hypothesis], ... {{num_hypotheses}}. [hypoth-esis]. Proposed hypotheses: esis]. Don't talk about any other words. Don't talk about any other words. Proposed hypotheses: Proposed hypotheses:Prompt for PneumoniaMNIST without demonstrationsThe hypotheses should analyze what kind of image-question pairs are more likely to lead to hallucinations. Please generate {{num_hypotheses}} possible hypotheses for hallucination patterns in the given context. Generate them in the format of 1. [hypothesis], 2. [hypothesis], ... {{num_hypotheses}}. [hypoth-esis]. Don't talk about any other words. Proposed hypotheses: Prompt for truthful reviews with demonstrations Prompt for funny reddit without demonstrations You're a professional radiologist. Prompt for unhealthy comments without demonstrations You're an expert comment analyst in online conversation. We want to generate hypotheses that are useful for predicting whether a comment is unhealthy. In other words, we want to know if the comment contributes to unhealthy conversations online. Please propose {{num_hypotheses}} possible hypotheses. These hypotheses should identify specific patterns that occur across common unhealthy comments. Each hypothesis should contain the following: A hypothesis about what makes comments more likely to be unhealthy. The hypotheses should analyze what kind of comments are likely to be unhealthy. Please generate hypotheses that are useful for predicting whether a comment is unhealthy or healthy. Propose {{num_hypotheses}} possible hypotheses for unhealthy comment patterns. Generate them in the format of 1. [hypothesis], 2. [hypothesis], ... {{num_hypotheses}}. [hypoth-esis]. Don't talk about any other words. Proposed hypotheses: We want to generate hypotheses that are useful for predicting whether a patient has pneumonia You're a professional Reddit content analyst. You're a professional hotel review analyst. based on their chest X-ray image. In other words, we want to know which patterns in the image We want to generate hypotheses that are useful for predicting whether a Reddit post is funny or not. We want to generate hypotheses that are useful for predicting whether a review is truthful or are indicative of pneumonia presence. In other words, we want to know what characteristics make a post likely to be perceived as humor-deceptive. In other words, we want to know whether the review is written by someone who actually Please propose {{num_hypotheses}} possible hypotheses. ous by the community. lived in the hotel. These hypotheses should identify specific visual patterns that occur in typical pneumonia cases. Please propose {{num_hypotheses}} possible hypotheses. Please propose {{num_hypotheses}} possible hypotheses. Each hypothesis should contain the following: A hypothesis about what makes an image more These hypotheses should identify specific patterns that occur across common funny posts. These hypotheses should identify specific patterns that occur across common truthful reviews. likely to show signs of pneumonia. Each hypothesis should contain the following: A hypothesis about what makes posts more likely Each hypothesis should contain the following: A hypothesis about what makes reviews more likely The hypotheses should analyze what kind of visual patterns or markers are likely to indicate to be perceived as funny. to be truthful. The hypotheses should analyze what kind of reviews are likely to be truthful or pneumonia. The hypotheses should analyze what kind of posts are likely to be considered humorous or non-deceptive. Please generate hypotheses that are useful for predicting whether a patient has pneumonia or not humorous. Please generate hypotheses that are useful for predicting whether a review is truthful or deceptive. based on the X-ray. Please generate hypotheses that are useful for predicting whether a post is funny or not. Propose {{num_hypotheses}} possible hypotheses for truthful review patterns. Propose {{num_hypotheses}} possible hypotheses for pneumonia-related visual patterns. Propose {{num_hypotheses}} possible hypotheses for funny Reddit post patterns. Generate them in the format of 1. [hypothesis], 2. [hypothesis], ... {{num_hypotheses}}. [hypoth-Generate them in the format of 1. [hypothesis], 2. [hypothesis], ... {{num_hypotheses}}. [hypoth-Generate them in the format of 1. [hypothesis], 2. [hypothesis], ... {{num_hypotheses}}. [hypoth-esis]. esis]. esis]. Don't talk about any other words. Don't include any additional context. Don't talk about any other words. Proposed hypotheses: Proposed hypotheses: Proposed hypotheses:</p>
<p>Researchagent: Iterative research idea generation over scientific literature with large language models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, arXiv:2404.077382024arXiv preprint</p>
<p>Language models are few-shot learners. Tom B Brown, arXiv:2005.141652020arXiv preprint</p>
<p>Kewei Cheng, Jingfeng Yang, Haoming Jiang, Zhengyang Wang, Binxuan Huang, Ruirui Li, Shiyang Li, Zheng Li, Yifan Gao, Xian Li, arXiv:2408.00114Inductive or deductive? rethinking the fundamental reasoning abilities of llms. 2024arXiv preprint</p>
<p>Describing differences in image sets with natural language. Lisa Dunlap, Yuhui Zhang, Xiaohan Wang, Ruiqi Zhong, Trevor Darrell, Jacob Steinhardt, Joseph E Gonzalez, Serena Yeung-Levy, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Properties of inductive reasoning. Evan Heit, Psychonomic bulletin &amp; review. 72000</p>
<p>Instruction induction: From few examples to natural language task descriptions. Or Honovich, Uri Shaham, Omer Samuel R Bowman, Levy, arXiv:2205.107822022arXiv preprint</p>
<p>Discovering and mitigating visual biases through keyword explanation. Younghyun Kim, Sangwoo Mo, Minkyu Kim, Kyungmin Lee, Jaeho Lee, Jinwoo Shin, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Mirage: Evaluating and explaining inductive reasoning process in language models. Jiachun Li, Pengfei Cao, Zhuoran Jin, Yubo Chen, Kang Liu, Jun Zhao, arXiv:2410.095422024arXiv preprint</p>
<p>Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, Ji-Rong Wen, arXiv:2305.10355Evaluating object hallucination in large vision-language models. 2023arXiv preprint</p>
<p>Literature meets data: A synergistic approach to hypothesis generation. Haokun Liu, Yangqiaoyu Zhou, Mingxuan Li, Chenfei Yuan, Chenhao Tan, arXiv:2410.173092024arXiv preprint</p>
<p>Prasad Bodhisattwa, Harshit Majumder, Dhruv Surana, Sanchaita Agarwal, Ashish Hazra, Peter Sabharwal, Clark, arXiv:2402.13610Data-driven discovery with large generative models. 2024aarXiv preprint</p>
<p>Prasad Bodhisattwa, Harshit Majumder, Dhruv Surana, Bhavana Agarwal, Abhijeetsingh Dalvi Mishra, Aryan Meena, Tirth Prakhar, Tushar Vora, Ashish Khot, Peter Sabharwal, Clark, arXiv:2407.01725Discoverybench: Towards data-driven discovery with large language models. 2024barXiv preprint</p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer, arXiv:2202.12837Rethinking the role of demonstrations: What makes in-context learning work?. 2022arXiv preprint</p>
<p>Minh Chau, Alexander Pham, Simeng Hoyle, Philip Sun, Mohit Resnik, Iyyer, arXiv:2311.01449Topicgpt: A promptbased topic modeling framework. 2023arXiv preprint</p>
<p>Ilan Price, Jordan Gifford-Moore, Jory Fleming, Saul Musker, Maayan Roichman, Guillaume Sylvain, Nithum Thain, Lucas Dixon, Jeffrey Sorensen, arXiv:2010.07410Six attributes of unhealthy conversation. 2020arXiv preprint</p>
<p>Large language models are zero shot hypothesis proposers. Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Bowen Zhang-Ren Chen, Zhou, arXiv:2311.059652023arXiv preprint</p>
<p>Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, arXiv:2310.08559Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. 2023arXiv preprint</p>
<p>Llm-sr: Scientific equation discovery via programming with large language models. Parshin Shojaee, Kazem Meidani, Shashank Gupta, Amir Barati Farimani, Chandan K Reddy, arXiv:2404.184002024arXiv preprint</p>
<p>Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.04109Can llms generate novel research ideas? a largescale human study with 100+ nlp researchers. 2024arXiv preprint</p>
<p>iprompt: Explaining data patterns in natural language via interpretable autoprompting. Chandan Singh, John X Morris, Jyoti Aneja, Alexander M Rush, Jianfeng Gao, 20222210ArXiv preprint</p>
<p>Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, Noah D Goodman, arXiv:2309.05660Hypothesis search: Inductive reasoning with language models. 2023arXiv preprint</p>
<p>Yurong Wu, Yan Gao, Bin Benjamin Zhu, Zineng Zhou, Xiaodi Sun, Sheng Yang, Jian-Guang Lou, Zhiming Ding, Linjun Yang, arXiv:2410.08601Strago: Harnessing strategic guidance for prompt optimization. 2024arXiv preprint</p>
<p>Verbalized machine learning: Revisiting machine learning with language models. Robert Tim Z Xiao, Bernhard Bamler, Weiyang Schölkopf, Liu, arXiv:2406.043442024arXiv preprint</p>
<p>Medmnist v2-a large-scale lightweight benchmark for 2d and 3d biomedical image classification. Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pfister, Bingbing Ni, Scientific Data. 101412023a</p>
<p>Zonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, Furu Wei, arXiv:2212.10923Language models as inductive reasoners. 2022arXiv preprint</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, arXiv:2309.027262023barXiv preprintSoujanya Poria, and Erik Cambria</p>
<p>Describing differences between text distributions with natural language. Ruiqi Zhong, Charlie Snell, Dan Klein, Jacob Steinhardt, International Conference on Machine Learning. PMLR2022</p>
<p>Goal driven discovery of distributional differences via language descriptions. Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, Jacob Steinhardt, Advances in Neural Information Processing Systems. 202336</p>
<p>Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, Chenhao Tan, arXiv:2404.04326Hypothesis generation with large language models. 2024arXiv preprint</p>
<p>Zhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, Hanjun Dai, arXiv:2310.07064Large language models can learn rules. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
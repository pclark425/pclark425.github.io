<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-199 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-199</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-199</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-10.html">extraction-schema-10</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <p><strong>Paper ID:</strong> paper-80376bdec5f534be78ba82821f540590ebce5559</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/80376bdec5f534be78ba82821f540590ebce5559" target="_blank">How Much Knowledge Can You Pack into the Parameters of a Language Model?</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> It is shown that this approach scales surprisingly well with model size and outperforms models that explicitly look up knowledge on the open-domain variants of Natural Questions and WebQuestions.</p>
                <p><strong>Paper Abstract:</strong> It has recently been observed that neural language models trained on unstructured text can implicitly store and retrieve knowledge using natural language queries. In this short paper, we measure the practical utility of this approach by fine-tuning pre-trained models to answer questions without access to any external context or knowledge. We show that this approach scales surprisingly well with model size and outperforms models that explicitly look up knowledge on the open-domain variants of Natural Questions and WebQuestions. To facilitate reproducibility and future work, we release our code and trained models.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e199.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e199.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Closed-Book QA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Closed-Book Question Answering (no external context in prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Fine-tuning pre-trained T5 models to answer open-domain questions without providing any external context or retrieved documents; the model must rely entirely on knowledge stored in its parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 (Text-to-Text Transfer Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>220M / 770M / 3B / 11B</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-domain factual question answering where no external evidence is provided in the prompt; the model must generate literal answer text from parametric knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>none (no evidence provided in prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>n/a (answers must be produced from parametric knowledge only)</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>unknown (no prompt evidence to compare against parametric knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Closed-book T5 results from Table 1. Example: T5-11B (no extra SSM) achieved NQ 32.6 (exact match %), WQ 37.2, TriviaQA dev 42.3, TriviaQA test 50.1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>Not applicable within the closed-book experimental condition (the paper does not report per-question metrics when external evidence is appended to the prompt for the same T5 models). Comparative open-book systems (see separate entry) report higher scores (e.g., Karpukhin et al. DPR NQ 41.5).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>neutral/inapplicable within the closed-book experiments; the paper demonstrates that removing evidence (closed-book) still yields useful performance for large models but does not provide per-prompt confidence/probability comparisons with/without evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Models must retrieve facts from distributed parametric representations learned during pre-training and fine-tuning; absence of explicit evidence forces reliance on memorized facts, leading to opaque provenance and potential confident hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large T5 models (especially ~11B parameters and with SSM pretraining) can answer many open-domain questions without any external context, showing that substantial factual knowledge can be stored in parameters; however, closed-book models lack interpretability about provenance and can produce plausible but incorrect answers when uncertain.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Much Knowledge Can You Pack into the Parameters of a Language Model?', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e199.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e199.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SSM pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Salient Span Masking (SSM) pre-training objective</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretraining objective that specifically masks named entities or dates in sentences (mined with BERT), training the model to reconstruct salient spans to better internalize world knowledge relevant to QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Realm: Retrieval-augmented language model pre-training</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5.1.1-XXL (and other T5 variants when continued pretraining applied)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>XXL (~11B) where reported</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Improve closed-book open-domain QA performance by continuing pretraining with an objective that emphasizes recovering entity/date spans.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>training-time evidence: sentences containing salient spans (named entities/dates) used during pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Wikipedia sentences mined for salient spans using BERT (Guu et al. dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>aligned (SSM specifically targets entity/date spans, aligning pretraining with factual QA needs)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Baseline T5.1.1-XXL (no SSM): NQ 32.8, WQ 35.6, TriviaQA dev 42.9, TriviaQA test 52.5 (exact match % where reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>With SSM continuing pretraining: T5.1.1-XXL + SSM achieves NQ 35.2, WQ 42.8, TriviaQA dev 51.9, TriviaQA test 61.6 — substantial absolute gains reported in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>positive — adding SSM pretraining (evidence-rich objective) increased closed-book QA accuracy substantially across datasets (not a prompt-time evidence addition but a pretraining modification that supplies evidence to the model's parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>By explicitly masking and reconstructing salient entity/date spans, SSM focuses pretraining on tokens carrying world knowledge, improving the model's ability to store and retrieve factual information from parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Continuing pretraining with SSM on entity-rich sentences markedly improves closed-book QA performance (several percentage points across datasets), indicating that the form of evidence seen during pretraining affects how well parametric knowledge supports later QA without prompt evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Much Knowledge Can You Pack into the Parameters of a Language Model?', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e199.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e199.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wikipedia-only pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pretraining on Wikipedia (span-corruption or from-scratch)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Experiments that further pretrain T5 on Wikipedia text (either continued pretraining with the same span-corruption objective or pretraining from scratch on Wikipedia) to test whether more domain-specific evidence improves closed-book QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 (various checkpoints)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (same sizes as T5 checkpoints used)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Assess effect of additional in-domain pretraining data (Wikipedia) on closed-book QA performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>training-time evidence: Wikipedia articles / sentences</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>English Wikipedia (either as additional continued pretraining data or exclusively when pretraining from scratch)</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>aligned in principle (Wikipedia contains answers), but empirically produced mixed/negative alignment</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Baseline T5 checkpoints (pretrained on diverse C4) as previously reported (e.g., T5.1.1-XXL baseline numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>Continued pretraining on Wikipedia with the standard span-corruption objective produced virtually no improvement. Pretraining from scratch on only Wikipedia produced dramatically worse performance regardless of pretraining amount (no numeric exact-match numbers are provided in text).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>mixed/negative — adding only Wikipedia data via standard span-corruption gave little-to-no benefit; restricting pretraining to Wikipedia (lower diversity) harmed downstream closed-book QA performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Lack of data diversity (Wikipedia alone is too small/richly correlated) leads to overfitting or insufficient generalization; the benefit depends on both objective and data diversity, not merely presence of domain evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>In-domain pretraining on Wikipedia with the original span-corruption objective had negligible effect, and pretraining from scratch solely on Wikipedia reduced performance — demonstrating that adding domain evidence can fail or be harmful if data diversity/objective are inappropriate.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Much Knowledge Can You Pack into the Parameters of a Language Model?', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e199.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e199.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Open-Book (retrieval-augmented) QA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open-book / retrieval-augmented question answering (external context provided)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard open-domain QA systems that retrieve passages/documents from an external knowledge source and provide them as context to a model (enabling extractive or grounded generation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various retrieval-augmented systems (e.g., DPR by Karpukhin et al., reader+retriever pipelines)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-domain factual QA where retrieved documents are provided to the model in the prompt or input, enabling the model to extract answers grounded in retrieved evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>retrieved documents / passages (supporting context)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>External corpora (typically Wikipedia or Web crawl), returned by a retrieval system</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>aligned (retrieved evidence typically contains the ground-truth answer and provides explicit support for answer correctness)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Closed-book T5 baselines (see closed-book entry) — e.g., T5-11B: NQ 32.6.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>Reported retrieval-based systems achieve higher scores in many cases; example from Table 1: Karpukhin et al. (DPR) NQ 41.5, WQ 42.4, TriviaQA dev 57.9 (exact match % where reported).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>positive — providing retrieved documents generally increases answer accuracy relative to closed-book baselines (at cost of retrieval computation and the need to attend to long contexts).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Access to explicit supporting text gives the model a grounded source to extract the answer from, improving correctness and providing interpretability (ability to show which document/span supported the answer).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Open-book systems typically outperform closed-book models of comparable settings and additionally offer provenance/interpretability by indicating which evidence was used; however, retrieval adds computational overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Much Knowledge Can You Pack into the Parameters of a Language Model?', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e199.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e199.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hallucination & False-Negatives</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model hallucinations and evaluation false-negatives from free-form generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observed phenomena where closed-book T5 generates realistic-looking but incorrect answers (hallucinations) and where automatic exact-match evaluation labels many correct or acceptable model outputs as incorrect due to phrasing or annotation mismatches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 (evaluated human analysis primarily on T5-11B + SSM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B (analysis examples drawn primarily from large models)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Free-form answer generation without context, and human re-evaluation of model outputs that automatic metrics marked as wrong.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>absence of prompt evidence; mismatch between generated answers and dataset annotations (phrasing differences / incomplete annotations)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>n/a (issue arises from lack of explicit prompt evidence and from dataset annotations)</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>sometimes misaligned — the model's generated answer can be semantically correct yet differ in phrasing or specificity from the annotated ground truth</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Automatic evaluation labeled many predictions incorrect; human evaluation of 150 automatic 'incorrect' NATURAL QUESTIONS outputs found only 62% to be true negatives. Adjusted accuracy after accounting for false negatives increases to 57.8 (recomputed metric described in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>Not reported for the same model; paper notes that open-book systems could also be affected but to a lesser degree.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>negative in the sense that absence of explicit evidence and free-form generation leads to plausible but incorrect answers and many apparent false negatives under exact-match metrics; adding explicit evidence (not measured here) is suggested to provide better grounding and reduce hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Maximum-likelihood training and free-form generation can produce confident, plausible outputs even when the model lacks correct grounded evidence; evaluation mismatches (phrasing, incomplete annotations) exacerbate perceived errors.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Closed-book generation can hallucinate and is frequently penalized by exact-match metrics even when semantically acceptable answers are produced; human evaluation reveals both genuine errors and annotation/evaluation issues, highlighting the role of explicit evidence and evaluation design in assessing truthfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Much Knowledge Can You Pack into the Parameters of a Language Model?', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Realm: Retrieval-augmented language model pre-training <em>(Rating: 2)</em></li>
                <li>Dense passage retrieval for open-domain question answering <em>(Rating: 2)</em></li>
                <li>Language models as knowledge bases? <em>(Rating: 2)</em></li>
                <li>Reading Wikipedia to answer open-domain questions <em>(Rating: 2)</em></li>
                <li>Language models are unsupervised multitask learners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-199",
    "paper_id": "paper-80376bdec5f534be78ba82821f540590ebce5559",
    "extraction_schema_id": "extraction-schema-10",
    "extracted_data": [
        {
            "name_short": "Closed-Book QA",
            "name_full": "Closed-Book Question Answering (no external context in prompt)",
            "brief_description": "Fine-tuning pre-trained T5 models to answer open-domain questions without providing any external context or retrieved documents; the model must rely entirely on knowledge stored in its parameters.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5 (Text-to-Text Transfer Transformer)",
            "model_size": "220M / 770M / 3B / 11B",
            "task_description": "Open-domain factual question answering where no external evidence is provided in the prompt; the model must generate literal answer text from parametric knowledge.",
            "evidence_type": "none (no evidence provided in prompt)",
            "evidence_source": "n/a (answers must be produced from parametric knowledge only)",
            "parametric_knowledge_alignment": "unknown (no prompt evidence to compare against parametric knowledge)",
            "performance_without_evidence": "Closed-book T5 results from Table 1. Example: T5-11B (no extra SSM) achieved NQ 32.6 (exact match %), WQ 37.2, TriviaQA dev 42.3, TriviaQA test 50.1.",
            "performance_with_evidence": "Not applicable within the closed-book experimental condition (the paper does not report per-question metrics when external evidence is appended to the prompt for the same T5 models). Comparative open-book systems (see separate entry) report higher scores (e.g., Karpukhin et al. DPR NQ 41.5).",
            "evidence_effect": "neutral/inapplicable within the closed-book experiments; the paper demonstrates that removing evidence (closed-book) still yields useful performance for large models but does not provide per-prompt confidence/probability comparisons with/without evidence.",
            "evidence_decreased_confidence": null,
            "proposed_mechanism": "Models must retrieve facts from distributed parametric representations learned during pre-training and fine-tuning; absence of explicit evidence forces reliance on memorized facts, leading to opaque provenance and potential confident hallucinations.",
            "key_findings": "Large T5 models (especially ~11B parameters and with SSM pretraining) can answer many open-domain questions without any external context, showing that substantial factual knowledge can be stored in parameters; however, closed-book models lack interpretability about provenance and can produce plausible but incorrect answers when uncertain.",
            "counterintuitive_behavior": false,
            "uuid": "e199.0",
            "source_info": {
                "paper_title": "How Much Knowledge Can You Pack into the Parameters of a Language Model?",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "SSM pretraining",
            "name_full": "Salient Span Masking (SSM) pre-training objective",
            "brief_description": "A pretraining objective that specifically masks named entities or dates in sentences (mined with BERT), training the model to reconstruct salient spans to better internalize world knowledge relevant to QA.",
            "citation_title": "Realm: Retrieval-augmented language model pre-training",
            "mention_or_use": "use",
            "model_name": "T5.1.1-XXL (and other T5 variants when continued pretraining applied)",
            "model_size": "XXL (~11B) where reported",
            "task_description": "Improve closed-book open-domain QA performance by continuing pretraining with an objective that emphasizes recovering entity/date spans.",
            "evidence_type": "training-time evidence: sentences containing salient spans (named entities/dates) used during pretraining",
            "evidence_source": "Wikipedia sentences mined for salient spans using BERT (Guu et al. dataset)",
            "parametric_knowledge_alignment": "aligned (SSM specifically targets entity/date spans, aligning pretraining with factual QA needs)",
            "performance_without_evidence": "Baseline T5.1.1-XXL (no SSM): NQ 32.8, WQ 35.6, TriviaQA dev 42.9, TriviaQA test 52.5 (exact match % where reported).",
            "performance_with_evidence": "With SSM continuing pretraining: T5.1.1-XXL + SSM achieves NQ 35.2, WQ 42.8, TriviaQA dev 51.9, TriviaQA test 61.6 — substantial absolute gains reported in Table 1.",
            "evidence_effect": "positive — adding SSM pretraining (evidence-rich objective) increased closed-book QA accuracy substantially across datasets (not a prompt-time evidence addition but a pretraining modification that supplies evidence to the model's parameters).",
            "evidence_decreased_confidence": false,
            "proposed_mechanism": "By explicitly masking and reconstructing salient entity/date spans, SSM focuses pretraining on tokens carrying world knowledge, improving the model's ability to store and retrieve factual information from parameters.",
            "key_findings": "Continuing pretraining with SSM on entity-rich sentences markedly improves closed-book QA performance (several percentage points across datasets), indicating that the form of evidence seen during pretraining affects how well parametric knowledge supports later QA without prompt evidence.",
            "counterintuitive_behavior": false,
            "uuid": "e199.1",
            "source_info": {
                "paper_title": "How Much Knowledge Can You Pack into the Parameters of a Language Model?",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "Wikipedia-only pretraining",
            "name_full": "Pretraining on Wikipedia (span-corruption or from-scratch)",
            "brief_description": "Experiments that further pretrain T5 on Wikipedia text (either continued pretraining with the same span-corruption objective or pretraining from scratch on Wikipedia) to test whether more domain-specific evidence improves closed-book QA.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5 (various checkpoints)",
            "model_size": "various (same sizes as T5 checkpoints used)",
            "task_description": "Assess effect of additional in-domain pretraining data (Wikipedia) on closed-book QA performance.",
            "evidence_type": "training-time evidence: Wikipedia articles / sentences",
            "evidence_source": "English Wikipedia (either as additional continued pretraining data or exclusively when pretraining from scratch)",
            "parametric_knowledge_alignment": "aligned in principle (Wikipedia contains answers), but empirically produced mixed/negative alignment",
            "performance_without_evidence": "Baseline T5 checkpoints (pretrained on diverse C4) as previously reported (e.g., T5.1.1-XXL baseline numbers).",
            "performance_with_evidence": "Continued pretraining on Wikipedia with the standard span-corruption objective produced virtually no improvement. Pretraining from scratch on only Wikipedia produced dramatically worse performance regardless of pretraining amount (no numeric exact-match numbers are provided in text).",
            "evidence_effect": "mixed/negative — adding only Wikipedia data via standard span-corruption gave little-to-no benefit; restricting pretraining to Wikipedia (lower diversity) harmed downstream closed-book QA performance.",
            "evidence_decreased_confidence": null,
            "proposed_mechanism": "Lack of data diversity (Wikipedia alone is too small/richly correlated) leads to overfitting or insufficient generalization; the benefit depends on both objective and data diversity, not merely presence of domain evidence.",
            "key_findings": "In-domain pretraining on Wikipedia with the original span-corruption objective had negligible effect, and pretraining from scratch solely on Wikipedia reduced performance — demonstrating that adding domain evidence can fail or be harmful if data diversity/objective are inappropriate.",
            "counterintuitive_behavior": true,
            "uuid": "e199.2",
            "source_info": {
                "paper_title": "How Much Knowledge Can You Pack into the Parameters of a Language Model?",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "Open-Book (retrieval-augmented) QA",
            "name_full": "Open-book / retrieval-augmented question answering (external context provided)",
            "brief_description": "Standard open-domain QA systems that retrieve passages/documents from an external knowledge source and provide them as context to a model (enabling extractive or grounded generation).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Various retrieval-augmented systems (e.g., DPR by Karpukhin et al., reader+retriever pipelines)",
            "model_size": null,
            "task_description": "Open-domain factual QA where retrieved documents are provided to the model in the prompt or input, enabling the model to extract answers grounded in retrieved evidence.",
            "evidence_type": "retrieved documents / passages (supporting context)",
            "evidence_source": "External corpora (typically Wikipedia or Web crawl), returned by a retrieval system",
            "parametric_knowledge_alignment": "aligned (retrieved evidence typically contains the ground-truth answer and provides explicit support for answer correctness)",
            "performance_without_evidence": "Closed-book T5 baselines (see closed-book entry) — e.g., T5-11B: NQ 32.6.",
            "performance_with_evidence": "Reported retrieval-based systems achieve higher scores in many cases; example from Table 1: Karpukhin et al. (DPR) NQ 41.5, WQ 42.4, TriviaQA dev 57.9 (exact match % where reported).",
            "evidence_effect": "positive — providing retrieved documents generally increases answer accuracy relative to closed-book baselines (at cost of retrieval computation and the need to attend to long contexts).",
            "evidence_decreased_confidence": null,
            "proposed_mechanism": "Access to explicit supporting text gives the model a grounded source to extract the answer from, improving correctness and providing interpretability (ability to show which document/span supported the answer).",
            "key_findings": "Open-book systems typically outperform closed-book models of comparable settings and additionally offer provenance/interpretability by indicating which evidence was used; however, retrieval adds computational overhead.",
            "counterintuitive_behavior": false,
            "uuid": "e199.3",
            "source_info": {
                "paper_title": "How Much Knowledge Can You Pack into the Parameters of a Language Model?",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "Hallucination & False-Negatives",
            "name_full": "Model hallucinations and evaluation false-negatives from free-form generation",
            "brief_description": "Observed phenomena where closed-book T5 generates realistic-looking but incorrect answers (hallucinations) and where automatic exact-match evaluation labels many correct or acceptable model outputs as incorrect due to phrasing or annotation mismatches.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5 (evaluated human analysis primarily on T5-11B + SSM)",
            "model_size": "11B (analysis examples drawn primarily from large models)",
            "task_description": "Free-form answer generation without context, and human re-evaluation of model outputs that automatic metrics marked as wrong.",
            "evidence_type": "absence of prompt evidence; mismatch between generated answers and dataset annotations (phrasing differences / incomplete annotations)",
            "evidence_source": "n/a (issue arises from lack of explicit prompt evidence and from dataset annotations)",
            "parametric_knowledge_alignment": "sometimes misaligned — the model's generated answer can be semantically correct yet differ in phrasing or specificity from the annotated ground truth",
            "performance_without_evidence": "Automatic evaluation labeled many predictions incorrect; human evaluation of 150 automatic 'incorrect' NATURAL QUESTIONS outputs found only 62% to be true negatives. Adjusted accuracy after accounting for false negatives increases to 57.8 (recomputed metric described in paper).",
            "performance_with_evidence": "Not reported for the same model; paper notes that open-book systems could also be affected but to a lesser degree.",
            "evidence_effect": "negative in the sense that absence of explicit evidence and free-form generation leads to plausible but incorrect answers and many apparent false negatives under exact-match metrics; adding explicit evidence (not measured here) is suggested to provide better grounding and reduce hallucinations.",
            "evidence_decreased_confidence": null,
            "proposed_mechanism": "Maximum-likelihood training and free-form generation can produce confident, plausible outputs even when the model lacks correct grounded evidence; evaluation mismatches (phrasing, incomplete annotations) exacerbate perceived errors.",
            "key_findings": "Closed-book generation can hallucinate and is frequently penalized by exact-match metrics even when semantically acceptable answers are produced; human evaluation reveals both genuine errors and annotation/evaluation issues, highlighting the role of explicit evidence and evaluation design in assessing truthfulness.",
            "counterintuitive_behavior": true,
            "uuid": "e199.4",
            "source_info": {
                "paper_title": "How Much Knowledge Can You Pack into the Parameters of a Language Model?",
                "publication_date_yy_mm": "2020-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Realm: Retrieval-augmented language model pre-training",
            "rating": 2
        },
        {
            "paper_title": "Dense passage retrieval for open-domain question answering",
            "rating": 2
        },
        {
            "paper_title": "Language models as knowledge bases?",
            "rating": 2
        },
        {
            "paper_title": "Reading Wikipedia to answer open-domain questions",
            "rating": 2
        },
        {
            "paper_title": "Language models are unsupervised multitask learners",
            "rating": 1
        }
    ],
    "cost": 0.014115749999999998,
    "model_str": null
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>How Much Knowledge Can You Pack Into the Parameters of a Language Model?</h1>
<p>Adam Roberts*<br>Google<br>adarob@google.com</p>
<p>Colin Raffel ${ }^{*}$
Google
craffel@gmail.com</p>
<h2>Noam Shazeer</h2>
<p>Google
noam@google.com</p>
<h4>Abstract</h4>
<p>It has recently been observed that neural language models trained on unstructured text can implicitly store and retrieve knowledge using natural language queries. In this short paper, we measure the practical utility of this approach by fine-tuning pre-trained models to answer questions without access to any external context or knowledge. We show that this approach scales with model size and performs competitively with open-domain systems that explicitly retrieve answers from an external knowledge source when answering questions. To facilitate reproducibility and future work, we release our code and trained models. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Big, deep neural language models that have been pre-trained on unlabeled text have proven to be extremely performant when fine-tuned on downstream Natural Language Processing (NLP) tasks (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019; Lan et al., 2019; Raffel et al., 2019). Interestingly, it has also recently been observed that these models can internalize a sort of implicit "knowledge base" after pre-training (Petroni et al., 2019; Jiang et al., 2019; Talmor et al., 2019). This behavior is potentially useful because 1) the knowledge is built up by pre-training on unstructured and unlabeled text data, which is freely available in huge quantities on the Internet (Raffel et al., 2019; Wenzek et al., 2019), and 2) it is possible to retrieve information using informal natural language queries since these pre-trained language models excel when fine-tuned on natural language understanding tasks.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: T5 is pre-trained to fill in dropped-out spans of text (denoted by $<M>$ ) from documents in a large, unstructured text corpus. We fine-tune T5 to answer questions without inputting any additional information or context. This forces T5 to answer questions based on "knowledge" that it internalized during pre-training.</p>
<p>Past work investigating "language models as knowledge bases" has typically tried to understand the scope of the information stored in the model using synthetic tasks that are similar to the pre-training objective (Petroni et al., 2019; Jiang et al., 2019) and/or measure reasoning capabilities (Talmor et al., 2019). In this work, we take a different approach by evaluating the capability of language models on the practical task of opendomain question answering - specifically, we finetune the model to answer questions without access to any external knowledge or context. To do so, the model must parse a natural language query and "look up information" stored in its parameters.</p>
<p>Most past work on question answering either explicitly feeds pertinent information to the model alongside the question (for example, an article that contains the answer (Rajpurkar et al., 2016; Zhang et al., 2018; Khashabi et al., 2018; Clark et al., 2019)) or allows the model to retrieve information from an external knowledge source (Berant et al., 2013; Chen et al., 2017). By feeding the model the input question alone, we can determine how much knowledge it has stored in its param-</p>
<p>eters while measuring its performance on a useful real-world problem. We refer to this task as "closed-book question answering".</p>
<p>A separate question we address in this work is whether models with more parameters end up storing more information. It has been shown that transfer learning performance on many downstream tasks tends to improve as the model size and amount of unsupervised pre-training increases (Radford et al., 2019; Liu et al., 2019; Raffel et al., 2019). In this work, we leverage the pre-trained "T5" models released by Raffel et al. (2019), the largest of which has around 11 billion parameters. By measuring knowledge retrieval capabilities on models of various sizes - including models that have an order of magnitude more parameters than considered in past work - we can explore how well our approach scales.</p>
<h2>2 Background</h2>
<p>Question Answering The task of training a model to either select or output the correct answer to a given question is referred to as "question answering". The most popular variant of this task feeds the model some "context" containing the answer (for example, a paragraph from an encyclopedia article) alongside the question (Rajpurkar et al., 2016; Zhang et al., 2018; Khashabi et al., 2018; Clark et al., 2019). Models can be trained either to indicate the span of the context that contains the answer or output the text of the answer itself. Since this format can be seen as reading some text and answering a question about it, it has been referred to as "reading comprehension".</p>
<p>A more difficult variant is "open-domain question answering" (Prager, 2006), where the model can be asked arbitrary context-independent questions (e.g. well-known facts or historical details). It is typically assumed that the model can access an external collection of knowledge when answering questions (e.g. a structured knowledge base or unstructured text corpus), but the model is not given any information about where in the collection the answer appears. The reading comprehension task can be considered a simplified version of open-domain question answering where the model is provided with the oracle context to answer a given question. As an analogy, the open-domain question answering system acts as if it is taking an open-book exam where it can find and use infor-
mation in an external source of knowledge. ${ }^{2}$
In this work, we consider open-domain question answering with the additional constraint that the model is not allowed to access any external knowledge whatsoever when answering questions. Instead, the model itself must be pre-trained to store knowledge in its parameters before being fine-tuned to answer questions. In one view, this can be seen as an alternative way to approach open-domain question answering where instead of learning to access external knowledge the model needs to have "memorized" it in order to answer questions; in another view, this constraint creates a third and potentially more ambitious variant of the question answering task. A model that answers questions in this way is metaphorically similar to a student taking a closed-book exam, where the student must study and memorize all pertinent information before taking the test.</p>
<p>Transfer Learning with Language Models In the past few years, it has become increasingly common to pre-train a language model using an unsupervised objective on a large, unstructured text corpus before fine-tuning it on a downstream task of interest (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018). The popularity of this form of "transfer learning" is attributable to its empirical success on many NLP tasks (Peters et al., 2018; Devlin et al., 2018; Yang et al., 2019; Lan et al., 2019; Raffel et al., 2019). Loosely speaking, the pre-training step may provide the model with some generally-useful awareness of meaning, syntax, and "world knowledge". In question answering in particular, most state-of-the-art systems use some form of transfer learning.</p>
<p>Currently, the most popular model architectures used in transfer learning for NLP are Transformerbased (Vaswani et al., 2017) "encoder-only" models like BERT (Devlin et al., 2018). These models can produce a single prediction for each input token and have been applied to reading comprehension-style question answering by predicting which tokens of the context contain the answer. Encoder-only models are not applicable to closed-book question answering because no context is provided to extract the answer span from. An alternative to encoder-only models, recently advocated by Raffel et al. (2019), is to treat ev-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>ery NLP task as a text-to-text problem using an encoder-decoder Transformer. When this framework is applied to question answering, the model is trained to generate the literal text of the answer in a free-form fashion. Despite the potential difficulty of generating rather than extracting the answer, Raffel et al. (2019) demonstrated state-of-the-art results on the SQuAD (Rajpurkar et al., 2016), MultiRC (Khashabi et al., 2018), BoolQ (Clark et al., 2019), and ReCoRD (Zhang et al., 2018) reading comprehension tasks.</p>
<p>The text-to-text framework is directly applicable to closed-book question answering since the model can be trained to generate an answer with or without any additional information in its input. Crucially, fine-tuning a text-to-text model to answer questions without any context requires that the model retrieve information from its parameters that it learned during pre-training. Radford et al. (2019) considered a similar task to evaluate the zero-shot question answering capabilities of a language model. The concurrent "RELIC" and "EAE" models of Ling et al. (2020) and Févry et al. (2020) learn representations for an explicitly predefined set of entities and are evaluated on the same closed-book variant of TriviaQA that we consider. Relatedly, Petroni et al. (2019) show that it is possible to manually convert some questions to a fill-in-the-blank format amenable to an encoder-only model (e.g. "Who developed the theory of relativity?" gets mapped to "The theory of relativity was developed by ___ ").</p>
<h2>3 Experiments</h2>
<p>Datasets We consider the following opendomain question answering datasets: Natural Questions (Kwiatkowski et al., 2019), a dataset of questions from web queries, each accompanied by a Wikipedia article containing the answer; WebQuestions (Berant et al., 2013), comprising questions from web queries matched to corresponding entries in FreeBase (Bollacker et al., 2008); and TriviaQA (Joshi et al., 2017), a collection of questions from quiz league websites where each question is accompanied by pages from web and Wikipedia searches that may contain the answer. In this work, we only make use of the questions from each dataset - we completely ignore the matching documents supplied for each question.</p>
<p>For WebQuestions and TriviaQA we follow the standard evaluation procedures where each pre-
dicted answer is compared to the ground-truth after both are lowercased and stripped of articles, punctuation, and duplicate whitespace (Rajpurkar et al., 2016). For Natural Questions, we evaluate using both 1) the standard "opendomain" version as used e.g. by (Lee et al., 2019; Min et al., 2019b,a; Asai et al., 2019) where the model is only required to produce a single normalized answer and 2) the standard multi-answer variant used with reading comprehension systems (Kwiatkowski et al., 2019). We review the details of Natural Questions evaluation in appendix A.</p>
<p>Note that Natural Questions and TriviaQA have private test sets, so standard practice on their opendomain variants is to report performance on the development sets. However, we also include our results on the official TriviaQA test set by finetuning on the unfiltered training set and submitting our test set predictions to the leaderboard for the Wikipedia domain. We urge future work to adopt this approach to help ensure the validity of results and avoid potentially overfitting to a public set.</p>
<p>Training We leverage the pre-trained models provided by Raffel et al. (2019), referred to as the "Text-to-Text Transfer Transformer" (T5). The original T5 models were pre-trained on a multitask mixture including an unsupervised "span corruption" task on the C4 dataset as well as supervised translation, summarization, classification, and reading comprehension tasks. Note that none of the reading comprehension datasets used for pre-training T5 overlap with the question answering datasets that we consider in this paper. In order to measure how performance scales with model size, we perform experiments with the Base (220 million parameters), Large ( 770 million), 3B (3 billion), and 11B (11 billion) variants of T5. Given that the T5 models were pre-trained on a multitask mixture including question answering, we also report performance using the "T5.1.1" checkpoints, which were pre-trained on unlabeled data only. ${ }^{3}$</p>
<p>For fine-tuning the T5 checkpoints, we follow the procedure used in Raffel et al. (2019) without any additional hyperparameter tuning: We use the AdaFactor optimizer (Shazeer and Stern, 2018) with a constant learning rate of $0.001,10 \%$ dropout rate, and a batch size of 196,608 tokens. We halve the batch and double the dropout rate for WebQuestions due to its small size. For the T5.1.1 checkpoints, we follow the same procedure</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>but with a dropout rate of $5 \%$ for all three datasets.
For evaluation, we follow the procedure used in Lee et al. (2019): for each dataset, we hold out $10 \%$ of the training set as a validation split, finetune a model from the remaining $90 \%$ of examples, and select the best-performing checkpoint for final evaluation on the test set. While we chose to train for 20,000 steps, our validation accuracy typically plateaued after only a few hundred steps and showed no signs of overfitting.</p>
<p>We decode the model's predictions by choosing the most likely token at each timestep. To map question answering tasks to the text-to-text format, we simply feed the question with a task-specific prefix into the model as input and train it to predict the literal answer text as output.</p>
<p>Salient Span Masking Recently, Guu et al. (2020) found that a "salient span masking" (SSM) pre-training objective produced substantially better results in open-domain question answering. This approach first uses BERT (Devlin et al., 2018) to mine sentences that contain salient spans (named entities and dates) from Wikipedia. The question answering model is then pre-trained to reconstruct masked-out spans from these sentences, which Guu et al. (2020) hypothesize helps the model "focus on problems that require world knowledge". We experimented with using the same SSM data and objective to continue pretraining the T5 checkpoints for 100,000 additional steps before fine-tuning for question answering.</p>
<p>Results Our results on the open-domain Natural Questions, WebQuestions, and TriviaQA tasks are shown in table 1. Notably, performance on each dataset improves as the model size increases, with either T5-11B or the comparably-sized T5.1.1XXL (pre-trained only on unlabeled data) performing best in every case. Further, we find that using Guu et al. (2020)'s SSM pre-training produces a substantial boost in performance. T5.1.1XXL with SSM ultimately achieves state-of-theart on WebQuestions and our largest models beat most other methods on Natural Questions and TriviaQA. Importantly, all previous methods except Ling et al. (2020) and Févry et al. (2020) operate in the "open-book" setting by explicitly retrieving and using information from an external knowledge source. While our largest models are computationally intensive, we note that most open-domain question answering systems must</p>
<p>Table 1: Scores achieved by fine-tuning T5 on the open-domain Natural Questions (NQ), WebQuestions (WQ), and TriviaQA (TQA) tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">NQ</th>
<th style="text-align: center;">WQ</th>
<th style="text-align: center;">TQA</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">dev</td>
<td style="text-align: center;">test</td>
</tr>
<tr>
<td style="text-align: left;">Chen et al. (2017)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">20.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Lee et al. (2019)</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">47.1</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Min et al. (2019a)</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Min et al. (2019b)</td>
<td style="text-align: center;">31.8</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Asai et al. (2019)</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Ling et al. (2020)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Guu et al. (2020)</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">40.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Févry et al. (2020)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">43.2</td>
<td style="text-align: center;">53.4</td>
</tr>
<tr>
<td style="text-align: left;">Karpukhin et al. (2020)</td>
<td style="text-align: center;">$\mathbf{4 1 . 5}$</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">$\mathbf{5 7 . 9}$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">T5-Base</td>
<td style="text-align: center;">25.9</td>
<td style="text-align: center;">27.9</td>
<td style="text-align: center;">23.8</td>
<td style="text-align: center;">29.1</td>
</tr>
<tr>
<td style="text-align: left;">T5-Large</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">30.6</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">35.9</td>
</tr>
<tr>
<td style="text-align: left;">T5-3B</td>
<td style="text-align: center;">30.4</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">43.4</td>
</tr>
<tr>
<td style="text-align: left;">T5-11B</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">42.3</td>
<td style="text-align: center;">50.1</td>
</tr>
<tr>
<td style="text-align: left;">T5-11B + SSM</td>
<td style="text-align: center;">34.8</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">60.5</td>
</tr>
<tr>
<td style="text-align: left;">T5.1.1-Base</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">28.2</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">30.6</td>
</tr>
<tr>
<td style="text-align: left;">T5.1.1-Large</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">37.2</td>
</tr>
<tr>
<td style="text-align: left;">T5.1.1-XL</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">45.1</td>
</tr>
<tr>
<td style="text-align: left;">T5.1.1-XXL</td>
<td style="text-align: center;">32.8</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">52.5</td>
</tr>
<tr>
<td style="text-align: left;">T5.1.1-XXL + SSM</td>
<td style="text-align: center;">35.2</td>
<td style="text-align: center;">$\mathbf{4 2 . 8}$</td>
<td style="text-align: center;">51.9</td>
<td style="text-align: center;">$\mathbf{6 1 . 6}$</td>
</tr>
</tbody>
</table>
<p>first do an expensive lookup step over the entire knowledge corpus and then attend to a long document to extract an answer. Our approach omits both of these steps, which ultimately saves a large amount of computation and memory.</p>
<p>Having established that our approach is competitive on open-domain question answering, we now evaluate it on the standard (and more difficult) multi-answer variant of Natural Questions. Virtually all models used on this task are reading comprehension systems that select the correct answer from an oracle context. After fine-tuning, T5-11B + SSM achieves a recall of 36.2 on the validation set, which lags behind the state-of-theart score of 51.9 from Pan et al. (2019) ${ }^{4}$ but outperforms the best baseline published alongside the dataset (recall of 33.2 (Kwiatkowski et al., 2019)). This shows that T5 can effectively answer questions with multiple answers. We discuss additional experiments and negative results in appendix B.</p>
<p>Human Evaluation The benchmarks we used and the "exact match" score assume that the model directly extracts answers from an external knowledge source. In contrast, our model generates answers in a free-form fashion. We hypothesize that this results in many false negatives when an-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2: A breakdown of the 150 hand-evaluated examples from Natural Questions where the T5 predictions were labelled as incorrect by the automatic procedure. We found only $62 \%$ of these to be true positives.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"></th>
<th style="text-align: left;">Example</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Category</td>
<td style="text-align: center;">Percentage</td>
<td style="text-align: left;">Question</td>
<td style="text-align: left;">Target(s)</td>
<td style="text-align: left;">T5 Prediction</td>
</tr>
<tr>
<td style="text-align: left;">True Negative</td>
<td style="text-align: center;">$62.0 \%$</td>
<td style="text-align: left;">what does the ghost of christmas <br> present sprinkle from his torch</td>
<td style="text-align: left;">little warmth, warmth</td>
<td style="text-align: left;">confetti</td>
</tr>
<tr>
<td style="text-align: left;">Phrasing Mismatch</td>
<td style="text-align: center;">$13.3 \%$</td>
<td style="text-align: left;">who plays red on orange is new <br> black</td>
<td style="text-align: left;">kate mulgrew</td>
<td style="text-align: left;">katherine kiernan <br> maria mulgrew</td>
</tr>
<tr>
<td style="text-align: left;">Incomplete Annotation</td>
<td style="text-align: center;">$13.3 \%$</td>
<td style="text-align: left;">where does the us launch space <br> shuttles from</td>
<td style="text-align: left;">florida</td>
<td style="text-align: left;">kennedy lc39b</td>
</tr>
<tr>
<td style="text-align: left;">Unanswerable</td>
<td style="text-align: center;">$11.3 \%$</td>
<td style="text-align: left;">who is the secretary of state for <br> northern ireland</td>
<td style="text-align: left;">karen bradley</td>
<td style="text-align: left;">james brokenshire</td>
</tr>
</tbody>
</table>
<p>swers do not exactly match the ground-truth context intended for each question. We therefore manually inspected 150 examples from the Natural Questions validation set where our model's prediction was counted as incorrect in hopes of identifying "false negatives" according to the exact match metric. We found that false negatives fell into three broad categories: First, answers with meaning-preserving differences in phrasing (e.g. "April 15" vs. "April 15th"); second, questions that were missing all possible correct answers (e.g. "where does the us launch space shuttles from" was annotated with the single ground-truth answer "florida", despite many possible correct answers such as "Kennedy Space Center", "Merritt Island", "Cape Canaveral", etc.); and finally, some questions were unanswerable without knowing the exact time or article they referred to (e.g. "what is the latest version of microsoft office 2010" depends on when the question is being asked). We provide examples of each of these false negative types in table 2. We note that open-book question answering systems could also be impacted to a lesser extent by these issues (e.g. if they select a slightly different answer span from the annotated one or retrieve a non-golden document that contains a different correct answer).</p>
<p>Of the 150 examples inspected, we found that 20 were marked as incorrect due to differences in phrasing, another 20 were not annotated with all correct answers, and 17 were unanswerable without appropriate context. Removing unanswerable questions from the validation set and recomputing our model's accuracy based on this false-negative rate produces a score of 57.8 . This suggests that the performance of closed-book question answering systems (in terms of how often it correctly answers questions) is substantially underestimated by the evaluation procedure used in these bench-
marks. For full transparency, we publicly release the results of our human evaluation and include an appropriate reference when we determined that a predicted answer was missing from ground-truth. ${ }^{5}$</p>
<h2>4 Conclusion</h2>
<p>In this short paper, we have shown that large language models pre-trained on unstructured text can attain competitive results on open-domain question answering benchmarks without any access to external knowledge. This suggests a fundamentally different approach to designing question answering systems, motivating many threads for future work: First, we obtained state-of-the-art results only with the largest models which had around 11 billion parameters. This model size can be prohibitively expensive in resource-constrained settings, prompting future work on more efficient language models. Second, "open-book" models typically provide some indication of what information they accessed when answering a question. This can provide a useful form of interpretability. In contrast, our model distributes knowledge in its parameters in an inexplicable way and hallucinates realistic-looking answers when it is unsure. Third, the maximum-likelihood objective used to train our model provides no guarantees as to whether a model will learn a fact or not. This makes it difficult to ensure that the model obtains specific knowledge over the course of pre-training and prevents us from explicitly updating or removing knowledge from a pre-trained model. Finally, the tasks we used in this paper mainly measure "trivia"-style knowledge. We are therefore interested in measuring performance on question answering tasks that require reasoning capabilities such as DROP (Dua et al., 2019).</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>Acknowledgments</h2>
<p>We thank Kelvin Guu, Kenton Lee, Ming-Wei Chang, Zora Tung, and Ice Pasupat for providing the open-domain question answering evaluation setup and access to their salient span-annotated data; Roy Frostig and Katherine Lee for comments and suggestions on this manuscript; Noah Constant for suggesting we try salience span masking; and Monica Dinculescu for building an interactive demonstration of our results. ${ }^{6}$</p>
<h2>References</h2>
<p>Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. 2019. Learning to retrieve reasoning paths over Wikipedia graph for question answering. arXiv preprint arXiv:1911.10470.</p>
<p>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data, pages 1247-1250.</p>
<p>Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to answer open-domain questions. arXiv preprint arXiv:1704.00051.</p>
<p>Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. BoolQ: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044.</p>
<p>Andrew M. Dai and Quoc V. Le. 2015. Semisupervised sequence learning. In Advances in Neural Information Processing Systems.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint arXiv:1903.00161.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Thibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski. 2020. Entities as experts: Sparse memory access with entity supervision. arXiv preprint arXiv:2004.07202.</p>
<p>Kelvin Guu, Kenton Lee, Zora Tung, Pasupat Panupong, and Ming-Wei Chang. 2020. Realm: Retrieval-augmented language model pre-training. arXiv preprint arXiv:2002.08909.</p>
<p>Jeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. arXiv preprint arXiv:1801.06146.</p>
<p>Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2019. How can we know what language models know? arXiv preprint arXiv:1911.12543.</p>
<p>Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551.</p>
<p>Vladimir Karpukhin, Barlas Ouguz, Sewon Min, Ledell Yu Wu, Sergey Edunov, Danqi Chen, and Wen tau Yih. 2020. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906.</p>
<p>Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. 2018. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of North American Chapter of the Association for Computational Linguistics (NAACL).</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7.</p>
<p>Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. ALBERT: A lite BERT for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942.</p>
<p>Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. arXiv preprint arXiv:1906.00300.</p>
<p>Jeffrey Ling, Nicholas FitzGerald, Zifei Shan, Livio Baldini Soares, Thibault Févry, David Weiss, and Tom Kwiatkowski. 2020. Learning crosscontext entity representations from text. arXiv preprint arXiv:2001.03765.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct electricity? a new dataset for open book question answering. In EMNLP.</p>
<p>Sewon Min, Danqi Chen, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2019a. A discrete hard EM approach for weakly supervised question answering. arXiv preprint arXiv:1909.04849.</p>
<p>Sewon Min, Danqi Chen, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2019b. Knowledge guided text retrieval and reading for open domain question answering. arXiv preprint arXiv:1911.03868.</p>
<p>Lin Pan, Rishav Chakravarti, Anthony Ferritto, Michael Glass, Alfio Gliozzo, Salim Roukos, Radu Florian, and Avirup Sil. 2019. Frustratingly easy natural question answering. arXiv preprint arXiv:1909.05286.</p>
<p>Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. arXiv preprint arXiv:1802.05365.</p>
<p>Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. 2019. Language models as knowledge bases? arXiv preprint arXiv:1909.01066.</p>
<p>John Prager. 2006. Open-domain question-answering. Foundations and Trends in Information Retrieval, $1(2)$.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250.</p>
<p>Noam Shazeer and Mitchell Stern. 2018. Adafactor: Adaptive learning rates with sublinear memory cost. arXiv preprint arXiv:1804.04235.</p>
<p>Alon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. 2019. olmpics-on what language model pre-training captures. arXiv preprint arXiv:1912.13283.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems.</p>
<p>Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzman, Armand Joulin, and Edouard Grave. 2019. Ccnet: Extracting high quality monolingual datasets from web crawl data. arXiv preprint arXiv:1911.00359.</p>
<p>Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019. XLNet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237.</p>
<p>Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. 2018. ReCoRD: Bridging the gap between human and machine commonsense reading comprehension. arXiv preprint arXiv:1810.12885.</p>
<h2>A Metrics for Natural Questions</h2>
<p>Compared to WebQuestions and TriviaQA, Natural Questions is distributed with a much richer set of annotations: Each question can be annotated either as unanswerable (given the oracle context), with a short answer, or with a yes/no answer; questions in the validation set can be annotated more than once; and some questions have multiple answers (e.g. "Who are the members of the Beatles?" has four answers). We consider two variants of Natural Questions. In both cases, we omit the "unanswerable" label and long answers, which are nearly impossible to predict without the oracle context.</p>
<p>The first variant is the standard "open-domain" version as used e.g. by (Lee et al., 2019; Min et al., 2019b,a; Asai et al., 2019), where 1) the model is only ever trained to output a single answer; 2) if a question has multiple answers, it is only trained to predict the first answer; 3) any questions with answers longer than five tokens are ignored; 4) answers are normalized before being compared (in the same manner as is typically done for WebQuestions and SQuAD); and 5) a predicted answer is considered correct if it matches any of the answers provided by any of the annotators (e.g. "Ringo Starr" would be considered a correct answer to "Who are the members of the Beatles?").</p>
<p>The second variant closely matches the official evaluation procedure used by the Natural Questions leaderboard, where our model is trained to predict all ground-truth answers and is only considered correct if it predicts all answers for any one of the annotators. As in the official evaluation, we consider questions with fewer than two non-null annotations unanswerable (given the context), but because we cannot predict unanswerability without the context, we only report the recall score. Further, because our model does not have access to the oracle context, we also normalize predicted and ground-truth answers when comparing them. The use of multiple possible answers also required minor modification of our text-totext format. In this case, we trained the model to output each answer delimited by the text "answer:" (for example, "answer: John Lennon answer: Ringo Starr answer: George Harrison answer: Paul McCartney"). We then split out each answer from the model's predictions as a postprocessing step before evaluating it against the set of answers provided by each annotation.</p>
<h2>B Other Things We Tried</h2>
<p>In the course of undertaking this study, we tried various ideas that ultimately did not improve performance. We briefly discuss them here.</p>
<p>Continued Pre-Training on Wikipedia The T5 checkpoints we used were primarily pre-trained on C4, a large and diverse dataset of unstructured web content. We were interested to see whether we could improve performance by doing further pretraining on data that was better tailored to the tasks we considered. Since both Natural Questions and TriviaQA source their answers from Wikipedia articles, we experimented with further pre-training on text data from English Wikipedia with the same unsupervised objective ("span corruption") as was used by T5. We found that this additional "indomain" pre-training had virtually no effect on performance. This may be because C4 already contains many articles from Wikipedia and the T5 checkpoints were pre-trained long enough to see plenty of this content.</p>
<h2>Pre-Training From Scratch On Wikipedia</h2>
<p>Since all of the answers to the questions in Natural Questions appeared in Wikipedia, we carried out an additional experiment where we pre-trained T5 from scratch only on data from Wikipedia. We pre-trained on up to 1 trillion tokens (the same amount the T5 checkpoints were pre-trained on) with the span corruption objective and measured fine-tuned performance after various amounts of pre-training. Unfortunately, this resulted in dramatically worse performance regardless of the amount of pre-training. We suspect that this is because Wikipedia is too small and results in detrimental overfitting.</p>
<p>Span-Corruption Pre-Training on Wikipedia Sentences with Salient Spans As described previously, we observed significant performance gains with additional pre-training using "salient span masking" (SSM) on the Wikipedia sentence dataset from Guu et al. (2020) but not when using the standard "span corruption" (SC) from Raffel et al. (2019) on longer Wikipedia articles. While SC masks random spans of the input by dropping $15 \%$ of its tokens (sampled each epoch) and replacing each consecutive span of dropped tokens with a unique sentinel, SSM specifically masks out one named entity or date in the input sentence.</p>
<p>We were interested in determining whether the</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Comparing additional pre-training using either salient span masking (SSM) or span corruption (SC). We further pre-trained T5.1.1-XXL on the Wikipedia sentence dataset from Guu et al. (2020) with each objective, fine-tuning on a mixture of our three closed-book QA tasks every 10,000 steps. For each fine-tuning run, we report the maximum exact match score achieved on the validation set over 10,000 steps of fine-tuning.
gains achieved were attributable to the use of a more task-specific dataset (pre-split into sentences that are known to contain at least one entity) or if the SSM objective itself was critical. As illustrated in fig. 2, the SSM objective is clearly an important ingredient in the improved performance; we saw no significant improvement versus the baseline T5 model when using SC.</p>
<h2>Fine-Tuning On All Question Answering Tasks</h2>
<p>The text-to-text framework used by T5 makes it simple to train multitask models simply by supplying a different task-specific prefix for each task and concatenating all of the constituent datasets. Since all of the question answering tasks we consider in this study follow the same basic structure, we were hopeful that training on a multitask
mixture of Natural Questions, WebQuestions, and TriviaQA would improve performance due to the additional supervised data. While multitask training improved performance on the Natural Questions by 0.5 , it produced slightly worse results on the other tasks.</p>
<h2>Randomly Sampling Answers For Natural</h2>
<p>Questions In the open-domain variant of Natural Questions, the model is only trained to generate a single answer at a time. For the results presented in the main text, when a question was annotated with multiple answers, we simply trained the model on the first annotated answer. We also experimented with sampling a random answer from the set of possible answers for pre-training and found that it did not affect performance.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ http://t5-trivia.glitch.me/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
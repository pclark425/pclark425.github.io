<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5839 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5839</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5839</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-06d7cb8c8816360feb33c3367073e0ef66d7d0b0</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/06d7cb8c8816360feb33c3367073e0ef66d7d0b0" target="_blank">Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> Tk-Instruct is built, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples) that outperforms existing instruction-following models such as InstructGPT by over 9% on the authors' benchmark despite being an order of magnitude smaller.</p>
                <p><strong>Paper Abstract:</strong> How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions—training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones.Furthermore, we build Tk-Instruct, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-Instruct outperforms existing instruction-following models such as InstructGPT by over 9% on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5839.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5839.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Default instruction encoding (Def + Pos(2))</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Task instruction encoding: Task Definition plus two positive demonstration examples</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's default prompt format for experiments: the task's natural-language definition followed by two positive example input-output pairs (no negative examples or explanations), prepended to each instance and used as the model input for open-ended generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T_k-InSTRUCT; mT_k-InSTRUCT; InstructGPT; GPT-3; T5-LM; T0</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B (T_k-InSTRUCT), 13B (mT_k-InSTRUCT), 175B (InstructGPT), 175B (GPT-3), 11B (T5-LM), 11B (T0)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SUP-NATURALINSTRUCTIONS cross-task generalization (English and X-lingual evaluation tracks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Cross-task generalization to held-out NLP tasks (119 English tasks; 35 non-English tasks) from a large meta-dataset of 1,616 tasks; models generate outputs in an open-ended setting and are evaluated with ROUGE-L.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Instruction appended to input: 'Definition: <task definition>' followed by two positive examples (each with input/output/explanation fields), then the test input; open-ended generation (no option ranking). This encoding is used as the default for training and evaluating instruction-following models in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Various alternative encodings studied (definition-only; positive examples only with 1/2/4 examples; definition+examples with/without negative examples; inclusion of explanations).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Aggregated ROUGE-L (English track): T_k-InSTRUCT (11B) = 62.0; InstructGPT (175B) = 52.1; GPT-3 (175B) = 45.0; T5-LM (11B) = 30.2; T0 (11B) = 32.3. (Table 3, default encoding = definition + 2 positive examples.) For X-lingual track: mT_k-InSTRUCT (13B) = 66.1; InstructGPT = 52.8; GPT-3 = 51.3; Copying Demo Output baseline = 50.3 (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Using the same default encoding, T_k-InSTRUCT (11B) outperforms InstructGPT (175B) by 9.9 ROUGE-L on the 119 unseen English tasks (62.0 vs 52.1). mT_k-InSTRUCT (13B) outperforms InstructGPT by 13.3 ROUGE-L on the 35 non-English tasks (66.1 vs 52.8).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+9.9 ROUGE-L (T_k-InSTRUCT vs InstructGPT on English) when both are evaluated with the default encoding; +13.3 ROUGE-L for multilingual comparison on X-lingual tasks (mT_k-InSTRUCT vs InstructGPT).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Instruction-tuning on a large, diverse set of task instructions (using task definitions plus examples) enables models to generalize to unseen tasks; the paper uses the definition+examples encoding as their most effective instruction combination. The authors hypothesize that the combination of declarative definitions and demonstration examples provides both an explicit mapping and concrete demonstrations that help the model learn to follow new instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5839.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5839.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instructional elements ablation (Table 4)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablation and cross-encoding study of instruction elements (definition, positive examples, negative examples, explanations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systematic experiments training models with different combinations of instruction elements and evaluating both in-matching and cross-encoding test conditions, showing which instruction parts help and whether models trained on one encoding generalize to other encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T_k-InSTRUCT (trained from T5-3B checkpoint for analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B (analysis checkpoint)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SUP-NATURALINSTRUCTIONS English evaluation tasks (subset used for ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Cross-task generalization across a diverse set of English NLP tasks (open-ended generation), evaluated with ROUGE-L.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Multiple instruction encodings were used for training/evaluation: Task ID only, Definition only, Pos(1), Def+Pos(1), Pos(2), Def+Pos(2), Def+Pos(2)+Neg(2), Def+Pos(2)+Neg(2)+Expl, Pos(4), Def+Pos(4). Each encoding prepends different elements before the input.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Each encoding is compared against all others (train-on-one-encoding, test-on-each-encoding), as presented in a training-vs-testing matrix (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Diagonal (train/test same encoding) average ROUGE-Ls (from Table 4 averages): Task ID = 33.9; Definition only = 39.9; Pos(1) = 43.1; Def+Pos(1) = 44.5; Pos(2) = 45.0; Def+Pos(2) = 46.4; Def+Pos(2)+Neg(2) = 45.9; Def+Pos(2)+Neg(2)+Expl = 44.3; Pos(4) = 44.5; Def+Pos(4) = 46.0. (Column 'Average' in Table 4.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Definition+Pos(2) (46.4) vs Definition only (39.9): +6.5 ROUGE-L; Pos(2) (45.0) vs Pos(4) (44.5): negligible change; adding negative examples to Def+Pos(2) gives a small decrease (46.4 -> 45.9); adding explanations reduces performance (46.4 -> 44.3).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+6.5 ROUGE-L improvement from adding 2 positive examples to a definition-only encoding; adding more positive examples (going from 2 to 4) yields negligible gains (~+0.6 at most in some cells); adding negative examples yields slight benefit or small decrease (~-0.5 average in this study); including explanations decreases performance (~-2.1 average).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>mixed: combining definition with positive examples improved performance; more examples beyond 2 gave negligible gain; negative examples gave minor/no consistent benefit; explanations decreased performance.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Definitions provide an explicit mapping specification while examples provide concrete behavioral evidence; combining them is synergistic. Explanations can hurt when model capacity is insufficient (consistent with prior observations), possibly because explanations introduce additional, sometimes noisy, textual material that confuses the model or encourages overfitting to spurious patterns. Negative examples give limited signal compared to positive examples.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Definition-only models cannot generalize to example-only test encodings; example-only models cannot generalize to definition-only test encodings. Models trained with both definitions and examples are robust across different test encodings (i.e., they generalize better to encoding variations).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5839.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5839.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instances-per-task scaling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of number of training instances per task on cross-task generalization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical study showing that increasing the number of instances per training task yields diminishing returns for cross-task generalization, with performance saturating at a small number (64) of instances per task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T_k-InSTRUCT (T5-3B for analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B (analysis checkpoint)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SUP-NATURALINSTRUCTIONS English track (cross-task generalization)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Training models on varying numbers of instances per task while keeping task diversity fixed, then evaluating on unseen tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Same instruction encoding (definition + 2 positive examples by default); vary number of training instances per task (e.g., 8, 16, 32, 64, 256, ...).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Performance saturates when using only 64 instances per task; adding many more instances per task does not meaningfully improve ROUGE-L and may increase training time or risk overfitting (Fig. 5b). Exact ROUGE-L curve values are shown in Fig. 5(b) (saturation point ~64 instances).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Saturation after ~64 instances per task (i.e., additional instances beyond 64 produce negligible gains).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>no effect beyond a modest number of examples per task (saturation)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>In the instruction-following, cross-task generalization setting, diversity of tasks matters more than per-task depth: once the model sees a modest number of exemplars per task it learns to map instructions to behavior, and extra per-task instances mainly cause overfitting to training tasks rather than better generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5839.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5839.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Training-task diversity scaling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of number of distinct training tasks on cross-task generalization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Study that varies the number of distinct tasks used for meta-training and finds model performance increases approximately log-linearly with the number of observed tasks (task diversity).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T_k-InSTRUCT (T5-3B for analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B (analysis checkpoint)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SUP-NATURALINSTRUCTIONS English track</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generalization to unseen tasks after meta-training on varying counts of distinct training tasks sampled from the full training set.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Default instruction encoding (definition + 2 positive examples); vary number of distinct tasks used in fine-tuning (e.g., 128, 256, 512, 757, ...).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Model generalization grows approx. log-linearly with number of observed tasks (Fig. 5a). Example mapping reported: a T5-large model trained with 757 tasks achieved ROUGE-L 48.0, comparable to a T5-3B model trained with 128 tasks (48.4), illustrating trade-off between task diversity and model size.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Substantial: expanding the number of distinct training tasks yields steady performance gains; growth is roughly linear in the log of task count.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (with more tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Exposure to diverse tasks teaches the model a wider variety of instruction-to-behavior mappings, improving zero/few-shot generalization to unseen tasks; thus diversity is a primary driver of instruction-following generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5839.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5839.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model size scaling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of model parameter size on instruction-following generalization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analysis showing that increasing model size (T5 family) consistently improves cross-task generalization performance (log-linear trend with parameter count).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T_k-InSTRUCT (initialized from different T5 checkpoints: small, base, large, XL, XXL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various: T5-small/base/large/XL/XXL (including 11B); specific experiments include 3B and 11B variants</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SUP-NATURALINSTRUCTIONS English track</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Cross-task generalization on held-out tasks after instruction-tuning; measure performance as model size varies.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Default instruction encoding (definition + 2 positive examples).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Increasing model size yields consistent and substantial ROUGE-L gains (Fig. 5c). Example from the paper: T_k-InSTRUCT 11B (T5-11B) achieves much higher ROUGE-L than smaller checkpoints; T_k-InSTRUCT (11B) reached 62.0 ROUGE-L on English unseen tasks (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Performance grows roughly log-linearly with parameter size (no single scalar given for all steps; see Fig. 5c); concrete example: 11B T_k-InSTRUCT (62.0 ROUGE-L) substantially outperforms 3B variants.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Larger models better absorb diverse instruction mappings and patterns and therefore generalize more strongly to unseen tasks; model scaling and task diversity act as complementary levers.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>This contradicts the claim in Xu et al. (2022) that 'model size has little impact with extremely large number of tasks'; the paper's experiments show model size does matter.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5839.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5839.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt style mismatch (PromptSource vs SUP-NATINST)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of prompt phrasing/style differences across prompt collections (PromptSource vs SUP-NATURALINSTRUCTIONS) on model performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observation that models trained on prompt collections with different prompt styles may not transfer well to a different instruction style; cited as a likely cause for T0's relatively weak performance on SUP-NATINST.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T0 (and comparison to T5-LM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B (T0)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SUP-NATURALINSTRUCTIONS English evaluation (cross-task generalization)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluation of T0 (trained on PromptSource prompts) on SUP-NATINST tasks whose instructions are longer and more detailed on average.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt style difference: PromptSource prompts are relatively concise (avg ~24.8 words) while SUP-NATINST definitions are longer (avg ~56.6 words); T0 was trained on PromptSource-style prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Short PromptSource-style prompts (used to train T0) vs longer, more comprehensive SUP-NATINST definitions + examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>T0 (11B) achieved 32.3 ROUGE-L on SUP-NATINST English track versus T5-LM (11B) 30.2 and T_k-InSTRUCT 62.0; T0 only slightly better than T5-LM despite instruction-tuning on another prompt collection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>T0's small gain over T5-LM suggests prompt/style mismatch harms transfer; the authors suspect differences in prompting style are responsible for T0 being only slightly better than T5-LM on SUP-NATINST.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Small: T0 (32.3) vs T5-LM (30.2) = +2.1 ROUGE-L, which the authors attribute to style mismatch rather than instruction-tuning efficacy on SUP-NATINST.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced/limited transfer (style mismatch likely reduced effectiveness)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The style and phrasing of prompts used during instruction-tuning matters; models tuned on concise PromptSource prompts may not perform well when evaluated with longer, more descriptive task definitions, indicating sensitivity to prompt wording and style.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5839.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5839.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Explanations in prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inclusion of natural-language explanations within instruction prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Study of whether adding explanation text (short natural-language rationales following examples) to instruction encodings affects performance; found that explanations decreased performance for models that are not sufficiently large.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T_k-InSTRUCT (T5-3B for analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B (analysis checkpoint)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SUP-NATURALINSTRUCTIONS English evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate whether adding explanatory text to examples in instruction encodings helps cross-task generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Def + Pos(2) + Neg(2) + Expl (i.e., including explanations associated with examples) versus the same encoding without explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Same encoding with explanations vs without explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average ROUGE-L with Def+Pos(2) = 46.4; with Def+Pos(2)+Neg(2)+Expl = 44.3 (Table 4 average column) — a decrease of ~2.1 ROUGE-L in the study's average.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Def+Pos(2) (46.4) vs Def+Pos(2)+Neg(2)+Expl (44.3): -2.1 ROUGE-L when explanations included (for the models/scale tested).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>-2.1 ROUGE-L (average decrease in this ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Explanations may introduce additional, potentially noisy text that confuses the model or encourage spurious correlations; prior work (Mishra et al., 2022b; Lampinen et al., 2022) observed similar effects when models are not large enough to benefit from explanation text.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Authors note that future work should explore whether larger models can benefit from explanations; the negative effect is reported here for not-large-enough models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5839.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5839.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Open-ended generation evaluation (vs option-ranking)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Choice of open-ended generation evaluation format instead of option-ranking or forced-choice</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methodology decision: evaluate models via open-ended text generation given instructions rather than scoring/ranking candidate options; argued to be a more realistic measure of instruction-following generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>All evaluated models (T_k-InSTRUCT, InstructGPT, GPT-3, T5-LM, T0, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SUP-NATURALINSTRUCTIONS cross-task generalization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Diverse tasks including classification and generation; paper adopts an open-ended generation set-up (no task-specific assumptions) and uses ROUGE-L as aggregated metric.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Open-ended text generation where model outputs free-form text; contrasts with option-ranking evaluation used in other works for classification tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Open-ended generation (this paper) vs option-ranking / candidate scoring (other works like Sanh et al. and Wei et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported ROUGE-L aggregated scores (Table 3) reflect models' performance under open-ended generation; authors argue ranking-based evaluations (option scoring) are not used here to avoid task-specific assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>different evaluation paradigm (not strictly 'improve' or 'reduce')</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Open-ended generation better reflects realistic instruction-following as it does not rely on format-specific assumptions (e.g., candidate lists) and thus tests models' ability to produce correct outputs under natural instruction prompt formats; the choice of evaluation format can affect which methods appear stronger (models optimized for option-ranking might perform differently in open-ended settings).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Cross-Task Generalization via Natural Language Crowdsourcing Instructions <em>(Rating: 2)</em></li>
                <li>Multitask Prompted Training Enables Zero-Shot Task Generalization <em>(Rating: 2)</em></li>
                <li>Finetuned Language Models are Zero-Shot Learners <em>(Rating: 2)</em></li>
                <li>Training Language Models to Follow Instructions with Human Feedback <em>(Rating: 2)</em></li>
                <li>Can Language Models Learn from Explanations in Context? <em>(Rating: 2)</em></li>
                <li>PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts <em>(Rating: 1)</em></li>
                <li>ZeroPrompt: Scaling Prompt-Based Pretraining to 1,000 Tasks Improves Zero-Shot Generalization <em>(Rating: 1)</em></li>
                <li>MetaICL: Learning to learn in context <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5839",
    "paper_id": "paper-06d7cb8c8816360feb33c3367073e0ef66d7d0b0",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "Default instruction encoding (Def + Pos(2))",
            "name_full": "Task instruction encoding: Task Definition plus two positive demonstration examples",
            "brief_description": "The paper's default prompt format for experiments: the task's natural-language definition followed by two positive example input-output pairs (no negative examples or explanations), prepended to each instance and used as the model input for open-ended generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T_k-InSTRUCT; mT_k-InSTRUCT; InstructGPT; GPT-3; T5-LM; T0",
            "model_size": "11B (T_k-InSTRUCT), 13B (mT_k-InSTRUCT), 175B (InstructGPT), 175B (GPT-3), 11B (T5-LM), 11B (T0)",
            "task_name": "SUP-NATURALINSTRUCTIONS cross-task generalization (English and X-lingual evaluation tracks)",
            "task_description": "Cross-task generalization to held-out NLP tasks (119 English tasks; 35 non-English tasks) from a large meta-dataset of 1,616 tasks; models generate outputs in an open-ended setting and are evaluated with ROUGE-L.",
            "problem_format": "Instruction appended to input: 'Definition: &lt;task definition&gt;' followed by two positive examples (each with input/output/explanation fields), then the test input; open-ended generation (no option ranking). This encoding is used as the default for training and evaluating instruction-following models in the paper.",
            "comparison_format": "Various alternative encodings studied (definition-only; positive examples only with 1/2/4 examples; definition+examples with/without negative examples; inclusion of explanations).",
            "performance": "Aggregated ROUGE-L (English track): T_k-InSTRUCT (11B) = 62.0; InstructGPT (175B) = 52.1; GPT-3 (175B) = 45.0; T5-LM (11B) = 30.2; T0 (11B) = 32.3. (Table 3, default encoding = definition + 2 positive examples.) For X-lingual track: mT_k-InSTRUCT (13B) = 66.1; InstructGPT = 52.8; GPT-3 = 51.3; Copying Demo Output baseline = 50.3 (Table 3).",
            "performance_comparison": "Using the same default encoding, T_k-InSTRUCT (11B) outperforms InstructGPT (175B) by 9.9 ROUGE-L on the 119 unseen English tasks (62.0 vs 52.1). mT_k-InSTRUCT (13B) outperforms InstructGPT by 13.3 ROUGE-L on the 35 non-English tasks (66.1 vs 52.8).",
            "format_effect_size": "+9.9 ROUGE-L (T_k-InSTRUCT vs InstructGPT on English) when both are evaluated with the default encoding; +13.3 ROUGE-L for multilingual comparison on X-lingual tasks (mT_k-InSTRUCT vs InstructGPT).",
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Instruction-tuning on a large, diverse set of task instructions (using task definitions plus examples) enables models to generalize to unseen tasks; the paper uses the definition+examples encoding as their most effective instruction combination. The authors hypothesize that the combination of declarative definitions and demonstration examples provides both an explicit mapping and concrete demonstrations that help the model learn to follow new instructions.",
            "counterexample_or_null_result": null,
            "uuid": "e5839.0",
            "source_info": {
                "paper_title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Instructional elements ablation (Table 4)",
            "name_full": "Ablation and cross-encoding study of instruction elements (definition, positive examples, negative examples, explanations)",
            "brief_description": "Systematic experiments training models with different combinations of instruction elements and evaluating both in-matching and cross-encoding test conditions, showing which instruction parts help and whether models trained on one encoding generalize to other encodings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T_k-InSTRUCT (trained from T5-3B checkpoint for analysis)",
            "model_size": "3B (analysis checkpoint)",
            "task_name": "SUP-NATURALINSTRUCTIONS English evaluation tasks (subset used for ablation)",
            "task_description": "Cross-task generalization across a diverse set of English NLP tasks (open-ended generation), evaluated with ROUGE-L.",
            "problem_format": "Multiple instruction encodings were used for training/evaluation: Task ID only, Definition only, Pos(1), Def+Pos(1), Pos(2), Def+Pos(2), Def+Pos(2)+Neg(2), Def+Pos(2)+Neg(2)+Expl, Pos(4), Def+Pos(4). Each encoding prepends different elements before the input.",
            "comparison_format": "Each encoding is compared against all others (train-on-one-encoding, test-on-each-encoding), as presented in a training-vs-testing matrix (Table 4).",
            "performance": "Diagonal (train/test same encoding) average ROUGE-Ls (from Table 4 averages): Task ID = 33.9; Definition only = 39.9; Pos(1) = 43.1; Def+Pos(1) = 44.5; Pos(2) = 45.0; Def+Pos(2) = 46.4; Def+Pos(2)+Neg(2) = 45.9; Def+Pos(2)+Neg(2)+Expl = 44.3; Pos(4) = 44.5; Def+Pos(4) = 46.0. (Column 'Average' in Table 4.)",
            "performance_comparison": "Definition+Pos(2) (46.4) vs Definition only (39.9): +6.5 ROUGE-L; Pos(2) (45.0) vs Pos(4) (44.5): negligible change; adding negative examples to Def+Pos(2) gives a small decrease (46.4 -&gt; 45.9); adding explanations reduces performance (46.4 -&gt; 44.3).",
            "format_effect_size": "+6.5 ROUGE-L improvement from adding 2 positive examples to a definition-only encoding; adding more positive examples (going from 2 to 4) yields negligible gains (~+0.6 at most in some cells); adding negative examples yields slight benefit or small decrease (~-0.5 average in this study); including explanations decreases performance (~-2.1 average).",
            "format_effect_direction": "mixed: combining definition with positive examples improved performance; more examples beyond 2 gave negligible gain; negative examples gave minor/no consistent benefit; explanations decreased performance.",
            "explanation_or_hypothesis": "Definitions provide an explicit mapping specification while examples provide concrete behavioral evidence; combining them is synergistic. Explanations can hurt when model capacity is insufficient (consistent with prior observations), possibly because explanations introduce additional, sometimes noisy, textual material that confuses the model or encourages overfitting to spurious patterns. Negative examples give limited signal compared to positive examples.",
            "counterexample_or_null_result": "Definition-only models cannot generalize to example-only test encodings; example-only models cannot generalize to definition-only test encodings. Models trained with both definitions and examples are robust across different test encodings (i.e., they generalize better to encoding variations).",
            "uuid": "e5839.1",
            "source_info": {
                "paper_title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Instances-per-task scaling",
            "name_full": "Effect of number of training instances per task on cross-task generalization",
            "brief_description": "Empirical study showing that increasing the number of instances per training task yields diminishing returns for cross-task generalization, with performance saturating at a small number (64) of instances per task.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T_k-InSTRUCT (T5-3B for analysis)",
            "model_size": "3B (analysis checkpoint)",
            "task_name": "SUP-NATURALINSTRUCTIONS English track (cross-task generalization)",
            "task_description": "Training models on varying numbers of instances per task while keeping task diversity fixed, then evaluating on unseen tasks.",
            "problem_format": "Same instruction encoding (definition + 2 positive examples by default); vary number of training instances per task (e.g., 8, 16, 32, 64, 256, ...).",
            "comparison_format": null,
            "performance": "Performance saturates when using only 64 instances per task; adding many more instances per task does not meaningfully improve ROUGE-L and may increase training time or risk overfitting (Fig. 5b). Exact ROUGE-L curve values are shown in Fig. 5(b) (saturation point ~64 instances).",
            "performance_comparison": null,
            "format_effect_size": "Saturation after ~64 instances per task (i.e., additional instances beyond 64 produce negligible gains).",
            "format_effect_direction": "no effect beyond a modest number of examples per task (saturation)",
            "explanation_or_hypothesis": "In the instruction-following, cross-task generalization setting, diversity of tasks matters more than per-task depth: once the model sees a modest number of exemplars per task it learns to map instructions to behavior, and extra per-task instances mainly cause overfitting to training tasks rather than better generalization.",
            "counterexample_or_null_result": null,
            "uuid": "e5839.2",
            "source_info": {
                "paper_title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Training-task diversity scaling",
            "name_full": "Effect of number of distinct training tasks on cross-task generalization",
            "brief_description": "Study that varies the number of distinct tasks used for meta-training and finds model performance increases approximately log-linearly with the number of observed tasks (task diversity).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T_k-InSTRUCT (T5-3B for analysis)",
            "model_size": "3B (analysis checkpoint)",
            "task_name": "SUP-NATURALINSTRUCTIONS English track",
            "task_description": "Generalization to unseen tasks after meta-training on varying counts of distinct training tasks sampled from the full training set.",
            "problem_format": "Default instruction encoding (definition + 2 positive examples); vary number of distinct tasks used in fine-tuning (e.g., 128, 256, 512, 757, ...).",
            "comparison_format": null,
            "performance": "Model generalization grows approx. log-linearly with number of observed tasks (Fig. 5a). Example mapping reported: a T5-large model trained with 757 tasks achieved ROUGE-L 48.0, comparable to a T5-3B model trained with 128 tasks (48.4), illustrating trade-off between task diversity and model size.",
            "performance_comparison": null,
            "format_effect_size": "Substantial: expanding the number of distinct training tasks yields steady performance gains; growth is roughly linear in the log of task count.",
            "format_effect_direction": "improved (with more tasks)",
            "explanation_or_hypothesis": "Exposure to diverse tasks teaches the model a wider variety of instruction-to-behavior mappings, improving zero/few-shot generalization to unseen tasks; thus diversity is a primary driver of instruction-following generalization.",
            "counterexample_or_null_result": null,
            "uuid": "e5839.3",
            "source_info": {
                "paper_title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Model size scaling",
            "name_full": "Effect of model parameter size on instruction-following generalization",
            "brief_description": "Analysis showing that increasing model size (T5 family) consistently improves cross-task generalization performance (log-linear trend with parameter count).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T_k-InSTRUCT (initialized from different T5 checkpoints: small, base, large, XL, XXL)",
            "model_size": "various: T5-small/base/large/XL/XXL (including 11B); specific experiments include 3B and 11B variants",
            "task_name": "SUP-NATURALINSTRUCTIONS English track",
            "task_description": "Cross-task generalization on held-out tasks after instruction-tuning; measure performance as model size varies.",
            "problem_format": "Default instruction encoding (definition + 2 positive examples).",
            "comparison_format": null,
            "performance": "Increasing model size yields consistent and substantial ROUGE-L gains (Fig. 5c). Example from the paper: T_k-InSTRUCT 11B (T5-11B) achieves much higher ROUGE-L than smaller checkpoints; T_k-InSTRUCT (11B) reached 62.0 ROUGE-L on English unseen tasks (Table 3).",
            "performance_comparison": null,
            "format_effect_size": "Performance grows roughly log-linearly with parameter size (no single scalar given for all steps; see Fig. 5c); concrete example: 11B T_k-InSTRUCT (62.0 ROUGE-L) substantially outperforms 3B variants.",
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Larger models better absorb diverse instruction mappings and patterns and therefore generalize more strongly to unseen tasks; model scaling and task diversity act as complementary levers.",
            "counterexample_or_null_result": "This contradicts the claim in Xu et al. (2022) that 'model size has little impact with extremely large number of tasks'; the paper's experiments show model size does matter.",
            "uuid": "e5839.4",
            "source_info": {
                "paper_title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Prompt style mismatch (PromptSource vs SUP-NATINST)",
            "name_full": "Effect of prompt phrasing/style differences across prompt collections (PromptSource vs SUP-NATURALINSTRUCTIONS) on model performance",
            "brief_description": "Observation that models trained on prompt collections with different prompt styles may not transfer well to a different instruction style; cited as a likely cause for T0's relatively weak performance on SUP-NATINST.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "T0 (and comparison to T5-LM)",
            "model_size": "11B (T0)",
            "task_name": "SUP-NATURALINSTRUCTIONS English evaluation (cross-task generalization)",
            "task_description": "Evaluation of T0 (trained on PromptSource prompts) on SUP-NATINST tasks whose instructions are longer and more detailed on average.",
            "problem_format": "Prompt style difference: PromptSource prompts are relatively concise (avg ~24.8 words) while SUP-NATINST definitions are longer (avg ~56.6 words); T0 was trained on PromptSource-style prompts.",
            "comparison_format": "Short PromptSource-style prompts (used to train T0) vs longer, more comprehensive SUP-NATINST definitions + examples.",
            "performance": "T0 (11B) achieved 32.3 ROUGE-L on SUP-NATINST English track versus T5-LM (11B) 30.2 and T_k-InSTRUCT 62.0; T0 only slightly better than T5-LM despite instruction-tuning on another prompt collection.",
            "performance_comparison": "T0's small gain over T5-LM suggests prompt/style mismatch harms transfer; the authors suspect differences in prompting style are responsible for T0 being only slightly better than T5-LM on SUP-NATINST.",
            "format_effect_size": "Small: T0 (32.3) vs T5-LM (30.2) = +2.1 ROUGE-L, which the authors attribute to style mismatch rather than instruction-tuning efficacy on SUP-NATINST.",
            "format_effect_direction": "reduced/limited transfer (style mismatch likely reduced effectiveness)",
            "explanation_or_hypothesis": "The style and phrasing of prompts used during instruction-tuning matters; models tuned on concise PromptSource prompts may not perform well when evaluated with longer, more descriptive task definitions, indicating sensitivity to prompt wording and style.",
            "counterexample_or_null_result": null,
            "uuid": "e5839.5",
            "source_info": {
                "paper_title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Explanations in prompts",
            "name_full": "Inclusion of natural-language explanations within instruction prompts",
            "brief_description": "Study of whether adding explanation text (short natural-language rationales following examples) to instruction encodings affects performance; found that explanations decreased performance for models that are not sufficiently large.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T_k-InSTRUCT (T5-3B for analysis)",
            "model_size": "3B (analysis checkpoint)",
            "task_name": "SUP-NATURALINSTRUCTIONS English evaluation",
            "task_description": "Evaluate whether adding explanatory text to examples in instruction encodings helps cross-task generalization.",
            "problem_format": "Def + Pos(2) + Neg(2) + Expl (i.e., including explanations associated with examples) versus the same encoding without explanations.",
            "comparison_format": "Same encoding with explanations vs without explanations.",
            "performance": "Average ROUGE-L with Def+Pos(2) = 46.4; with Def+Pos(2)+Neg(2)+Expl = 44.3 (Table 4 average column) — a decrease of ~2.1 ROUGE-L in the study's average.",
            "performance_comparison": "Def+Pos(2) (46.4) vs Def+Pos(2)+Neg(2)+Expl (44.3): -2.1 ROUGE-L when explanations included (for the models/scale tested).",
            "format_effect_size": "-2.1 ROUGE-L (average decrease in this ablation).",
            "format_effect_direction": "reduced",
            "explanation_or_hypothesis": "Explanations may introduce additional, potentially noisy text that confuses the model or encourage spurious correlations; prior work (Mishra et al., 2022b; Lampinen et al., 2022) observed similar effects when models are not large enough to benefit from explanation text.",
            "counterexample_or_null_result": "Authors note that future work should explore whether larger models can benefit from explanations; the negative effect is reported here for not-large-enough models.",
            "uuid": "e5839.6",
            "source_info": {
                "paper_title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Open-ended generation evaluation (vs option-ranking)",
            "name_full": "Choice of open-ended generation evaluation format instead of option-ranking or forced-choice",
            "brief_description": "Methodology decision: evaluate models via open-ended text generation given instructions rather than scoring/ranking candidate options; argued to be a more realistic measure of instruction-following generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "All evaluated models (T_k-InSTRUCT, InstructGPT, GPT-3, T5-LM, T0, etc.)",
            "model_size": null,
            "task_name": "SUP-NATURALINSTRUCTIONS cross-task generalization",
            "task_description": "Diverse tasks including classification and generation; paper adopts an open-ended generation set-up (no task-specific assumptions) and uses ROUGE-L as aggregated metric.",
            "problem_format": "Open-ended text generation where model outputs free-form text; contrasts with option-ranking evaluation used in other works for classification tasks.",
            "comparison_format": "Open-ended generation (this paper) vs option-ranking / candidate scoring (other works like Sanh et al. and Wei et al.).",
            "performance": "Reported ROUGE-L aggregated scores (Table 3) reflect models' performance under open-ended generation; authors argue ranking-based evaluations (option scoring) are not used here to avoid task-specific assumptions.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "different evaluation paradigm (not strictly 'improve' or 'reduce')",
            "explanation_or_hypothesis": "Open-ended generation better reflects realistic instruction-following as it does not rely on format-specific assumptions (e.g., candidate lists) and thus tests models' ability to produce correct outputs under natural instruction prompt formats; the choice of evaluation format can affect which methods appear stronger (models optimized for option-ranking might perform differently in open-ended settings).",
            "counterexample_or_null_result": null,
            "uuid": "e5839.7",
            "source_info": {
                "paper_title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks",
                "publication_date_yy_mm": "2022-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Cross-Task Generalization via Natural Language Crowdsourcing Instructions",
            "rating": 2
        },
        {
            "paper_title": "Multitask Prompted Training Enables Zero-Shot Task Generalization",
            "rating": 2
        },
        {
            "paper_title": "Finetuned Language Models are Zero-Shot Learners",
            "rating": 2
        },
        {
            "paper_title": "Training Language Models to Follow Instructions with Human Feedback",
            "rating": 2
        },
        {
            "paper_title": "Can Language Models Learn from Explanations in Context?",
            "rating": 2
        },
        {
            "paper_title": "PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts",
            "rating": 1
        },
        {
            "paper_title": "ZeroPrompt: Scaling Prompt-Based Pretraining to 1,000 Tasks Improves Zero-Shot Generalization",
            "rating": 1
        },
        {
            "paper_title": "MetaICL: Learning to learn in context",
            "rating": 1
        }
    ],
    "cost": 0.02161825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>SUPER-NATURALINSTRUCTIONS: Generalization via Declarative Instructions on 1600+ NLP Tasks</h1>
<p>Yizhong Wang ${ }^{2}$ Swaroop Mishra ${ }^{3}$ Pegah Alipoormolabashi ${ }^{1}$ Yeganeh Kordi ${ }^{5}$<br>Amirreza Mirzaei ${ }^{1}$ Anjana Arunkumar ${ }^{3}$ Arjun Ashok ${ }^{6}$ Arut Selvan Dhanasekaran ${ }^{5}$ Atharva Naik ${ }^{7}$ David Stap ${ }^{5}$ Eshaan Pathak ${ }^{9}$ Giannis Karamanolakis ${ }^{10}$ Haizhi Gary Lai ${ }^{11}$<br>Ishan Purohit ${ }^{12}$ Ishani Mondal ${ }^{13}$ Jacob Anderson ${ }^{3}$ Kirby Kuznia ${ }^{4}$ Krima Doshi ${ }^{3}$ Maitreya Patel ${ }^{3}$<br>Kuntal Kumar Pal ${ }^{3}$ Mehrad Moradshahi ${ }^{14}$ Mihir Parmar ${ }^{3}$ Mirali Purohit ${ }^{15}$ Neeraj Varshney ${ }^{3}$<br>Phani Rohitha Kaza ${ }^{3}$ Pulkit Verma ${ }^{3}$ Ravsehaj Singh Puri ${ }^{3}$ Rushang Karia ${ }^{5}$ Shailaja Keyur Sampat ${ }^{3}$<br>Savan Doshi ${ }^{3}$ Siddhartha Mishra ${ }^{16}$ Sujan Reddy ${ }^{17}$ Sumanta Patro ${ }^{18}$ Tanay Dixit ${ }^{19}$ Xudong Shen ${ }^{20}$<br>Chitta Baral ${ }^{3}$ Yejin Choi ${ }^{1,2}$ Noah A. Smith ${ }^{1,2}$ Hannaneh Hajishirzi ${ }^{1,2}$ Daniel Khashabi ${ }^{21}$<br>${ }^{1}$ Allen Institute for AI ${ }^{2}$ Univ. of Washington ${ }^{3}$ Arizona State Univ. ${ }^{4}$ Sharif Univ. of Tech. ${ }^{5}$ Tehran Polytechnic ${ }^{6}$ PSG College of Tech. ${ }^{7}$ IIT Kharagpur<br>${ }^{8}$ Univ. of Amsterdam ${ }^{9}$ UC Berkeley ${ }^{10}$ Columbia Univ. ${ }^{11}$ Factored AI ${ }^{12}$ Govt. Polytechnic Rajkot ${ }^{13}$ Microsoft Research ${ }^{14}$ Stanford Univ. ${ }^{15}$ Zyzus Infotech<br>${ }^{16}$ Univ. of Massachusetts Amherst ${ }^{17}$ National Inst. of Tech. Karnataka ${ }^{18}$ TCS Research ${ }^{19}$ IIT Madras ${ }^{20}$ National Univ. of Singapore ${ }^{21}$ Johns Hopkins Univ.</p>
<h2>Abstract</h2>
<p>How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce SUPER-NATURALINSTRUCTIONS, ${ }^{1}$ a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructionstraining models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones.
Furthermore, we build T $k$-InSTRUCT, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or $k$-shot examples). Our experiments show that T $k$-InSTRUCT outperforms existing instruction-following models such as InstructGPT by over $9 \%$ on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models. ${ }^{2}$</p>
<h2>1 Introduction</h2>
<p>The NLP community has witnessed great progress in building models for generalization to unseen tasks via in-context instructions (Mishra et al.,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An example task from SuP-NATInST adopted from Chawla et al. (2021). A successful model is expected to use the provided instructions (including task definition and demonstration examples) to output responses to a pool of evaluation instances.</p>
<p>2022b; Sanh et al., 2022; Wei et al., 2022) using large pretrained language models (Raffel et al., 2020; Brown et al., 2020). As remarkable as models like InstructGPT (Ouyang et al., 2022) are, the contribution of various design choices to their success is opaque. In particular, the role of supervised data has remained understudied due to limited data released by the corporate entities behind major models. In addition, it is nearly impossible for the research community to extend and re-train these gigantic models. Addressing these two chal-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Resource $\rightarrow$</th>
<th style="text-align: center;">SUP-NATINST <br> (this work)</th>
<th style="text-align: center;">NATINST <br> (Mishra et al., 2022b)</th>
<th style="text-align: center;">CROSSFIT <br> (Ye et al., 2021)</th>
<th style="text-align: center;">PROMPTSOURCE <br> (Bach et al., 2022)</th>
<th style="text-align: center;">FLAN <br> (Wei et al., 2022)</th>
<th style="text-align: center;">INSTRUCTGPT <br> (Ouyang et al., 2022)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Has task instructions?</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Has negative examples?</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Has non-English tasks?</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Is public?</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Number of tasks</td>
<td style="text-align: center;">1616</td>
<td style="text-align: center;">61</td>
<td style="text-align: center;">269</td>
<td style="text-align: center;">176</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Number of instructions</td>
<td style="text-align: center;">1616</td>
<td style="text-align: center;">61</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">2052</td>
<td style="text-align: center;">620</td>
<td style="text-align: center;">14378</td>
</tr>
<tr>
<td style="text-align: left;">Number of annotated tasks types</td>
<td style="text-align: center;">76</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">$13^{\times}$</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: left;">Avg. task definition length (words)</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">134.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">8.2</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 1: A comparison of SUP-NATINST to a few notable datasets in the field. We obtain the number of tasks, instructions, and task types of other datasets from their original paper. "-" indicates the fields are not applicable or unknown. Standards for categorizing task types vary across different datasets (see Fig. 2). *PROMPTSOURCE does not provide task type annotation for all their tasks, for which we report only the 13 task types annotated for training T0 (Sanh et al., 2022) instead.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Compared to other datasets, SUP-NATINST covers a more diverse range of task types. InstructGPT reports a very coarse categorization of their task types. Bubble size represents the number of tasks of each type in log scale.
lenges necessitates the availability of large-scale public benchmarks of a broad range of NLP tasks and their instructions to facilitate developing and evaluating models that can generalize to unseen tasks.</p>
<p>In this paper, we construct a meta-dataset (i.e., dataset of datasets; Triantafillou et al., 2019) that consists of a wide variety of NLP tasks with their instructions, and train a model that can perform a new task given the instruction, outperforming InstructGPT (which uses $16 \times$ more parameters).</p>
<p>Our dataset, SUPER-NATURALINSTRUCTIONS (SUP-NATINST for short), is a large benchmark of 1,616 NLP tasks and their natural language instructions. It brings in a diverse variety of tasks-76 broad task types spanning 55 different languages. Each task is paired up with an instruction that consists of the task definition for mapping an input text to a task output and several examples for demon-
strating the desired or undesired output (see Fig. 1 as an example task). These tasks and their instructions are contributed by 88 NLP practitioners, in response to our public call. These contributions are consolidated after several rounds of peer-review and crowdsourced feedback to ensure quality. Having this diverse and large-scale data enables us to carefully split the tasks into training and test sets and systematically study how state-of-the-art methods perform on them. Table 1 and Figure 2 highlight properties of SUP-NATINST compared to relevant benchmarks, emphasizing the diversity of tasks and instruction types in our benchmark.</p>
<p>Our model, T $k$-InSTRUCT, is a generative model for transforming task inputs given declarative in-context instructions (task definition or $k$ shot examples). It is built by multi-task training of the T5 model (Raffel et al., 2020) over all the task instructions in our training set, and is eval-</p>
<p>uated on unseen tasks in the test set. Interestingly, an 11B-parameter T $k$-InSTRUCT can outperform the 175B-parameter InstructGPT model by 9.9 ROUGE-L points when evaluated on 119 unseen English tasks, and the multilingual variant $\mathrm{mT} k$-InSTRUCT outperforms InstructGPT by 13.3 points on 35 non-English tasks (§6.1). According to human evaluation, $\mathrm{T} k$-InSTRUCT generates responses at least as well as the ground truth for $77 \%$ of the testing instances (§6.2), confirming its strong generalization to unseen tasks.</p>
<p>The compelling empirical performance of $\mathrm{T} k$ InSTRUCT confirms the importance of super-sized meta datasets such as our Sup-NatInSt to facilitate research towards generalizable NLP models. We conduct extensive analysis to understand the important factors for this generalization (§7). Our analysis shows that scaling up the diversity of training tasks and the model size are both important for strong generalization to unseen tasks. Finally, we estimate performance upper bounds, suggesting further room for improvement.</p>
<h2>2 Related Work</h2>
<p>Language instructions are a versatile way of defining goals, which is why they have been studied in the context of a variety of applications, such as instructions in grounded environments (Shridhar et al., 2020; Stepputtis et al., 2020; Min et al., 2022b; Weir et al., 2022) and database commands (Kim et al., 2020). Here, we focus on applications of instructions for general NLP tasks.</p>
<p>Recent literature has been motivated by building models that are generalizable across a variety of NLP tasks, when prompted with either a few examples (Ye and Ren, 2021; Bragg et al., 2021) or language definitions (Efrat and Levy, 2020; Weller et al., 2020; Zhong et al., 2021; Mishra et al., 2022b,a; Parmar et al., 2022). Our work is related to the existing benchmarks in this line of work, as delineated in Table 1 along various dimensions. Our benchmark extends NatInSt (Mishra et al., 2022b) with $26 \times$ more tasks and greater variety of task types (Fig. 2). While CrossFit (Ye et al., 2021) focuses on benchmarking with a few in-context examples, our benchmark also offers task instructions.</p>
<p>Concurrent to our work, PromptSource (Bach et al., 2022) is another benchmark of tasks and their language instructions (prompts). An important distinction between this benchmark and
ours is the phrasing of the task definitions: while PromptSource task definitions are relatively concise, our task definitions are collected with the intention of providing a complete definition of each task and therefore are longer ( 24 tokens vs. 56 tokens on average; Table 1). More recently, BIGBENCH (Srivastava et al., 2022) introduces a collection of 204 tasks and also provides short task descriptions and input prefixes that can be used for prompting LMs. With little overlap to our collection of tasks, they focus more on finding challenging tasks that can be used to test different behaviors of current LMs. Nevertheless, we believe that all these efforts in collecting different tasks as well as the task instructions are complementary, and the community will benefit from considering different benchmarks. Finally, the well-adopted InstructGPT model (Ouyang et al., 2022) is partially enabled by a large dataset of prompts that are collected via various synthetic data augmentation which, unfortunately, is not publicly available.</p>
<p>Beyond cross-task generalization, our benchmark can also be used to study multi-task learning more broadly, which is a longstanding goal for AI (Caruana, 1997). Traditionally, this literature focuses on setups that involve evaluation on tasks that are observed during training (Collobert and Weston, 2008; Hashimoto et al., 2017). More recent studies show promise that large-scale multi-task learning can enable strong generalization to similar tasks via unified encoding (Khashabi et al., 2020; Xie et al., 2022) or better finetuning results on downstream tasks (McCann et al., 2018; Aribandi et al., 2022). Our proposed benchmark provides diverse tasks for studying multi-tasking at a massive scale.</p>
<h2>3 SUPER-NATURALINSTRUCTIONS</h2>
<p>Super-NaturalInstructions is a metadataset (Triantafillou et al., 2019) consisting of a variety of NLP tasks (see Fig. 2a) and instructions that describe them in plain language.
Instruction schema. All task instructions follow the same uniform schema (see Fig. 1) which is composed of the following parts:</p>
<ul>
<li>Definition defines a given task in natural language. This is a complete definition of how an input text (e.g., a sentence or a document) is expected to be mapped to an output text.</li>
<li>
<p>Positive Examples are samples of inputs and their correct outputs, along with a short explanation for each.</p>
</li>
<li>
<p>Negative Examples are samples of inputs and their incorrect/invalid outputs, along with a short explanation for each.
The above schema is based on that of Mishra et al. (2022b), though it is simplified. See Appendix C for the comparison.
Task instances. Given the instructions for each task, a model is expected to solve instances of that task. We use a unified format to organize the instances of all our tasks. More precisely, each instance consists of a textual input and a list of acceptable textual outputs. We limit the number of instances in each task to 6.5 K to avoid an imbalance of instances between tasks.
Benchmark collection. The benchmark was collected through a large community effort on GitHub. ${ }^{3}$ Tasks were collected and contributed by NLP practitioners who were either responding to our public invitation ${ }^{4}$ or students who were encouraged to contribute as part of their class project. ${ }^{5}$ Contributors were encouraged to be creative and source the tasks from several resources: (a) existing public NLP datasets, (b) available intermediate annotations in crowdsourcing experiments (e.g., paraphrasing questions or rating their quality during crowdsourcing a QA dataset), or (c) synthetic tasks that can be communicated to an average human in a few sentences (e.g., basic algebraic operations like number comparison, finding the longest palindrome substring, etc.). When using existing datasets or crowdsourcing annotations, contributors were encouraged to adopt the instructions used to create this dataset whenever available. This was done to ensure that the instructions were sufficient to define the tasks to average human readers. Tasks along with instructions and other meta information were contributed as JSON files via GitHub pull requests, which were reviewed by automated checks and peers. We had 88 contributors from diverse locations and backgrounds contribute to our repository.
Quality control. Controlling the quality of this community-contributed data was done in several phases: (1) Upon creating a GitHub pull request of the proposed task, it immediately went through an automatic test. This process verified that the introduced file contained the expected fields and adhered to our desired properties (e.g., no duplicate</p>
</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>instances, the output labels are not heavily imbalanced, etc.) and (2) The proposed task was then peer-reviewed by 1-2 other expert contributors to ensure the clarity and sufficiency of instruction content. The review process was done iteratively until the reviewers were content with the quality of the proposed instruction. Specifically, reviewers were asked to verify that the instruction is clear and sufficient for an average language speaker to solve the underlying task (evaluation instances) while being grammatical, fluent, and concise. On average, the review of each GitHub pull request took about 46 iterations over the span of multiple days before being merged. (3) Lastly, the added tasks were presented to crowdworkers in order to collect feedback on the quality of the provided instructions, such as typos, clarity, or other issues (details in §A). Subsequently, one of the authors used this feedback to improve the task definitions of the instances. This feedback was done only for English tasks, as finding high-quality crowdworkers in other languages is nontrivial (Pavlick et al., 2014).</p>
<p>Diversity of tasks. Collecting tasks for SuPNatInST was carefully supervised to cover a wide variety of natural language understanding tasks, domains, and languages. To better understand this diversity, we comprehensively categorize tasks along three different dimensions:</p>
<ul>
<li>TASK TYPE defines the nature of the mapping from instance inputs to outputs (e.g., question answering, classification, etc.).</li>
<li>LANGUAGE indicates the language(s) of the instances.</li>
<li>Domain indicates the domain(s) to which the text of the tasks belong to (e.g., politics, medicine, dialogue, etc.).</li>
</ul>
<p>These different measures of categorization can be used to study different senses of generalization. In our empirical studies (§5), we study generalization along the axis of task types. We refer the reader to Fig. 10 in the appendix for the distribution of tasks among different task types, languages, and domains.</p>
<p>Statistics. Table 2 shows various statistics for the benchmark. In total, the dataset includes 1616 tasks and 5 M instances. On average, each instruction is paired with 2.8 positive and 2.4 negative examples. The average definition length is 56.6 in words.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">statistic</th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"># of tasks</td>
<td style="text-align: right;">1616</td>
</tr>
<tr>
<td style="text-align: left;"># of task types</td>
<td style="text-align: right;">76</td>
</tr>
<tr>
<td style="text-align: left;"># of languages</td>
<td style="text-align: right;">55</td>
</tr>
<tr>
<td style="text-align: left;"># of domains</td>
<td style="text-align: right;">33</td>
</tr>
<tr>
<td style="text-align: left;"># of non-English tasks</td>
<td style="text-align: right;">576</td>
</tr>
<tr>
<td style="text-align: left;">avg. definition length (words per task)</td>
<td style="text-align: right;">56.6</td>
</tr>
<tr>
<td style="text-align: left;">avg. # of positive examples (per task)</td>
<td style="text-align: right;">2.8</td>
</tr>
<tr>
<td style="text-align: left;">avg. # of negative examples (per task)</td>
<td style="text-align: right;">2.4</td>
</tr>
<tr>
<td style="text-align: left;">avg. # of instances (per task)</td>
<td style="text-align: right;">3106.0</td>
</tr>
</tbody>
</table>
<p>Table 2: Statistics of Sup-NatInST.</p>
<h2>4 T $k$-InStrUCT: Learning to Follow Instructions at Scale</h2>
<p>Defining Generalization to Unseen Tasks. Each task $t$ is defined via its natural language instruction $I_{t}$, and each task has a set of input/output instances $\left(X_{t}, Y_{t}\right)$. A model $M$ is expected to produce the output $y$, given the input $x$ and the task instruction $I_{t}: M\left(I_{t}, x\right)=y$, for $(x, y) \in\left(X_{t}, Y_{t}\right)$. In particular, we would like to evaluate model $M$ on tasks that are not observed (i.e., their instances were not used for training $M$ ). The only source of signal for learning the task at inference time is in-context instructions $I_{t}$ that contain a definition and demonstration examples of the task.
T $k$-InStruCt. We introduce T $k$-InStruCt, a model that is meta-trained on Sup-NatInST for solving tasks given their in-context instructions. Previous work has shown the effectiveness of such meta-training in improving model's ability to do incontext learning with either prompts (Zhong et al., 2021; Sanh et al., 2022) or demonstration examples (Min et al., 2022a). Because of the large variety of tasks in Sup-NatInST, we are able to do this multi-task meta-training at a larger scale than before. We conduct our experiments and analysis based on the T5 model (Raffel et al., 2020). Since each instruction $I_{t}$ consists of multiple elements as described in our instruction schema (§3), we map these elements to textual format and append them before the input instance. Fig. 8 in the appendix shows how we encode the full instructions. We study different combinations of these instruction elements in $\S 7.2$. By default, we will use our most effective instruction elements (i.e., task definition and two positive examples) unless otherwise specified. In the same manner, we train the multilingual variant $\mathrm{mT} k$-InStruCt based on the mT5 model (Xue et al., 2021).</p>
<h2>5 Benchmarking Cross-Task Generalization with SuP-NatInST</h2>
<p>Here we provide our recommended recipe for benchmarking generalization via SuP-NatInST.</p>
<h3>5.1 Evaluation Setup</h3>
<p>An Evaluation Split of Unseen Tasks. We split the large collection of tasks in SuP-NatInST into two subsets: one for evaluation and the other for supervision. For evaluation tasks, we fix a manuallyselected collection of 12 categories that represent 154 tasks. The large variety of tasks in SuPNatInST enables us to choose a diverse set of tasks for evaluation - such as those at word, sentence, and document levels, covering both classification and generation formats. Appendix G lists our evaluation tasks with examples for representative tasks. For an efficient evaluation, we sample a maximum of 100 instances for each task, which results in 15,310 testing instances in total. The remaining tasks are used for training models. ${ }^{6}$
Divided Tracks for English and X-lignual Tasks. Sup-NatInST consists of tasks across multiple languages, which enables evaluating the model's generalization to unseen tasks not only in English but also in other languages. Therefore, we divide our evaluation tasks into two tracks: one for English cross-task generalization (119 tasks) and the other for cross-lingual cross-task generalization ( 35 tasks). To the best of our knowledge, this is the first study in cross-lingual cross-task generalization (i.e., generalization to unseen tasks in different languages). Fig. 11 and Fig. 12 in the appendix contain the evaluation tasks for each track.
Evaluation Metrics. Due to the diversity of our tasks and the open-ended generation nature of our formulation, ${ }^{7}$ we adopt ROUGE-L (Lin, 2004) for reporting aggregated performance results. This is a soft string overlap metric that can be applied to a wide range of text generation tasks. We show that the ranking from this metric correlates well with accuracy for classification tasks in Appendix E. We also conduct a human evaluation in $\S 6.2$.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>5.2 Baselines and Existing Models</h1>
<p>Here we discuss a variety of baselines and competitive models for our target application. See Appendix D for implementation details.</p>
<p>Heuristic baselines. We first evaluate the following heuristics to evaluate the possible shortcuts in the data. Copying Demo Output copies the output of a random demonstration example. Since we balance the labels for our test tasks, the performance of this baseline will roughly equal a random guess or a majority baseline for classification tasks. Copying Instance Input copies the given instance input. This strategy performs well on tasks where the target output largely overlaps with the input (e.g., question rewriting, grammar error correction).</p>
<p>Off-the-shelf pretrained language models. We evaluate existing LMs that are not fine-tuned with instruction-specific data. Specifically, we evaluate the 11B-parameter T5 (Raffel et al., 2020) as a direct counterpart of T $k$-InSTRUCT. Due to the infilling pretraining objective of the original T5 model, it cannot continue text well. Therefore, we evaluate its "LM-adapted" version, which is further trained with a language modeling objective (Lester et al., 2021). Additionally, we evaluate GPT-3 (Brown et al., 2020), a 175B-parameter autoregressive LM that has shown remarkable ability in following demonstrations provided in its prompt.</p>
<p>Instruction-tuned models. In addition to our T $k$ InSTRUCT (§4), we evaluate existing models that are fine-tuned to follow language instructions. In particular, we evaluate InstructGPT (Ouyang et al., 2022) which uses reinforcement learning to incorporate human preferences into a GPT-3 pretrained model, and T0 (Sanh et al., 2022) which finetunes T5 on a collection of task prompts in PromptSource (Bach et al., 2022).</p>
<p>Upper bound estimates. We estimate an upper bound on models' generalization to unseen tasks by fine-tuning an oracle model on the tasks' labeled instances. Since this model observes the hidden instances of the evaluation tasks, it is, by definition, an estimated upper bound to our generalizationbased models. Specifically, we fine-tune a T5-11B model on the 119 English evaluation tasks, and a mT5-13B model on the 35 non-English tasks, with 1 K random training instances per task, without overlap with the evaluation instances.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: left;">Methods $\downarrow /$ Evaluation $\rightarrow$</th>
<th style="text-align: center;">En</th>
<th style="text-align: center;">X-lingual</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Heuristic</td>
<td style="text-align: left;">Copying Instance Input</td>
<td style="text-align: center;">14.2</td>
<td style="text-align: center;">5.4</td>
</tr>
<tr>
<td style="text-align: center;">Baselines</td>
<td style="text-align: left;">Copying Demo Output</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">50.3</td>
</tr>
<tr>
<td style="text-align: center;">Pretrained LMs</td>
<td style="text-align: left;">T5-LM (11B)</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">GPT3 (175B)</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">51.3</td>
</tr>
<tr>
<td style="text-align: center;">Instruction-tuned</td>
<td style="text-align: left;">T0 (11B)</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">InstructGPT (175B)</td>
<td style="text-align: center;">52.1</td>
<td style="text-align: center;">52.8</td>
</tr>
<tr>
<td style="text-align: center;">Models</td>
<td style="text-align: left;">T $k$-InSTRUCT (ours, 11B)</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: left;">mT $k$-InSTRUCT (ours, 13B)</td>
<td style="text-align: center;">57.1</td>
<td style="text-align: center;">66.1</td>
</tr>
<tr>
<td style="text-align: center;">Upper-bound (est.)</td>
<td style="text-align: left;">Supervised Training</td>
<td style="text-align: center;">74.3</td>
<td style="text-align: center;">94.0</td>
</tr>
</tbody>
</table>
<p>Table 3: The overall performance of different methods on unseen tasks in the test set of Sup-NATInST (§6.1). We report ROUGE-L here as our aggregated metric. Models that leverage instructions show stronger generalization to unseen tasks. In particular, our model that is fine-tuned on a diverse set of tasks outperforms InstructGPT and T0 by a large margin.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Human evaluation vs. ROUGE-L for several methods (§6.2). The trends of these two metrics are highly correlated with a Pearson coefficient of 0.998 .</p>
<h2>6 Experimental Results</h2>
<h3>6.1 Overall Results</h3>
<p>Table 3 summarizes our overall benchmarking results. We use the same input encoding that contains the most effective instructional elements (task definition and two positive examples without the negative examples and explanations) for all the methods. To better understand models' generalization to different tasks, we also break down the performance according to the task categories in Fig. 4. We refer the reader to Appendix H for more detailed analysis on each individual task.
Instruction-tuning enables stronger generalization to unseen tasks. Generally instruction-tuned models perform better compared to their untuned LM counterparts (T $k$-InSTRUCT vs. T5-LM, InstructGPT vs. GPT-3) and heuristic baselines. This indicates models do learn to follow instructions by finetuning on instruction data, and this can generalize to new instructions for unseen tasks. T0 is an exception, which is only slightly better than</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Performance per evaluation task type. T $k$-InSTRUCT consistently performs better than other generalizationbased methods on all task types, while there is still a sizable gap compared to supervised training.</p>
<p>T5-LM. We suspect this is because the style of prompting in T0's training data is very different from our style of instructions.</p>
<p>Our T $k$-InStruCt outperforms InstructGPT. Our T $k$-InStruCt and mT $k$-InStruCt models, which are trained with a variety of tasks, generalize best to unseen tasks for both English and non-English tasks in all evaluation task categories. InstructGPT also shows a great extent of generalization to our evaluation tasks. However, we want to note it is not clear if InstructGPT's training data overlaps with our evaluation tasks since their data is unavailable.</p>
<p>There is a sizable gap for improvement. Despite the impressive performance of current models, there is a sizable gap between the generalization of instruction-based models and the supervised training approach, leaving more room for improvement.</p>
<h3>6.2 Human Evaluation</h3>
<p>For language generation tasks, automatic metrics are only an approximation of human judgments; we conduct a human evaluation to confirm the findings so far. Specifically, we ask crowdworkers to indicate if they prefer the predicted answer by the model or the ground truth outputs for each instance with ties being allowed (see Appendix B for details). The resulting human evaluation metric indicates how often model predictions were rated as at least as good as our ground truth labels. The theoretical upper bound of this metric is $100 \%$ when the model is rated at least as good as the ground truth for all the instances. The results of human evaluation (shown in Fig. 3) align quite well with our automatic metrics and confirm the human-perceived quality of our models.</p>
<h2>7 Further Analysis</h2>
<p>We conduct further analysis to understand the important factors for models to generalize across tasks. Due to the computational cost, this analysis is done on the English track and using the T5-3B checkpoint, except for the experiments on model sizes.</p>
<h3>7.1 Scaling Trends of Generalization</h3>
<p>We study T $k$-InSTRUCT's generalization performance with respect to three scaling factors: the number of training tasks, the number of instances per task, and the model sizes. Fig. 5 presents the performance change by scaling each of them.
More observed tasks improve the generalization. We fine-tune T $k$-InSTRUCT with different numbers of tasks that are randomly sampled from the whole training set (Fig. 5a). The model generalization performance grows log-linearly ${ }^{8}$ as we increase the set of tasks used for training. Previous work (Mishra et al., 2022b; Sanh et al., 2022; Wei et al., 2022) has made similar observations on a much smaller scale, while we show that this trend holds even with 757 diverse training tasks.
A large number of training instances do not help generalization. We then vary the number of instances per task that are used for finetuning (Fig. 5b). While the conventional wisdom in supervised learning is that more training instances usually helps (Banko and Brill, 2001; Sun et al., 2017; Hestness et al., 2017), in our setup, the model's performance saturates when only 64 instances per task are used for training. A large number of training instances would instead lead to longer training time and risk overfitting to the training tasks.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Scaling trends of models performance (\$7.1) as a function of (a) the number of training tasks; (b) the number of instances per training task; (c) model sizes. $x$-axes are in log scale. The linear growth of model performance with exponential increase in observed tasks and model size is a promising trend. Evidently, the performance gain from more instances is limited.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Testing Encoding $\rightarrow$ <br> Training Encoding $\downarrow$</th>
<th style="text-align: center;">Task ID</th>
<th style="text-align: center;">Def</th>
<th style="text-align: center;">Pos (1)</th>
<th style="text-align: center;">Def <br> + Pos (1)</th>
<th style="text-align: center;">Pos (2)</th>
<th style="text-align: center;">Def <br> + Pos (2)</th>
<th style="text-align: center;">Def <br> + Pos (2) <br> + Neg (2)</th>
<th style="text-align: center;">Def <br> + Pos (2) <br> + Neg (2) <br> + Expl</th>
<th style="text-align: center;">Pos (4)</th>
<th style="text-align: center;">Def <br> + Pos (4)</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Task ID</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">30.9</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">33.7</td>
<td style="text-align: center;">33.9</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">33.9</td>
</tr>
<tr>
<td style="text-align: center;">Def</td>
<td style="text-align: center;">17.3</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">31.1</td>
<td style="text-align: center;">43.8</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">46.4</td>
<td style="text-align: center;">44.2</td>
<td style="text-align: center;">44.3</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;">39.9</td>
</tr>
<tr>
<td style="text-align: center;">Pos (1)</td>
<td style="text-align: center;">10.9</td>
<td style="text-align: center;">32.1</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">46.6</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">46.2</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">46.6</td>
<td style="text-align: center;">49.5</td>
<td style="text-align: center;">43.1</td>
</tr>
<tr>
<td style="text-align: center;">Def + Pos (1)</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;">42.2</td>
<td style="text-align: center;">43.8</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">47.4</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">53.1</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">53.7</td>
<td style="text-align: center;">44.5</td>
</tr>
<tr>
<td style="text-align: center;">Pos (2)</td>
<td style="text-align: center;">12.7</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">47.1</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">49.3</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">49.8</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">45.0</td>
</tr>
<tr>
<td style="text-align: center;">Def + Pos (2)</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">42.1</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">54.2</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">52.7</td>
<td style="text-align: center;">50.3</td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">46.4</td>
</tr>
<tr>
<td style="text-align: center;">Def + Pos (2) + Neg (2)</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">42.3</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">48.6</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">54.3</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">53.8</td>
<td style="text-align: center;">45.9</td>
</tr>
<tr>
<td style="text-align: center;">Def + Pos (2) + Neg (2) + Expl</td>
<td style="text-align: center;">15.4</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">43.8</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">47.6</td>
<td style="text-align: center;">51.9</td>
<td style="text-align: center;">52.5</td>
<td style="text-align: center;">52.6</td>
<td style="text-align: center;">48.6</td>
<td style="text-align: center;">52.2</td>
<td style="text-align: center;">44.3</td>
</tr>
<tr>
<td style="text-align: center;">Pos (4)</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">23.9</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">49.8</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">49.5</td>
<td style="text-align: center;">47.5</td>
<td style="text-align: center;">49.8</td>
<td style="text-align: center;">51.3</td>
<td style="text-align: center;">44.5</td>
</tr>
<tr>
<td style="text-align: center;">Definition + Pos (4)</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">44.3</td>
<td style="text-align: center;">51.9</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">53.7</td>
<td style="text-align: center;">53.4</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">46.0</td>
</tr>
</tbody>
</table>
<p>Table 4: Performance (ROUGE-L) of models trained and evaluated with various encodings. Diagonal numbers (underlined) represent performances of models trained and evaluated with the same instruction encoding. Each encoding is a combination of the elements in the instructions (Fig. 1). Task ID is a short string composed of dataset name and task category; Def represents the task definition; Pos (k) represents $k$ positive examples; Neg (k) represents $k$ negative examples; Expl represents explanation. These results (a) show the gains from various instructional elements, and (b) indicate surprising reliability of the models to various input encoding. A model trained with definition and positive examples (i.e., the last row) remains robust for different encodings.</p>
<p>Tuning larger models with instructions consistently lead to gains. We study the effect of model scaling by initializing T $k$-InSTRUCT from different sizes of pretrained T5 checkpoints, including the small, base, large, xl and xxl sizes (Fig. 5c). We found that increasing the model sizes consistently bring significant improvement (log-linearly with parameter size). This finding contradicts the claim in Xu et al. (2022) that "model size has little impact on performance with an extremely large number of tasks." Combining Fig. 5(a) and Fig. 5(c), one can create a correspondence between model size and task size. For example, a T5-large model trained with 757 tasks can achieve comparable performance (48.0 ROUGE-L) to the T5-3B model trained with 128 tasks (48.4 ROUGE-L), indicating that increasing the diversity of training tasks is an alternative to scaling model sizes.</p>
<h3>7.2 Instructing with Different Elements</h3>
<p>We evaluate the performance of $\mathrm{T} k$-InStrUCT under different instructional elements.
Benefit of different instructional elements. As shown in Fig. 1, SuP-NatInST provides multiple elements for instructing a task. We train multiple models with different combinations of these elements. The diagonal cells of Table 4 show the performance of our models when trained and evaluated on a particular instruction encoding. Based on the diagonal numbers, including the task definition consistently helps the model to generalize better. Moreover, combining the task definition with positive demonstration examples yields further improvement. However, adding more demonstration examples is negligible. Negative examples help a little bit; explanations decrease performance, which is consistent with the observations of Mishra et al. (2022b) and Lampinen et al. (2022) when</p>
<p>the model is not large enough. Future work can explore whether more powerful models can benefit from these elements.</p>
<p>Generalization to different input encodings. We further investigate whether a model trained on a particular encoding can generalize to other encodings. This can be read from the non-diagonal cells of Table 4. The negative result here is that definitiononly models cannot generalize to example-only test encodings; and similarly, example-only models cannot generalize to definition-only test encodings. However, models trained on encodings that contain both definition and examples are surprisingly robust across different encoding variations.</p>
<h2>8 Conclusion</h2>
<p>We construct a large-scale benchmark consisting of a diverse set of NLP tasks and their instructions. This benchmark can serve as a rich playground for training or evaluation of models that can generalize to unseen tasks by following instructions. Furthermore, we train T $k$-InSTRUCT using this data, and demonstrate its capability to perform unseen tasks to a surprising extent. We provide extensive analysis to understand the important factors for such generalization. We hope our data and model will facilitate future work towards more general-purpose models.</p>
<h2>9 Limitations</h2>
<p>While the presented data offers a notable variety (e.g., diverse task types), its underlying distributions suffer from skews which should be addressed in future work (see Appendix F). On language diversity, the proposed benchmark is biased toward English. On output diversity, the collected tasks are generally still skewed to short responses, which might reflect the distribution of the available tasks in the field. This under-representation of the longtail of tasks poses a challenge for building generalpurpose models in the future. We hope future work addresses such distributional imbalances. Moreover, we see natural extensions of the instructionfollowing setup here in the context of other modalities such as vision or speech.</p>
<p>Automatic evaluation of models' performance is another challenge, considering the diverse set of tasks in our benchmark, and many of them being open-ended generation tasks. We use ROUGE-L as an aggregated metric in this paper and find it as a good proxy for the overall performance of the mod-
els, aligning well with human evaluation. However, there are specific tasks for which ROUGE-L might not serve as an effective proxy of quality (such as rewriting tasks or error correction tasks where copying the input can result in a high ROUGE-L score). We hope these issues will be addressed with the development of more powerful evaluation metrics for text generation.</p>
<p>In terms of computing power, we have experimented with models that were accessible to us and have made the resulting models publicly available. We also acknowledge that there are larger models that we were not able to train due to the limitations of our computational budget.</p>
<h2>Acknowledgments</h2>
<p>We thank the anonymous reviewers, our colleagues from AI2 and UWNLP, especially Matthew Peters for his encouraging conversations that motivated this project. We also thank the student contributors of Arizona State University's CSE 576 "Topics in NLP" course and all other contributors to our data repository. All experiments were run on AI2's Beaker GPU clusters and Google's research TPUs. This work was supported in part by ONR MURI N00014-18-1-2670, ONR N00014-18-1-2826, and DARPA MCS N66001-19-2-4031 grants.</p>
<h2>References</h2>
<p>Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei Zhuang, Vinh Q Tran, Dara Bahri, Jianmo Ni, et al. 2022. ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning. In International Conference on Learning Representations (ICLR).</p>
<p>Stephen H Bach, Victor Sanh, Zheng-Xin Yong, Albert Webson, Colin Raffel, Nihal V Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, et al. 2022. PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts. In Annual Meeting of the Association for Computational Linguistics (ACL) System Demonstrations.</p>
<p>Michele Banko and Eric Brill. 2001. Scaling to Very Very Large Corpora for Natural Language Disambiguation. In Annual Meeting of the Association for Computational Linguistics (ACL).</p>
<p>Max Bartolo, Alastair Roberts, Johannes Welbl, Sebastian Riedel, and Pontus Stenetorp. 2020. Beat the ai: Investigating adversarial human annotation for reading comprehension. Transactions of the Association for Computational Linguistics (TACL), 8:662-678.</p>
<p>Mohaddeseh Bastan, Mahnaz Koupaee, Youngseo Son, Richard Sicoli, and Niranjan Balasubramanian. 2020. Author's sentiment prediction. In International Conference on Computational Linguistics (COLING).</p>
<p>Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. 2008. The sixth pascal recognizing textual entailment challenge. In Text Analysis Conference (TAC).</p>
<p>Jonathan Bragg, Arman Cohan, Kyle Lo, and Iz Beltagy. 2021. Flex: Unifying evaluation for few-shot nlp. In Advances in Neural Information Processing Systems (NeurIPS).</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, and et al. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems (NeurIPS).</p>
<p>Rich Caruana. 1997. Multitask learning. Machine learning, 28(1):41-75.</p>
<p>Kushal Chawla, Jaysa Ramirez, Rene Clever, Gale Lucas, Jonathan May, and Jonathan Gratch. 2021. CaSiNo: A Corpus of Campsite Negotiation Dialogues for Automatic Negotiation Systems. In Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</p>
<p>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In International Conference on Machine Learning (ICML).</p>
<p>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. In Machine Learning Challenges Workshop.</p>
<p>Avia Efrat and Omer Levy. 2020. The Turking Test: Can Language Models Understand Instructions? arXiv preprint arXiv:2010.11982.</p>
<p>Nancy Fulda, Nathan Tibbetts, Zachary Brown, and David Wingate. 2017. Harvesting common-sense navigational knowledge for robotics from uncurated text corpora. In Conference on Robot Learning (IJCAI).</p>
<p>Aditya Gupta, Jiacheng Xu, Shyam Upadhyay, Diyi Yang, and Manaal Faruqui. 2021. Disfl-qa: A benchmark dataset for understanding disfluencies in question answering. In Annual Meeting of the Association for Computational Linguistics (ACL) - Findings.</p>
<p>Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, and Richard Socher. 2017. A joint many-task model: Growing a neural network for multiple nlp tasks. In Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>William Hersh, Chris Buckley, TJ Leone, and David Hickam. 1994. Ohsumed: An interactive retrieval evaluation and new large test collection for research.</p>
<p>In Conference of the Association for Computing Machinery Special Interest Group in Information Retrieval (SIGIR).</p>
<p>Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md Patwary, Mostofa Ali, Yang Yang, and Yanqi Zhou. 2017. Deep Learning Scaling is Predictable, Empirically. arXiv preprint arXiv:1712.00409.</p>
<p>Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. 2020. UnifiedQA: Crossing Format Boundaries With a Single QA System. In Conference on Empirical Methods in Natural Language Processing (EMNLP) - Findings.</p>
<p>Hyeonji Kim, Byeong-Hoon So, Wook-Shin Han, and Hongrae Lee. 2020. Natural language to sql: Where are we today? Proceedings of the VLDB Endowment, 13(10):1737-1750.</p>
<p>Andrew K Lampinen, Ishita Dasgupta, Stephanie CY Chan, Kory Matthewson, Michael Henry Tessler, Antonia Creswell, James L McClelland, Jane X Wang, and Felix Hill. 2022. Can Language Models Learn from Explanations in Context? arXiv preprint arXiv:2204.02329.</p>
<p>Logan Lebanoff, John Muchovej, Franck Dernoncourt, Doo Soon Kim, Lidan Wang, Walter Chang, and Fei Liu. 2020. Understanding points of correspondence between sentences for abstractive summarization. In Annual Meeting of the Association for Computational Linguistics (ACL) - Student Research Workshop.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In ACL Workshop on Text Summarization Branches Out.</p>
<p>Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. 2018. The natural language decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730.</p>
<p>Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2022a. MetaICL: Learning to learn in context. In NAACL-HLT.</p>
<p>So Yeon Min, Devendra Singh Chaplot, Pradeep Ravikumar, Yonatan Bisk, and Ruslan Salakhutdinov. 2022b. FILM: Following Instructions in Language with Modular Methods. In International Conference on Learning Representations (ICLR).</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi. 2022a. Reframing instructional prompts to gptk's language. In Annual Meeting of the Association for Computational Linguistics (ACL) - Findings.</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022b. Cross-Task Generalization via Natural Language Crowdsourcing Instructions. In Annual Meeting of the Association for Computational Linguistics (ACL).</p>
<p>Courtney Napoles, Keisuke Sakaguchi, and Joel Tetreault. 2017. Jfleg: A fluency corpus and benchmark for grammatical error correction. In Conference of the European Chapter of the Association for Computational Linguistics (EACL).</p>
<p>Jekaterina Novikova, Ondřej Dušek, and Verena Rieser. 2017. The e2e dataset: New challenges for end-to-end generation. In Annual SIGdial Meeting on Discourse and Dialogue.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training Language Models to Follow Instructions with Human Feedback. arXiv preprint arXiv:2203.02155.</p>
<p>Mihir Parmar, Swaroop Mishra, Mirali Purohit, Man Luo, Murad Mohammad, and Chitta Baral. 2022. InBoXBART: Get instructions into biomedical multitask learning. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 112-128, Seattle, United States. Association for Computational Linguistics.</p>
<p>Ellie Pavlick, Matt Post, Ann Irvine, Dmitry Kachaev, and Chris Callison-Burch. 2014. The Language Demographics of Amazon Mechanical Turk. Transactions of the Association for Computational Linguistics (TACL).</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research (JMLR).</p>
<p>Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. 2011. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In AAAI spring symposium: logical formalizations of commonsense reasoning.</p>
<p>Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2020. WINOGRANDE: an adversarial winograd schema challenge at scale. In Conference on Artificial Intelligence (AAAI).</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong,</p>
<p>Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. 2022. Multitask Prompted Training Enables Zero-Shot Task Generalization. In International Conference on Learning Representations (ICLR).</p>
<p>Igor Shalyminov, Alessandro Sordoni, Adam Atkinson, and Hannes Schulz. 2020. Fast domain adaptation for goal-oriented dialogue using a hybrid generativeretrieval transformer. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).</p>
<p>Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. 2020. ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.</p>
<p>Simon Stepputtis, Joseph Campbell, Mariano Phielipp, Stefan Lee, Chitta Baral, and Heni Ben Amor. 2020. Language-Conditioned Imitation Learning for Robot Manipulation Tasks. In Advances in Neural Information Processing Systems (NeurIPS).</p>
<p>Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. 2017. Revisiting Unreasonable Effectiveness of Data in Deep Learning Era. In International Conference on Computer Vision (ICCV).</p>
<p>Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, et al. 2019. Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples. In International Conference on Learning Representations (ICLR).</p>
<p>Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. 2022. Finetuned Language Models are Zero-Shot Learners. In International Conference on Learning Representations (ICLR).</p>
<p>Nathaniel Weir, Xingdi Yuan, Marc-Alexandre Côté, Matthew Hausknecht, Romain Laroche, Ida Momennejad, Harm Van Seijen, and Benjamin Van Durme. 2022. One-Shot Learning from a Demonstration with Hierarchical Latent Language. arXiv preprint arXiv:2203.04806.</p>
<p>Orion Weller, Nicholas Lourie, Matt Gardner, and Matthew Peters. 2020. Learning from Task Descriptions. In Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-Art Natural Language Processing. In Conference on Empirical Methods in Natural Language Processing (EMNLP) - System Demonstrations.</p>
<p>Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I Wang, et al. 2022. UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models. arXiv preprint arXiv:2201.05966.</p>
<p>Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yanggang Wang, Haiyu Li, and Zhilin Yang. 2022. ZeroPrompt: Scaling Prompt-Based Pretraining to 1,000 Tasks Improves Zero-Shot Generalization. arXiv preprint arXiv:2201.06910.</p>
<p>Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer. In Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</p>
<p>Qinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021. CrossFit: A Few-shot Learning Challenge for Crosstask Generalization in NLP. In Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Qinyuan Ye and Xiang Ren. 2021. Learning to Generate Task-Specific Adapters from Task Description. In Annual Meeting of the Association for Computational Linguistics (ACL).</p>
<p>Ruiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein. 2021. Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections. In Conference on Empirical Methods in Natural Language Processing (EMNLP) - Findings.</p>
<h2>Supplemental Material</h2>
<h2>A Crowdsourcing Human Feedback</h2>
<p>We use Amazon Mechanical Turk (AMT) to crowdsource feedback on the quality of the collected instructions. We limit our crowdworkers to predominantly English-speaking countries (USA, UK, Canada, and Australia), and to those who have finished over 1 K HITs with an approval rating of over $99 \%$.</p>
<p>Fig. 6 shows the crowdsourcing template used for collecting crowdworker feedback on our instructions. We show the instructions (the task definition, along with positive and negative examples) followed by forms for their feedback. We allow the crowdworkers to give us a qualitative measure of their perceived quality as well as text boxes for more concrete items (such as typos or phrasings that may benefit from more clear articulation). For each task, we solicit the feedback of 3 crowdworkers and then use this feedback to improve the task definitions or the examples for each task.</p>
<h2>B Crowdsourcing Human Judgements of Generation Quality</h2>
<p>We perform a crowdsourcing experiment on Amazon Mechanical Turk (AMT) to assess the quality of the generated responses of models. Specifically, we ask crowdworkers to indicate if they prefer the predicted answer by the model or the ground truth outputs for each instances. The annotation interface is shown in Fig. 7. It is essentially the same template used for the quality assessment of the dataset (§A), except that here the crowdworkers are shown a pair of responses for each instancesthe reference text (from our benchmark) and the one generated by the model-turning the task into a comparative evaluation.</p>
<p>For each instance, we obtain annotations from an annotator as to whether they prefer either response over the other or they would rate them equally ("tie"). The model receives a credit of 1.0 if the worker favors the model's prediction at least as well as the ground truth label (otherwise, the model would receive a credit of 0.0 ). The overall accuracy score for the model is computed by averaging instance-level scores. To reduce the costs, the human evaluation of our models is done on 60 randomly selected tasks (about half of our evaluation tasks), and on 10 random instances of each task.</p>
<p>Since it is non-trivial to find non-English speaking crowdworkers (Pavlick et al., 2014), this evaluation was restricted to English language tasks. Therefore, since our task is focused on English tasks, we required workers to be based in a country with a population predominantly of native English speakers (USA, Canada, UK, and Australia) and have completed at least 5000 HITs with $\geq 99 \%$ assignment approval rate.</p>
<p>The resulting human-evaluation metric indicates how often were model predictions equal or preferred to our ground truth labels. In this evaluation, the theoretical upper bound is $100 \%$ where the model is rated at least as well as the ground truth. The results of human evaluation are shown in the bottom row of Fig. 3.</p>
<h2>C Instruction Schema</h2>
<p>Our instruction schema is based on that of NatInSt (Mishra et al., 2022b), but we simplify it to make data collection easier. Our Definition field serves as the union of Mishra et al. (2022b)'s Definition, Things to Avoid, and Emphasis \&amp; Caution. Additionally, we drop their Title and Prompt as their content is most often covered by Definition.</p>
<h2>D Model Implementation Details</h2>
<p>T5 experiments. We use T5 for training our T $k$ INSTRUCT, estimating the performance of the supervised approach and conducting analysis.</p>
<p>Our experiments that finetune the T5-11B model are conducted based on the Google's T5 library ${ }^{9}$ and we use their T5.1.1.xxl checkpoint ${ }^{10}$ by default, which is pre-trained only on C4. ${ }^{11}$ These experiments are run on Google V3-256 TPUs using a batch size of 1,048,576 tokens (1,024 examples), a constant learning rate of $1 \mathrm{e}-5$ and a total of 1000 steps. Each training run takes 4 hours to complete.</p>
<p>Our analyses that use T5 models smaller than 11B parameters are conducted based on Huggingface's transformers library and model checkpoints ${ }^{12}$ (Wolf et al., 2020) on GPU machines.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: The crowdsourcing template we use to receive feedback on our collected tasks.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Crowdsourcing interface used for human assessment of our baselines (§6.2).</p>
<p>When fine-tuning models, we train them for two epochs with a batch size of 16 and a constant learning rate of $1 \mathrm{e}-5$. The maximum input length is set to 1024 , and the maximum output length is set to 128 . These experiments are conducted with 8 A100 GPUs with 48GB GPU memory per each. We use DeepSpeed ${ }^{13}$ for model parallelization, with bfloat16 precision enabled to save the GPU mem-</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>ory. Each training run takes 6 hours to complete.
GPT-3 and InstructGPT experiments. We use the OpenAI API ${ }^{14}$ for conducting the GPT-3 experiments. We use their "davinci" engine for the GPT-3 language model experiments and their "text-davinci-001" engine for the InstructGPT experiments. When making the requests, we set the temperature as 0 , top_p as 1 and the maximum gen-</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>eration length as 128. Due to the high cost, we randomly sample 20 instances from each of our 119 test tasks to estimate the performance of GPT3 and InstructGPT. All API requests were made on May 30, 2022.</p>
<p>Encoding instruction with input For every problem setup, we map a given instruction $I_{t}$ and an input instance $x$ into a textual format, obtaining $\operatorname{enc}\left(I_{t}, x\right)$. Each instruction $I_{t}$ consists of multiple elements as described in our instruction schema (§3). We map each element of the instruction to a textual format and prepend it to the input instance. Fig. 8 shows how we encode the full instruction. We study different combinations of these instruction elements in $\S 7.2$. The encoded instance is then fed to an encoder-decoder model to predict $y$ : $M: \operatorname{enc}\left(I_{t}, x\right) \rightarrow y$.</p>
<div class="codehilite"><pre><span></span><code><span class="x">Definition : </span><span class="cp">{{</span><span class="nv">definition</span><span class="cp">}}</span>
<span class="x">Positive Example 1-</span>
<span class="x">    input : </span><span class="cp">{{</span><span class="nv">p_ex1.input</span><span class="cp">}}</span>
<span class="x">    output : </span><span class="cp">{{</span><span class="nv">p_ex1.output</span><span class="cp">}}</span>
<span class="x">    explanation : </span><span class="cp">{{</span><span class="nv">p_ex1.exp</span><span class="cp">}}</span>
<span class="x">Positive Example 2-</span>
<span class="x">    Negative Example 1-</span>
<span class="x">    input : </span><span class="cp">{{</span><span class="nv">n_ex1.input</span><span class="cp">}}</span>
<span class="x">    output : </span><span class="cp">{{</span><span class="nv">n_ex1.output</span><span class="cp">}}</span>
<span class="x">    explanation : </span><span class="cp">{{</span><span class="nv">n_ex1.exp</span><span class="cp">}}</span>
<span class="x">Negative Example 2-</span>
<span class="x">    Now complete the following example-</span>
<span class="x">    input : </span><span class="cp">{{</span><span class="nv">x.input</span><span class="cp">}}</span>
<span class="x">    output :</span>
</code></pre></div>

<p>Figure 8: Encoding task instruction with input.</p>
<h2>E Evaluation Metrics</h2>
<p>We adopt ROUGE-L as our automatic evaluation metric in this work. However, it remains a question for how much ROUGE-L can reflect model's performance on different tasks. Although we cannot test ROUGE-L's correlation with each task-specific metric of the tasks included in our data, we do investigate whether ROUGE-L can be used for classification tasks. Fig. 9 plots the ROUGE-L scores and accuracy of several models on different types of tasks. These task types are usually regarded as classification tasks and have very short ground truth output. We can see that for all these task types, the trend of ROUGE-L correlates well with the trend of accuracy. For some task types, we do see some gap between these two metrics. The reason is because
there are some generation tasks categorized into these types. These results indicate that ROUGEL is a good proxy for accuracy for classification tasks.</p>
<h2>F Distribution of Tasks</h2>
<p>As is described in §3, Sup-NATInSt provides the annotation for categorizing tasks along three different dimensions: task type, language, and domain. Fig. 10 shows the distribution of tasks among these three dimensions. This meta-information can be used to study model's generalization ability in different senses. Despite the diversity of the data, we acknowledge the skew toward certain tasks and languages, which we leave to be addressed by future work.</p>
<h2>G Evaluation Tasks</h2>
<p>Table 5 lists the 12 task categories used for our evaluation and all the tasks included in each category (introduced in $\S 5.1$ ). To provide a better sense of what those tasks look like, we also select one representative task from each category and list them in Tables 6-17. Due to the large number of tasks in our dataset, we cannot list all 1,616 tasks in this paper. We refer the reader to our dataset.</p>
<h2>H Performance Improvement per Evaluation Task</h2>
<p>To provide more detailed analysis of T $k$-InSTRUCT on each individual task, Fig. 11 presents the per-task improvement of our T $k$-InSTRUCT (3B) model over the best of two heuristic baselines on the English evaluation tasks, and Fig. 12 presents the per-task improvement of the $\mathrm{mT} k$-InSTRUCT model on the cross-lingual evaluation tasks. For most of the evaluation tasks, we see a notable extent of generalization by T $k$-InSTRUCT.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Rouge-L v.s. Accuracy for task types that are usually regarded as classification tasks. The trends of these two metrics are highly correlated with a Pearson coefficient of 0.970 .
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: Distribution of SuP-NATINST tasks in terms of their (a) task types (b) languages (c) domains. $y$-axes are in log scale.</p>
<table>
<thead>
<tr>
<th>Task Category</th>
<th>Metric</th>
<th>List of Tasks</th>
</tr>
</thead>
<tbody>
<tr>
<td>Textual</td>
<td>Exact Match</td>
<td>task937_defeasible_nti_atomic_textual_entailment</td>
</tr>
<tr>
<td>Entailment</td>
<td>task202_multinli_textual_entailment</td>
<td>task463_pusinfo_textual_entailment</td>
</tr>
<tr>
<td></td>
<td>task936_defeasible_nti_atomic_textual_entailment</td>
<td>task1387_anli_textual_entailment</td>
</tr>
<tr>
<td></td>
<td>task641_e_snli_textual_entailment</td>
<td>task738_perspectrum_textual_entailment</td>
</tr>
<tr>
<td></td>
<td>task1344_rte_textual_entailment</td>
<td>task1529_scitalv1.1_textual_entailment</td>
</tr>
<tr>
<td></td>
<td>task1615_sick_textual_entailment</td>
<td>task190_snli_textual_entailment</td>
</tr>
<tr>
<td></td>
<td>task1385_anli_textual_entailment</td>
<td>task200_multinli_textual_entailment</td>
</tr>
<tr>
<td></td>
<td>task935_defeasible_nti_atomic_textual_entailment</td>
<td>task1612_sick_textual_entailment</td>
</tr>
<tr>
<td></td>
<td>task199_multinli_textual_entailment</td>
<td>task970_sherbic_textual_entailment</td>
</tr>
<tr>
<td></td>
<td>task1388_cb_textual_entailment</td>
<td>task890_gwad_textual_entailment</td>
</tr>
<tr>
<td></td>
<td>task3554_scitall_textual_entailment</td>
<td>task464_pusinfo_textual_entailment</td>
</tr>
<tr>
<td></td>
<td>task640_e_snli_textual_entailment</td>
<td>task1516_inuppws_textual_entailment</td>
</tr>
<tr>
<td></td>
<td>task534_hrrtnal_textual_entailment</td>
<td>task642_e_snli_textual_entailment</td>
</tr>
<tr>
<td></td>
<td>task201_multinli_textual_entailment</td>
<td></td>
</tr>
<tr>
<td>Cause</td>
<td>Exact Match</td>
<td>task1178_xcopa_cause_effect_classification</td>
</tr>
<tr>
<td>Effect</td>
<td>task391_cod3s_cause_effect_classification</td>
<td>task1184_xcopa_cause_effect_classification</td>
</tr>
<tr>
<td>Classification</td>
<td>task939_indicntp_cause_effect_classification</td>
<td>task1176_xcopa_cause_effect_classification</td>
</tr>
<tr>
<td></td>
<td>task392_cod3s_cause_effect_classification</td>
<td>task614_glucose_cause_effect_classification</td>
</tr>
<tr>
<td></td>
<td>task938_indicntp_cause_effect_classification</td>
<td>task1629_copa_hr_cause_effect_classification</td>
</tr>
<tr>
<td></td>
<td>task1168_xcopa_cause_effect_classification</td>
<td>task1175_xcopa_cause_effect_classification</td>
</tr>
<tr>
<td></td>
<td>task828_copa_cause_effect_classification</td>
<td>task827_copa_cause_effect_classification</td>
</tr>
<tr>
<td></td>
<td>task1628_copa_hr_cause_effect_classification</td>
<td>task1173_xcopa_cause_effect_classification</td>
</tr>
<tr>
<td></td>
<td>task943_indicntp_cause_effect_classification</td>
<td>task1180_xcopa_cause_effect_classification</td>
</tr>
<tr>
<td></td>
<td>task1182_xcopa_cause_effect_classification</td>
<td>task1170_xcopa_cause_effect_classification</td>
</tr>
<tr>
<td></td>
<td>task1171_xcopa_cause_effect_classification</td>
<td>task1183_xcopa_cause_effect_classification</td>
</tr>
<tr>
<td></td>
<td>task968_xcopa_cause_effect_classification</td>
<td>task969_xcopa_cause_effect_classification</td>
</tr>
<tr>
<td></td>
<td>task942_indicntp_cause_effect_classification</td>
<td>task941_indicntp_cause_effect_classification</td>
</tr>
<tr>
<td></td>
<td>task1181_xcopa_cause_effect_classification</td>
<td>task1626_copa_hr_cause_effect_classification</td>
</tr>
<tr>
<td></td>
<td>task1172_xcopa_cause_effect_classification</td>
<td>task940_indicntp_cause_effect_classification</td>
</tr>
<tr>
<td></td>
<td>task1393_copa_cause_effect_classification</td>
<td>task393_cod3s_cause_effect_classification</td>
</tr>
<tr>
<td></td>
<td>task1174_xcopa_cause_effect_classification</td>
<td>task1169_xcopa_cause_effect_classification</td>
</tr>
<tr>
<td></td>
<td>task1627_copa_hr_cause_effect_classification</td>
<td>task1179_xcopa_cause_effect_classification</td>
</tr>
<tr>
<td></td>
<td>task1177_xcopa_cause_effect_classification</td>
<td></td>
</tr>
<tr>
<td>Coreference</td>
<td>Exact Match</td>
<td>task1391_winogrande_coreference_resolution</td>
</tr>
<tr>
<td>Resolution</td>
<td>task1664_wino_bias_coreference_resolution</td>
<td>task133_winowby_coreference_resolution</td>
</tr>
<tr>
<td></td>
<td>task304_mummy_fused_head_coreference_resolution</td>
<td>task329_gap_coreference_resolution</td>
</tr>
<tr>
<td></td>
<td>task892_gap_coreference_resolution</td>
<td>task249_enhanced_wsc_coreference_resolution</td>
</tr>
<tr>
<td></td>
<td>task891_gap_coreference_resolution</td>
<td>task648_winograd_wsc_coreference_resolution</td>
</tr>
<tr>
<td></td>
<td>task330_gap_coreference_resolution</td>
<td>task1390_wsc_fuexed_coreference_resolution</td>
</tr>
<tr>
<td></td>
<td>task401_mummy_fused_head_coreference_resolution</td>
<td>task893_gap_coreference_resolution</td>
</tr>
<tr>
<td>Dialogue</td>
<td>Exact Match</td>
<td>task879_schema_guided_dm:8_dialogue_act_recognition</td>
</tr>
<tr>
<td>Act</td>
<td>task362_upstin_dialogue_act_recognition</td>
<td>task1531_dialydialog_dialogue_act_recognition</td>
</tr>
<tr>
<td>Recognition</td>
<td>task1533_dialydialog_dialogue_act_recognition</td>
<td>task1394_meta_wor_dialogue_act_recognition</td>
</tr>
<tr>
<td></td>
<td>task1534_dialydialog_dialogue_act_recognition</td>
<td></td>
</tr>
<tr>
<td>Anowerability</td>
<td>Exact Match</td>
<td>task020_inctaco_anowerability_classification</td>
</tr>
<tr>
<td>Classification</td>
<td>task050_multiro_anowerability_classification</td>
<td>task1442_doga_anowerability_classification</td>
</tr>
<tr>
<td></td>
<td>task1439_doga_anowerability_classification</td>
<td>task242_tworiqa_anoworability_classification</td>
</tr>
<tr>
<td></td>
<td>task233_tirc_anoworability_classification</td>
<td>task1624_diofl_qa_anoworability_classification</td>
</tr>
<tr>
<td></td>
<td>task226_nuck_overflow_anoworability_classification</td>
<td>task520_aquamore_anoworability_classification</td>
</tr>
<tr>
<td></td>
<td>task396_persianqa_anoworability_classification</td>
<td>task290_tellmowby_anoworability_classification</td>
</tr>
<tr>
<td></td>
<td>task1640_advowerial_qa_anoworability_classification</td>
<td>task349_squal2.0_anoworability_classification</td>
</tr>
<tr>
<td>Word Analogy</td>
<td>Exact Match</td>
<td>task1155_hard_word_analogy task1152_hard_word_analogy</td>
</tr>
<tr>
<td></td>
<td></td>
<td>task1158_hard_word_analogy task1156_hard_word_analogy task1157_hard_word_analogy</td>
</tr>
<tr>
<td>Overlap</td>
<td>ROUGE-L</td>
<td>task039_qasc_overlap_extraction</td>
</tr>
<tr>
<td>Extraction</td>
<td></td>
<td>task281_points_of_correspondence_overlap_extraction</td>
</tr>
<tr>
<td>Keyword</td>
<td>ROUGE-L</td>
<td>task613_liar_keyword_tugging</td>
</tr>
<tr>
<td>Tagging</td>
<td>task645_wiki_auto_all_data_keyword_tugging</td>
<td>task036_qasc_keyword_tugging</td>
</tr>
<tr>
<td></td>
<td>task620_obsumed_keyword_tugging</td>
<td>task623_obsumed_keyword_tugging</td>
</tr>
<tr>
<td>Question</td>
<td>ROUGE-L</td>
<td>task670_ambigqa_question_rewriting</td>
</tr>
<tr>
<td>Rewriting</td>
<td>task121_rest_question_rewriting</td>
<td>task671_ambigqa_question_rewriting</td>
</tr>
<tr>
<td></td>
<td>task1195_diofl_qa_question_rewriting</td>
<td>task1562_rest_question_rewriting</td>
</tr>
<tr>
<td></td>
<td>task442_com_qa_question_rewriting</td>
<td>task1622_diof_qa_question_rewriting</td>
</tr>
<tr>
<td></td>
<td>task1195_gap_question_rewriting</td>
<td>task034_winogrande_question_rewriting</td>
</tr>
<tr>
<td></td>
<td>task035_winogrande_question_rewriting</td>
<td>task402_grailqa_question_rewriting</td>
</tr>
<tr>
<td>Title</td>
<td>ROUGE-L</td>
<td>task1356_visual_title_generation</td>
</tr>
<tr>
<td>Generation</td>
<td>task1540_pest_read_title_generation</td>
<td>task602_wikitext_title_generation</td>
</tr>
<tr>
<td></td>
<td>task1659_bilbam_title_generation</td>
<td>task1586_scifact_title_generation</td>
</tr>
<tr>
<td></td>
<td>task589_recips_ntg_title_generation</td>
<td>task743_eastex_title_generation</td>
</tr>
<tr>
<td></td>
<td>task1342_amazon_us_reviews_title_generation</td>
<td>task500_scruples_title_generation</td>
</tr>
<tr>
<td></td>
<td>task220_rocatories_title_generation</td>
<td>task619_obsumed_title_generation</td>
</tr>
<tr>
<td></td>
<td>task1561_clickbait_news_bg_title_generation</td>
<td>task510_reddit_tifu_dataset_title_generation</td>
</tr>
<tr>
<td></td>
<td>task418_poisson_title_generation</td>
<td>task518_gapword_title_generation</td>
</tr>
<tr>
<td></td>
<td>task1358_visum_title_generation task769_qed_title_generation</td>
<td>task1161_coda_19_title_generation</td>
</tr>
<tr>
<td>Data to Text</td>
<td>ROUGE-L</td>
<td>task957_c2c_data_to_text task1631_open_pt_data_to_text</td>
</tr>
<tr>
<td></td>
<td>task1598_nyc_data_to_text task1728_web_ntg_data_to_text</td>
<td>task1407_dart_data_to_text task1409_dart_data_to_text task760_msr_eqa_data_to_text</td>
</tr>
<tr>
<td></td>
<td>task102_commongen_data_to_text</td>
<td></td>
</tr>
<tr>
<td>Grammar Error</td>
<td>ROUGE-L</td>
<td>task1557_flfeg_grammar_error correction</td>
</tr>
<tr>
<td>Correction</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 5: 12 Evaluation categories (§5.1), their evaluation metrics (Exact Matching or ROUGE-L, §5.1), and all the tasks in each category.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task Type</th>
<th style="text-align: left;">Textual Entailment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Task ID</td>
<td style="text-align: left;">task1344_rie_textual_entailment</td>
</tr>
<tr>
<td style="text-align: left;">Definition</td>
<td style="text-align: left;">In this task, you're given two sentences. Indicate if the first sentence clearly entails the second sentence (i.e., one <br> can conclude the 2nd sentence by reading the 1st one). Indicate your answer with "1" if the first sentence entails the <br> second sentence, otherwise answer with "0".</td>
</tr>
<tr>
<td style="text-align: left;">Positive Ex- <br> ample</td>
<td style="text-align: left;">Input: Sentence 1: No Weapons of Mass Destruction Found in Iraq Yet. Sentence 2:Weapons of Mass Destruction <br> Found in Iraq. <br> Output: 0 <br> Explanation: In our first statement we clearly say that Iraq does not have any weapon of mass destruction but the <br> second sentence says that weapon of mass destruction is found in Iraq which is a contradiction. Hence output will <br> be 0 for non entailment.</td>
</tr>
<tr>
<td style="text-align: left;">Negative Ex- <br> ample</td>
<td style="text-align: left;">Input: Sentence 1: Valero Energy Corp., on Monday, said it found "extensive" additional damage at its 250,000- <br> barrel-per-day Port Arthur refinery. Sentence 2: Valero Energy Corp. produces 250,000 barrels per day. <br> Output: 0 <br> Explanation: The first statement mentions that there was damage found in the 250,000 barrel-per-day Port Aurhur <br> refinery. Which means that they produce 250,000 barrels a day. Hence the output should have been 1 for entailment.</td>
</tr>
<tr>
<td style="text-align: left;">Instance</td>
<td style="text-align: left;">Input: Sentence 1: Like the United States, U.N. officials are also dismayed that Aristide killed a conference called <br> by Prime Minister Robert Malval in Port-au-Prince in hopes of bringing all the feuding parties together. Sentence 2: <br> Aristide had Prime Minister Robert Malval murdered in Port-au-Prince. <br> Valid Output: ["0"]</td>
</tr>
</tbody>
</table>
<p>Table 6: An example task in the Textual Entailment category of our dataset, adopted from RTE (Dagan et al., 2005; Bentivogli et al., 2008).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task Type</th>
<th style="text-align: left;">Cause Effect Classification</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Task ID</td>
<td style="text-align: left;">task828_copa_cause_effect_classification</td>
</tr>
<tr>
<td style="text-align: left;">Definition</td>
<td style="text-align: left;">In this task your given two statements. You must judge whether the second sentence is the cause or effect of the first <br> one. Label the instances as "cause" or "effect" based on your judgment. The sentences are separated by a newline <br> character.</td>
</tr>
<tr>
<td style="text-align: left;">Positive Ex- <br> ample</td>
<td style="text-align: left;">Input: The women met for coffee. They wanted to catch up with each other. <br> Output: cause <br> Explanation: The women met for coffee because they wanted to catch up with each other.</td>
</tr>
<tr>
<td style="text-align: left;">Negative Ex- <br> ample</td>
<td style="text-align: left;">Input: My body cast a shadow over the grass. The sun was rising. <br> Output: effect <br> Explanation: The rising of the sun isn't an effect of casting a shadow over the grass.</td>
</tr>
<tr>
<td style="text-align: left;">Instance</td>
<td style="text-align: left;">Input: The woman tolerated her friend's difficult behavior. The woman knew her friend was going through a hard <br> time. <br> Valid Output: ["cause"]</td>
</tr>
</tbody>
</table>
<p>Table 7: An example task in the Cause Effect Classification category of our dataset, adopted from COPA (Roemmele et al., 2011).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task Type</th>
<th style="text-align: left;">Coreference Resolution</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Task ID</td>
<td style="text-align: left;">task1391_winogrande_coreference_resolution</td>
</tr>
<tr>
<td style="text-align: left;">Definition</td>
<td style="text-align: left;">In this task, you are given a question containing a blank (_) and two options. You should pick the best option to <br> answer the question. Please answer with "A" or "B".</td>
</tr>
<tr>
<td style="text-align: left;">Positive Ex- <br> ample</td>
<td style="text-align: left;">Input: Katrina gave Christine a stuffed animal for their birthday, but _ already had this one. (A) Katrina (B) <br> Christine <br> Output: B <br> Explanation: Since the blank is someone who received the gift and already had a stuffed animal, the answer must <br> be "Christine".</td>
</tr>
<tr>
<td style="text-align: left;">Negative Ex- <br> ample</td>
<td style="text-align: left;">Input: Kevin had to use less sunscreen when at the beach tanning than Justin because _ had less sensitive skin. (A) <br> Kevin (B) Justin <br> Output: (A) <br> Explanation: Here, an additonal parentheses has been added to the answer. Note that, a valid answer must be "A" <br> or "B".</td>
</tr>
<tr>
<td style="text-align: left;">Instance</td>
<td style="text-align: left;">Input: Benjamin hated being in the sand and just watched Nelson make castle since _ hated to be messy. (A) <br> Benjamin (B) Nelson <br> Valid Output: ["A"]</td>
</tr>
</tbody>
</table>
<p>Table 8: An example task in the Cause Effect Classification category of our dataset, adopted from WinoGrande (Sakaguchi et al., 2020).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task Type</th>
<th style="text-align: left;">Dialogue Act Recognition</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Task ID</td>
<td style="text-align: left;">task1394_meta_woz_dialogue_act_recognition</td>
</tr>
<tr>
<td style="text-align: left;">Definition</td>
<td style="text-align: left;">In this task, you are given four sentences: a bot task sentence, a bot role sentence, a user task sentence <br> and a user role sentence. Your job is to classify given sentences into one of the 47 different domains. The <br> domains are: "UPDATE_CALENDAR", "PRESENT_IDEAS", "MOVIE_LISTINGS", "AUTO_SORT", <br> "GAME_RULES", "CONTACT_MANAGER", "BANK_BOT", "MUSIC_SUGGESTER", "CHECK_STATUS", <br> "PET_ADVICE", "HOW_TO_BASIC", "NAME_SUGGESTER", "QUOTE_OF_THE_DAY_BOT", "GUI- <br> NESS_CHECK", "INSURANCE", "RESTAURANT_PICKER", "MAKE_RESTAURANT_RESERVATIONS", <br> "WEDDING_PLANNER", "SKI_BOT", "HOME_BOT", "PLAY_TIMES", "BUS_SCHEDULE_BOT", <br> "WHAT_IN_IT", "PHONE_PLAN_BOT", "DECIDER_BOT", "PHONE_SETTINGS", "TIME_ZONE", <br> "LIBRARY_REQUEST", "UPDATE_CONTACT", "CADALOGUE_BOT", "PROMPT_GENERATOR", <br> "SCAM_LOOKUP", "SPORTS_INFO", "POLICY_BOT", "CITY_INFO", "APARTMENT_FINDER", <br> "EVENT_RESERVE", "SHOPPING", "EDIT_PLAYLIST", "LOOK_UP_INFO", "ORDER_PIZZA", <br> "WEATHER_CHECK", "APPOINTMENT_REMINDER", "GEOGRAPHY", "STORE_DETAILS", "AGREE- <br> MENT_BOT", "ALARM_SET".</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Table 9: An example task in the Dialogue Act Recognition category of our dataset, adopted from MetaLWOz (Shalyminov et al., 2020).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task Type</th>
<th style="text-align: left;">Answerability Classification</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Task ID</td>
<td style="text-align: left;">task1640_adverserial_qa_answerability_classification</td>
</tr>
<tr>
<td style="text-align: left;">Definition</td>
<td style="text-align: left;">Given a paragraph from a wikipedia article about some topic, and a question related to the topic, determine whether <br> the question is answerable from the paragraph. If the question is answerable, answer "True", otherwise, answer <br> "False".</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Table 10: An example task in the Answerability Classification category of our dataset, adopted from AdversarialQA (Bartolo et al., 2020).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task Type</th>
<th style="text-align: left;">Word Analogy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Task ID</td>
<td style="text-align: left;">task1156_bard_word_analogy</td>
</tr>
<tr>
<td style="text-align: left;">Definition</td>
<td style="text-align: left;">Two analogies that relate actions to the tools used to perform the action is given in the form "A : B. C : ?". "A : B" <br> relates action A to tool B. Your task is to replace the question mark (?) with the appropriate tool for the given action <br> C, following the "A : B" relation.</td>
</tr>
<tr>
<td style="text-align: left;">Positive Ex- <br> ample</td>
<td style="text-align: left;">Input: eat : fork. cook : ? <br> Output: pan <br> Explanation: The given analogy relates actions to the tools used to perform them. A fork can be used to eat. To <br> cook, a pan can be used.</td>
</tr>
<tr>
<td style="text-align: left;">Negative Ex- <br> ample</td>
<td style="text-align: left;">Input: dig : shovel. wash : ? <br> Output: sink <br> Explanation: The given analogy relates actions to the tools used to perform them. A knife can be used to cut. To <br> wash, a sink CANNOT be used.</td>
</tr>
<tr>
<td style="text-align: left;">Instance</td>
<td style="text-align: left;">Input: cut : knife. wash : ? <br> Valid Output: ["soap", "washcloth", "detergent", "rag"]</td>
</tr>
</tbody>
</table>
<p>Table 11: An example task in the Word Analogy category of our dataset, adopted from BARD (Fulda et al., 2017).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task Type</th>
<th style="text-align: left;">Overlap Extraction</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Task ID</td>
<td style="text-align: left;">task281_points_of_correspondence_overlap_extraction</td>
</tr>
<tr>
<td style="text-align: left;">Definition</td>
<td style="text-align: left;">You will be given three sentences. Read them, then identify a noun phrase (person, place, or thing) or event that is <br> shared between all three sentences. As the output, write the span of the text corresponding to that phrase in each <br> sentence. Keep the order of the sentences, that is, your answer should look like: 1: <em>a phrase from sentence 1</em> 2: <em>a <br> phrase from sentence 2</em> 3: <em>a phrase from sentence 3</em></td>
</tr>
<tr>
<td style="text-align: left;">Positive Ex- <br> ample</td>
<td style="text-align: left;">Input: 1: Four employees of the store have been arrested, but its manager - herself a woman - was still at large <br> Saturday, said Goa police superintendent Kartik Kashyap. 2: If convicted, they could spend up to three years in <br> jail, Kashyap said. 3: The four store workers arrested could spend 3 years each in prison if convicted <br> Output: 1: Four employees of the store 2: they 3: The four store workers <br> Explanation: All three mentioned parts refer to the same entity, the four employees. "of the store" in first sentence <br> must be included, since it is part of the same noun phrase describing the employees.</td>
</tr>
<tr>
<td style="text-align: left;">Negative Ex- <br> ample</td>
<td style="text-align: left;">Input: 1: But an Arizona official told CNN Bates never trained with the agency. 2: He didn't come to Arizona, the <br> official from the Maricopa County Sheriff 's Office said, and he certainly didn't train with us. 3: Maricopa County <br> Sheriff's Office in Arizona says Robert Bates never trained with them. <br> Output: 1: the agency 3: Maricopa County Sheriff 's Office in Arizona <br> Explanation: The two noun phrases given in this example are correct, but there's no noun phrase from sentence 2. <br> You should include all three sentences in your response.</td>
</tr>
<tr>
<td style="text-align: left;">Instance</td>
<td style="text-align: left;">Input: 1: The President is headed to Panama for a regional summit, and Julie Pace of The Associated Press reports <br> one of the big questions is whether he 'll make history and have a face-to-face meeting with Cuban leader Raul <br> Castro. 2: And so what the White House is going to be weighing is whether this meeting would be a way to <br> generate more progress or whether it would be a premature reward for the Castros. 3: White House weighing <br> whether Obama should meet with Raul Castro. <br> Valid Output: ["1: Cuban leader Raul Castro 2: the Castros 3: Raul Castro", "1: face-to-face meeting 2: this <br> meeting 3: meet"]</td>
</tr>
</tbody>
</table>
<p>Table 12: An example task in the Overlap Extraction category of our dataset, adopted from PointsOfCorrespondence (Lebanoff et al., 2020).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task Type</th>
<th style="text-align: left;">Keyword Tagging</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Task ID</td>
<td style="text-align: left;">task620_ohsumed_keyword_tagging</td>
</tr>
<tr>
<td style="text-align: left;">Definition</td>
<td style="text-align: left;">Given an abstract, generate a keyword (a noun phrase) that best describes the focus or contribution of the paper. <br> Such keywords can be directly from the given abstract or outside it.</td>
</tr>
<tr>
<td style="text-align: left;">Positive Ex- <br> ample</td>
<td style="text-align: left;">Input: Abstract: Our results suggest that ethylene oxide retention after sterilization is increased in cuprammonium <br> cellulose plate dialyzers containing potting compound. In contrast, cuprammonium cellulose plate dialyzers without <br> potting compound were characterized by a rapid disappearance of retained ethylene oxide after sterilization. Whether <br> these findings explain the low incidence of SARD with cuprammonium cellulose plate dialyzers that do not contain <br> potting material is a matter for continued study and experimentation. <br> Output: Sterilization <br> Explanation: This term is directly present in the abstract and it is one of the main topic in it. So can be chosen as <br> the medical subject heading.</td>
</tr>
<tr>
<td style="text-align: left;">Negative Ex- <br> ample</td>
<td style="text-align: left;">Input: Abstract: Our results suggest that ethylene oxide retention after sterilization is increased in cuprammonium <br> cellulose plate dialyzers containing potting compound. In contrast, cuprammonium cellulose plate dialyzers without <br> potting compound were characterized by a rapid disappearance of retained ethylene oxide after sterilization. Whether <br> these findings explain the low incidence of SARD with cuprammonium cellulose plate dialyzers that do not contain <br> potting material is a matter for continued study and experimentation. <br> Output: Plasma Volume <br> Explanation: This term is not directly present in the abstract and it is no way related to the abstract. So can not be <br> chosen as the medical subject heading. "Cellulose" can be become a mesh term</td>
</tr>
<tr>
<td style="text-align: left;">Instance</td>
<td style="text-align: left;">Input: Abstract: There is controversy regarding the appropriate utilization of health care resources in the man- <br> agement of tricyclic antidepressant overdosage. Antidepressant overdose patients presenting to the emergency <br> department (ED) are routinely admitted to intensive care units, but only a small proportion develop cardiac arrhyth- <br> mias or other complications requiring such an environment. The authors reviewed the findings in 165 patients <br> presenting to an ED with antidepressant overdose. They found that major manifestations of toxicity on ED evaluation <br> (altered mental status, seizures, arrhythmias, an d conduction defects) were commonly associated with a complicated <br> hospital course. Patients with the isolated findings of sinus tachycardia or QTc prolongation had no complications. <br> No patient experienced a serious toxic event without major evidence of toxicity on ED evaluation and continued <br> evidence of toxicity during the hospital course. These data support the concept that proper ED evaluation can <br> identify a large body of patients with trivial ingestions who may not require hospital observation. <br> Valid Output: ["Antidepressive Agents, Tricyclic", "Arrhythmia", "California", "Electrocardiography", "Emergen- <br> cies", "Emergency Service, Hospital", "Female", "Human", "Length of Stay", "Male", "Prognosis", "Retrospective <br> Studies", "Tachycardia, Sinus"]</td>
</tr>
</tbody>
</table>
<p>Table 13: An example task in the Keyword Tagging category of our dataset, adopted from OHSUMED (Hersh et al., 1994).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task Type</th>
<th style="text-align: left;">Question Rewriting</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Task ID</td>
<td style="text-align: left;">task1622_disfl_qp_question_rewriting</td>
</tr>
<tr>
<td style="text-align: left;">Definition</td>
<td style="text-align: left;">Convert a disfluent question to a proper question. A disfluent question is a question that has some interruptions in it <br> while framing. A proper question is the correct form of the question without any disfluency.</td>
</tr>
<tr>
<td style="text-align: left;">Positive Ex- <br> ample</td>
<td style="text-align: left;">Input: Why was uh where was the Rhine regulated with an upper canal? <br> Output: Where was the Rhine regulated with an upper canal? <br> Explanation: The above disfluent question is correctly converted to a proper question. The 'uh' and the 'why' <br> before the correction should be removed to make it fluent.</td>
</tr>
<tr>
<td style="text-align: left;">Negative Ex- <br> ample</td>
<td style="text-align: left;">Input: When did Maududi exert the least impact no where did he exert? <br> Output: When did Maududi exert the least impact? <br> Explanation: The above disfluent question is not correctly converted to a proper question since the original meaning <br> in the disfluent question is where did Mauduli exert the least impact but the output asks when did Maudidi exert the <br> least impact.</td>
</tr>
<tr>
<td style="text-align: left;">Instance</td>
<td style="text-align: left;">Input: What kind of, no hold up, what describes the proportionality of acceleration to force and mass? <br> Valid Output: ["What describes the proportionality of acceleration to force and mass?"]</td>
</tr>
</tbody>
</table>
<p>Table 14: An example task in the Question Rewriting category of our dataset, adopted from Disfl-QA (Gupta et al., 2021).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{13}$ https://github.com/microsoft/DeepSpeed&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>GPT-3 and InstructGPT experiments. We use the OpenAI API ${ }^{14}$ for conducting the GPT-3 experiments. We use their "davinci" engine for the GPT-3 language model experiments and their "text-davinci-001" engine for the InstructGPT experiments. When making the requests, we set the temperature as 0 , top_p as 1 and the maximum gen-&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
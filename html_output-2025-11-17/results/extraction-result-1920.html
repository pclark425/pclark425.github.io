<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1920 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1920</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1920</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-39.html">extraction-schema-39</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <p><strong>Paper ID:</strong> paper-278534657</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.08548v2.pdf" target="_blank">From Seeing to Doing: Bridging Reasoning and Decision for Robotic Manipulation</a></p>
                <p><strong>Paper Abstract:</strong> Achieving generalization in robotic manipulation remains a critical challenge, particularly for unseen scenarios and novel tasks. Current Vision-Language-Action (VLA) models, while building on top of general Vision-Language Models (VLMs), still fall short of achieving robust zero-shot performance due to the scarcity and heterogeneity prevalent in embodied datasets. To address these limitations, we propose FSD (From Seeing to Doing), a novel vision-language model that generates intermediate representations through spatial relationship reasoning, providing fine-grained guidance for robotic manipulation. Our approach combines a hierarchical data pipeline for training with a self-consistency mechanism that aligns spatial coordinates with visual signals. Through extensive experiments, we comprehensively validated FSD's capabilities in both"seeing"and"doing,"achieving outstanding performance across 8 benchmarks for general spatial reasoning and embodied reference abilities, as well as on our proposed more challenging benchmark VABench. We also verified zero-shot capabilities in robot manipulation, demonstrating significant performance improvements over baseline methods in both SimplerEnv and real robot settings. Experimental results show that FSD achieves 40.6% success rate in SimplerEnv and 72% success rate across 8 real-world tasks, outperforming the strongest baseline by 30%.</p>
                <p><strong>Cost:</strong> 0.027</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1920.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1920.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FSD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>From Seeing to Doing (FSD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language model that generates object-centric intermediate spatial representations (affordance boxes/points and visual traces) via a Spatial Relationship-Focused Visual Chain-of-Thought (SrCoT) and a self-consistency alignment mechanism, enabling zero-shot robotic manipulation across simulation and real robots.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FSD</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal vision-language model: frozen CLIP-ViT-L image encoder (336px) → two-layer linear projector → Vicuna-13B LLM (instruction tuned). Generates discretized 2D coordinates as text (<ref>, <point>, <box>) and produces object-centric visual traces/affordances using SrCoT and self-consistency; outputs are used to drive motion planners or back-projected to SE(3) trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining (pretrained CLIP image encoder + pretrained LLM) followed by two-stage instruction tuning on mixed visual-language and embodied datasets</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Instruction-tuning mixture (~1.4M samples): LLaVA-665k and ASMv2-4M instruction-following data (~838k sampled), ~295k spatial reasoning data (LLaVA-OneVision, RoboPoint, SpatialBot, SAT), plus FSD's weak-to-strong embodied spatial dataset (~250k) including affordance boxes/points and visual traces derived from human demonstrations; contains object descriptions, spatial relations, affordance labels and inferred trajectories (2D), but limited high-quality 3D coordinate pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>robotic manipulation (visual-aid-guided manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Tabletop object manipulation tasks in SimplerEnv simulation (WidowX) and real-world xArm-6; action execution uses either sampling center of predicted affordance boxes/points with a motion planner (CuRobo) or back-projecting 2D visual traces with depth then interpolating to SE(3) continuous trajectories; object types are household items, tasks include pick/place, folding, sponge manipulation; environment complexity varies (clutter, occlusion); evaluated zero-shot (no task-specific finetuning).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Discussed explicitly: pretraining on internet-scale VLM data provides visual semantics but coordinate/discrete 2D coordinate spaces do not appear in pretraining, so FSD introduces self-consistency (inverse task) and weak-to-strong data construction to align image-language knowledge with coordinate-based affordance/action representations; dataset contains explicit spatial relationships and affordance annotations improving overlap with target tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported metrics (from this paper): VABench-Point accuracy = 61.82% (FSD) vs RoboPoint 19.09%; VABench-VisualTrace RMSE = 78.26, MAE = 63.44 (FSD) (lower is better); SimplerEnv zero-shot average success rate = 40.6% (WidowX tasks); real-world xArm zero-shot success across 8 tabletop tasks = 72%; spatial-benchmarks: strong results (e.g., 3D depth perception 88.0%, 3D distance estimation 86.7%, spatial relationship 78.3%), overall ranking top among tested open models (rank 1.3 across 18 subtasks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Compared baselines: DINOv2 Predictor (vision-centric) VABench-VisualTrace RMSE = 113.53 (worse than FSD's 78.26); RoboPoint VABench-Point accuracy = 19.09% (vs FSD 61.82%); ASMv2 and other open VLMs have lower spatial benchmark scores than FSD (see Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Paper does not report explicit sample-efficiency curves or counts (e.g., episodes-to-convergence) comparing language-pretrained vs non-language models; claims improved zero-shot transfer rather than fewer demonstration examples quantitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No explicit attention map or attention-layer visualization analysis is reported; the paper enforces syntactic binding (<ref>, <point>, <box>) and self-consistency to reduce hallucinations rather than analyzing attention.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No analyses (PCA/clustering/representation visualization) of embedding spaces or feature clustering are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Yes — multiple lines of evidence: (1) FSD generates object-centric affordance boxes/points and ordered visual traces that are bound to object references and coordinates; (2) self-consistency inverse-task training forces the model to map coordinates ↔ language, strengthening semantic grounding; (3) FSD's generated 2D traces are back-projected using depth and converted to SE(3) trajectories and executed on xArm with reported 72% zero-shot success; (4) ablation shows SrCoT and alignment improve visual-trace accuracy and downstream success.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>The paper constructs a hierarchical (weak-to-strong) dataset (levels 1–5) to develop capabilities but does not present internal analyses that separate low-level vs high-level feature benefits from language pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Reported important factors: diversity and mixture of training data (internet-scale VQA + embodied spatial reasoning) improve transfer; object-centric 2D representations help cross-embodiment transfer; availability of approximate depth enables 2D→3D mapping for execution; embodiment heterogeneity and lack of coordinate supervision in pretraining limit naive VLM→VLA transfer, necessitating SrCoT and self-consistency. Transfer works best when spatial relationships and affordance semantics exist in the training mixture.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>The evaluation emphasizes zero-shot performance on novel tasks/scenes (e.g., SimplerEnv and 8 real-world tasks) showing improved generalization, but the paper does not provide granular per-object statistics contrasting familiar vs novel object performance.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Zero-shot: SimplerEnv average success = 40.6% (no fine-tuning), real-world xArm zero-shot success = 72% across 8 tasks; few-shot learning curves are not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No layer-wise importance/ablation (e.g., freezing LLM layers, probing vision projector layers) is reported beyond freezing the visual encoder and fine-tuning projector + LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Paper qualitatively notes that directly applying general VLMs to embodied action prediction yields poor zero-shot performance due to coverage/diversity and embodiment heterogeneity; no quantified negative-transfer numbers are provided, but baseline results (e.g., GPT-4o/ASMv2 lower VABench/VLM performance) illustrate limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Yes — direct comparison to a vision-only trajectory predictor (DINOv2 Predictor) shows FSD (vision-language + reasoning) achieves substantially lower trajectory error (RMSE 78.26 vs 113.53) and better qualitative GPT Scores; also outperforms RoboPoint on affordance point accuracy (61.82% vs 19.09%).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No analysis of representation or performance dynamics over the course of finetuning (early vs late phases) is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No intrinsic dimensionality or PCA-style dimensionality measurements are reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1920.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1920.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DINOv2 Predictor</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DINOv2 Predictor (Transformer visual trajectory predictor baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-centric baseline built for VABench: pre-trained DINOv2 visual encoder + multi-layer transformer head trained to predict fixed-length (8-point) 2D visual traces for manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DINOv2 Predictor</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses pre-trained DINOv2 visual encoder to produce image features, concatenated with frozen T5-Base language encoder outputs and learnable trajectory embeddings; passed to a Transformer encoder and linear readout to predict eight 2D points (visual trace).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-only self-supervised pretraining for the encoder (DINOv2) + frozen language encoder; downstream training supervised on visual-trajectory data produced by FSD pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>DINOv2 encoder pretraining (self-supervised on large image datasets) — no explicit action/affordance supervision; downstream training uses FSD-generated visual trajectory dataset (equidistant 8-point traces inferred from human demonstrations).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>visual trace prediction for robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Predict 8-point ordered 2D trajectories (object-centric) from single images and language instructions; used to evaluate VABench-VisualTrace (simulation/benchmarks), intended to be mapped to 3D for execution.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Limited: encoder captures general visual features but lacks explicit spatial-chain reasoning anchoring on object relationships; downstream supervised training imposes some alignment to affordance/trace labels.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not language-pretrained; baseline reported VABench-VisualTrace RMSE ≈ 113.53 (higher error than FSD), indicating weaker performance on trajectory prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>This model itself is essentially vision-centric; performance is the reported baseline above.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>No explicit sample-efficiency comparisons reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention/interpretability analyses provided.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No representation-space analyses provided.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Limited: purely supervised prediction of traces; inferior error and GPT-score relative to FSD suggests weaker grounding of language/action to visual affordances compared to reasoning-enabled VLM.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Requires training on trajectory-labeled data; generalization beyond training distribution is limited compared to FSD's reasoning-driven approach.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not analyzed in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not intended for zero-shot generalization; trained on trajectory data for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>None specifically reported, but worse performance vs FSD implies constrained transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>This is a vision-only baseline; FSD outperforms it on visual-trace error and downstream qualitative scores.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1920.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1920.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoboPoint</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoboPoint</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language model specialized for spatial affordance prediction (points/boxes) used as a baseline for object/region reference and VABench-Point comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robopoint: A vision-language model for spatial affordance prediction for robotics.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoboPoint</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A 13B vision-language model tailored to predict spatial affordance points/boxes from image+instruction; used for object-free-space localization.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining and task-specific instruction tuning on affordance/object reference datasets (per cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Trained with datasets containing object locations and affordance annotations (e.g., Where2Place and related embodied datasets), with labels for target regions/points.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>spatial affordance prediction / object-free-space reference</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Predict target bounding boxes or points in images given natural-language instructions; discrete outputs used by planners for manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Designed for affordance semantics; however, the paper reports RoboPoint often makes incorrect predictions on tasks requiring deeper spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported: VABench-Point accuracy = 19.09% (substantially lower than FSD 61.82%); RoboRef object reference ≈ 49.8% (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Produces affordance outputs (points/boxes) that can guide planners, but poorer zero-shot performance and inability to predict full traces limit action grounding evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Per paper, works better on simpler, direct instructions and where training data distribution matches test tasks; struggles on complex spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Paper implies lower generalization to unseen tasks/objects; no fine-grained numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Evaluated zero-shot; performance lower than FSD.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not quantified, but comparatively lower accuracy suggests limitations transferring to complex or novel embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Compared primarily to VLM-based approaches; FSD (VLM+SrCoT+self-consistency) outperforms RoboPoint by large margin on VABench-Point.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1920.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1920.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoboBrain</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoboBrain</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large unified model for robotic manipulation that can generate visual traces (agent-centric) and is compared qualitatively and quantitatively to FSD in the paper's analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robobrain: A unified brain model for robotic manipulation from abstract to concrete.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoboBrain</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A multimodal model intended to unify manipulation reasoning and action representations; generates agent-centric visual traces according to the citation and is used as a qualitative baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Multimodal pretraining with robotics/manipulation datasets (details in RoboBrain paper — not exhaustively presented here).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Likely contains manipulation demonstrations and hierarchical annotations; the current paper does not detail RoboBrain's pretraining corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>visual trace generation and robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Generates sequences representing manipulation trajectories; in RoboBrain's case often agent-centric (robot-centric) traces; evaluated qualitatively against FSD for accuracy and generalizability.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Produces manipulable traces but tends to be tied to agent representation; the paper reports this limits generalization to images without robot embodiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>No numeric comparison provided in main text; qualitative statement: FSD's traces have higher accuracy under zero-shot conditions compared to RoboBrain.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>N/A in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>RoboBrain generates traces (evidence of grounding) but tends to be agent-centric, making cross-embodiment grounding weaker versus FSD's object-centric traces.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Per paper, agent-centric generation reduces transfer when images lack explicit robot embodiment; FSD's object-centric design is more robust across heterogeneous ontologies.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Compared under zero-shot; FSD reported higher zero-shot accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Paper suggests RoboBrain's agent-centric outputs can negatively impact transfer across heterogeneous scenarios, but no numerical magnitude is given.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not directly compared to vision-only models here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1920.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1920.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source multimodal large language model (MLLM) used in this paper as a high-performance reference for spatial reasoning and to generate CoT exemplars; it provides strong general visual reasoning but limited coordinate-precise outputs for embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT-4o system card.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source multimodal LLM capable of image+text understanding and chain-of-thought generation; used as a performance reference and to generate CoT reasoning templates in dataset construction.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Large-scale multimodal pretraining on web image-text and language corpora (proprietary).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Broad internet-scale image-text pairs and web text; contains object descriptions and general spatial language but not discrete coordinate supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>spatial reasoning and visual-aid prediction</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Evaluated on general spatial reasoning benchmarks and VABench; constrained in producing exact numeric coordinate outputs so bounding-box outputs are sampled for point evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Strong high-level semantic/world knowledge alignment but lacks native training for coordinate-grounded outputs; the paper highlights that GPT-4o cannot directly output reliable point coordinates without prompting workarounds.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported benchmarks: example VABench-VisualTrace RMSE ≈ 128.32, GPT-Score ≈ 4.37 (lower than FSD); spatial reasoning strong but lower precise affordance/trace accuracy than FSD.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>N/A.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No per-layer attention analysis reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Limited in this context: GPT-4o demonstrates strong semantic reasoning but struggles to produce grounded coordinate outputs needed for direct manipulation; performs worse on VABench metrics compared to FSD.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>General-purpose reasoning transfers broadly but does not directly map to coordinate-level embodied actions; requires pipelines like FSD's SrCoT and alignment to bridge the gap.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Evaluated zero-shot; output formatting limitations reduce effectiveness for embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>No explicit quantified negative transfer, but inability to generate exact coordinates limits direct application to manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not applicable directly; acts as a strong multimodal baseline but lacks coordinate grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1920.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1920.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ASMv2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ASMv2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source vision-language model and instruction-tuning base used by FSD as initialization and also evaluated as a baseline on spatial reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ASMv2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source VLM used for visual instruction tuning; in FSD it served as a starting point / baseline (13B) and contributed to training data/architecture choices; in experiments ASMv2-13B is a comparative baseline on spatial benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language pretraining with instruction-tuning (per cited prior work), used as VLM backbone.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Large mixed visual instruction datasets (ASMv2 training data referenced) with some spatial reasoning instances; not specifically coordinate-grounded.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>spatial reasoning and instruction following (benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Evaluated on five spatial-reasoning benchmarks (CVBench, BLINK, CRPE, SAT, EmbSpatial-Bench) and VABench; not primarily used for end-to-end manipulation in this paper but serves as comparative VLM.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Moderate; trained with visual instruction data but lacks FSD's SrCoT/self-consistency coordinate binding specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported but lower than FSD on spatial benchmarks (Table 1 shows ASMv2 performance below FSD across multiple subtasks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>N/A.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Limited in this paper: ASMv2 is used as a reasoning baseline but does not include the explicit coordinate-binding and affordance-generation mechanisms central to FSD.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Per authors, general VLMs like ASMv2 require augmented datasets and specialized mechanisms (SrCoT, self-consistency) to transfer effectively to embodied coordinate-based tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Evaluated zero-shot on spatial benchmarks; lower capability than FSD for embodied visual-aid generation.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Paper argues general VLMs without coordinate alignment suffer limited zero-shot performance for manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>ASMv2 is a multimodal baseline; FSD outperforms it after targeted training for spatial reasoning and visual-aid generation.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Robopoint: A vision-language model for spatial affordance prediction for robotics. <em>(Rating: 2)</em></li>
                <li>Robobrain: A unified brain model for robotic manipulation from abstract to concrete. <em>(Rating: 2)</em></li>
                <li>Rt-2: Vision-language-action models transfer web knowledge to robotic control. <em>(Rating: 2)</em></li>
                <li>Llarva: Vision-action instruction tuning enhances robot learning. <em>(Rating: 2)</em></li>
                <li>Where2act: From pixels to actions for articulated 3d objects. <em>(Rating: 1)</em></li>
                <li>Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. <em>(Rating: 2)</em></li>
                <li>SpatialVLA <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1920",
    "paper_id": "paper-278534657",
    "extraction_schema_id": "extraction-schema-39",
    "extracted_data": [
        {
            "name_short": "FSD",
            "name_full": "From Seeing to Doing (FSD)",
            "brief_description": "A vision-language model that generates object-centric intermediate spatial representations (affordance boxes/points and visual traces) via a Spatial Relationship-Focused Visual Chain-of-Thought (SrCoT) and a self-consistency alignment mechanism, enabling zero-shot robotic manipulation across simulation and real robots.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "FSD",
            "model_description": "Multimodal vision-language model: frozen CLIP-ViT-L image encoder (336px) → two-layer linear projector → Vicuna-13B LLM (instruction tuned). Generates discretized 2D coordinates as text (&lt;ref&gt;, &lt;point&gt;, &lt;box&gt;) and produces object-centric visual traces/affordances using SrCoT and self-consistency; outputs are used to drive motion planners or back-projected to SE(3) trajectories.",
            "pretraining_type": "vision-language pretraining (pretrained CLIP image encoder + pretrained LLM) followed by two-stage instruction tuning on mixed visual-language and embodied datasets",
            "pretraining_data_description": "Instruction-tuning mixture (~1.4M samples): LLaVA-665k and ASMv2-4M instruction-following data (~838k sampled), ~295k spatial reasoning data (LLaVA-OneVision, RoboPoint, SpatialBot, SAT), plus FSD's weak-to-strong embodied spatial dataset (~250k) including affordance boxes/points and visual traces derived from human demonstrations; contains object descriptions, spatial relations, affordance labels and inferred trajectories (2D), but limited high-quality 3D coordinate pretraining.",
            "target_task_name": "robotic manipulation (visual-aid-guided manipulation)",
            "target_task_description": "Tabletop object manipulation tasks in SimplerEnv simulation (WidowX) and real-world xArm-6; action execution uses either sampling center of predicted affordance boxes/points with a motion planner (CuRobo) or back-projecting 2D visual traces with depth then interpolating to SE(3) continuous trajectories; object types are household items, tasks include pick/place, folding, sponge manipulation; environment complexity varies (clutter, occlusion); evaluated zero-shot (no task-specific finetuning).",
            "semantic_alignment": "Discussed explicitly: pretraining on internet-scale VLM data provides visual semantics but coordinate/discrete 2D coordinate spaces do not appear in pretraining, so FSD introduces self-consistency (inverse task) and weak-to-strong data construction to align image-language knowledge with coordinate-based affordance/action representations; dataset contains explicit spatial relationships and affordance annotations improving overlap with target tasks.",
            "performance_with_language_pretraining": "Reported metrics (from this paper): VABench-Point accuracy = 61.82% (FSD) vs RoboPoint 19.09%; VABench-VisualTrace RMSE = 78.26, MAE = 63.44 (FSD) (lower is better); SimplerEnv zero-shot average success rate = 40.6% (WidowX tasks); real-world xArm zero-shot success across 8 tabletop tasks = 72%; spatial-benchmarks: strong results (e.g., 3D depth perception 88.0%, 3D distance estimation 86.7%, spatial relationship 78.3%), overall ranking top among tested open models (rank 1.3 across 18 subtasks).",
            "performance_without_language_pretraining": "Compared baselines: DINOv2 Predictor (vision-centric) VABench-VisualTrace RMSE = 113.53 (worse than FSD's 78.26); RoboPoint VABench-Point accuracy = 19.09% (vs FSD 61.82%); ASMv2 and other open VLMs have lower spatial benchmark scores than FSD (see Table 1).",
            "sample_efficiency_comparison": "Paper does not report explicit sample-efficiency curves or counts (e.g., episodes-to-convergence) comparing language-pretrained vs non-language models; claims improved zero-shot transfer rather than fewer demonstration examples quantitatively.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No explicit attention map or attention-layer visualization analysis is reported; the paper enforces syntactic binding (&lt;ref&gt;, &lt;point&gt;, &lt;box&gt;) and self-consistency to reduce hallucinations rather than analyzing attention.",
            "embedding_space_analysis": "No analyses (PCA/clustering/representation visualization) of embedding spaces or feature clustering are reported.",
            "action_grounding_evidence": "Yes — multiple lines of evidence: (1) FSD generates object-centric affordance boxes/points and ordered visual traces that are bound to object references and coordinates; (2) self-consistency inverse-task training forces the model to map coordinates ↔ language, strengthening semantic grounding; (3) FSD's generated 2D traces are back-projected using depth and converted to SE(3) trajectories and executed on xArm with reported 72% zero-shot success; (4) ablation shows SrCoT and alignment improve visual-trace accuracy and downstream success.",
            "hierarchical_features_evidence": "The paper constructs a hierarchical (weak-to-strong) dataset (levels 1–5) to develop capabilities but does not present internal analyses that separate low-level vs high-level feature benefits from language pretraining.",
            "transfer_conditions": "Reported important factors: diversity and mixture of training data (internet-scale VQA + embodied spatial reasoning) improve transfer; object-centric 2D representations help cross-embodiment transfer; availability of approximate depth enables 2D→3D mapping for execution; embodiment heterogeneity and lack of coordinate supervision in pretraining limit naive VLM→VLA transfer, necessitating SrCoT and self-consistency. Transfer works best when spatial relationships and affordance semantics exist in the training mixture.",
            "novel_vs_familiar_objects": "The evaluation emphasizes zero-shot performance on novel tasks/scenes (e.g., SimplerEnv and 8 real-world tasks) showing improved generalization, but the paper does not provide granular per-object statistics contrasting familiar vs novel object performance.",
            "zero_shot_or_few_shot": "Zero-shot: SimplerEnv average success = 40.6% (no fine-tuning), real-world xArm zero-shot success = 72% across 8 tasks; few-shot learning curves are not reported.",
            "layer_analysis": "No layer-wise importance/ablation (e.g., freezing LLM layers, probing vision projector layers) is reported beyond freezing the visual encoder and fine-tuning projector + LLM.",
            "negative_transfer_evidence": "Paper qualitatively notes that directly applying general VLMs to embodied action prediction yields poor zero-shot performance due to coverage/diversity and embodiment heterogeneity; no quantified negative-transfer numbers are provided, but baseline results (e.g., GPT-4o/ASMv2 lower VABench/VLM performance) illustrate limitations.",
            "comparison_to_vision_only": "Yes — direct comparison to a vision-only trajectory predictor (DINOv2 Predictor) shows FSD (vision-language + reasoning) achieves substantially lower trajectory error (RMSE 78.26 vs 113.53) and better qualitative GPT Scores; also outperforms RoboPoint on affordance point accuracy (61.82% vs 19.09%).",
            "temporal_dynamics": "No analysis of representation or performance dynamics over the course of finetuning (early vs late phases) is reported.",
            "dimensionality_analysis": "No intrinsic dimensionality or PCA-style dimensionality measurements are reported.",
            "uuid": "e1920.0"
        },
        {
            "name_short": "DINOv2 Predictor",
            "name_full": "DINOv2 Predictor (Transformer visual trajectory predictor baseline)",
            "brief_description": "A vision-centric baseline built for VABench: pre-trained DINOv2 visual encoder + multi-layer transformer head trained to predict fixed-length (8-point) 2D visual traces for manipulation tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DINOv2 Predictor",
            "model_description": "Uses pre-trained DINOv2 visual encoder to produce image features, concatenated with frozen T5-Base language encoder outputs and learnable trajectory embeddings; passed to a Transformer encoder and linear readout to predict eight 2D points (visual trace).",
            "pretraining_type": "vision-only self-supervised pretraining for the encoder (DINOv2) + frozen language encoder; downstream training supervised on visual-trajectory data produced by FSD pipeline.",
            "pretraining_data_description": "DINOv2 encoder pretraining (self-supervised on large image datasets) — no explicit action/affordance supervision; downstream training uses FSD-generated visual trajectory dataset (equidistant 8-point traces inferred from human demonstrations).",
            "target_task_name": "visual trace prediction for robotic manipulation",
            "target_task_description": "Predict 8-point ordered 2D trajectories (object-centric) from single images and language instructions; used to evaluate VABench-VisualTrace (simulation/benchmarks), intended to be mapped to 3D for execution.",
            "semantic_alignment": "Limited: encoder captures general visual features but lacks explicit spatial-chain reasoning anchoring on object relationships; downstream supervised training imposes some alignment to affordance/trace labels.",
            "performance_with_language_pretraining": "Not language-pretrained; baseline reported VABench-VisualTrace RMSE ≈ 113.53 (higher error than FSD), indicating weaker performance on trajectory prediction.",
            "performance_without_language_pretraining": "This model itself is essentially vision-centric; performance is the reported baseline above.",
            "sample_efficiency_comparison": "No explicit sample-efficiency comparisons reported.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No attention/interpretability analyses provided.",
            "embedding_space_analysis": "No representation-space analyses provided.",
            "action_grounding_evidence": "Limited: purely supervised prediction of traces; inferior error and GPT-score relative to FSD suggests weaker grounding of language/action to visual affordances compared to reasoning-enabled VLM.",
            "hierarchical_features_evidence": "No.",
            "transfer_conditions": "Requires training on trajectory-labeled data; generalization beyond training distribution is limited compared to FSD's reasoning-driven approach.",
            "novel_vs_familiar_objects": "Not analyzed in the paper.",
            "zero_shot_or_few_shot": "Not intended for zero-shot generalization; trained on trajectory data for evaluation.",
            "layer_analysis": "No.",
            "negative_transfer_evidence": "None specifically reported, but worse performance vs FSD implies constrained transfer.",
            "comparison_to_vision_only": "This is a vision-only baseline; FSD outperforms it on visual-trace error and downstream qualitative scores.",
            "temporal_dynamics": "No.",
            "dimensionality_analysis": "No.",
            "uuid": "e1920.1"
        },
        {
            "name_short": "RoboPoint",
            "name_full": "RoboPoint",
            "brief_description": "A vision-language model specialized for spatial affordance prediction (points/boxes) used as a baseline for object/region reference and VABench-Point comparisons.",
            "citation_title": "Robopoint: A vision-language model for spatial affordance prediction for robotics.",
            "mention_or_use": "use",
            "model_name": "RoboPoint",
            "model_description": "A 13B vision-language model tailored to predict spatial affordance points/boxes from image+instruction; used for object-free-space localization.",
            "pretraining_type": "vision-language pretraining and task-specific instruction tuning on affordance/object reference datasets (per cited work).",
            "pretraining_data_description": "Trained with datasets containing object locations and affordance annotations (e.g., Where2Place and related embodied datasets), with labels for target regions/points.",
            "target_task_name": "spatial affordance prediction / object-free-space reference",
            "target_task_description": "Predict target bounding boxes or points in images given natural-language instructions; discrete outputs used by planners for manipulation.",
            "semantic_alignment": "Designed for affordance semantics; however, the paper reports RoboPoint often makes incorrect predictions on tasks requiring deeper spatial reasoning.",
            "performance_with_language_pretraining": "Reported: VABench-Point accuracy = 19.09% (substantially lower than FSD 61.82%); RoboRef object reference ≈ 49.8% (Table 2).",
            "performance_without_language_pretraining": "Not provided.",
            "sample_efficiency_comparison": "Not provided.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No.",
            "embedding_space_analysis": "No.",
            "action_grounding_evidence": "Produces affordance outputs (points/boxes) that can guide planners, but poorer zero-shot performance and inability to predict full traces limit action grounding evidence.",
            "hierarchical_features_evidence": "No.",
            "transfer_conditions": "Per paper, works better on simpler, direct instructions and where training data distribution matches test tasks; struggles on complex spatial reasoning.",
            "novel_vs_familiar_objects": "Paper implies lower generalization to unseen tasks/objects; no fine-grained numbers provided.",
            "zero_shot_or_few_shot": "Evaluated zero-shot; performance lower than FSD.",
            "layer_analysis": "No.",
            "negative_transfer_evidence": "Not quantified, but comparatively lower accuracy suggests limitations transferring to complex or novel embodied tasks.",
            "comparison_to_vision_only": "Compared primarily to VLM-based approaches; FSD (VLM+SrCoT+self-consistency) outperforms RoboPoint by large margin on VABench-Point.",
            "temporal_dynamics": "No.",
            "dimensionality_analysis": "No.",
            "uuid": "e1920.2"
        },
        {
            "name_short": "RoboBrain",
            "name_full": "RoboBrain",
            "brief_description": "A large unified model for robotic manipulation that can generate visual traces (agent-centric) and is compared qualitatively and quantitatively to FSD in the paper's analysis.",
            "citation_title": "Robobrain: A unified brain model for robotic manipulation from abstract to concrete.",
            "mention_or_use": "use",
            "model_name": "RoboBrain",
            "model_description": "A multimodal model intended to unify manipulation reasoning and action representations; generates agent-centric visual traces according to the citation and is used as a qualitative baseline.",
            "pretraining_type": "Multimodal pretraining with robotics/manipulation datasets (details in RoboBrain paper — not exhaustively presented here).",
            "pretraining_data_description": "Likely contains manipulation demonstrations and hierarchical annotations; the current paper does not detail RoboBrain's pretraining corpus.",
            "target_task_name": "visual trace generation and robotic manipulation",
            "target_task_description": "Generates sequences representing manipulation trajectories; in RoboBrain's case often agent-centric (robot-centric) traces; evaluated qualitatively against FSD for accuracy and generalizability.",
            "semantic_alignment": "Produces manipulable traces but tends to be tied to agent representation; the paper reports this limits generalization to images without robot embodiments.",
            "performance_with_language_pretraining": "No numeric comparison provided in main text; qualitative statement: FSD's traces have higher accuracy under zero-shot conditions compared to RoboBrain.",
            "performance_without_language_pretraining": "N/A in this paper.",
            "sample_efficiency_comparison": "Not reported.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No.",
            "embedding_space_analysis": "No.",
            "action_grounding_evidence": "RoboBrain generates traces (evidence of grounding) but tends to be agent-centric, making cross-embodiment grounding weaker versus FSD's object-centric traces.",
            "hierarchical_features_evidence": "No.",
            "transfer_conditions": "Per paper, agent-centric generation reduces transfer when images lack explicit robot embodiment; FSD's object-centric design is more robust across heterogeneous ontologies.",
            "novel_vs_familiar_objects": "Not detailed.",
            "zero_shot_or_few_shot": "Compared under zero-shot; FSD reported higher zero-shot accuracy.",
            "layer_analysis": "No.",
            "negative_transfer_evidence": "Paper suggests RoboBrain's agent-centric outputs can negatively impact transfer across heterogeneous scenarios, but no numerical magnitude is given.",
            "comparison_to_vision_only": "Not directly compared to vision-only models here.",
            "temporal_dynamics": "No.",
            "dimensionality_analysis": "No.",
            "uuid": "e1920.3"
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o",
            "brief_description": "A closed-source multimodal large language model (MLLM) used in this paper as a high-performance reference for spatial reasoning and to generate CoT exemplars; it provides strong general visual reasoning but limited coordinate-precise outputs for embodied tasks.",
            "citation_title": "GPT-4o system card.",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Closed-source multimodal LLM capable of image+text understanding and chain-of-thought generation; used as a performance reference and to generate CoT reasoning templates in dataset construction.",
            "pretraining_type": "Large-scale multimodal pretraining on web image-text and language corpora (proprietary).",
            "pretraining_data_description": "Broad internet-scale image-text pairs and web text; contains object descriptions and general spatial language but not discrete coordinate supervision.",
            "target_task_name": "spatial reasoning and visual-aid prediction",
            "target_task_description": "Evaluated on general spatial reasoning benchmarks and VABench; constrained in producing exact numeric coordinate outputs so bounding-box outputs are sampled for point evaluations.",
            "semantic_alignment": "Strong high-level semantic/world knowledge alignment but lacks native training for coordinate-grounded outputs; the paper highlights that GPT-4o cannot directly output reliable point coordinates without prompting workarounds.",
            "performance_with_language_pretraining": "Reported benchmarks: example VABench-VisualTrace RMSE ≈ 128.32, GPT-Score ≈ 4.37 (lower than FSD); spatial reasoning strong but lower precise affordance/trace accuracy than FSD.",
            "performance_without_language_pretraining": "N/A.",
            "sample_efficiency_comparison": "No.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No per-layer attention analysis reported in this paper.",
            "embedding_space_analysis": "No.",
            "action_grounding_evidence": "Limited in this context: GPT-4o demonstrates strong semantic reasoning but struggles to produce grounded coordinate outputs needed for direct manipulation; performs worse on VABench metrics compared to FSD.",
            "hierarchical_features_evidence": "No.",
            "transfer_conditions": "General-purpose reasoning transfers broadly but does not directly map to coordinate-level embodied actions; requires pipelines like FSD's SrCoT and alignment to bridge the gap.",
            "novel_vs_familiar_objects": "Not detailed.",
            "zero_shot_or_few_shot": "Evaluated zero-shot; output formatting limitations reduce effectiveness for embodied tasks.",
            "layer_analysis": "No.",
            "negative_transfer_evidence": "No explicit quantified negative transfer, but inability to generate exact coordinates limits direct application to manipulation.",
            "comparison_to_vision_only": "Not applicable directly; acts as a strong multimodal baseline but lacks coordinate grounding.",
            "temporal_dynamics": "No.",
            "dimensionality_analysis": "No.",
            "uuid": "e1920.4"
        },
        {
            "name_short": "ASMv2",
            "name_full": "ASMv2",
            "brief_description": "An open-source vision-language model and instruction-tuning base used by FSD as initialization and also evaluated as a baseline on spatial reasoning benchmarks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ASMv2",
            "model_description": "Open-source VLM used for visual instruction tuning; in FSD it served as a starting point / baseline (13B) and contributed to training data/architecture choices; in experiments ASMv2-13B is a comparative baseline on spatial benchmarks.",
            "pretraining_type": "Vision-language pretraining with instruction-tuning (per cited prior work), used as VLM backbone.",
            "pretraining_data_description": "Large mixed visual instruction datasets (ASMv2 training data referenced) with some spatial reasoning instances; not specifically coordinate-grounded.",
            "target_task_name": "spatial reasoning and instruction following (benchmarks)",
            "target_task_description": "Evaluated on five spatial-reasoning benchmarks (CVBench, BLINK, CRPE, SAT, EmbSpatial-Bench) and VABench; not primarily used for end-to-end manipulation in this paper but serves as comparative VLM.",
            "semantic_alignment": "Moderate; trained with visual instruction data but lacks FSD's SrCoT/self-consistency coordinate binding specifics.",
            "performance_with_language_pretraining": "Reported but lower than FSD on spatial benchmarks (Table 1 shows ASMv2 performance below FSD across multiple subtasks).",
            "performance_without_language_pretraining": "N/A.",
            "sample_efficiency_comparison": "No.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No.",
            "embedding_space_analysis": "No.",
            "action_grounding_evidence": "Limited in this paper: ASMv2 is used as a reasoning baseline but does not include the explicit coordinate-binding and affordance-generation mechanisms central to FSD.",
            "hierarchical_features_evidence": "No.",
            "transfer_conditions": "Per authors, general VLMs like ASMv2 require augmented datasets and specialized mechanisms (SrCoT, self-consistency) to transfer effectively to embodied coordinate-based tasks.",
            "novel_vs_familiar_objects": "Not detailed.",
            "zero_shot_or_few_shot": "Evaluated zero-shot on spatial benchmarks; lower capability than FSD for embodied visual-aid generation.",
            "layer_analysis": "No.",
            "negative_transfer_evidence": "Paper argues general VLMs without coordinate alignment suffer limited zero-shot performance for manipulation.",
            "comparison_to_vision_only": "ASMv2 is a multimodal baseline; FSD outperforms it after targeted training for spatial reasoning and visual-aid generation.",
            "temporal_dynamics": "No.",
            "dimensionality_analysis": "No.",
            "uuid": "e1920.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Robopoint: A vision-language model for spatial affordance prediction for robotics.",
            "rating": 2
        },
        {
            "paper_title": "Robobrain: A unified brain model for robotic manipulation from abstract to concrete.",
            "rating": 2
        },
        {
            "paper_title": "Rt-2: Vision-language-action models transfer web knowledge to robotic control.",
            "rating": 2
        },
        {
            "paper_title": "Llarva: Vision-action instruction tuning enhances robot learning.",
            "rating": 2
        },
        {
            "paper_title": "Where2act: From pixels to actions for articulated 3d objects.",
            "rating": 1
        },
        {
            "paper_title": "Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies.",
            "rating": 2
        },
        {
            "paper_title": "SpatialVLA",
            "rating": 1
        }
    ],
    "cost": 0.026917749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>From Seeing to Doing: Bridging Reasoning and Decision for Robotic Manipulation
27 May 2025</p>
<p>Yifu Yuan yuanyf@tju.edu.cn 
Tianjin University</p>
<p>Haiqin Cui 
Tianjin University</p>
<p>‡ Yibin Chen 
Tianjin University</p>
<p>Zibin Dong 
Tianjin University</p>
<p>Fei Ni 
Tianjin University</p>
<p>Longxin Kou 
Tianjin University</p>
<p>Jinyi Liu 
Tianjin University</p>
<p>Pengyi Li 
Tianjin University</p>
<p>Yan Zheng 
Tianjin University</p>
<p>Jianye Hao jianye.hao@tju.edu.cn 
Tianjin University</p>
<p>Tianjin University</p>
<p>From Seeing to Doing: Bridging Reasoning and Decision for Robotic Manipulation
27 May 202589AE693071DAE6204E1CBF78E58195A3arXiv:2505.08548v2[cs.RO]General Robotic ManipulationVLM ReasoningSpatial Chain-of-ThoughtSpatial VLMs
Achieving generalization in robotic manipulation remains a critical challenge, particularly for unseen scenarios and novel tasks.Current Vision-Language-Action (VLA) models, while building on top of general Vision-Language Models (VLMs), still fall short of achieving robust zero-shot performance due to the scarcity and heterogeneity prevalent in embodied datasets.To address these limitations, we propose FSD (From Seeing to Doing), a novel vision-language model that generates intermediate representations through spatial relationship reasoning, providing fine-grained guidance for robotic manipulation.Our approach combines a hierarchical data construction pipeline for training with a self-consistency mechanism that aligns spatial coordinates with visual signals.Through extensive experiments, we comprehensively validated FSD's capabilities in both "seeing" and "doing", achieving outstanding performance across 8 benchmarks for general spatial reasoning and embodied reference abilities, as well as on our proposed more challenging benchmark VABench.We also verified zero-shot capabilities in robot manipulation, demonstrating significant performance improvements over baseline methods in both SimplerEnv and real robot settings.Experimental results show that FSD achieves 40.6% success rate in SimplerEnv and 72% success rate across 8 real-world tasks, outperforming the strongest baseline by 30%.More visualizations and datasets are available on website.</p>
<p>Introduction</p>
<p>A driving force behind robotics research is the pursuit of generalization: creating agents capable of versatile action across diverse robotic platforms, extending beyond familiar tasks, objects, and environments while adapting to dynamic visual inputs.Current approaches Kim et al. (2024), Brohan et al. (2023) leverage pre-trained Vision-Language Models (VLMs) and transform them into Vision-Language-Action Models (VLAs) using large-scale embodied datasets.This enables systems to interpret natural language instructions and generate robotic manipulation actions Black et al. (2024).The intention is to capitalize on the generalization capabilities of VLMs pre-trained on internet-scale data, with the hope that resulting VLAs will adapt to novel scenarios involving unseen objects and tasks.However, empirical evidence Zheng et al. (2024), Zawalski et al. (2024), Liu et al. (2024c) demonstrates that directly applying the generalization power of VLMs falls short of achieving strong zero-shot performance on completely novel tasks.coverage and diversity prevent robust zero-shot generalization.Additionally, robotic embodiment heterogeneity Wang et al. (2024) causes significant variation in VLA outputs, making end-to-end supervised learning from vision and language to diverse action outputs a potentially unrealistic path toward generalization.</p>
<p>Large Language Model</p>
<p>We present a novel pipeline addressing generalization challenges in robotic manipulation.Our approach leverages VLMs' visual understanding capabilities, augmented with step-by-step reasoning to extract unified mid-level structural representations independent of robot embodiment.This representation is key to generalizing learning across diverse physical interactions and dynamic behaviors.Specifically, this mid-level representation includes spatial affordance boxes/points and visual traces, each represented as marked coordinates within visual images.These visual aids provide expressive yet compact spatial information that enables effective reasoning and decisions, overcoming both scarcity and heterogeneity limitations.We introduce FSD (From Seeing to Doing), a model that generates visual intermediate representations through spatial reasoning (Fig. 1).FSD comprises three key components: (1) Spatial Relationship-Focused Visual Chain-of-Thought (SrCoT), which conducts multi-step reasoning anchored by object coordinates and spatial relationships, treating visual aid generation as a reasoning process;</p>
<p>(2) A hierarchical data construction pipeline combining large-scale embodied datasets with common sense data, establishing a weak-to-strong capability enhancement training process; and (3) A self-consistency mechanism that aligns understanding and generation by binding spatial coordinates with specific visual signals.To evaluate the accuracy and generalization ability of visual aids generated in complex scenes, we also manually annotated 300 images from both real-world and simulation tasks across various scenarios, forming a Visual Aids Generation Benchmark (VABench).Through carefully crafted training and evaluation methods, FSD can achieve precise generation of visual aids in different scenarios, then the robot follows spatial affordances and visual traces through simple planning methods to complete action execution.</p>
<p>FSD generalizes effectively to new instructions, and scenes through enhanced reasoning abilities.Our contributions include: (1) A novel paradigm bridging VLM reasoning with embodied decisions via visual aids;</p>
<p>(2) The SrCoT method enabling multi-step reasoning for visual aid generation and guiding zero-shot manipulation;</p>
<p>(3) Our weak-to-strong spatial reasoning and visual aids datasets, along with VABench, a manually annotated challenging benchmark for visual aids generation; (4) Superior performance across 8 benchmarks in spatial reasoning, free space reference, and visual aids generation, with zero-shot deployment achieving 40.6% success in SimplerEnv and 72% in 8 real-world tasks, outperforming RoboPoint baseline by 30%.</p>
<p>Related Work</p>
<p>Spatial Understanding and Reasoning with VLMs Spatial understanding and reasoning Liu et al. (2023b), Song et al. (2024), Du et al. (2024), Liao et al. (2024), Ray et al. (2024) require VLMs to infer spatial information beyond 2D RGB images, a capability crucial for embodied AI applications such as navigation Song et al. ( 2024), Hong et al. (2023), Li et al. (2024b) and manipulation Yuan et al. (2024b).Recent advances include SpatialVLM Chen et al. (2024), which explicitly incorporates spatial primitives and coordinate systems to enhance geometric reasoning capabilities.Similarly, SpatialRGPT Cheng et al. (2024) and SpatialBot Cai et al. (2024) improve spatial capabilities through more precise spatial relationship modeling.FSD specifically targets embodied manipulation scenarios by enhancing spatial reasoning capabilities through novel SrCoT mechanisms and sophisticated self-consistency alignment techniques.</p>
<p>Visual Chain-of-thought Reasoning</p>
<p>The integration of Chain-of-thought (CoT) Wei et al. (2022) and its variants Zhang et al. (2022), Yao et al. (2023bYao et al. ( , 2024) ) (2024).In contrast, FSD uniquely integrates visual-spatial reasoning using spatial relationship graphs as reasoning anchors.2025) demonstrating its utility in robotic manipulation tasks.Our key insight is that the scarcity of embodied data fundamentally limits purely data-driven visual aid prediction approaches, impeding zero-shot generalization to unseen scenarios.Therefore, we employ a reasoning-driven paradigm to activate general spatial abilities and enhance generalization.</p>
<p>Visual Aids Empowered Robotic Manipulation</p>
<p>Bridging Reasoning and Decision through Visual Aids Generation</p>
<p>To harness VLMs' visual perception capabilities for cross-domain generalization, we introduce FSD (From Seeing to Doing)-a model exhibiting robust spatial reasoning and generation abilities.We first establish visual aids through spatial affordances and visual traces, then develop Spatial Relationship-Focused CoT (SrCoT), which leverages object-centric coordinates and their spatial relationships as reasoning anchors.Supporting this approach requires precise spatial understanding and complex instruction-following capabilities to generate coordinate representations.We implement a progressive weak-to-strong data construction pipeline across five capability levels, complemented by a self-consistency alignment mechanism that enhances understanding and generation abilities.</p>
<p>Definition of Visual Aids</p>
<p>Put the sushi into the silver pot  .., T} represent ordered coordinate sequences that describe manipulation trajectories, where T denotes the sequence length.These traces enable complex instruction execution, cross-embodiment transfer, and collision avoidance.We implement these representations in 2D rather than 3D due to limited high-quality 3D data availability Zhang et al. (2024).Our object-centric rather than agent-centric visual aids approach effectively circumvents the limitations of heterogeneous embodied data while leveraging general visual datasets without robots, thus enabling robust generalization to novel scenarios and tasks.</p>
<p>Spatial Relationship-Focused Visual Chain-of-thought</p>
<p>To enable VLMs to generate spatial visual aids, a direct approach is supervised fine-tuning Niu et al. (2024), Yuan et al. (2024b), Li et al. (2024c), Zawalski et al. (2024) using these aids as a new action space or employing generative models Shridhar et al. (2024), Xu et al. (2024).However, the heterogeneity and scarcity of embodied datasets limit this method.Directly aligning RGB images with coordinate points is challenging and prone to overfitting, hindering generalization.How can we stimulate VLMs' spatial reasoning abilities to guide the generation rather than merely relying on extensive demonstrations?Inspired by human cognition (Fig. 3 (Top)), when executing tasks like "putting broccoli into a pot," humans first locate relevant objects, then plan movement paths based on relative positions while considering feasibility and obstacles.During this process, humans construct reasoning chains, repeatedly referencing object positions and establishing spatial relationships.</p>
<p>Based on these considerations, we introduce Spatial Relationship-Focused Visual Chain-of-thought (SrCoT).This approach guides VLMs to generate visual aids through structured reasoning based on spatial relationship graphs.SrCoT consists of two essential phases: 1 Description: We generate object-centric region captions establishing a task-relevant spatial relationship graph where nodes represent objects with their coordinates and edges denote relative relationships (above, below, left, right, etc.). 2 Reasoning: Using the spatial relationship graph as anchor points, we determine start and end coordinates through object references and free space reasoning, then iteratively derive intermediate points with explicit logical connections between steps.Thus, we prescribe a templated reasoning path for VLMs, enabling FSD to perform analogical reasoning in the spatial domain.While VLMs struggle to directly map future actions to image coordinates, our method leverages known object relationships as reference points for multi-hop analysis, simplifying the reasoning process.Fig. 3 (Bottom) demonstrates a complete reasoning sequence.This step-by-step SrCoT approach, though powerful, fundamentally depends on precise spatial understanding capabilities.To improve the stability and reliability of reasoning paths and reduce model hallucinations, SrCoT requires the model to generate coordinates in a specified format and bind them to objects while performing object-centric reasoning Wang et al. (2025), Li et al. (2024f).We use <ref> to mark objects, and <point> and <box> to mark points and boxes, respectively, ensuring that each object is strictly bound to its corresponding coordinates.This explicit visual-spatial coordinate alignment enhances the FSD 's understanding of the spatial positions and relationships of objects.All coordinates are treated as text and are discretized as integers normalized between 0 and 999.</p>
<p>Weak-to-Strong Capability Dataset Construction Pipeline</p>
<p>The SrCoT mechanism demands enhanced capabilities from VLMs, including precise reference grounding, spatial understanding, and complex instruction-following capabilities (directly predicting future point trajectories) where mainstream VLMs Liu et al. (2024b), Lin et al. (2024b), Ray et al. (2024), Du et al. (2024) show limitations.Consequently, we designed a weak-to-strong data construction pipeline to progressively develop these abilities.We only generate relative depth sorting data to infer positional relationships, so the accuracy requirement is not high.To improve data quality, in particular, we selected objects with a relative depth gap of 20% for subsequent generation.Afterward, we randomly sample spatial relationship graphs to construct spatial reasoning QA.We provide image captions, object coordinates, and relationships as a context for GPT4o to create complex QAs (Level 3 Dataset).</p>
<p>A core aspect of the FSD dataset is the visual aids generation.We employ a simple method with successful human demonstrations from embodied datasets and infer the process from the results.Spatial affordance represents the designated completion area for manipulation tasks.To create spatial affordance labels (Level 4 Dataset), we extract the manipulated object's final position from the terminal frame, combine it with reference object positioning, calculate the precise affordance region, and re-render this information onto the initial frame.For visual trace generation (Level 5 Dataset), we employ a two-stage approach: first applying self-supervised keypoint extraction Huang et al. (2024) to identify grasp points on manipulated objects, then utilizing Cotracker Karaev et al. (2024) to capture temporal dynamics from human demonstrations, subsequently projecting these trajectories onto the initial frame.Throughout this process, we employed strict rule-based filters and continually validated our approach against manually annotated test sets, iteratively refining our filtering criteria based on empirical feedback to ensure the resulting dataset met our quality requirements.The dataset presentation, data filtering process, and prompts used to generate the data are provided in Appendix A.</p>
<p>Self-Consistent Alignment for Spatial Understanding and Generation</p>
<p>High-quality SFT datasets enable VLMs to generate visual aids Yuan et al. (2024b), yet these models struggle to understand the physical meaning of such annotations since coordinate spaces never appeared in pretraining data.The alignment between image coordinates and actual spatial positions presents a significant challenge.Therefore, we propose a self-consistency mechanism to further align FSD capabilities in spatial understanding and generation.We frame generation tasks inversely as understanding problems: if the forward task requires inferring visual trace τ from an image X v and task instruction X q , i.e. (X v , X q ) → τ, we construct the inverse task of predicting possible instructions given an image and visual traces (X v , τ) → X q .This bidirectional approach helps the model comprehend spatial coordinates' meanings and aligns coordinate space with image-text modalities, unifying visual aids as both understanding and generation signals while enhancing FSD spatial reasoning capabilities.</p>
<p>Training and Action Execution of FSD</p>
<p>Training: We follow the instruction tuning pipeline proposed by LLaVA-1.5 Liu et al. (2023cLiu et al. ( , 2024b)).As shown in Fig. 1, FSD's architecture comprises an image encoder (CLIP-ViT-L-336px Gao et al. ( 2024)), a linear projector, a language tokenizer, and a LLM (Vicuna-13B Zheng et al. (2023b)).The image encoder processes images into tokens.These visual tokens are then projected into the same embedding space as language tokens through a two-layer linear projector.Only the projector and LLM weights are updated during fine-tuning while the vision encoder and tokenizer remain frozen.We built upon ASMv2 Wang et al. (2025)  * .For relatively fixed scenes, we may also use predetermined grasp poses.Subsequently, we optimize the path trajectory using gradient descent-based interpolation, generating complete motion trajectories in SE(3) space, enabling the robotic arm to follow the 3D visual trajectory.When using only spatial affordance, we utilize CuRobo Sundaralingam et al. (2023) as the motion planner to determine execution trajectories T based on the target position .More details are provided in Appendix C. Unlike methods such as LLARVA Niu et al. (2024) and EmbodiedCOT Zawalski et al. (2024) which also utilize visual auxiliary aids, FSD transforms prediction tasks into reasoning tasks, better leveraging visual-spatial common knowledge without requiring scenario-specific fine-tuning.</p>
<p>Visual Aids Generation Benchmark</p>
<p>Few datasets exist for evaluating visual aid generation, Where2Place Yuan et al. (2024b) provides 100 real-world images from homes and offices but is limited to direct and simple language instructions.And no benchmarks for trajectory prediction.To address this gap, we propose the Visual Aids Generation Benchmark (VABench).we manually annotated 300 problems from real-world and simulation datasets (OXE O 'Neill et al. (2023), BridgeData Walke et al. ( 2023), Droid Khazatsky et al. (2024) and Libero Liu et al. (2023a)), requiring models to infer visual aids given only natural language instructions similar to everyday human commands.For spatial affordance (VABench-Point), we measure the proportion of points falling within target regions.For models that only output bounding boxes, we sample uniformly within boxes.For visual trace (VABench-VisualTrace), we compute MAE and RMSE between ground truth τ = {x t | t = 1, 2, ..., T} and
predictions τ = { xt | t = 1, 2, ..., T}: MAE = 1 T ∑ T t=1 ∥x t − xt ∥, RMSE = √ 1 T ∑ T t=1 ∥x t − xt ∥ 2 .
We interpolate when trajectory lengths differ and normalize coordinates to a 1000×1000 space for consistent evaluation.Since multiple valid solutions exist for each instruction, we simulated human evaluation standards by establishing detailed assessment criteria and added metrics with MLLM-based qualitative scoring (1-10) of visualized trajectories, named GPT Score.We provide detailed dataset information and evaluation procedures in Appendix D.</p>
<p>Experiments</p>
<p>We evaluated FSD across two dimensions: Seeing and Doing.For Seeing, we tested its general spatial reasoning and visual aids generation capabilities.For Doing, we conducted zero-shot manipulation experiments in both SimplerEnv Li et al. (2024e) simulation and real-world xArm robotic platforms to assess its practical generalization performance.</p>
<p>Evaluation of Spatial Understanding and Reasoning Capabilities</p>
<p>Benchmarks and Baselines.General Spatial Reasoning Capabilities: We evaluated general spatial reasoning capabilities using five popular benchmarks: CVBench Tong et al. (2024), BLINK Fu et al. (2024), CRPE Wang et al. (2025), SAT Ray et al. (2024), andEmbSpatial-Bench Du et al. (2024).These benchmarks encompass 15 subtasks measuring various spatial competencies including counting, spatial relationships, distance estimation and so on.We included two leading closed-source models: GPT-4o Hurst et al. (2024) and GPT-4V as performance reference.Subsequently, we conducted comparative analyses against other open-  2025)).We used the proportion of predicted points within specified objects/regions as the accuracy metric.For models without point output support, we asked models to output bounding boxes of target objects/free space regions, then sampled evenly within these bounding boxes.Spatial Affordance and Visual Trace Capabilities: We utilized our VABench to evaluate the capabilities.We found very few models with this capability for Visual Trace prediction, so we trained an end-to-end prediction baseline model using a pre-trained DINOv2 Oquab et al. ( 2023) encoder coupled with a multi-layer transformer Vaswani et al. (2017) architecture to predict visual trajectories, trained on the same visual trajectory data, named DINOv2 Predictor.We conducted this comparison to demonstrate the advantages of our reasoning-based FSD approach.More detailed descriptions of benchmarks and baselines are provided in Appendix E.</p>
<p>FSD exhibits superior general spatial reasoning capabilities.As shown in Table 1, FSD achieves a top ranking of 1.3 across 18 subtasks from 5 spatial benchmarks, surpassing other 13B VLMs and competing with GPT-4o.FSD demonstrates particular strengths in 3D depth perception (88.0%), 3D distance estimation (86.7%), and spatial relationship (78.3%).This confirms the effectiveness of our data mixing strategy and capability construction in enhancing spatial reasoning abilities.We believe stronger spatial foundational capabilities enable enhanced embodied perception.<Description></p>
<p>The image shows a <ref>red spoon</ref><box> [[476, 533, 751, 622]]</box> placed next to a <ref>orange towel</ref><box> [[163, 329, 384, 488]]</box>.In the middle, there is a metal bowl, and to the left of the bowl, there is a yellow chicken leg.</Description> <Reasoning> The task is to place the red spoon at <point> [[488, 576]]</point> on top of the green towel at <box> [[163, 329, 384, 488]]</box>.The current position of the spoon is above the towel, so it needs to be moved to overlap with the towel's position <box> [[163, 329, 384, 488]]</box>.</Reasoning> <Answer> To complete the task, the target position for the red spoon is <box> [[166, 371, 418, 435]]</box>.The new position can also be roughly defined by the following several free points <point> [[201, 402]</p>
<p>FSD demonstrates breakthrough performance in visual aid generation. As shown in Table Table 3b, FSD significantly outperforms all baseline methods in generating precise spatial affordances and visual</p>
<p>The image shows a <ref>green plate</ref><box> [[264, 324, 504, 516]]</box> …… The carrot<box> [[553, 506, 751, 844]]</box> is currently <pred>right</pred> of the green plate.</p>
<p>You are currently a robot performing robotic manipulation tasks.The task instruction is: put carrot on plate.Use 2D points to mark the manipulated object-centric waypoints to guide the robot to successfully complete the task.</p>
<p>Description:</p>
<p>The start by identifying the current position of the carrot at <point> [[680, 708]]</point>.</p>
<p>Reasoning Step 1:</p>
<p>First, lift the carrot slightly upwards to <point> [[663, 663]]</point> to clear any obstacles.</p>
<p>Reasoning</p>
<p>Step 2: … Finally, place the carrot on the plate at <point> [[390, 416]]</point>, within the final bounding box <box> [[208, 437, 440, 521]]</box>.</p>
<p>Reasoning Step n:</p>
<p>The visual trace for placing the orange carrot on the green plate is <point> [[680, 708] traces.For VABench-Point, FSD achieves 61.82% accuracy-three times higher than RoboPoint (19.09%), while for VABench-VisualTrace, FSD demonstrates significantly lower error rates and better GPT Score compared to the DINOv2 Predictor.Ablation studies confirm the critical contributions of both SrCoT and self-consistency alignment, validating our reasoning-based approach where step-by-step spatial analysis enables more accurate predictions than purely data-driven methods.We provide detailed comparisons with RoboBrain Ji et al. ( 2025) in Appendix I.</p>
<p>Visualization of visual aids generated by FSD</p>
<p>We present examples of generation in Fig. 5. FSD is able to adapt to diverse scenes, perspectives, and tasks, and the three types of visual traces generated can well match the task instructions.We also presented the reasoning process of FSD in Fig. 9, where the CoT provides clear guidance for the results.Subsequently, we selected several specific examples to showcase the complete outputs of FSD in Fig. 8 and Fig. 7.More visualizations are in Appendix F.</p>
<p>Evaluation of the Decision Capability</p>
<p>SimplerEnv Simulation SimplerEnv Li et al. ( 2024e) is a simulation environment specifically designed for evaluating real-world robotic manipulation.It provides a standardized platform, emphasizing reproducibility with real-world scenarios.We utilized FSD to generate visual traces and perform zero-shot deployment on the WidowX robotic arm.We compared its performance against state-of-the-art general manipulation VLA, including Octo  et al. (2025).The results are presented in Table 4. FSD surpasses the baseline in most tasks without FT with a 40.6% success rate, demonstrating the strong generalization ability of FSD.Notably, FSD achieves the best zero-shot average performance and is competitive with specially fine-tuned VLA models.This superior generalization capability demonstrates that visual aids representations provide better zero-shot transfer compared to direct action prediction approaches, bridging the gap between perception and control more effectively.</p>
<p>Real World Robot Evaluation</p>
<p>As shown in Fig. 10, we conducted zero-shot tests with FSD on an xArm 6 robot across 8 tabletop manipulation tasks.The setup included an Intel RealSense L515 LiDAR camera.To test the capabilities of different visual aids, we used visual trace for the sponge and folding tasks, while affordance points were used for other tasks.We compared against RoboPoint and GPT-4o baselines, which were limited to predicting only start/end points rather than full trajectories.Robopoint often makes incorrect predictions in tasks involving spatial understanding.Under zero-shot conditions, FSD achieved 72% success rate, outperforming the strongest baseline by more than 30%.Notably, FSD successfully completed complex tasks with visual trace generation, e.g.cloth folding, which was beyond baseline capabilities.Full results are presented in Fig. 10 and visualization of control tasks executed using FSD in Fig. 6.We refer to Appendix G for detailed setup.</p>
<p>Conclusion</p>
<p>In this paper, we introduced FSD (From Seeing to Doing), bridging visual reasoning and robotic manipulation through intermediate spatial representations.Our approach addresses the challenges of data scarcity and embodiment heterogeneity through three key innovations: Spatial Relationship-Focused Visual Chain-of-  Level 2: Spatial Relationship</p>
<p>Human: Tell me about the items in your view and their relationships to each other.Answer the question with scene graph.GPT: <ref>front left burner</ref><box> [[597, 422, 817, 596]]</box> is positioned <pred>to the right of</pred><box> [[597, 422, 817, 596]]</box><box> [[171, 275, 446, 409]]</box> <ref>metal pot</ref><box> [[171, 275, 446, 409]]</box>.<ref>metal pot</ref><box> [[171, 275, 446, 409]]</box> appears <pred>to the left of</pred><box> [[171, 275, 446, 409]]</box><box> [[431, 474, 715, 608]]</box> <ref>green spatula</ref><box> [[431, 474, 715, 608]]</box>.<ref>metal pot</ref><box> [[171, 275, 446, 409]]</box> can be seen <pred>to the left of</pred><box> [[171, 275, 446, 409]]</box><box> [[519, 408, 600, 490]]</box> <ref>orange cheese slice</ref><box> [[519, 408, 600, 490]]</box>.2023) and Droid Khazatsky et al. (2024).Our methodology for extracting visual aids, specifically Affordance Boxes, Affordance Points, and Visual Traces, involves a multi-stage process leveraging state-of-the-art vision models.We also incorporate a rigorous data validation procedure to ensure high-quality output.</p>
<p>First, we acquire the initial and final frames of the video sequence.To determine the Affordance Box, we utilize GroundingDINO Liu et al. (2023d) and GroundedSam Ren et al. (2024) to detect the mask of the manipulated_object in the final frame.The bounding box of this detected object in the final frame defines the Affordance Box, representing the ultimate spatial location of the manipulated object.Subsequently, we extract Affordance Points.This involves performing an erosion operation on the mask of the manipulated_object obtained from the final frame.Erosion reduces the mask's area, facilitating the sampling of points that are more central or internal to the object, thereby mitigating the risk of sampling edge points.From this eroded mask, we uniformly sample 8 points, which constitute the Affordance Points.</p>
<p>For Visual Trace extraction, we begin by detecting the mask of the manipulated_object in the first frame.</p>
<p>From this mask, we sample 3 points, which serve as the initial query points for the CoTracker Karaev et al. (2024) model.Selecting multiple points enhances the robustness of trajectory tracking.The processed video sequence and these sampled points are then fed into the CoTracker model, which outputs the predicted trajectory for each query point across every frame of the video sequence.We calculate the total distance of each trajectory and select the longest trajectory as the representative trajectory for the manipulated_object.</p>
<p>This chosen trajectory is then subjected to cubic spline interpolation for smoothing.Finally, 8 equidistant points are uniformly sampled from the smoothed trajectory, forming the Visual Trace.</p>
<p>A critical aspect of this process is addressing potential prediction errors from the pre-trained visual models, such as incorrect object identification or incomplete tracking of object motion.To mitigate these issues, we implement stringent rule-based filtering using hyperparameters like size thresholds and trajectory length thresholds.Before annotating each dataset, we iteratively adjust these rules and conduct manual inspections on 100 examples.Only when the accuracy of these filtered results surpasses 95% do we deem the filtering rules robust enough to proceed with the full data generation pipeline.This meticulous validation process ensures the high quality of the generated data.</p>
<p>Following the generation of these visual aids, we also pre-generate the thinking processes for Chain-of-Thought (CoT) reasoning.We input templates, questions, and answers into GPT-4o, querying it to complete the thinking process.The complete query prompt is constructed accordingly:</p>
<p>Prompt and In-context Example for Completing Thinking Process</p>
<p>You are an AI visual assistant that can analyze a single image.You receive one image and corresponding caption, task instruction, manipulated object in the task and target place to finish the task, and bounding box position and relative position of these objects.Also, you receive the answer of points and bbox.Now, you need to generate a <Description>...</Description>, <Reasoning>...</Reasoning>, <Answer>...</Answer> format.<Description> First, using the provided caption, task instruction, describe the scene.If there are errors in the caption, please ignore them and do not point them out in your description.Instead of directly mentioning the bounding box coordinates, utilize this data to explain the scene using natural language with its bounding box in the format like "<ref>object</ref><box> [[x1, y1, x2, y2]]</box>".When mentioning the predicate between two objects, you should mention it in the format like "<pred>predicate</pred><box> [[x1, y1, x2, y2]]</box><box> [[x3, y3, x4, y4]]</box>", where "<box> [[x1, y1, x2, y2]]</box>" denotes the bounding box coordinates of the subject and "<box> [[x3, y3, x4, y4]]</box>" denotes the bounding box coordinates of the object.</Description> <Reasoning> According to the task instruction and the answer of points and bbox, provide a chain-of-thought, logical explanation of the problem.</Reasoning> <Answer> State the final bounding box answer and point answer in a clear and direct format here.Bounding box answer is in the format like "<box> [[x1, y1, x2, y2]]</box>".Point answer is in the format like "<point> [[x1, y1] 174,788,357]]</box> that is positioned on a table.It is situated <pred>above</pred><box> [[686, 174, 788, 357]]</box><box> [[135, 411, 852, 920]]</box> the <ref>top drawer</ref><box> [[135, 411, 852, 920]]</box> which is open.</Description> <Reasoning> The 7up can is above the top drawer now and should be placed into the top drawer.So the target position of the 7up can might be similar to the position of the drawer's interior.</Reasoning> <Answer> To complete the task, the target position is <box> [[569, 553, 651, 679]]</box>.The new position can also be roughly defined by the following several free points <point> [[603, 609], [593,637], [596,643], [615,596], [612,593], [590,637], [603,606], [615,631]]</point>, which can outline the position of the 7up can after it is placed in the top drawer.</Answer></p>
<p>Next, we present two examples from the dataset: one involves generating Affordance Boxes/Points (Level 4), and the other involves generating visual trace (Level 5).We consider Affordance Points as samples within the box; thus, generating the box first and then the points is a refinement process from coarse to fine granularity.Therefore, both types are generated for each data, and one can choose which to use as needed.In the task of generating visual trace, since we performed equidistant interpolation in advance, the visual trace in the dataset is fixed at 8 points.Consequently, models trained on such a dataset will also generate 8 points as the visual trace.</p>
<p>Level 4: Spatial Affordance Box/Points Generation Human: "You are currently a robot performing robotic manipulation tasks.Your task instruction: Moved the can to the left side of the green cloth.</p>
<p>Observe the image, use 2D points and bounding box to mark the target location where the manipulated object will be moved.In your answer, use <box> [[x1, y1, x2, y2]]</box> to present the bounding box of the target region, and use <point> [[x1, y1], [x2, y2], ...]</point> to mark the points of the free space."</p>
<p>GPT: "<Description> The image shows a <ref>white soup can</ref><box> [[544, 177, 700, 368]]</box> positioned to the right of a <ref>green towel</ref><box> [[244, 389, 664, 676]]</box> on a table.A small bowl containing a yellow object and a brush is next to the towel.</Description> <Reasoning> The task is to move the white soup can to the left side of the green towel.Currently, the can is to the right of the towel, so it needs to be repositioned to the left of the towel's current location.</Reasoning> <Answer> The target position for the white soup can is <box> [[250, 181, 400, 392]]</box>.The new position can also be roughly defined by the following points: <point> [[346, 248], [302,365], [377,251], [330,295], [357,291], [354,362], [329,355], [312,352]]</point>.</Answer>"</p>
<p>Level 5: Visual Trace Generation</p>
<p>Human: "You are currently a robot performing robotic manipulation tasks.Your task instruction: put the green spatula in the silver pot.</p>
<p>Observe the image, use 2D points to mark the manipulated object-centric waypoints to guide the robot to manipulate the object.Typically, the waypoints consist of an ordered sequence of eight 2D points.The format is <point> [[x1, y1], [x2, y2], ...]</point>."</p>
<p>GPT: "<Description> The image shows a <ref>green spatula</ref><box> [[762, 536, 856, 711]]</box> positioned to the right of an orange towel on a table.A <ref>silver pot</ref><box> [[0, 723, 242, 872]]</box> is located above a robotic arm.</p>
<p>Other objects, such as a gray can and a mushroom, are also present in the scene.</Description> <Reasoning> To move the green spatula into the silver pot, start by identifying the initial position of the spatula at <point> [[802, 613]]</point>.First, lift the spatula upwards to clear the table, moving to <point> [[774, 501]]</point>.Then, move it horizontally towards the left, passing through <point> [[685, 394]]</point>.Finally, lower the spatula into the silver pot, ending at <point> [[657, 401]]</point> within the target bounding box <box> [[225, 529, 407, 640]]</box>.</Reasoning> <Answer> The visual trace for placing the green spatula into the silver pot is <point> [[802, 613], [780,582], [774,501], [744,465], [685,394], [657,349], [668,354], [657,401]]</point>.</Answer>"</p>
<p>B. Training Details and Datasets</p>
<p>The training of FSD is a two-stage process, building upon the LLaVA Liu et al. (2024a) architecture through continued fine-tuning of ASMv2 Wang et al. (2025).In order to train a VLM with powerful spatiotemporal reasoning abilities that can also generate visual aids, we leveraged a comprehensive dataset of approximately 1.4M samples from various sources.Note: The position coordinate format used by FSD normalizes the image coordinates to a range of 0-999 after padding the image to a square shape.For all datasets mentioned below, the coordinates have been pre-processed in this manner.</p>
<p>Stage 1: Spatial Reasoning Enhancement The primary goal of the first stage is to enhance the model's spatial reasoning capabilities.This dataset is primarily composed of two parts: common-sense image QA and conversations data and spatial reasoning data.The inclusion of GeneralQA is crucial for FSD to maintain broad instruction-following abilities after fine-tuning.First, we selected some datasets and randomly sampled around 838k data from LLaVA-665k and ASMv2-4M instruction-following data.Next, we incorporated approximately 295k samples of spatial reasoning data from the LLaVA-OneVision Li et al.</p>
<p>(2024a), RoboPoint Yuan et al. (2024b), SpatialBot Cai et al. (2024) and SAT Ray et al. (2024) training datasets.Finally, the first three levels of FSD's collected embodied spatial reasoning datasets (250k samples) were also integrated into this training phase.A summary of the datasets used can be found in Table 5.</p>
<p>Stage 2: Visual Aids Generation and Understanding We specifically focused on training for visual aids generation.Here, "generation" refers to generating visual aids based on a given question, while "understanding" represents its inverse problem.We excluded some inverse problems with ambiguous semantics.The final total amount of data used is shown in Table 5.</p>
<p>Training Configuration</p>
<p>We used exactly the same hyperparameters in both stages of training.We utilized a global batch size of 128 and the AdamW optimizer, configured with β 1 = 0.9, β 2 = 0.999, and a weight decay coefficient of 0. The learning rate was set to 2 × 10 −5 .Both training stages employ a linear warmup for the first 3% of training steps, followed by a cosine decay strategy to a minimum learning rate of 0. We simultaneously train both the vision-language connector and the language model.The image resolution was set to 336 × 336, and the visual encoder remained frozen throughout the entire training process.In the first phase, we train for 1 epoch on the complete dataset, while in the second phase, we train for 3 epochs.The FSD model has approximately 13B trainable parameters and we conducted training using 8 A100 40G GPUs, with Stage 1 requiring approximately 72 hours and Stage 2 requiring 8 hours.</p>
<p>C. Details of Action Execution</p>
<p>When utilizing FSD for robotic manipulation tasks, we can select from various visual aids.As described in Fig. 2, these include spatial affordance boxes (ℬ), spatial affordance points (), and object-centric visual traces (τ).The choice of visual aids dictates the subsequent motion planning strategy.</p>
<p>Motion Planning with Spatial Affordances</p>
<p>For spatial affordance boxes (ℬ), the target point for manipulation is derived by sampling the center of the box.In the case of spatial affordance points (), a point is directly sampled.When relying on spatial affordance information, whether from boxes or points, we employ CuRobo Sundaralingam et al. (2023) as our motion planner.CuRobo is responsible for generating collision-free paths that guide the robot's end-effector to the inferred target affordance point.</p>
<p>Motion Planning with Object-Centric Visual Traces When leveraging object-centric visual traces (τ), the process involves mapping 2D visual traces into 3D space and then interpolating these discrete points to form a complete motion trajectory in SE(3) space.The detailed procedure is as follows:</p>
<p>We directly acquire 2D keypoint information, denoted as
k i = (u i , v i ) ∈ R
2 , where u i and v i represent the x and y coordinates of the i-th point in the image, for i ∈ [1, T].Initial depth information, d i ∈ R, is obtained from a depth camera.Using the Pinhole camera model Hartley and Zisserman (2004), we can transform these 2D keypoints into 3D Cartesian coordinates P i = (x i , y i , z i ) ∈ R 3 via:
s i ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ u i v i 1 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ = ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ f x 0 c x 0 f y c y 0 0 1 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ x i y i z i ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦
Here, s i is the normalized depth, calculated as s i = d i /depth_scale.The intrinsic camera parameters, f x , f y , c x , c y , and the depth scaling factor, depth_scale, are all camera-specific.</p>
<p>However, a naive use of the raw depth values d i obtained from the depth camera often results in trajectories that closely hug the object's surface, which is undesirable for robust robot manipulation.To address this, we formulate an optimization problem to address it.We fix the depth values for the start and end points of the path (d 1 and d T ) and optimize the intermediate depth values d 2∶T−1 to minimize the total Euclidean distance between consecutive points in Cartesian space.</p>
<p>The objective function for this optimization is:
di = arg min d 2∶T−1 ∑ i d(P i , P i+1 )
where d(P i , P i+1 ) represents the Euclidean distance between points P i and P i+1 .We employ a gradient descent method from scipy library to optimize this objective function.This refined approach allows for more robust and practical robot motion planning by addressing the limitations of raw depth data and providing a structured framework for integrating various visual aids.</p>
<p>D. Details of Visual Aids Generation Benchmark</p>
<p>To address the gap in evaluating visual assistance signal generation, we established VABench, a comprehensive benchmark.VABench comprises two distinct tasks: VABench-Point and VABench-VisualTrace, each featuring 300 meticulously hand-annotated questions.These tasks require models to infer visual auxiliary information solely from natural language instructions, mimicking everyday human commands.</p>
<p>For VABench-Point, we provide ground truth bounding boxes for each question.The model's performance is then assessed by calculating the proportion of predicted points that fall within the target region.For models that only output bounding boxes, we explored two scoring methods: Intersection Over Union (IOU) and uniformly sampling points within the predicted box.We ultimately opted for the latter approach, as the physical interaction between a robotic arm and an object in real-world tasks is fundamentally determined by specific point coordinates.For VABench-VisualTrace, we provide ground truth trajectories consisting of eight points.When the predicted trajectory length deviates from the ground truth, we employ interpolation to align their lengths.To ensure consistent evaluation across varying image resolutions, all coordinates are uniformly normalized to a range of 0 to 1000.Subsequently, we employ a combination of Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and GPT Score to comprehensively evaluate performance.</p>
<p>Here, we detail the design philosophy and evaluation procedure of the GPT Score.Given that multiple valid solutions can exist for each task instruction, relying solely on trajectory similarity to the ground truth as a scoring criterion can be one-sided.Therefore, we established detailed evaluation criteria to simulate realistic human assessment.Based on these rules, we introduced a visualized trajectory scoring method leveraging Multimodal Large Language Models (MLLMs), termed GPT-score.</p>
<p>Specifically, we designed an evaluation prompt to guide GPT-4.1 in assessing predicted object manipulation trajectories based on both task instructions and visual inputs.The prompt provides clear instructions and criteria, positioning the model as an expert evaluator in robotic manipulation and visual reasoning.Each evaluation instance consists of a task instruction and an accompanying image that visualizes the predicted trajectory, where a red circle indicates the start point and a blue diamond marks the end point.The model is instructed to assess the trajectory according to three key criteria: (1) task alignment and success, determining whether the predicted path correctly fulfills the instruction by starting and ending in appropriate locations;</p>
<p>(2) feasibility, evaluating the physical plausibility and smoothness of the motion; and (3) obstacle avoidance, considering whether the trajectory avoids potential collisions.The prompt emphasizes that completing the task correctly is the most important factor; any major deviation in goal achievement results in a low score, even if the trajectory appears smooth or feasible.Then, the model returns a structured response consisting of a numerical score from 1 to 10, along with a concise explanation.Scores are interpreted based on task success and quality: low scores (1-4) indicate failure to accomplish the task, mid-range scores (6-8) reflect successful but imperfect trajectories, and high scores (9-10) are reserved for trajectories that are both accurate and high-quality.This scoring scheme allows for nuanced, human-like evaluation that integrates both semantic understanding and visual reasoning.By leveraging this multimodal prompt framework, GPT-score enables a robust and interpretable evaluation process that aligns closely with human judgment, overcoming the limitations of purely geometric or distance-based metrics.The complete prompt is presented as follows:</p>
<p>Complete Prompt for GPT Score Metrics</p>
<p>You are an expert evaluator in robotic manipulation and visual reasoning.Your job is to assess the quality of predicted trajectories based on task instructions and visual inputs.-If the task is completed but the trajectory has issues (e.g., roughness, minor risk of collision), a score in the <strong>6-8</strong> range is appropriate.</p>
<p>-A <strong>score of 9-10</strong> should be given only when the trajectory clearly completes the task, with good start/end accuracy, smooth motion, and reasonable safety.</p>
<p>Based on these criteria, provide a single overall score from 1 (very poor) to 10 (excellent), reflecting how well the task is accomplished.</p>
<p>Respond strictly in the following format: Score: &lt;1-10&gt; Explanation: <brief justification></p>
<p>The task instruction is: {task_instruction} Please give your response.</p>
<p>G. Real World Experiment Results</p>
<p>In our real-world desktop manipulation tasks, we used an xArm 6 robotic arm for evaluation.This setup included an Intel RealSense L515 LiDAR camera and a force-torque sensor on the xArm to enable compliance control, which improved interaction with the environment.A computer running Ubuntu 24.04 and equipped with an NVIDIA GTX 1660 was directly connected to the arm and camera to execute low-level control policies.Notably, a single RealSense L515 depth camera was sufficient for task completion, especially when performing visual trace execution.This approach eliminated the need for object segmentation and 3D mapping; instead, we directly mapped 2D visual trajectories to 3D for execution, with no strict requirements on depth information accuracy.Demonstrations are available in Fig. 6 and on our website.where the manipulated object will be moved.In your answer, use <box> [[x1, y1, x2, y2]]</box> to present the bounding box of the target region, and use <point> [[x1, y1], [x2, y2], ...]</point> to mark the points of the free space.</p>
<p>H. Prompt for Using FSD Model</p>
<p>I. Comparison of FSD and RoboBrain</p>
<p>Both FSD and RoboBrain Ji et al. (2025) have the capability to generate visual trace.However, RoboBrain tends to produce agent-centric visual trace, whereas FSD generates object-centric visual trace.FSD adopts a taskcentric design principle, allowing it to perform effectively even in more heterogeneous ontological scenarios, including those that completely lack robotic arms in the image, thus exhibiting stronger generalizability.Due to the differences in the methods of generating visual trace, we conducted several sets of visual trajectory visualizations for qualitative analysis, as shown in Fig. 15.Under the same zero-shot setting, the visual trace generated by FSD has higher accuracy compared to those generated by RoboBrain, confirming that FSD's reasoning-based pipeline possesses greater generalizability when facing unknown tasks.</p>
<p>Figure 1 :
1
Figure 1: Overview of FSD.FSD unlocks visual aids reasoning and generation through Spatial Relationship-Focused CoT, demonstrating exceptional generalization capabilities that enable zero-shot robot manipulation and achieving remarkable performance across multiple benchmarks.</p>
<p>Extracting embodiment-agnostic visual aids to enhance training efficiency has emerged as a promising paradigm in robotic manipulation.Numerous studies Bharadhwaj et al. (2024), Wen et al. (2023), Xu et al. (2024), Zheng et al. (2024), Yuan et al. (2024a) have explored robotic policy learning based on visual traces, though most remain confined to specific tasks with cross-embodiment applicability.LLaRVA Niu et al. (2024) advances this field by predicting visual traces to better align visual and action spaces for robot learning, compiling an impressive large visual trace dataset, yet struggles to generalize to novel downstream tasks without task-specific fine-tuning.Spatial affordance represents another effective visual aid, with several works Yuan et al. (2024b), Mo et al. (2021), Qin et al. (2020), Song et al. (2024), Ji et al. (2025), Yang et al. (2025), Li et al. (</p>
<p>Figure 2 :
2
Figure 2: Diagrams of Visual Aid Types</p>
<p>Figure 3 :
3
Figure 3: Inspired by the process of human reasoning, FSD uses a spatial relationship graph as an anchor to derive a visual chain-of-thought reasoning process for visual trace generation.</p>
<p>Figure 4 :
4
Figure 4: FSD screens data from large-scale embodied datasets, generates ground truth spatial relationship graph.We finally collected 300K data for 10+ embodiments with 5-level capabilities.</p>
<p>Figure 5 :
5
Figure 5: FSD directly generates visual aids based on task instructions for novel tasks and scenarios.1st row: affordance bounding boxes; 2nd row: affordance points; 3rd and 4th rows: visual traces.</p>
<p>Figure 6 :Figure 7 :
67
Figure 6: Visualization of FSD executing tasks.The first six rows are real-world experiments, and the last four rows are from SIMPLEREnv.</p>
<p>Figure 8 :
8
Figure 8: An example and visualization of FSD for generating affordance box and points.</p>
<p>Figure 9 :Figure 10 :
910
Figure 9: Example of the FSD Reasoning Process FSD performs point-by-point reasoning and localization, ultimately generating a feasible visual trace.</p>
<p>Figure 11 :
11
Figure 11: RGB-D images before transformation and the visualized point clouds after transformation.</p>
<p>You are given: -A task instruction describing an object manipulation task.-An image showing a predicted trajectory.<strong>Note:</strong> -In the image, the red circle indicates the start point, and the blue diamond indicates the end point.-The trajectory represents the predicted movement path of the manipulated object, not the robot or end-effector.-You should <strong>evaluate the predicted trajectory as a proposed motion for the object that is supposed to be moved</strong>, based on the task instruction -<strong>not based on the static positions of objects in the image</strong>.The objects have not actually moved.<strong>Evaluation Criteria (listed in order of importance):</strong> 1. <strong>Task Alignment and Success (most important)</strong> -Does the trajectory clearly and accurately fulfill the task instruction?-<strong>The trajectory must start at the correct location and end at a target position that aligns with the task goal.</strong>-Large deviations in the starting or ending point (e.g., wrong object, wrong destination, or stopping short of the goal) should result in a low score, even if the rest of the trajectory is smooth.-If the task is not accomplished (due to incorrect goal interpretation or spatial execution), the score should be low regardless of other qualities.2. <strong>Feasibility</strong> -Is the movement physically plausible, smooth, and continuous?-Are there any unrealistic discontinuities, sharp turns, or impossible transitions?-Even if the movement is feasible, it should not receive a high score if the task is not completed.3. <strong>Obstacle Avoidance / Safety</strong> -Does the trajectory reasonably avoid collisions with surrounding objects?-Minor risks may be tolerated if the task is completed successfully, but major or clear collisions should reduce the score.<strong>Scoring Guideline:</strong> -If the task is <strong>not accomplished</strong>, or if the start or end point is significantly incorrect, the score should typically be <strong>4 or below</strong>.</p>
<p>Figure 12 :
12
Figure 12: Visualization of visual aids generated by FSD in the Where2Place benchmark.</p>
<p>Generate</p>
<p>Spatial Affordance Points &amp; Bounding Box You are currently a robot performing robotic manipulation tasks.Your task instruction: {Task Instruction} .Observe the image, use 2D points and bounding box to mark the target location</p>
<p>Figure 13 :
13
Figure 13: Visualization of visual aids generated by FSD in the RoboRefit benchmark.</p>
<p>How to draw visual trace for the task?
1.Where is the broccoli? (Start Point)2.Which area is in the pot? (End Point)3.How to connect the start and end points to form atrace?(A straight line is the simplest.) ×4. How to avoid collisions?(Pick up the vegetables with a curve.) √Task instruction: put the broccoli from the left burner into the silver potBroccoliLeft ofSilver Pot<Description> <ref>broccoli</ref><box>…</box> is positioned <pred>left of</pred> <ref>silver … (Spatial Relationship) </Description><Reasoning>OnBehindTo move the broccoli from the left burner to inside the silver pot, start by identifying the initial position of the broccoli atNext to<point>[[561, 369]]</point>. Lift the broccoli slightly upwards and to the right, moving towards the pot, reaching approximately<point>[[633, 319]]</point>. Continue moving it over the pot atLeft BurnerKnife<point>[[761, 304]]</point>. Finally, lower the broccoli into the pot,ending at the target position <box>[[736, 377, 876, 478]]</box><Answer>The visual trace for moving the broccoli into the silver pot iswith the final point at <point>[[833, 443]]</point>.<point>[[561, 369], …, [761, 304], …, [833, 443]]</point>. </Answer></Reasoning></p>
<p>For visual trace execution, we first generate 2D visual traces τ and obtain preliminary depth information from depth cameras.Following the pinhole camera model, we employ depth-based back-projection to map these to 3D space, yielding τ 3d = {x 3d t | t = 1, 2, ..., T}.Next, based on the spatial position of the first point x 1 , we query GraspNet's Fang et al. (2020) grasp candidates G to match the nearest grasp pose G</p>
<p>Brohan et al. (2023)hich already incorporates basic relation conversation and reference grounding capabilities.The training process of FSD is divided into two stages: General Spatial Reasoning Enhancement: Using data from levels 1-3, we focus on improving the model's embodied spatial reasoning capabilities.FollowingYuan et al. (2024b)andBrohan et al. (2023), we discovered that an appropriate data mixture is crucial for downstream performance.Joint training with mixed robotic and internet data ensures the model retains knowledge acquired during pre-training.Consequently, our instruction tuning utilizes a diverse 1.4M sample mixture including general visual question answering (VQA) data.This comprehensive training ensures FSD maintains robust general spatial knowledge while developing embodied capabilities.Visual Aids Generation and Understanding: Using data from levels 4-5 with the self-consistency mechanism, we specifically train visual aids generation and understanding abilities.FSD predicts a fixed set of 8 points for simplification when generating spatial visual traces.Additional training details and the summary of mixture datasets are provided in Appendix B.Action Execution: FSD can reason from initial or intermediate task steps, freely selecting needed visual aids.When using bounding boxes, we sample the center as the target point; with affordance points, we directly sample one point.</p>
<p>Table 1 :
1
Performance comparison on 5 spatial reasoning benchmarks.Bold and underlined values show best and second-best performance among open-source models.
CVBenchCRPESATBLINKEmbSp RankCount 2DRel 3DDep 3DDis Avg. Exist. Subj. Pred. Obj. Avg.ValReal Count MV RelDepth SpRel Avg.TestClosed-sourceGPT-4V62.471.179.868.3 70.4 90.6 76.7 65.1 68.5 75.2 44.8 50.760.8 55.659.772.7 62.236.1-GPT-4o65.985.587.878.2 79.4 93.3 81.9 71.8 73.6 80.2 49.4 57.549.2 60.274.269.2 63.249.1-Open-sourceLLaVA-1.5-13B58.246.653.047.8 51.4 88.7 57.4 54.2 55.2 63.9 51.4 41.645.0 41.453.269.9 52.435.14.8SAT-Dynamic-13B 61.589.780.773.0 76.2 87.5 60.6 57.6 65.2 67.7 87.7 54.935.8 44.473.466.4 55.051.32.8RoboPoint-13B56.577.281.557.7 68.2 93.2 66.3 62.4 70.9 73.2 53.3 46.648.3 44.462.165.7 55.151.42.8ASMv2-13B58.968.968.968.9 66.4 92.1 69.2 59.0 65.3 71.4 63.9 46.759.2 44.456.565.0 56.357.43.1FSD-13B62.486.588.086.7 80.9 94.0 75.2 65.1 70.4 76.2 73.2 63.3 60.0 46.670.278.3 63.863.31.3</p>
<p>Table 2 :
2
Performance comparison on object/free space reference benchmarks.The best results are highlighted in bold.
BenchmarkGPT-4o</p>
<p>SpaceLLaVA LLaVA-NeXT-34B SpatialBot-3B ASMv2-13B RoboBrain-7B RoboPoint-13B FSD-13B
RoboRef It15.321.319.923.648.410.149.856.7Where2Place29.111.815.015.022.016.646.045.8
FSD excels in object reference and free space localization.The results in Table 2 demonstrate FSD's ability to accurately identify objects and free spaces from language instructions.For object reference (RoboRef It), FSD achieves 56.7% accuracy, surpassing both GPT-4o (15.3%) and specialist models like RoboPoint (49.8%) by significant margins.On the more challenging free space reference task (Where2Place), FSD performs competitively with RoboPoint (45.8% vs. 46.0%)while substantially outperforming other models.This improvement stems from enhanced spatial understanding through our SrCoT mechanism.See Appendix F for more visualizations.</p>
<p>Table 3 :
3
Performance comparison on VABench.The best results are highlighted in bold.
(a) VABench-Point(b) VABench-VisualTraceModelAccuracy ↑ModelRMSE↓ MAE↓ GPT Score↑GPT4o ASMv2 RoboPoint FSD w/o SrCoT w/o Alignment9.30 10.07 19.09 61.82 26.21 55.92GPT4o DINOv2 Predictor 128.32 117.49 136.13 113.53 FSD 78.26 63.44 w/o SrCoT 99.53 80.06 w/o Alignment 80.48 66.804.37 4.01 6.21 5.07 5.92</p>
<p>Table 4 : SimplerEnv Evaluation on WidowX Robot.
4
Qu et al. (2025)aselines are derived fromQu et al. (2025).ZS: zero-shot, FT: fine-tuning using BridgeData.Each task is tested 24 episodes.
ModelSpoon→TowelCarrot→PlateGreen→YellowEggplant→BasketAvg.GraspSucc.GraspSucc.GraspSucc.GraspSucc.RT-1-X O'Neill et al. (2023)16.7%0%20.8%4.2%8.3%0%0.0%0%1.1%Octo-S Team et al. (2024)77.8% 47.2%27.8%9.7%40.3%4.2%87.5%56.9%30.0%OpenVLA Kim et al. (2024)4.1%0%33.3%0%12.5%0%8.3%4.1%1.0%RoboVLM (ZS) Li et al. (2024d)37.5%20.8%33.3%25.0%8.3%8.3%0.0%0%13.5%RoboVLM (FT) Li et al. (2024d)54.2%29.2%25.0%25.0%45.8%12.5%58.3%58.3%31.3%SpatialVLA (ZS) Qu et al. (2025)25.0%20.8%41.7%20.8%58.3%25.0%79.2%70.8%34.4%SpatialVLA (FT) Qu et al. (2025)20.8%16.7%29.2%25.0%62.5%29.2%100%100%42.7%FSD58.3%41.7%58.3% 50.0% 91.7% 33.3% 37.5%37.5%40.6%</p>
<p>Li et al. (2024d)), OpenVLA Kim et al. (2024), RoboVLMLi et al. (2024d)and SpatialVLA Qu</p>
<p>Spatial Affordance and Visual Trace Dataset Generation</p>
<p>Next, we provide a detailed description of how to extract the required controllable points/boxes and visual trajectories from the embodied dataset like BridgeData Walke et al. (2023), RT-X O'Neill et al. (</p>
<p>Cheng et al. (2024), Du et al. (2024) generating the spatial relation diagrams, we can easily create various template questions for spatial reasoningCheng et al. (2024), Du et al. (2024), such as: How are[A]and[B]positioned in relation to each other in the image?or From your perspective, which object in the image is at the shortest distance?In addition to template-based QA, we also combine task instructions, existing spatial information, and images to query GPT-4o, thereby generating more diverse multi-turn dialogues to enhance the model's generalization ability in spatial reasoning.</p>
<p>AcknowledgmentWe would like to thank Zhongwen Xu, Liang Wang, Shuyang Gu, and Chen Li for their participation in the discussions of this paper and for providing valuable insights.In addition, we would especially like to thank Yiyang Huang for the constructive suggestions on improving the figures in the manuscript.https://embodied-fsd.github.io/AppendixA. Weak-to-Strong Dataset ConstructionRegion Grounding Dataset Generation Regional grounding enables robots to focus on key objects within a scene.In embodied AI, robots need to concentrate on task-relevant local scenes in the image according to task instructions and be able to accurately locate objects.In Level 1 of our dataset, we integrate grounding with captioning, aiming for the agent to provide positional information when describing images.Unlike traditional image captioning, objects in embodied scenes are often cluttered, and we only seek to extract captions for task-relevant regions, avoiding redundant information.We prompt GPT-4o to exclude task-irrelevant regions based on task instructions and generate both image captions and the corresponding object names.Subsequently, we use GroundingDINO Liu et al. (2023d)to capture the locations of various objects in the image and embed them into the caption sentences, forming captions with object location information.Below is an example from the dataset: Level 1: Region Grounding Human: Give a short and clear explanation of the subsequent image.GPT: A <ref>robotic arm</ref><box>[[338, 126, 994, 861]]</box> is positioned above a <ref>blue bottle</ref><box>[[257, 201, 381, 413]]</box>, with a <ref>grey toy</ref><box>[[391, 413, 538, 518]]</box> to the left and a <ref>green oval object</ref><box>[[592, 481, 702, 601]]</box> to the right on a wooden table.Spatial Relationship Dataset GenerationTo accurately infer object spatial relationships from RGB images, a multi-stage pipeline is employed: encompassing object detection, instance segmentation, 2D-to-3D mapping, and subsequent relationship calculation.Initially, for pre-identified objects of interest within the scene, GroundedSAMRen et al. (2024)is utilized to perform instance segmentation, yielding precise object masks.Subsequently, the 2D RGB information is elevated to a 3D spatial representation.This transformation begins with leveraging PerspectiveFieldsJin et al. (2023)to estimate the Z-axis orientation, serving as a coarse approximation for camera extrinsics.Concurrently, the WildCameraZhu et al. (2024)model is employed to estimate intrinsic camera parameters, including focal length and resolution.Metric3Dv2Hu et al. (2024)is then used for robust depth estimation.By integrating the RGB image, estimated depth image, and computed camera intrinsics and extrinsics, the 2D RGB image is transformed into a 3D point cloud.Given the prior object segmentation, the specific 3D spatial position and size of each individual object can be precisely derived from the generated point cloud.These comprehensive 3D data then enable the calculation of relative spatial relationships between objects, which are subsequently exported as a spatial relationship graph.We present the 2D RGB-D images before transformation and the visualized point clouds after transformation in Fig.11.It is important to note that for inferring spatial relationships, only relative depth sorting data is generated, which means stringent accuracy requirements for absolute depth are relaxed.To further enhance data quality, particularly for robust relationship inference, objects exhibiting a relative depth gap of at least 20% are preferentially selected for subsequent generation.An example from the dataset:E. Details of Benchmarks and BaselinesFor general spatial reasoning tasks, the answers are typically multiple-choice questions with clear options.However, some spatial reasoning models show reduced instruction-following ability after fine-tuning, preventing them from directly outputting the correct option.To address this, we use a lenient matching rule, considering an answer correct if it includes either the correct content or the corresponding option.For object/region reference tasks, we carefully fine-tuned and used a tailored prompt for each model.Most models, such as GPT-4o and ASMV2Wang et al. (2025), cannot directly output specific points.Similar to the validation process for RoboPointYuan et al. (2024b), we also found that using in-context learning to specify point output formats resulted in worse performance compared to directly outputting bounding boxes.Therefore, for these models, we instructed them to output bounding boxes directly.From these bounding boxes, we either uniformly sampled nine points or took the midpoint.Then, we calculated the proportion of points within the specified region to determine the final average accuracy.For the VABench-VisualTrace task, due to a lack of strong baselines, we developed an additional Transformerbased prediction model, trained from scratch using the same data, which we named DINOv2 Predictor.In the DINOv2 Predictor, the visual encoder uses a pre-trained DINOv2Oquab et al. (2023), encoding images to output a (196, 768) feature vector.The language encoder uses a pre-trained T5-BaseRaffel et al. (2020), outputting (32, 768).These are concatenated with learnable embeddings (8, 768) and passed through a Transformer encoder base.The output of the learnable embeddings is then read and passed through a linear layer to predict eight points.During training, we keep the language encoder fully frozen and train the visual encoder along with the other remaining parameters.F. More Visualizations and ExamplesWe present the prediction results of FSD on Where2placeYuan et al. (2024b), RoborefitLu et al. (2023), and VABench in Fig.12, Fig.13, and Fig.14, respectively.J. Future WorksWe have made preliminary attempts to use visual aids as intermediate states in FSD, achieving promising results in object/target/region reference and actual manipulation task execution.Future work can focus on the following aspects to further enhance the applicability of this paradigm: 1. Task Decomposition for Complex and Long-Horizon Instructions: The current version of FSD primarily targets clear and explicit language instructions.When dealing with long-horizon tasks or ambiguous/complex instructions, the model needs to decompose them into atomic, executable sub-tasks.We believe that decomposing instructions into a sequence of visual aids to guide each sub-task execution is a promising avenue.2.Downstream Execution and Visual-Aid-Guided Control: Currently, FSD relies mainly on training-free motion planning methods for downstream execution.In extremely complex or dynamic scenarios, this may lead to a bottleneck in success rates.A potential improvement is to use the generated visual aids as explicit guidance for downstream VLA models, replacing language-conditioned training.
Homanga Bharadhwaj, Roozbeh Mottaghi, Abhinav Gupta, Shubham Tulsiani, arXiv:2405.01527arXiv:2410.24164Track2act: Predicting point tracks from internet videos enables generalizable robot manipulation. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, 2024. 2024arXiv preprint0 : A vision-language-action flow model for general robot control</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, arXiv:2307.158182023arXiv preprint</p>
<p>Spatialbot: Precise spatial understanding with vision language models. Wenxiao Cai, Iaroslav Ponomarenko, Jianhao Yuan, Xiaoqi Li, Wankou Yang, Hao Dong, Bo Zhao, arXiv:2406.136422024arXiv preprint</p>
<p>Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, Fei Xia, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, Rui Zhao, arXiv:2306.15195Shikra: Unleashing multimodal llm's referential dialogue magic. 2023arXiv preprint</p>
<p>Embspatial-bench: Benchmarking spatial understanding for embodied tasks with large vision-language models. An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, Sifei Liu, Binhao Spatialrgpt ; Mengfei Du, Zejun Wu, Xuanjing Li, Zhongyu Huang, Wei, arXiv:2406.01584arXiv:2406.05756Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. Chenxi Hao-Shu Fang, Minghao Wang, Cewu Gou, Lu, the IEEE/CVF conference on computer vision and pattern recognition2024. 2024. 2020arXiv preprintGrounded spatial reasoning in vision language model</p>
<p>Blink: Multimodal large language models can see but not perceive. Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A Smith, Wei-Chiu Ma, Ranjay Krishna, European Conference on Computer Vision. Springer2024</p>
<p>Clip-adapter: Better vision-language models with feature adapters. Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, Yu Qiao, International Journal of Computer Vision. 13222024</p>
<p>Multiple View Geometry in Computer Vision. Richard Hartley, Andrew Zisserman, 2004Cambridge University Press2nd edition</p>
<p>3d-llm: Injecting the 3d world into large language models. Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, Chuang Gan, Advances in Neural Information Processing Systems. 202336</p>
<p>Metric3d v2: A versatile monocular geometric foundation model for zero-shot metric depth and surface normal estimation. Mu Hu, Wei Yin, Chi Zhang, Zhipeng Cai, Xiaoxiao Long, Hao Chen, Kaixuan Wang, Gang Yu, Chunhua Shen, Shaojie Shen, arXiv:2404.155062024arXiv preprint</p>
<p>Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, Li Fei-Fei, arXiv:2409.01652Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation. 2024arXiv preprint</p>
<p>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint</p>
<p>Yuheng Ji, Huajie Tan, Jiayu Shi, Xiaoshuai Hao, Yuan Zhang, Hengyuan Zhang, Pengwei Wang, Mengdi Zhao, Yao Mu, Pengju An, Xinda Xue, Qinghang Su, Huaihai Lyu, Xiaolong Zheng, Jiaming Liu, Zhongyuan Wang, Shanghang Zhang, arXiv:2502.21257Robobrain: A unified brain model for robotic manipulation from abstract to concrete. 2025arXiv preprint</p>
<p>Perspective fields for single image camera calibration. Linyi Jin, Jianming Zhang, Yannick Hold-Geoffroy, Oliver Wang, Kevin Blackburn-Matzen, Matthew Sticha, David F Fouhey, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>Nikita Karaev, Iurii Makarov, Jianyuan Wang, Natalia Neverova, arXiv:2410.11831Andrea Vedaldi, and Christian Rupprecht. Cotracker3: Simpler and better point tracking by pseudo-labelling real videos. 2024arXiv preprint</p>
<p>Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, arXiv:2403.12945A large-scale in-the-wild robot manipulation dataset. 2024arXiv preprint</p>
<p>Openvla: An open-source vision-language-action model. Jin Moo, Karl Kim, Siddharth Pertsch, Ted Karamcheti, Ashwin Xiao, Suraj Balakrishna, Rafael Nair, Ethan Rafailov, Grace Foster, Pannag Lam, Sanketi, arXiv:2406.092462024arXiv preprint</p>
<p>Llava-onevision: Easy visual task transfer. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, arXiv:2408.03326arXiv:2406.02537Topviewrs: Visionlanguage models as top-view spatial reasoners. Chengzu Li, Caiqi Zhang, Han Zhou, Nigel Collier, Anna Korhonen, Ivan Vulić, 2024a. 2024barXiv preprint</p>
<p>Xiang Li, Cristina Mata, Jongwoo Park, Kumara Kahatapitiya, Sung Yoo, Jinghuan Jang, Kanchana Shang, Ryan Ranasinghe, Mu Burgert, Yong Jae Cai, Lee, arXiv:2406.20095Supercharging robot learning data for vision-language policy. 2024carXiv preprint</p>
<p>Towards generalist robot policies: What matters in building vision-language-action models. Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong, Hanbo Zhang, Huaping Liu, arXiv:2412.140582024darXiv preprint</p>
<p>Evaluating real-world robot manipulation policies in simulation. Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, arXiv:2405.059412024earXiv preprint</p>
<p>Yi Li, Yuquan Deng, Jesse Zhang, Joel Jang, Marius Memmel, Raymond Yu, Caelan Reed Garrett, Fabio Ramos, Dieter Fox, Anqi Li, arXiv:2502.05485Hierarchical action models for open-world robot manipulation. 2025arXiv preprint</p>
<p>Zejun Li, Ruipu Luo, Jiwen Zhang, Minghui Qiu, Zhongyu Wei, arXiv:2405.16919Vocot: Unleashing visually grounded multi-step reasoning in large multi-modal models. 2024farXiv preprint</p>
<p>Fanqi Lin, Yingdong Hu, Pingyue Sheng, Chuan Wen, Jiacheng You, and Yang Gao. Data scaling laws in imitation learning for robotic manipulation. Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, David Acuna, arXiv:2409.09788arXiv:2410.186472024. 2024aarXiv preprintReasoning paths with reference objects elicit quantitative spatial reasoning in large vision-language models</p>
<p>Vila: On pre-training for visual language models. Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, Song Han, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024b</p>
<p>Libero: Benchmarking knowledge transfer for lifelong robot learning. Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, Peter Stone, Advances in Neural Information Processing Systems. 2023a36</p>
<p>Visual spatial reasoning. Fangyu Liu, Guy Emerson, Nigel Collier, Transactions of the Association for Computational Linguistics. 112023b</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, arXiv:2310.03744Improved baselines with visual instruction tuning. 2023carXiv preprint</p>
<p>Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, Yong Jae Lee, Llava-next: Improved reasoning, ocr, and world knowledge. 2024a</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Advances in neural information processing systems. 2024b36</p>
<p>Grounding dino: Marrying dino with grounded pre-training for open-set object detection. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, arXiv:2303.054992023darXiv preprint</p>
<p>Rdt-1b: a diffusion foundation model for bimanual manipulation. Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, Jun Zhu, arXiv:2410.078642024carXiv preprint</p>
<p>Vl-grasp: a 6-dof interactive grasp policy for language-oriented objects in cluttered indoor scenes. Yuhao Lu, Yixuan Fan, Beixing Deng, Fangfu Liu, Yali Li, Shengjin Wang, 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2023</p>
<p>Compositional chain-of-thought prompting for large multimodal models. Chancharik Mitra, Brandon Huang, Trevor Darrell, Roei Herzig, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Where2act: From pixels to actions for articulated 3d objects. Kaichun Mo, Leonidas J Guibas, Mustafa Mukadam, Abhinav Gupta, Shubham Tulsiani, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>Llarva: Vision-action instruction tuning enhances robot learning. Dantong Niu, Yuvan Sharma, Giscard Biamby, Jerome Quenum, Yutong Bai, Baifeng Shi, Trevor Darrell, Roei Herzig, arXiv:2406.118152024arXiv preprint</p>
<p>Open x-embodiment: Robotic learning datasets and rt-x models. Jake O' Neill, Abraham Arthurs, Fábio Avila Belbute-Peres, Julian Balaguer, Sarah Bechtle, Gemma Bidoia, Kyle Burden, Erwin Chang, Sheila Chen, Todor Davchev, arXiv:2310.088642023arXiv preprint</p>
<p>Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, arXiv:2304.07193Learning robust visual features without supervision. 2023arXiv preprint</p>
<p>Keto: Learning keypoint representations for tool manipulation. Zengyi Qin, Kuan Fang, Yuke Zhu, Li Fei-Fei, Silvio Savarese, 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEE2020</p>
<p>Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, Jiayuan Gu, Bin Zhao, Dong Wang, arXiv:2501.15830Exploring spatial representations for visual-language-action model. 2025arXiv preprint</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 212020</p>
<p>Arijit Ray, Jiafei Duan, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan A Plummer, Ranjay Krishna, Kuo-Hao Zeng, arXiv:2412.07755Spatial aptitude training for multimodal language models. 2024arXiv preprint</p>
<p>Grounded sam: Assembling open-world models for diverse visual tasks. Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, arXiv:2401.141592024arXiv preprint</p>
<p>Visual cot: Advancing multi-modal language models with a comprehensive dataset and benchmark for chain-of-thought reasoning. Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, Hongsheng Li, The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2024</p>
<p>Mohit Shridhar, Yat , Long Lo, Stephen James, arXiv:2407.07875Generative image as action models. 2024arXiv preprint</p>
<p>Hee Chan, Valts Song, Jonathan Blukis, Stephen Tremblay, Yu Tyree, Stan Su, Birchfield, arXiv:2411.16537Robospatial: Teaching spatial understanding to 2d and 3d vision-language models for robotics. 2024arXiv preprint</p>
<p>Curobo: Parallelized collision-free robot motion generation. Balakumar Sundaralingam, Siva Kumar Sastry, Adam Hari, Caelan Fishman, Karl Garrett, Valts Van Wyk, Alexander Blukis, Helen Millane, Ankur Oleynikova, Fabio Handa, Ramos, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Octo Team, Rt-X Team, Anthony Brohan, Noah Brown, Lauren Chen, Michael Cheng, Krzysztof Choromanski, Eamonn Cullina, Gabe Dalal, Chelsea Fu, Florian Golemo, arXiv:2403.10164An open-source generalist robot policy. 2024arXiv preprint</p>
<p>Cambrian-1: A fully open, vision-centric exploration of multimodal llms. Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Charitha Sai, Jihan Akula, Shusheng Yang, Adithya Yang, Xichen Iyer, Austin Pan, Rob Wang, Yann Fergus, Saining Lecun, Xie, 2024</p>
<p>Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730Attention is all you need</p>
<p>Bridgedata v2: A dataset for robot learning at scale. Homer Rich Walke, Kevin Black, Tony Z Zhao, Quan Vuong, Chongyi Zheng, Philippe Hansen-Estruch, Andre Wang He, Vivek Myers, Jin Moo, Max Kim, Du, Conference on Robot Learning. PMLR2023</p>
<p>Scaling proprioceptive-visual learning with heterogeneous pre-trained transformers. Lirui Wang, Xinlei Chen, Jialiang Zhao, Kaiming He, arXiv:2409.205372024arXiv preprint</p>
<p>The all-seeing project v2: Towards general relation comprehension of the open world. Weiyun Wang, Yiming Ren, Haowen Luo, Tiantong Li, Chenxiang Yan, Zhe Chen, Wenhai Wang, Qingyun Li, Lewei Lu, Xizhou Zhu, European Conference on Computer Vision. Springer2025</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Any-point trajectory modeling for policy learning. Chuan Wen, Xingyu Lin, John So, Kai Chen, Qi Dou, Yang Gao, Pieter Abbeel, arXiv:2401.000252023arXiv preprint</p>
<p>Dettoolchain: A new prompting paradigm to unleash detection ability of mllm. Yixuan Wu, Yizhou Wang, Shixiang Tang, Wenhao Wu, Tong He, Wanli Ouyang, Philip Torr, Jian Wu, European Conference on Computer Vision. Springer2025</p>
<p>Flow as the cross-domain manipulation interface. Mengda Xu, Zhenjia Xu, Yinghao Xu, Cheng Chi, Gordon Wetzstein, Manuela Veloso, Shuran Song, arXiv:2407.152082024arXiv preprint</p>
<p>Magma: A foundation model for multimodal ai agents. Jianwei Yang, Reuben Tan, Qianhui Wu, Ruijie Zheng, Baolin Peng, Yongyuan Liang, Yu Gu, Mu Cai, Seonghyeon Ye, Joel Jang, arXiv:2502.131302025arXiv preprint</p>
<p>Thinking like an expert: Multimodal hypergraph-of-thought (hot) reasoning to boost foundation modals. Fanglong Yao, Changyuan Tian, Jintao Liu, Zequn Zhang, Qing Liu, Li Jin, Shuchao Li, Xiaoyu Li, Xian Sun, arXiv:2308.062072023aarXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Beyond chain-of-thought, effective graph-of-thought reasoning in language models. Yao Yao, Zuchao Li, Hai Zhao, arXiv:2305.165822023barXiv preprint</p>
<p>Ferret: Refer and ground anything anywhere at any granularity. Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, Yinfei Yang, arXiv:2310.077042023arXiv preprint</p>
<p>Chengbo Yuan, Chuan Wen, Tong Zhang, Yang Gao, arXiv:2401.11439General flow as foundation affordance for scalable robot learning. 2024aarXiv preprint</p>
<p>Robopoint: A vision-language model for spatial affordance prediction for robotics. Wentao Yuan, Jiafei Duan, Valts Blukis, Wilbert Pumacay, Ranjay Krishna, Adithyavairavan Murali, Arsalan Mousavian, Dieter Fox, arXiv:2406.107212024barXiv preprint</p>
<p>Robotic control via embodied chain-of-thought reasoning. Michał Zawalski, William Chen, Karl Pertsch, Oier Mees, Chelsea Finn, Sergey Levine, arXiv:2407.086932024arXiv preprint</p>
<p>Empowering embodied manipulation: A bimanual-mobile robot manipulation dataset for household tasks. Tianle Zhang, Dongjiang Li, Yihang Li, Zecui Zeng, Lin Zhao, Lei Sun, Yue Chen, Xuelong Wei, Yibing Zhan, Lusong Li, arXiv:2405.188602024arXiv preprint</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, arXiv:2210.034932022arXiv preprint</p>
<p>Ddcot: Duty-distinct chain-of-thought prompting for multimodal reasoning in language models. Ge Zheng, Bin Yang, Jiajin Tang, Hong-Yu Zhou, Sibei Yang, Advances in Neural Information Processing Systems. 362023a</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 2023b36</p>
<p>Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. Ruijie Zheng, Yongyuan Liang, Shuaiyi Huang, Jianfeng Gao, Hal Daumé, Iii , Andrey Kolobov, Furong Huang, Jianwei Yang, arXiv:2412.103452024arXiv preprint</p>
<p>Tame a wild camera: in-the-wild monocular camera calibration. Shengjie Zhu, Abhinav Kumar, Masa Hu, Xiaoming Liu, Advances in Neural Information Processing Systems. 202436</p>            </div>
        </div>

    </div>
</body>
</html>
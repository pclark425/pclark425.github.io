<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3870 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3870</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3870</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-91.html">extraction-schema-91</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-354dcdebf3f8b5feeed5c62090e0bc1f0c28db06</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/354dcdebf3f8b5feeed5c62090e0bc1f0c28db06" target="_blank">Augmenting large language models with chemistry tools</a></p>
                <p><strong>Paper Venue:</strong> Nature Machine Intelligence</p>
                <p><strong>Paper TL;DR:</strong> ChemCrow, an LLM chemistry agent designed to accomplish tasks across organic synthesis, drug discovery and materials design, is introduced by integrating 18 expert-designed tools and using GPT-4 as the LLM, augments the LLM performance in chemistry, and new capabilities emerge.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have shown strong performance in tasks across domains but struggle with chemistry-related problems. These models also lack access to external knowledge sources, limiting their usefulness in scientific applications. We introduce ChemCrow, an LLM chemistry agent designed to accomplish tasks across organic synthesis, drug discovery and materials design. By integrating 18 expert-designed tools and using GPT-4 as the LLM, ChemCrow augments the LLM performance in chemistry, and new capabilities emerge. Our agent autonomously planned and executed the syntheses of an insect repellent and three organocatalysts and guided the discovery of a novel chromophore. Our evaluation, including both LLM and expert assessments, demonstrates ChemCrowâ€™s effectiveness in automating a diverse set of chemical tasks. Our work not only aids expert chemists and lowers barriers for non-experts but also fosters scientific advancement by bridging the gap between experimental and computational chemistry. Large language models can be queried to perform chain-of-thought reasoning on text descriptions of data or computational tools, which can enable flexible and autonomous workflows. Bran et al. developed ChemCrow, a GPT-4-based agent that has access to computational chemistry tools and a robotic chemistry platform, which can autonomously solve tasks for designing or synthesizing chemicals such as drugs or materials.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3870.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3870.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemCrow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemCrow (LLM chemistry agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-augmented chemistry agent that integrates an LLM (GPT-4) with a suite of expert-designed chemistry tools to plan, reason about, and in some cases execute chemical syntheses and discovery workflows, using iterative Thought/Action/Observation loops.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>ChemCrow</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>An agent framework that prompts an LLM with a list of tool names and descriptions and instructs it to follow a Thought / Action / Action Input / Observation loop (ReAct-style) to decide which tools to call and how to combine their outputs; integrates 18 tools across literature/web search, molecule manipulation, safety checks, reaction prediction/planning and robotic execution to ground LLM outputs in tool results.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Varies by task; when literature is required ChemCrow uses a LitSearch tool that accepts user-supplied PDFs/text (including raw HTML) or literature retrieved from web search; corpus size unspecified and depends on user-provided documents or web search results. Other inputs include single-molecule identifiers (names, SMILES, CAS) and reaction specifications.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Natural-language user prompts describing tasks (e.g., 'Plan and execute the synthesis of an insect repellent'), provided to the LLM; the agent can then translate the prompt into tool calls and retrieval queries.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Retrieval-augmented generation using tool calls: the LLM issues retrieval/summarization calls (LitSearch/WebSearch) to get top-k passages (OpenAI embeddings + FAISS) which are summarized and included in prompts; reasoning is done via ReAct-like chain-of-thought (explicit Thought/Action steps) with iterative tool use and observation incorporation.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Natural-language answers that may include structured synthesis plans, stepwise procedures, safety summaries, proposed molecules (names/SMILES), and machine-readable action sequences for robotic execution; final outputs optionally summarized into a readable assistant-style response.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Human expert panel evaluated outputs across correctness of chemistry, quality of reasoning, and degree of task completion; an LLM-based evaluator (EvaluatorGPT) was also used to grade responses and give feedback; in experimental cases, physical synthesis was validated by chemical analysis (HPLC/MS) and robotic execution logs for syntheses performed on RoboRXN.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>ChemCrow successfully planned and executed the syntheses of DEET and three thiourea organocatalysts, and guided discovery and synthesis of a novel chromophore; expert human evaluators preferred ChemCrow over an unaugmented GPT-4 baseline on factuality, reasoning, and task completion (particularly for complex/novel tasks), while the LLM-based evaluator tended to prefer GPT-4 due to fluency.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Performance depends on quality and coverage of integrated tools and the underlying LLM; risks include LLM hallucinations if tools are misused or return poor outputs, incomplete or invalid procedural outputs that may require human fixes to run on robotic platforms, safety/dual-use concerns, and reproducibility limits imposed by using closed-source API LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Compared against GPT-4 prompted to 'act like an expert chemist' (tool-less baseline): experts rated ChemCrow higher on chemistry factuality and reasoning, especially for complex/novel tasks; GPT-4 sometimes scored higher on tasks that require memorization of well-known facts, and was favored by the LLM-based evaluator due to more fluent completions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Augmenting large language models with chemistry tools', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3870.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3870.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LitSearch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LitSearch (literature search tool used in ChemCrow)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A literature search and question-answering tool that embeds and searches user-supplied scientific documents (PDF/text/HTML) using OpenAI embeddings and FAISS and uses an LLM to summarize top passages relative to a query, anchoring LLM responses in the scientific literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>LitSearch (paper-qa + embeddings + FAISS)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Embeds documents and queries into vector space (OpenAI Embeddings), retrieves top-k relevant passages using FAISS, creates passage-level summaries in relation to the query, and feeds those summaries into the LLM prompt so the model generates answers grounded in the retrieved literature.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Arbitrary collections of scientific documents provided as PDFs or text files (including raw HTML); corpus size unspecified and determined by user-provided documents and any literature/web search results; documents are embedded into a vector index (FAISS).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Natural-language queries issued by the LLM agent or user; LitSearch returns top-k passages and summaries relevant to that query.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Retrieval-augmented generation (RAG): embed + retrieve top passages, summarize them with an LLM, and incorporate those summaries into the final answer; this anchors generation to source passages and includes references to papers.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Summaries of relevant passages, citations to source papers, and LLM-generated answers anchored to the retrieved literature (textual summaries suitable to be included in agent prompts and final responses).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Indirectly evaluated as part of ChemCrow by downstream task performance and human expert review; no standalone benchmarking numbers reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Used by ChemCrow as the preferred literature retrieval tool (preferred over generic WebSearch) to ground answers in literature; contributed to more factual outputs and to features such as providing references and literature-backed safety/chemical information.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No corpus size or coverage guarantees; relies on quality of embeddings and retrieval hyperparameters (top-k); LLM summarization may still introduce omissions or misinterpretations if retrieved passages are insufficient or noisy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Compared implicitly to a generic WebSearch tool (SerpAPI) within ChemCrow; LitSearch was preferred by the agent and authors when literature grounding was needed, but no quantitative head-to-head metrics are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Augmenting large language models with chemistry tools', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3870.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3870.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EvaluatorGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EvaluatorGPT (LLM-based evaluator used in this work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM instructed to act as a teacher/evaluator that grades model outputs on whether tasks are addressed and the correctness of the thought process, and highlights strengths/weaknesses and suggestions for improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>EvaluatorGPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>A prompt-engineered use of an LLM (GPT-4) to grade responses: the evaluator LLM is instructed to assign grades (task addressed or not, thought process correctness), provide qualitative feedback, and explain scores; used as an automated evaluation signal alongside human expert review.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Model outputs (textual task solutions from ChemCrow and GPT-4 baseline) presented to the evaluator LLM; does not itself index or search external scholarly corpora for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Evaluator is given the task prompt, the model completions, and an evaluation rubric via natural-language instructions and asked to provide grades and feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Not a literature distillation method per se; it synthesizes evaluation judgments from the provided model outputs using an LLM's internal reasoning and the supplied rubric.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Graded scores (binary/graded items), qualitative comments on strengths/weaknesses, and suggestions for improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Compared against a panel of human expert graders; the paper reports a discrepancy: EvaluatorGPT often preferred GPT-4 outputs based on fluency, while human experts preferred ChemCrow based on factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>EvaluatorGPT tended to favor more fluent GPT-4 completions, failing to reliably penalize factual errors/hallucinations; the paper concludes that LLM-based self-evaluation can be unreliable when factuality is central.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Evaluator LLMs can be misled by fluency and completeness, failing to detect factual inaccuracies or flawed reasoning; reliance on the same model family can cause blind spots in assessment of factuality.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Direct comparison in the paper: human expert evaluations disagreed with EvaluatorGPT in favoring ChemCrow; EvaluatorGPT favored GPT-4 due to style/fluency rather than factual correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Augmenting large language models with chemistry tools', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3870.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3870.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LangChain</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LangChain (framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A software framework for building applications with LLMs that provides interfaces for prompts, document loaders, chains, indexes, agents, memory, and tool integration, used here to integrate ChemCrow's external tools with the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LangChain</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>LangChain</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Provides the infrastructure to register external tools, build agent loops, manage prompts and chains, and connect LLM calls with tool executions; used as the application framework that orchestrates ChemCrow's LLM + tool interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Framework-level: supports arbitrary documents and connectors (web, PDFs, vector stores). In this work it was used to connect to the set of tools (LitSearch, WebSearch, python REPL, ReactionPlanner, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>User natural-language prompts are passed into LangChain agent constructs which map them to sequences of LLM calls and tool invocations.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Not a distillation algorithm; it is an orchestration layer that enables retrieval-augmented and tool-augmented generation by hooking LLM prompts to retrieval and tool outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Orchestrated LLM outputs and tool results, structured according to the agent's Thought/Action loop and final summarization steps.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Used as part of the system implementation; no separate evaluation reported for LangChain itself in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Used successfully to implement ChemCrow's agent behavior and tool integrations; contributed to modularity and extensibility of the system.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Framework-level limitations not detailed; general concerns include orchestration complexity and potential propagation of errors from tools into LLM prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>No direct baseline comparison; LangChain served as the chosen integration framework for implementing the agent.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Augmenting large language models with chemistry tools', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3870.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3870.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method/architecture that interleaves chain-of-thought style reasoning ('Thought') with explicit actions ('Action' and 'Action Input') that call external tools, enabling LLMs to perform multi-step reasoning grounded by tool outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>React: Synergizing reasoning and acting in language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>A prompting/interaction protocol that instructs LLMs to alternate between internal reasoning traces and actions that call external tools, feeding tool outputs back as observations to continue the reasoning loop.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Architecture-agnostic; ReAct does not require a corpus but can be applied with retrieval tools or document corpora when needed.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Natural-language prompts specifying tasks; the model is guided to produce Thought/Action steps in its generation.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Chain-of-thought + tool invocation loop; when combined with retrieval tools it supports retrieval-augmented synthesis grounded in external documents.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Interleaved reasoning traces and tool action calls; final answer produced after iterative interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Referenced as prior work and as inspiration for ChemCrow's interaction format; the paper cites ReAct as an established method but does not evaluate ReAct directly.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Used conceptually to shape ChemCrow's Thought/Action/Observation loop; shown in prior work to improve reasoning+action tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Mentioned in this paper as part of the conceptual foundation; known limitations include potential for verbose reasoning traces and sensitivity to prompt design.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Mentioned as part of the architectural inspiration; no direct comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Augmenting large language models with chemistry tools', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3870.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3870.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MRKL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MRKL Systems (Modular, neuro-symbolic architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular architecture that composes LLMs with external knowledge sources and discrete reasoning modules (tools) to extend capabilities beyond pure generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>MRKL</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>A conceptual framework for integrating LLMs with symbolic modules and external knowledge sources to perform tasks requiring exact computation, factual grounding, or discrete reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Framework-level; MRKL can ingest structured knowledge sources and documents via connected modules; no specific corpus used in this paper's mention.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Natural-language task prompts, routed to a combination of LLM reasoning and specialized modules as needed.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Neuro-symbolic composition: LLM for flexible language reasoning plus external modules (calculators, search, retrieval) to ground and disambiguate outputs; supports retrieval-augmented workflows when connected to document indexes.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Structured or textual outputs that combine symbolic module outputs and LLM-generated explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Referenced as conceptual background; not evaluated directly in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as part of prior art motivating tool-augmented LLM agents like ChemCrow.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>General challenges include module integration complexity and ensuring correct orchestration between LLM and symbolic modules.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Mentioned as related work; no empirical comparison provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Augmenting large language models with chemistry tools', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3870.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3870.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Toolformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Toolformer: Language models can teach themselves to use tools</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method demonstrating that LLMs can be fine-tuned to learn when and how to call external tools by self-supervised training, improving tool-usage behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Toolformer: Language models can teach themselves to use tools.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Toolformer</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>A self-supervised fine-tuning approach where synthetic supervision is created for tool usage tokens, enabling LLMs to learn triggers and appropriate call patterns for external tools.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not a literature-distillation system per se; operates on large text corpora augmented with synthesized tool-call annotations during fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Standard language prompts/tasks during pretraining/fine-tuning; the model learns to insert tool calls where helpful.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Self-supervised insertion of tool-call annotations into training data to teach LLMs to use tools; not specifically retrieval of scholarly papers.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>LM outputs containing learned tool-call invocations and resultant composed outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Mentioned in related work as motivation for tool-augmented LLMs; not experimentally evaluated within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as evidence that LLMs can be taught to call tools effectively, supporting the design choice of ChemCrow to rely on LLM-driven tool invocation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not discussed in detail here; general limitations include dependency on fine-tuning data and mismatch between training-time and deployment-time tool behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Mentioned as related literature; no direct comparisons reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Augmenting large language models with chemistry tools', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>React: Synergizing reasoning and acting in language models. <em>(Rating: 2)</em></li>
                <li>MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning. <em>(Rating: 2)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools. <em>(Rating: 2)</em></li>
                <li>Emergent autonomous scientific research capabilities of large language models. <em>(Rating: 2)</em></li>
                <li>MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action. <em>(Rating: 1)</em></li>
                <li>HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3870",
    "paper_id": "paper-354dcdebf3f8b5feeed5c62090e0bc1f0c28db06",
    "extraction_schema_id": "extraction-schema-91",
    "extracted_data": [
        {
            "name_short": "ChemCrow",
            "name_full": "ChemCrow (LLM chemistry agent)",
            "brief_description": "An LLM-augmented chemistry agent that integrates an LLM (GPT-4) with a suite of expert-designed chemistry tools to plan, reason about, and in some cases execute chemical syntheses and discovery workflows, using iterative Thought/Action/Observation loops.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "ChemCrow",
            "system_or_method_description": "An agent framework that prompts an LLM with a list of tool names and descriptions and instructs it to follow a Thought / Action / Action Input / Observation loop (ReAct-style) to decide which tools to call and how to combine their outputs; integrates 18 tools across literature/web search, molecule manipulation, safety checks, reaction prediction/planning and robotic execution to ground LLM outputs in tool results.",
            "input_corpus_description": "Varies by task; when literature is required ChemCrow uses a LitSearch tool that accepts user-supplied PDFs/text (including raw HTML) or literature retrieved from web search; corpus size unspecified and depends on user-provided documents or web search results. Other inputs include single-molecule identifiers (names, SMILES, CAS) and reaction specifications.",
            "topic_or_query_specification": "Natural-language user prompts describing tasks (e.g., 'Plan and execute the synthesis of an insect repellent'), provided to the LLM; the agent can then translate the prompt into tool calls and retrieval queries.",
            "distillation_method": "Retrieval-augmented generation using tool calls: the LLM issues retrieval/summarization calls (LitSearch/WebSearch) to get top-k passages (OpenAI embeddings + FAISS) which are summarized and included in prompts; reasoning is done via ReAct-like chain-of-thought (explicit Thought/Action steps) with iterative tool use and observation incorporation.",
            "output_type_and_format": "Natural-language answers that may include structured synthesis plans, stepwise procedures, safety summaries, proposed molecules (names/SMILES), and machine-readable action sequences for robotic execution; final outputs optionally summarized into a readable assistant-style response.",
            "evaluation_or_validation_method": "Human expert panel evaluated outputs across correctness of chemistry, quality of reasoning, and degree of task completion; an LLM-based evaluator (EvaluatorGPT) was also used to grade responses and give feedback; in experimental cases, physical synthesis was validated by chemical analysis (HPLC/MS) and robotic execution logs for syntheses performed on RoboRXN.",
            "results_summary": "ChemCrow successfully planned and executed the syntheses of DEET and three thiourea organocatalysts, and guided discovery and synthesis of a novel chromophore; expert human evaluators preferred ChemCrow over an unaugmented GPT-4 baseline on factuality, reasoning, and task completion (particularly for complex/novel tasks), while the LLM-based evaluator tended to prefer GPT-4 due to fluency.",
            "limitations_or_challenges": "Performance depends on quality and coverage of integrated tools and the underlying LLM; risks include LLM hallucinations if tools are misused or return poor outputs, incomplete or invalid procedural outputs that may require human fixes to run on robotic platforms, safety/dual-use concerns, and reproducibility limits imposed by using closed-source API LLMs.",
            "comparison_to_baselines_or_humans": "Compared against GPT-4 prompted to 'act like an expert chemist' (tool-less baseline): experts rated ChemCrow higher on chemistry factuality and reasoning, especially for complex/novel tasks; GPT-4 sometimes scored higher on tasks that require memorization of well-known facts, and was favored by the LLM-based evaluator due to more fluent completions.",
            "uuid": "e3870.0",
            "source_info": {
                "paper_title": "Augmenting large language models with chemistry tools",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "LitSearch",
            "name_full": "LitSearch (literature search tool used in ChemCrow)",
            "brief_description": "A literature search and question-answering tool that embeds and searches user-supplied scientific documents (PDF/text/HTML) using OpenAI embeddings and FAISS and uses an LLM to summarize top passages relative to a query, anchoring LLM responses in the scientific literature.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "LitSearch (paper-qa + embeddings + FAISS)",
            "system_or_method_description": "Embeds documents and queries into vector space (OpenAI Embeddings), retrieves top-k relevant passages using FAISS, creates passage-level summaries in relation to the query, and feeds those summaries into the LLM prompt so the model generates answers grounded in the retrieved literature.",
            "input_corpus_description": "Arbitrary collections of scientific documents provided as PDFs or text files (including raw HTML); corpus size unspecified and determined by user-provided documents and any literature/web search results; documents are embedded into a vector index (FAISS).",
            "topic_or_query_specification": "Natural-language queries issued by the LLM agent or user; LitSearch returns top-k passages and summaries relevant to that query.",
            "distillation_method": "Retrieval-augmented generation (RAG): embed + retrieve top passages, summarize them with an LLM, and incorporate those summaries into the final answer; this anchors generation to source passages and includes references to papers.",
            "output_type_and_format": "Summaries of relevant passages, citations to source papers, and LLM-generated answers anchored to the retrieved literature (textual summaries suitable to be included in agent prompts and final responses).",
            "evaluation_or_validation_method": "Indirectly evaluated as part of ChemCrow by downstream task performance and human expert review; no standalone benchmarking numbers reported in the paper.",
            "results_summary": "Used by ChemCrow as the preferred literature retrieval tool (preferred over generic WebSearch) to ground answers in literature; contributed to more factual outputs and to features such as providing references and literature-backed safety/chemical information.",
            "limitations_or_challenges": "No corpus size or coverage guarantees; relies on quality of embeddings and retrieval hyperparameters (top-k); LLM summarization may still introduce omissions or misinterpretations if retrieved passages are insufficient or noisy.",
            "comparison_to_baselines_or_humans": "Compared implicitly to a generic WebSearch tool (SerpAPI) within ChemCrow; LitSearch was preferred by the agent and authors when literature grounding was needed, but no quantitative head-to-head metrics are reported.",
            "uuid": "e3870.1",
            "source_info": {
                "paper_title": "Augmenting large language models with chemistry tools",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "EvaluatorGPT",
            "name_full": "EvaluatorGPT (LLM-based evaluator used in this work)",
            "brief_description": "An LLM instructed to act as a teacher/evaluator that grades model outputs on whether tasks are addressed and the correctness of the thought process, and highlights strengths/weaknesses and suggestions for improvement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "EvaluatorGPT",
            "system_or_method_description": "A prompt-engineered use of an LLM (GPT-4) to grade responses: the evaluator LLM is instructed to assign grades (task addressed or not, thought process correctness), provide qualitative feedback, and explain scores; used as an automated evaluation signal alongside human expert review.",
            "input_corpus_description": "Model outputs (textual task solutions from ChemCrow and GPT-4 baseline) presented to the evaluator LLM; does not itself index or search external scholarly corpora for evaluation.",
            "topic_or_query_specification": "Evaluator is given the task prompt, the model completions, and an evaluation rubric via natural-language instructions and asked to provide grades and feedback.",
            "distillation_method": "Not a literature distillation method per se; it synthesizes evaluation judgments from the provided model outputs using an LLM's internal reasoning and the supplied rubric.",
            "output_type_and_format": "Graded scores (binary/graded items), qualitative comments on strengths/weaknesses, and suggestions for improvement.",
            "evaluation_or_validation_method": "Compared against a panel of human expert graders; the paper reports a discrepancy: EvaluatorGPT often preferred GPT-4 outputs based on fluency, while human experts preferred ChemCrow based on factuality.",
            "results_summary": "EvaluatorGPT tended to favor more fluent GPT-4 completions, failing to reliably penalize factual errors/hallucinations; the paper concludes that LLM-based self-evaluation can be unreliable when factuality is central.",
            "limitations_or_challenges": "Evaluator LLMs can be misled by fluency and completeness, failing to detect factual inaccuracies or flawed reasoning; reliance on the same model family can cause blind spots in assessment of factuality.",
            "comparison_to_baselines_or_humans": "Direct comparison in the paper: human expert evaluations disagreed with EvaluatorGPT in favoring ChemCrow; EvaluatorGPT favored GPT-4 due to style/fluency rather than factual correctness.",
            "uuid": "e3870.2",
            "source_info": {
                "paper_title": "Augmenting large language models with chemistry tools",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "LangChain",
            "name_full": "LangChain (framework)",
            "brief_description": "A software framework for building applications with LLMs that provides interfaces for prompts, document loaders, chains, indexes, agents, memory, and tool integration, used here to integrate ChemCrow's external tools with the LLM.",
            "citation_title": "LangChain",
            "mention_or_use": "use",
            "system_or_method_name": "LangChain",
            "system_or_method_description": "Provides the infrastructure to register external tools, build agent loops, manage prompts and chains, and connect LLM calls with tool executions; used as the application framework that orchestrates ChemCrow's LLM + tool interactions.",
            "input_corpus_description": "Framework-level: supports arbitrary documents and connectors (web, PDFs, vector stores). In this work it was used to connect to the set of tools (LitSearch, WebSearch, python REPL, ReactionPlanner, etc.).",
            "topic_or_query_specification": "User natural-language prompts are passed into LangChain agent constructs which map them to sequences of LLM calls and tool invocations.",
            "distillation_method": "Not a distillation algorithm; it is an orchestration layer that enables retrieval-augmented and tool-augmented generation by hooking LLM prompts to retrieval and tool outputs.",
            "output_type_and_format": "Orchestrated LLM outputs and tool results, structured according to the agent's Thought/Action loop and final summarization steps.",
            "evaluation_or_validation_method": "Used as part of the system implementation; no separate evaluation reported for LangChain itself in this paper.",
            "results_summary": "Used successfully to implement ChemCrow's agent behavior and tool integrations; contributed to modularity and extensibility of the system.",
            "limitations_or_challenges": "Framework-level limitations not detailed; general concerns include orchestration complexity and potential propagation of errors from tools into LLM prompts.",
            "comparison_to_baselines_or_humans": "No direct baseline comparison; LangChain served as the chosen integration framework for implementing the agent.",
            "uuid": "e3870.3",
            "source_info": {
                "paper_title": "Augmenting large language models with chemistry tools",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct: Synergizing reasoning and acting in language models",
            "brief_description": "A method/architecture that interleaves chain-of-thought style reasoning ('Thought') with explicit actions ('Action' and 'Action Input') that call external tools, enabling LLMs to perform multi-step reasoning grounded by tool outputs.",
            "citation_title": "React: Synergizing reasoning and acting in language models.",
            "mention_or_use": "mention",
            "system_or_method_name": "ReAct",
            "system_or_method_description": "A prompting/interaction protocol that instructs LLMs to alternate between internal reasoning traces and actions that call external tools, feeding tool outputs back as observations to continue the reasoning loop.",
            "input_corpus_description": "Architecture-agnostic; ReAct does not require a corpus but can be applied with retrieval tools or document corpora when needed.",
            "topic_or_query_specification": "Natural-language prompts specifying tasks; the model is guided to produce Thought/Action steps in its generation.",
            "distillation_method": "Chain-of-thought + tool invocation loop; when combined with retrieval tools it supports retrieval-augmented synthesis grounded in external documents.",
            "output_type_and_format": "Interleaved reasoning traces and tool action calls; final answer produced after iterative interactions.",
            "evaluation_or_validation_method": "Referenced as prior work and as inspiration for ChemCrow's interaction format; the paper cites ReAct as an established method but does not evaluate ReAct directly.",
            "results_summary": "Used conceptually to shape ChemCrow's Thought/Action/Observation loop; shown in prior work to improve reasoning+action tasks.",
            "limitations_or_challenges": "Mentioned in this paper as part of the conceptual foundation; known limitations include potential for verbose reasoning traces and sensitivity to prompt design.",
            "comparison_to_baselines_or_humans": "Mentioned as part of the architectural inspiration; no direct comparison in this paper.",
            "uuid": "e3870.4",
            "source_info": {
                "paper_title": "Augmenting large language models with chemistry tools",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "MRKL",
            "name_full": "MRKL Systems (Modular, neuro-symbolic architecture)",
            "brief_description": "A modular architecture that composes LLMs with external knowledge sources and discrete reasoning modules (tools) to extend capabilities beyond pure generation.",
            "citation_title": "MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.",
            "mention_or_use": "mention",
            "system_or_method_name": "MRKL",
            "system_or_method_description": "A conceptual framework for integrating LLMs with symbolic modules and external knowledge sources to perform tasks requiring exact computation, factual grounding, or discrete reasoning.",
            "input_corpus_description": "Framework-level; MRKL can ingest structured knowledge sources and documents via connected modules; no specific corpus used in this paper's mention.",
            "topic_or_query_specification": "Natural-language task prompts, routed to a combination of LLM reasoning and specialized modules as needed.",
            "distillation_method": "Neuro-symbolic composition: LLM for flexible language reasoning plus external modules (calculators, search, retrieval) to ground and disambiguate outputs; supports retrieval-augmented workflows when connected to document indexes.",
            "output_type_and_format": "Structured or textual outputs that combine symbolic module outputs and LLM-generated explanations.",
            "evaluation_or_validation_method": "Referenced as conceptual background; not evaluated directly in this paper.",
            "results_summary": "Cited as part of prior art motivating tool-augmented LLM agents like ChemCrow.",
            "limitations_or_challenges": "General challenges include module integration complexity and ensuring correct orchestration between LLM and symbolic modules.",
            "comparison_to_baselines_or_humans": "Mentioned as related work; no empirical comparison provided here.",
            "uuid": "e3870.5",
            "source_info": {
                "paper_title": "Augmenting large language models with chemistry tools",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Toolformer",
            "name_full": "Toolformer: Language models can teach themselves to use tools",
            "brief_description": "A method demonstrating that LLMs can be fine-tuned to learn when and how to call external tools by self-supervised training, improving tool-usage behavior.",
            "citation_title": "Toolformer: Language models can teach themselves to use tools.",
            "mention_or_use": "mention",
            "system_or_method_name": "Toolformer",
            "system_or_method_description": "A self-supervised fine-tuning approach where synthetic supervision is created for tool usage tokens, enabling LLMs to learn triggers and appropriate call patterns for external tools.",
            "input_corpus_description": "Not a literature-distillation system per se; operates on large text corpora augmented with synthesized tool-call annotations during fine-tuning.",
            "topic_or_query_specification": "Standard language prompts/tasks during pretraining/fine-tuning; the model learns to insert tool calls where helpful.",
            "distillation_method": "Self-supervised insertion of tool-call annotations into training data to teach LLMs to use tools; not specifically retrieval of scholarly papers.",
            "output_type_and_format": "LM outputs containing learned tool-call invocations and resultant composed outputs.",
            "evaluation_or_validation_method": "Mentioned in related work as motivation for tool-augmented LLMs; not experimentally evaluated within this paper.",
            "results_summary": "Cited as evidence that LLMs can be taught to call tools effectively, supporting the design choice of ChemCrow to rely on LLM-driven tool invocation.",
            "limitations_or_challenges": "Not discussed in detail here; general limitations include dependency on fine-tuning data and mismatch between training-time and deployment-time tool behavior.",
            "comparison_to_baselines_or_humans": "Mentioned as related literature; no direct comparisons reported in this paper.",
            "uuid": "e3870.6",
            "source_info": {
                "paper_title": "Augmenting large language models with chemistry tools",
                "publication_date_yy_mm": "2023-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "React: Synergizing reasoning and acting in language models.",
            "rating": 2
        },
        {
            "paper_title": "MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.",
            "rating": 2
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools.",
            "rating": 2
        },
        {
            "paper_title": "Emergent autonomous scientific research capabilities of large language models.",
            "rating": 2
        },
        {
            "paper_title": "MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action.",
            "rating": 1
        },
        {
            "paper_title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace.",
            "rating": 1
        }
    ],
    "cost": 0.01729075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Augmenting large language models with chemistry tools</h1>
<p>Andres M. Bran ${ }^{12 <em>}$ Sam Cox $^{3 </em>}$ Oliver Schilter ${ }^{24}$<br>Carlo Baldassari ${ }^{4}$ Andrew D. White ${ }^{3}$ Philippe Schwaller ${ }^{12}$<br>${ }^{1}$ Laboratory of Artificial Chemical Intelligence (LIAC), ISIC, EPFL<br>${ }^{2}$ National Centre of Competence in Research (NCCR) Catalysis, EPFL<br>${ }^{3}$ Department of Chemical Engineering, University of Rochester<br>${ }^{4}$ Accelerated Discovery, IBM Research - Europe<br>*Contributed equally.<br>andrew.white@rochester.edu<br>philippe.schwaller@epfl.ch</p>
<h4>Abstract</h4>
<p>Over the last decades, excellent computational chemistry tools have been developed. Integrating them into a single platform with enhanced accessibility could help reaching their full potential by overcoming steep learning curves. Recently, large-language models (LLMs) have shown strong performance in tasks across domains, but struggle with chemistry-related problems. Moreover, these models lack access to external knowledge sources, limiting their usefulness in scientific applications. In this study, we introduce ChemCrow, an LLM chemistry agent designed to accomplish tasks across organic synthesis, drug discovery, and materials design. By integrating 18 expert-designed tools, ChemCrow augments the LLM performance in chemistry, and new capabilities emerge. Our agent autonomously planned and executed the syntheses of an insect repellent, three organocatalysts, and guided the discovery of a novel chromophore. Our evaluation, including both LLM and expert assessments, demonstrates ChemCrow's effectiveness in automating a diverse set of chemical tasks. Surprisingly, we find that GPT-4 as an evaluator cannot distinguish between clearly wrong GPT-4 completions and Chemcrow's performance. Our work not only aids expert chemists and lowers barriers for non-experts, but also fosters scientific advancement by bridging the gap between experimental and computational chemistry. Publicly available code can be found at https://github.com/ur-whitelab/chemcrow-public.</p>
<h2>1 Introduction</h2>
<p>In the last few years, Language Language Models (LLMs) ${ }^{1-5}$ have transformed various sectors by automating natural language tasks. A prime example of this is the introduction of GitHub Copilot in $2021^{6}$ and more recently StarCoder ${ }^{7}$, which provides proposed code completions based on the context of a file and open windows that increases developers' productivity ${ }^{8}$. Most recent advances are based on the Transformer architecture ${ }^{9}$, introduced for neural machine translation and extended to various natural language processing tasks demonstrating remarkable few-shot and zero-shot performance ${ }^{2}$. Nevertheless, it is crucial to recognize the limitations of LLMs, which often struggle with seemingly simple tasks like basic mathematics and chemistry operations ${ }^{10,11}$. For instance, GPT-4 ${ }^{12}$ and GPT-3.5 ${ }^{13}$ cannot consistently and accurately multiply $12345 * 66765$ or convert IUPAC names into the corresponding molecular graph ${ }^{14}$. These shortcomings can be attributed to the models' core design, which focuses on predicting subsequent words. To address these limitations, one viable approach is to augment large language models with dedicated external tools or plugins, such as a calculator for mathematical operations or OPSIN ${ }^{15}$ for IUPAC to structure conversion. These specialized tools provide exact answers, thereby compensating for the inherent deficiencies of LLMs in specific domains and enhancing their overall performance and applicability.</p>
<p>Chemistry, as a field, has been impacted through expert-designed artificial intelligence (AI) systems that tackle specific problems, such as reaction prediction ${ }^{16-20}$, retrosynthesis planning ${ }^{21-27}$, molecular property prediction ${ }^{28-32}$, de-novo molecular generation ${ }^{33,34}$, materials design ${ }^{35,36}$ and, more recently, Bayesian Optimization ${ }^{37-39}$. Due to the nature of their training, it has been shown that code-generating LLMs do possess some understanding of chemistry ${ }^{14}$. By understanding, we mean that LLMs are capable of adapting to observations, planning over multiple steps, and responding correctly to intent ${ }^{13,40-44}$. However, the automation levels achieved in chemistry remain relatively low compared to other domains, primarily due to its highly experimental nature, the lack of data, as well as the limited scope and applicability of computational tools, even within their designated areas ${ }^{45}$.
Integrating such tools tends to occur within isolated environments, such as RXN for Chemistry ${ }^{18,24,46-48}$ and AIZynthFinder ${ }^{25,49,50}$, facilitated by corporate directives that promote integrability. Although most tools are developed by the open-source community or made accessible through application programming interfaces (API), their integration and interoperability pose considerable challenges for experimental chemists, mainly due to their lack of computational skill set and the diversity of tools with steep learning curves, thereby preventing the full exploitation of their potential.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview and toolset. a) An overview of the task-solving process. Using a variety of chemistryrelated packages and software, a set of tools is created. These tools and a user input are then given to an LLM. The LLM then proceeds through an automatic, iterative chain-of-thought process, deciding on its path, choice of tools, and inputs before coming to a final answer. The example shows the synthesis of DEET, a common insect repellent. b) Toolsets implemented in ChemCrow: reaction, molecule, safety, search, and standard tools.</p>
<p>Inspired by successful applications in other fields ${ }^{10,51,52}$, we propose an LLM-powered chemistry engine, ChemCrow, designed to streamline the reasoning process for various common chemical tasks across areas such as drug and materials design and synthesis. ChemCrow harnesses the power of multiple expertdesigned tools for chemistry and operates by prompting an LLM (GPT-4 in our experiments) with specific instructions about the task and the desired format, as shown in Figure 1a. The LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output. It is then instructed to answer a user-given prompt using the tools provided when necessary. The model is guided to follow the Thought, Action, Action Input, Observation format ${ }^{43}$, which requires it to reason about the current state of the task, consider its relevance to the final goal, and plan the next steps accordingly, demonstrating its level of understanding. After the reasoning in the Thought step, the LLM requests a tool (preceded by the</p>
<p>keyword "Action") and the input for this tool (with the keyword "Action Input"). The text generation then pauses, and the program attempts to execute the requested function using the provided input. The result is returned to the LLM prepended by the keyword "Observation", and the LLM proceeds to the Thought step again. It continues iteratively until the final answer is reached.
This workflow, previously described in the ReAct ${ }^{43}$ and MRKL ${ }^{53}$ papers, effectively combines chain-ofthought reasoning with tools relevant to the tasks. As a result, and as will be shown in the following sections, the LLM transitions from a hyperconfident - although typically wrong - information source, to a reasoning engine that is prompted to reflect on a task, act using a suitable tool to gather additional information, observe the tool's responses, and repeat this loop until the final answer is reached. Contemporaneously with this work, ${ }^{54}$ describes a similar approach of augmenting an LLM with tools for accomplishing tasks in chemistry that are out of reach of GPT-4 alone. Their focus is specifically on cloud labs, while ours investigates an extensive range of tasks and tools including the connection to a cloud-connected robotic synthesis platform. We implemented 18 tools, including web and literature search, as well as molecule-specific and reaction-specific tools, as shown in Figure 1b and described in Section 5.3, that endow ChemCrow not only with knowledge about molecular and reaction properties, but also with the capacity to directly execute tasks in a physical lab. While the list of tools included is not exhaustive, ChemCrow has been designed to be easily adapted to new applications by providing the tool, along with a description of its intended use, all through natural language. ChemCrow serves as an assistant to expert chemists while simultaneously lowering the entry barrier for non-experts by offering a simple interface to access accurate chemical knowledge. We analyze the capabilities of ChemCrow on 14 use cases (see Appendix G), including synthesizing target molecules, safety controls, and searching for molecules with similar modes of action.</p>
<h1>2 Results \&amp; Discussion</h1>
<h3>2.1 Autonomous chemical synthesis</h3>
<p>From simple user inputs such as Plan and execute the synthesis of an insect repellent (Figure 1a) and Find and synthesize a thiourea organocatalyst, which accelerates a Diels-Alder reaction. (Figure 2b), ChemCrow found corresponding molecules, planned syntheses, and executed the syntheses on the cloudconnected, proprietary RoboRXN platform from IBM Research ${ }^{55}$. Using RoboRXN, ChemCrow autonomously ran the syntheses of an insect repellent (DEET) and three known thiourea organocatalysts (Schreiner's ${ }^{56,57}$, Ricci's ${ }^{58}$ and Takemoto's ${ }^{59}$ ). To do so, ChemCrow sequentially queried tools, such as LitSearch/WebSearch, Name2SMILES, ReactionPlanner, and finally, ReactionExecute, to combine the information and solve the tasks. The synthesized structures are shown in Figure 2d, and the detailed description of the tools in Section 5.3. The four syntheses yielded the anticipated compounds successfully, demonstrating synthesis planning and execution-related LLM agent interactions with the physical world.
Standardized synthesis procedures are key for successful execution. However, the predicted procedures ${ }^{46}$ are not always directly executable on the RoboRXN platform; typical problems include "not enough solvent" or "invalid purify action". Addressing these issues requires human interaction to fix the invalid actions before attempting to execute the synthesis. ChemCrow is able to autonomously query the synthesis validation data from the platform and iteratively adapt the synthesis procedure (such as increasing solvent quantity) until the synthesis procedure is fully valid. The ActionCleaner functionality is included in the ReactionExecute tool, and does not require human intervention. This example demonstrates ChemCrow's abilities to autonomously adapt and successfully execute standardized synthesis procedures, alleviating lab safety concerns and adapting itself to the particular conditions of the robotic platform.</p>
<h3>2.2 Human-AI collaboration</h3>
<p>The interaction between humans and computers results particularly fruitful, especially in the realm of chemistry where decisions are often taken based on experimental results, and the execution of experiments themselves can turn out to be challenging, even beyond the capabilities of cutting-edge self-driving labs. Here we demonstrate how such an interaction can lead to the discovery of a novel chromophore. For this example, ChemCrow was instructed to train a machine-learning model to help screen a library of candidate chromophores. As can be seen in Figure 3, ChemCrow is capable of loading, cleaning, and processing the data, training and evaluating a Random Forest model, and finally providing a suggestion based on the model and the given target absorption maximum wavelength of 369 nm . The proposed molecule (see</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Experimental validation. a) Example of the script run by a user to initiate ChemCrow. b) Query and synthesis of a thiourea organocatalyst. c) The IBM Research RoboRXN synthesis platform on which the experiments were executed (pictures reprinted courtesy of International Business Machines Corporation). d) Experimentally validated compounds.</p>
<p>Figure 3) was subsequently synthesized and analyzed, confirming the discovery of a new chromophore with approximately the desired property (measured absorption maximum wavelength of 336 nm ).
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Human/Model interaction leading to the discovery of a novel chromophore. Left: The human input, actions, and observation. Right: ChemCrow actions and final answer with the suggestion of the novel chromophore.</p>
<h1>2.3 Evaluation across diverse chemical use cases</h1>
<p>In recent years, there has been a surge in the application of machine learning to chemistry, resulting in a wealth of datasets and benchmarks in the field ${ }^{60,61}$. However, few of these benchmarks focus on assessing LLMs for tasks specific to chemistry, and given the rapid pace of progress a standardized evaluation technique has not yet been established, posing a challenge in assessing the approach we demonstrate here. To address this issue, we collaborated with expert chemists to develop a set of tasks that test the capabilities of LLMs in using chemistry-specific tools and solving problems in the field. The selected tasks are executed by both ChemCrow and GPT-4 (the latter prompted to assume the role of an expert chemist), and these results are evaluated with a combination of LLM-based and expert human assessments. For the former, we draw inspiration from the evaluation methods described in ${ }^{5,62,63}$, where the authors use an evaluator LLM that is instructed to assume the role of a teacher assessing their students. In our case, we adapted the prompt so that the evaluator LLM (which we call EvaluatorGPT) only gives a grade based on whether the task is addressed or not, and whether the overall thought process is correct. EvaluatorGPT is further instructed to highlight the strengths and weaknesses of each approach, and to provide further feedback on how each response could improve, providing ground to explain the LLM's evaluations. Full results for several tasks, spanning synthetic planning for drugs, design of novel compounds with similar properties and modes of actions, and explaining reaction mechanisms, are presented in the Appendix G. The full examples are also available at https://github.com/ur-whitelab/chencrow-runs.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Evaluation results. Comparative performance of GPT-4 and ChemCrow across a range of tasks. a. Per-task preference. For each task, evaluators were asked which response they are more satisfied with. Tasks are split in three categories: Synthesis, Molecular design, and Chemical logic. Tasks are sorted by order of difficulty within the classes. b. Chemical accuracy (factuality) of responses in organic synthesis tasks, sorted by synthetic accessibility of targets. c. Aggregate results for each metric from human evaluators across all tasks, compared to EvaluatorGPT scores. The error bars represent the confidence interval $(95 \%)$.</p>
<p>It is worth noting that the validity of ChemCrow's responses depend on both the quality of the tools and the agent's reasoning process, each of which affects one another throughout ChemCrow's execution. For instance, synthetic planning capabilities can benefit from an improved underlying synthesis engine, an active area of research ${ }^{23,64,65}$. Even then, any tool becomes useless if the reasoning behind its usage is flawed, and garbage inputs are given to the tool. Similarly, inaccurate outputs from the tools can lead the agent to wrong conclusions. For these reasons, a panel of expert chemists were asked to evaluate each model's performance for each task across three dimensions: 1. Correctness of the chemistry, 2. Quality of reasoning, and 3. Degree of task completion, see the Appendix B. As shown in Figure 4 ChemCrow outperforms the tool-less LLM, especially on more complex tasks where more grounded</p>
<p>chemical reasoning is required. GPT-4 on the other hand systematically fails to provide factually accurate information, however using a more fluent and complete style, making it preferred by EvaluatorGPT; the hallucinations it produces are nevertheless unveiled upon thorough inspection. As shown in Figure 4a and 4b, GPT-4 only outperforms ChemCrow at easier tasks, where the objective is very clear and all necessary information is a part of GPT-4's training data, allowing it to offer more complete answers based almost purely on memorization of training data (e.g. synthesis of DEET and paracetamol). In contrast ChemCrow consistently offers better solutions across multiple objectives and difficulties, resulting in a strong preference from expert chemists in favor of ChemCrow, showing its potential as a tool for the practitioner chemist.</p>
<p>Note the difference between the human and the LLM-powered evaluations in Figure 4. While humans prefer and highly score ChemCrow's responses across all three proposed metrics, EvaluatorGPT concludes that on average GPT-4 is a better model, typically basing its results in the fluency and apparent completeness of its responses. GPT-4 has been recently presented and used as a self-evaluation method ${ }^{5,62}$, but these results indicate that when it lacks the required understanding to answer a prompt, it also lacks information to evaluate the prompt completions and thus fails to provide a trustworthy assessment, rendering it unusable for the benchmarking of LLM capabilities whenever factuality plays key roles in evaluation.</p>
<h1>3 Risk Mitigation Strategies</h1>
<p>The implementation and use of LLM-driven chemistry engines, like ChemCrow, empower non-expert researchers by facilitating streamlined combination of different expert-designed tools' outputs. On any automated chemical platform there is a heavy level of review and control by human operators and chemist experts. Nevertheless, it is crucial to ensure responsible development and use of LLM agents ${ }^{66-68}$.</p>
<p>We discuss the unintended risks and propose possible mitigation strategies. Those can be achieved through foresight and safeguards, still promoting open and transparent science to enable broad oversight and feedback from the research community.</p>
<p>Unintended Risks It is a worldwide standard safety guideline to provide access to chemical laboratories only to trained chemists. However, attempting to perform experiments based on the LLM-powered engine's recommendations may lead to accidents or hazardous situations. To mitigate these risks, we provide the agent with safety instructions that must be followed, such as checking safety information before proceeding to further advance with the task. As shown in Figure 5, ChemCrow follows a set of hard-coded guidelines (see Appendix D) by checking that the queried molecules are not known controlled chemicals and other safety information; execution stops in such a case. In case not, execution proceeds, and this information is reused by the model to provide a more complete answer including safety concerns of the suggested substances, as well as grounded recommendations on how to safely handle them. An additional and more general mitigation strategy could involve incorporating safety checks and expert review systems, ensuring that recommendations adhere to established safety standards and protocols.</p>
<p>Inaccurate or incomplete reasoning due to a lack of sufficient chemistry knowledge in the LLM-powered engine poses a another risk, as it may lead to flawed decision-making or problematic experiment results. One of the key points of this paper is that the integration of expert-designed tools can help mitigate the hallucination issues commonly associated with these models, thus reducing the risk of inaccuracy. However, concerns may still arise when the model is unable to adequately analyze different observations due to a limited understanding of chemistry concepts, potentially leading to suboptimal outcomes. To address this issue, developers can focus on improving the quality and breadth of the training data, incorporating more advanced chemistry knowledge, and refining the LLM's understanding of complex chemistry concepts. Additionally, a built-in validation or peer-review system, analog to the RLHF implemented for GPT-3.5 ${ }^{69,70}$, could be incorporated to help ensure the reliability of the engine's recommendations.</p>
<p>Encouraging users to critically evaluate the information provided by the LLM-powered engine and crossreference it with established literature and expert opinions can further mitigate the risk of relying on flawed reasoning ${ }^{71}$. By combining these approaches, developers can work towards minimizing the impact of insufficient chemistry knowledge on the engine's reasoning process and enhancing the overall effectiveness of LLM-powered chemistry engines ${ }^{72}$ like ChemCrow.
Addressing intellectual property issues is crucial for the responsible development and use of generative AI models ${ }^{73}$, like ChemCrow. Clearer guidelines and policies regarding the ownership of generated syntheses of chemical structures or materials, their predicted applications, as well as the potential infringement</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Safety guidelines provided by ChemCrow Example task, where safety information is explicitly requested along with the synthesis procedure for paracetamol (left). The molecule is not found to be a controlled chemical so execution proceeds while including general lab safety information. In cases where the input molecule is found to be a controlled chemical (right), execution stops with a warning indicating that it is illegal and unethical to propose compounds with similar properties to a controlled chemical.
of proprietary information, need to be established. Collaboration with legal experts, as well as industry stakeholders, can help in navigating these complex issues and implementing appropriate measures to protect intellectual property.
In summary, it is crucial to carefully consider and address the potential drawbacks associated with LLMpowered chemistry engines, such as ChemCrow, to ensure their safe and responsible application. By integrating expert-designed tools, the issue of model hallucination can be mitigated, while improving the quality and breadth of training data can enhance the engine's understanding of complex chemistry concepts. Implementing effective mitigation strategies, such as access controls, safety guidelines, and ethical policies, further contributes to minimizing risks and maximizing the positive impact of these engines on the field of chemistry. As the technology continues to evolve, collaboration and vigilance among developers, users, and industry stakeholders are essential in identifying and addressing new risks and challenges ${ }^{74,75}$, fostering responsible innovation and progress in the domain of LLM-powered chemistry engines.</p>
<h1>4 Conclusion</h1>
<p>In this study, we have demonstrated the development of ChemCrow, a novel LLM-powered method for integrating computational tools in chemistry. By combining the reasoning power of LLMs with the chemical expert knowledge from computational tools, ChemCrow showcases one of the first chemistry-related LLM agent interactions with the physical world. ChemCrow has successfully planned and synthesized an insect repellent, three organocatalysts, and guided the screening and synthesis of a novel chromophore with target</p>
<p>properties. Furthermore, ChemCrow is capable of independently solving reasoning tasks in chemistry, ranging from simple drug discovery loops to synthesis planning of substances across a wide range of molecular complexity, indicating its potential as a future chemical assistant Ã  la ChatGPT.</p>
<p>Although the current results are limited by the amount and quality of the chosen tools, the space of possibilities is vast, particularly as potential tools are not restricted to the chemistry domain. The incorporation of other language-based tools, image processing tools, and more could significantly enhance ChemCrow's capabilities. Additionally, while the selected evaluation tasks are limited, further research and development can expand and diversify these tasks to truly push the limits of what these systems can achieve.</p>
<p>Evaluation by expert chemists revealed that ChemCrow outperforms GPT-4 in terms of chemical factuality, reasoning and completeness of responses, particularly for increasingly complex tasks. Although GPT-4 may perform better for tasks that involve memorization, such as the synthesis of well-known molecules like paracetamol and aspirin, ChemCrow excels when tasks are novel or less known, which are the most useful and challenging cases. In contrast, LLM-powered evaluation tends to favor GPT-4, primarily due to the more fluent and complete-looking nature of its responses. However, it is important to note that the LLM-powered evaluation may not be as reliable as human evaluation in assessing the true effectiveness of the models in chemical reasoning. This discrepancy highlights the need for further refining evaluation methods to better capture the unique capabilities of systems like ChemCrow in solving complex, real-world chemistry problems.</p>
<p>The evaluation process is not without its challenges, and improved experimental design could enhance the validity of the results. One major challenge is the lack of reproducibility of individual results under the current API-based approach to LLMs, as closed-source models provide limited control, see Appendix E. Recent open-source models ${ }^{76-78}$ offer a potential solution to this issue, albeit with a possible trade-off in reasoning power. Additionally, implicit bias in task selection and the inherent limitations of testing chemical logic behind task solutions on a large scale present difficulties for evaluating ML systems. Despite these challenges, our results demonstrate the promising capabilities and potential of systems like ChemCrow to serve as valuable assistants in chemical laboratories and to address chemical tasks across diverse domains.</p>
<h1>5 Methods</h1>
<h3>5.1 LLMs</h3>
<p>The rise of LLMs in the last years, and their quick advancement, availability, and scaling in the last months, have opened the door to a wide range of applications and ideas. Usage of LLMs is further overpowered when used as part of some frameworks designed to exploit their zero-shot reasoning capabilities, as can be demonstrated by architectures like ReAct ${ }^{43}$ and MRKL ${ }^{53}$. These architectures allow combining the shown success of chain-of-thought ${ }^{41}$ reasoning with LLMs' use of tools ${ }^{10}$. For our experiments, we used OpenAI's GPT-4 ${ }^{12}$ with a temperature of 0.1 .</p>
<h3>5.2 LLMs application framework - LangChain</h3>
<p>LangChain ${ }^{79}$ is a comprehensive framework designed to facilitate the development of language model applications by providing support for various modules, including access to various LLMs, prompts, document loaders, chains, indexes, agents, memory, and chat functionality. With these modules, LangChain enables users to create various applications such as chatbots, question answering systems, summarization tools, and data-augmented generation systems. LangChain not only offers standard interfaces for these modules but also assists in integrating with external tools, experimenting with different prompts and models, and evaluating the performance of generative models. In our implementation, we integrate external tools through LangChain, as LLMs have been shown to perform better with tools ${ }^{10,32,80}$.</p>
<h3>5.3 Tools</h3>
<p>Although our implementation uses a limited set of tools, it must be noted that this tool set can very easily be expanded depending on needs and availability.</p>
<p>The tools used can be classified into general tools, molecular tools, and chemical reaction tools.</p>
<h1>5.3.1 General tools</h1>
<p>WebSearch The web search tool is designed to provide the language model with the ability to access relevant information from the web. Utilizing SerpAPI ${ }^{81}$, the tool queries search engines and compiles a selection of impressions from the first page of Google search results. This allows the model to collect current and relevant information across a broad range of scientific topics. A distinct characteristic of this instrument is its capacity to act as a launching pad when the model encounters a query it cannot tackle or is unsure of the suitable tool to apply. Integrating this tool enables the language model to efficiently expand its knowledge base, streamline the process of addressing common scientific challenges, and verify the precision and dependability of the information it offers. By default, LitSearch is preferred by the agent over the WebSearch tool.</p>
<p>LitSearch The literature search tool focuses on extracting relevant information from scientific documents such as PDFs or text files (including raw HTML) to provide accurate and well-grounded answers to questions. This tool utilizes the paper-qa python package (https://github.com/whitead/paper-qa). By leveraging OpenAI Embeddings ${ }^{82}$ and FAISS $^{83}$, a vector database, the tool embeds and searches through documents efficiently. A language model then aids in generating answers based on these embedded vectors.
The literature search process involves embedding documents and queries into vectors and searching for the top k passages in the documents. Once these relevant passages have been identified, the tool creates a summary of each passage in relation to the query. These summaries are then incorporated into the prompt, allowing the language model to generate an informed answer. By anchoring responses in the existing scientific literature, the literature search tool significantly enhances the model's capacity to provide reliable and accurate information for routine scientific tasks, while also including references to the relevant papers.</p>
<p>Python REPL One of Langchain's standard tools, python REPL provides ChemCrow with a functional Python shell. This tool enables the LLM to write and run Python code directly, making it easier to accomplish a wide range of complex tasks. These tasks can range from performing numerical computations to training AI models and performing data analysis.</p>
<p>Human This tool serves as a direct interface for human interaction, allowing the engine to ask a question and expect a response from the user. The LLM may request such tool whenever encounters difficulty or uncertainty regarding the next step. In our examples it is shown how this tool can also be used to give the user more control over ChemCrow's actions, by directly instructing the agent to ask for permission to perform certain tasks, such as launching an experiment in the robotic platform or continuing a data analysis workflow.</p>
<h3>5.3.2 Molecule tools</h3>
<p>Name2SMILES This tool is specifically designed to obtain the SMILES representation of a given molecule. By taking the name (or CAS number) of a molecule as input, it returns the corresponding SMILES string. The tool allows users to request tasks involving molecular analysis and manipulation, by referencing the molecule in natural language (e.g. caffeine, novastatine, etc), IUPAC names, etc. Our implementation queries chem-space ${ }^{84}$ as a primary source, and upon failure queries PubChem ${ }^{85}$ and the IUPAC to SMILES converter OPSIN ${ }^{86}$ as a last option.</p>
<p>SMILES2Price The purpose of this tool is to provide information on the purchasability and commercial cost of a specific molecule. By taking a molecule as input, it first utilizes molbloom ${ }^{87}$ to check whether the molecule is available for purchase (in ZINC20 ${ }^{88}$ ). Then, using chem-space API ${ }^{84}$, it returns the cheapest price available on the market, enabling the LLM to make informed decisions about the affordability and availability of the queried molecule toward the resolution of a given task.</p>
<p>Name2CAS The tool is designed to determine the Chemical Abstracts Service (CAS) number of a given molecule, using either a various types of input references such as common names, IUPAC names, or SMILES strings by querying the PubChem ${ }^{85}$ database. By converting these molecular representations into the unique CAS number, it greatly facilitates web searches and information retrieval for any molecule. The CAS number serves as a precise and universally recognized chemical identifier, enabling researchers to access relevant data and resources with ease, and ensuring that they obtain accurate and consistent information about the target molecule ${ }^{89}$.</p>
<p>Similarity The primary function of this tool is to evaluate the similarity between two molecules, utilizing the Tanimoto similarity measure ${ }^{90}$ based on the ECFP2 molecular fingerprints ${ }^{91}$ of the input molecules. This tool receives two molecules and returns a measure of the molecules' structural similarity, which is valuable for assessing the potential of molecular analogs in various applications, such as drug discovery and chemical research. This tool allows the model to calculate and compare the similarity between pairs of molecules. The Tanimoto similarity approach provides a robust and reliable comparison of molecular structures, allowing scientists to make informed decisions when exploring new molecular candidates or investigating structure-activity relationships.</p>
<p>ModifyMol This tool is designed to make alterations to a given molecule by generating a local chemical space around it using retro and forward synthesis rules. It employs the SynSpace package ${ }^{92}$, originally applied in counterfactual explanations for molecular machine learning ${ }^{93}$. The modification process utilizes 50 robust medchem reactions ${ }^{94}$, and the retrosynthesis is performed either via PostEra Manifold ${ }^{18,95}$ (upon availability of an API key) or by reversing the 50 robust reactions. The purchasable building blocks come from the Purchasable Mcule supplier building block catalogs ${ }^{96}$, although customization options are available. By taking the SMILES representation of a molecule as input, this tool returns a single modified molecule resulting from a small change. This tool gives the model the ability to explore structurally similar molecules and generate novel molecules. This enables researchers to explore new molecular structures, derivatives, and fine-tune their molecular candidates for specific applications, such as drug discovery and chemical research.</p>
<p>PatentCheck The patent checker tool is designed to verify whether a molecule has been patented or not, without the need for a web request. It utilizes molbloom ${ }^{87}$, a C library to check strings against a bloom filter, making it an efficient tool to assess compounds against known databases. The primary application of this tool, which is used in our implementations, is to determine if a molecule can be purchased by checking against the ZINC database of purchasable compounds. By taking a molecule's SMILES representation as input, the patent checker tool informs the LLM if a patent exists for that particular molecule, thus helping it avoid potential intellectual property conflicts and determine whether a given compound is novel.</p>
<p>FuncGroups This tool is designed to identify functional groups within a given molecule by analyzing a list of named SMARTS (SMiles ARbitrary Target Specification) patterns. By taking the SMILES representation of a single molecule as input, the functional group finder searches for matches between the molecule's structure and the predefined SMARTS patterns representing various functional groups.
Upon identifying these matches, the tool returns a list of functional groups present in the molecule. This information is essential for understanding the molecule's reactivity, properties, and potential applications in various scientific domains, such as drug discovery, chemical research, and materials science. By providing a comprehensive overview of a molecule's functional groups, the LLM can make informed decisions when designing experiments, synthesizing compounds, or exploring new molecular candidates.</p>
<p>SMILES2Weight The purpose of this tool is to calculate the molecular weight of a molecule, given a SMILES representation of that molecule. This tool utilizes RDKit ${ }^{97}$ to get the exact molecular weight from a SMILES string.</p>
<h1>5.3.3 Safety tools</h1>
<p>As mentioned in previous sections, safety is one of the most prominent issues regarding the development of tools like ChemCrow. One of the risk mitigation strategies that has been proposed is to provide built-in safety-assessment functionalities, that allow the LLM to assess the potential risks of any proposed molecule, reaction or procedure.</p>
<p>ControlledChemicalCheck Created to reduce unintended risks, this tool takes a molecule's CAS number and checks it against several lists of recognized Chemical Weapons and Precursors (Organisation for the Prohibition of Chemical Weapons Schedules 1-3 ${ }^{98}$ and The Australia Group's Export Control List: Chemical Weapons Precursors ${ }^{99}$ ). This tool is automatically invoked when a request is made for a synthesis method or execution for a given molecule. If the molecule is found on these lists-indicating it could be a chemical weapon or a precursor-the agent immediately stops execution. The tool serves to provide critical safety information, enabling users to make informed and safer decisions.</p>
<p>ExplosiveCheck This tool utilizes the Globally Harmonized System (GHS) to identify explosive molecules. It queries the PubChem database using molecular identifiers like common name, IUPAC name, or CAS number. If the molecule's GHS rating is "Explosive", the tool confirms its explosive nature. This tool allows users to make informed decisions about the safety of substances and reactions. In addition, ChemCrow automatically invokes this tool when a user requests a synthesis method, giving an appropriate warning or error to the user, thereby mitigating associated risks.</p>
<p>SafetySummary This tool provides a general safety overview for any given molecule. It produces a safety summary by querying data from the PubChem database ${ }^{85}$ and uses an LLM as interface to highlight four central aspects: Operational safety (potential risks for the operator, i.e. health concerns of handling the given substance), GHS information (general hazards and recommendations to handle the substance), environmental risks (any environmental concerns of the handling of the substance, along with recommendations for how to handle it), and societal impact: whether the substance is a known controlled chemical. Whenever no information is available, the LLM is permitted to fill in the gaps while explicitly stating so. In that case, GPT-4 is permitted to fill in the gaps, but must explicitly state so. This tool provides comprehensive and digestible safety information from the PubChem database, enabling users to make informed decisions and to take appropriate safety measures. Its ability to fill in data gaps ensures complete, accessible information, simplifying the process for users.</p>
<h1>5.3.4 Chemical reaction tools</h1>
<p>NameRXN This tool, powered by the proprietary software NameRxn from NextMove Software ${ }^{100}$, is designed to identify and classify a given chemical reaction based on its internal database of several hundred named reactions. By taking a reaction SMILES, the tool returns a classification code and the reaction name in natural language. The classification code corresponds to a position in the hierarchy proposed by Carey, Laffan, Thomson, and Williams ${ }^{101}$. This information is essential for understanding reaction mechanisms, selecting appropriate catalysts, and optimizing experimental conditions.</p>
<p>ReactionPredict The reaction prediction tool leverages the RXN4Chemistry API from IBM Research ${ }^{48}$, which utilizes a transformer model specifically tailored for predicting chemical reactions and retrosynthesis paths based on the Molecular Transformer ${ }^{18,24}$ and provides highly accurate predictions. This tool takes as input a set of reactants and returns the predicted product, allowing the LLM to have accurate chemical information that can't typically be obtained by a simple database query, but that requires a sort of abstract reasoning chemists are trained to perform. While the API is free to use, registration is required.</p>
<p>ReactionPlanner This powerful tool also employs the RXN4Chemistry API from IBM Research ${ }^{18,24,48}$, utilizing the same Transformer approach for translation tasks as the reaction prediction tool, but adding search algorithms to handle multi-step synthesis, and an action prediction algorithm that converts a reaction sequence into actionable steps in machine-readable format, including conditions, additives, and solvents ${ }^{102}$. To interface with ChemCrow, we added an LLM processing step that converts these machine-readable actions into natural language. The molecular synthesis planner is designed to assist the LLM in planning a synthetic route to prepare a desired target molecule. By taking the SMILES representation of the desired product as input, this tool enables ChemCrow to devise and compare efficient synthetic pathways toward the target compound.</p>
<p>ReactionExecute This tool allows ChemCrow direct interaction with the physical world through a robotic chemistry lab platform. Also based on the RXN4Chemistry API, the tool allows the agent to plan, adapt, and execute the synthesis of a given molecule. Internally, the tool requests a synthesis plan (using the RXNPlanner tool), obtains the action sequence to be executed on the robot, and uses a LLM-powered loop to adapt the errors and warnings in the action sequence. Finally it requests permission from the user to launch the synthesis, and returns a success message upon successful launching the action sequence.</p>
<h2>Data \&amp; Code availability</h2>
<p>All the experiments carried out in this study can be found under https://github.com/ur-whitelab/ chemcrow-runs. Additionally, an open-source version of the ChemCrow platform has been released at https://github.com/ur-whitelab/chemcrow-public, which includes the main agent setup and a subset of 12 tools used in the original implementation.</p>
<h1>Acknowledgements</h1>
<p>A.M.B., O.S. and P.S. acknowledge support from the NCCR Catalysis (grant number 180544), a National Centre of Competence in Research funded by the Swiss National Science Foundation. S.C. and A.D.W. acknowledge support from the NSF under grant number 1751471. Research reported in this work was supported by the National Institute of General Medical Sciences of the National Institutes of Health under award number R35GM137966. The authors thank the wider RXN for Chemistry team for the support and having granted limited access to the platform for the sole scope of executing the reported syntheses.</p>
<h2>References</h2>
<p>[1] Devlin, J.; Chang, M.-W.; Lee, K.; Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 2018,
[2] Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems 2020, 33, 1877-1901.
[3] Bommasani, R.; Hudson, D. A.; Adeli, E.; Altman, R.; Arora, S.; von Arx, S.; Bernstein, M. S.; Bohg, J.; Bosselut, A.; Brunskill, E., et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 2021,
[4] Chowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra, G.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.; Gehrmann, S., et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 2022,
[5] Bubeck, S.; Chandrasekaran, V.; Eldan, R.; Gehrke, J.; Horvitz, E.; Kamar, E.; Lee, P.; Lee, Y. T.; Li, Y.; Lundberg, S., et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712 2023,
[6] GitHub Copilot: Your AI pair programmer. https://copilot.github.com.
[7] Li, R. et al. StarCoder: may the source be with you! 2023.
[8] Ziegler, A.; Kalliamvakou, E.; Li, X. A.; Rice, A.; Rifkin, D.; Simister, S.; Sittampalam, G.; Aftandilian, E. Productivity assessment of neural code completion. 2022, 21-29.
[9] Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Å.; Polosukhin, I. Attention is all you need. Advances in neural information processing systems 2017, 30 .
[10] Schick, T.; Dwivedi-Yu, J.; DessÃ¬, R.; Raileanu, R.; Lomeli, M.; Zettlemoyer, L.; Cancedda, N.; Scialom, T. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761 2023,
[11] Castro Nascimento, C. M.; Pimentel, A. S. Do Large Language Models Understand Chemistry? A Conversation with ChatGPT. Journal of Chemical Information and Modeling 2023, 63, 1649-1655.
[12] OpenAI, GPT-4 Technical Report. 2023.
[13] Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A., et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems 2022, 35, 27730-27744.
[14] White, A. D.; Hocky, G. M.; Gandhi, H. A.; Ansari, M.; Cox, S.; Wellawatte, G. P.; Sasmal, S.; Yang, Z.; Liu, K.; Singh, Y., et al. Assessment of chemistry knowledge in large language models that generate code. Digital Discovery 2023,
[15] Lowe, D. M.; Corbett, P. T.; Murray-Rust, P.; Glen, R. C. Chemical Name to Structure: OPSIN, an Open Source Solution. Journal of Chemical Information and Modeling 2011, 51, 739-753, PMID: 21384929.
[16] Coley, C. W.; Barzilay, R.; Jaakkola, T. S.; Green, W. H.; Jensen, K. F. Prediction of organic reaction outcomes using machine learning. ACS central science 2017, 3, 434-443.</p>
<p>[17] Coley, C. W.; Jin, W.; Rogers, L.; Jamison, T. F.; Jaakkola, T. S.; Green, W. H.; Barzilay, R.; Jensen, K. F. A graph-convolutional neural network model for the prediction of chemical reactivity. Chem. Sci. 2019, 10, 370-377.
[18] Schwaller, P.; Laino, T.; Gaudin, T.; Bolgar, P.; Hunter, C. A.; Bekas, C.; Lee, A. A. Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction. ACS central science 2019, 5, 1572-1583.
[19] Pesciullesi, G.; Schwaller, P.; Laino, T.; Reymond, J.-L. Transfer learning enables the molecular transformer to predict regio-and stereoselective reactions on carbohydrates. Nat. Commun. 2020, 11, $1-8$.
[20] Irwin, R.; Dimitriadis, S.; He, J.; Bjerrum, E. J. Chemformer: a pre-trained transformer for computational chemistry. Machine Learning: Science and Technology 2022, 3, 015022.
[21] SzymkuÄ‡, S.; Gajewska, E. P.; Klucznik, T.; Molga, K.; Dittwald, P.; Startek, M.; Bajczyk, M.; Grzybowski, B. A. Computer-assisted synthetic planning: the end of the beginning. Angew. Chem. Int. Ed. 2016, 55, 5904-5937.
[22] Segler, M. H.; Preuss, M.; Waller, M. P. Planning chemical syntheses with deep neural networks and symbolic AI. Nature 2018, 555, 604-610.
[23] Coley, C. W.; Thomas, D. A.; Lummiss, J. A.; Jaworski, J. N.; Breen, C. P.; Schultz, V.; Hart, T.; Fishman, J. S.; Rogers, L.; Gao, H., et al. A robotic platform for flow synthesis of organic compounds informed by AI planning. Science 2019, 365.
[24] Schwaller, P.; Petraglia, R.; Zullo, V.; Nair, V. H.; Haeuselmann, R. A.; Pisoni, R.; Bekas, C.; Iuliano, A.; Laino, T. Predicting retrosynthetic pathways using transformer-based models and a hyper-graph exploration strategy. Chemical science 2020, 11, 3316-3325.
[25] Genheden, S.; Thakkar, A.; ChadimovÃ¡, V.; Reymond, J.-L.; Engkvist, O.; Bjerrum, E. AiZynthFinder: a fast, robust and flexible open-source software for retrosynthetic planning. J. Cheminf. 2020, 12, 1-9.
[26] Molga, K.; SzymkuÄ‡, S.; Grzybowski, B. A. Chemist Ex Machina: Advanced Synthesis Planning by Computers. Acc. Chem. Res. 2021, 54, 1094-1106.
[27] Schwaller, P.; Vaucher, A. C.; Laplaza, R.; Bunne, C.; Krause, A.; Corminboeuf, C.; Laino, T. Machine intelligence for chemical reaction space. Wiley Interdisciplinary Reviews: Computational Molecular Science 2022, 12, e1604.
[28] Mayr, A.; Klambauer, G.; Unterthiner, T.; Hochreiter, S. DeepTox: toxicity prediction using deep learning. Frontiers in Environmental Science 2016, 3, 80.
[29] Yang, K.; Swanson, K.; Jin, W.; Coley, C.; Eiden, P.; Gao, H.; Guzman-Perez, A.; Hopper, T.; Kelley, B.; Mathea, M., et al. Analyzing learned molecular representations for property prediction. Journal of chemical information and modeling 2019, 59, 3370-3388.
[30] Chithrananda, S.; Grand, G.; Ramsundar, B. Chemberta: Large-scale self-supervised pretraining for molecular property prediction. arXiv preprint arXiv:2010.09885 2020,
[31] van Tilborg, D.; Alenicheva, A.; Grisoni, F. Exposing the limitations of molecular machine learning with activity cliffs. Journal of Chemical Information and Modeling 2022, 62, 5938-5951.
[32] Jablonka, K. M.; Schwaller, P.; Ortega-Guerrero, A.; Smit, B. Is GPT-3 all you need for low-data discovery in chemistry? 2023,
[33] GÃ³mez-Bombarelli, R.; Wei, J. N.; Duvenaud, D.; HernÃ¡ndez-Lobato, J. M.; SÃ¡nchez-Lengeling, B.; Sheberla, D.; Aguilera-Iparraguirre, J.; Hirzel, T. D.; Adams, R. P.; Aspuru-Guzik, A. Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules. ACS Cent. Sci. 2018, 4, 268-276, PMID: 29532027.
[34] Blaschke, T.; ArÃºs-Pous, J.; Chen, H.; Margreitter, C.; Tyrchan, C.; Engkvist, O.; Papadopoulos, K.; Patronov, A. REINVENT 2.0: an AI tool for de novo drug design. Journal of chemical information and modeling 2020, 60, 5918-5922.</p>
<p>[35] Tao, Q.; Xu, P.; Li, M.; Lu, W. Machine learning for perovskite materials design and discovery. $n p j$ Computational Materials 2021, 7, 1-18, Number: 1 Publisher: Nature Publishing Group.
[36] GÃ³mez-Bombarelli, R. et al. Design of efficient molecular organic light-emitting diodes by a highthroughput virtual screening and experimental approach. Nature Materials 2016, 15, 1120-1127, Number: 10 Publisher: Nature Publishing Group.
[37] Shields, B. J.; Stevens, J.; Li, J.; Parasram, M.; Damani, F.; Alvarado, J. I. M.; Janey, J. M.; Adams, R. P.; Doyle, A. G. Bayesian reaction optimization as a tool for chemical synthesis. Nature 2021, 590, 89-96.
[38] Torres, J. A. G.; Lau, S. H.; Anchuri, P.; Stevens, J. M.; Tabora, J. E.; Li, J.; Borovika, A.; Adams, R. P.; Doyle, A. G. A Multi-Objective Active Learning Platform and Web App for Reaction Optimization. Journal of the American Chemical Society 2022, 144, 19999-20007.
[39] Ramos, M. C.; Michtavy, S. S.; Porosoff, M. D.; White, A. D. Bayesian Optimization of Catalysts With In-context Learning. arXiv preprint arXiv:2304.05341 2023,
[40] Marra, G.; Giannini, F.; Diligenti, M.; Gori, M. Integrating learning and reasoning with deep logic models. 2020, 517-532.
[41] Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Chi, E.; Le, Q.; Zhou, D. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903 2022,
[42] Ho, N.; Schmid, L.; Yun, S.-Y. Large Language Models Are Reasoning Teachers. arXiv preprint arXiv:2212.10071 2022,
[43] Yao, S.; Zhao, J.; Yu, D.; Du, N.; Shafran, I.; Narasimhan, K.; Cao, Y. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629 2022,
[44] Zelikman, E.; Wu, Y.; Mu, J.; Goodman, N. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems 2022, 35, 15476-15488.
[45] Zhao, Z.-W.; del Cueto, M.; Troisi, A. Limitations of machine learning models when predicting compounds with completely new chemistries: possible improvements applied to the discovery of new non-fullerene acceptors. Digital Discovery 2022, 1, 266-276.
[46] Vaucher, A. C.; Schwaller, P.; Geluykens, J.; Nair, V. H.; Iuliano, A.; Laino, T. Inferring experimental procedures from text-based representations of chemical reactions. Nature communications 2021, 12, 2573.
[47] Schwaller, P.; Probst, D.; Vaucher, A. C.; Nair, V. H.; Kreutter, D.; Laino, T.; Reymond, J.-L. Mapping the space of chemical reactions using attention-based neural networks. Nature machine intelligence 2021, 3, 144-152.
[48] rxn4Chemistry, rxn4Chemistry. https://github.com/rxn4chemistry/rxn4chemistry, 2020; Accessed: April 2023.
[49] Thakkar, A.; Kogej, T.; Reymond, J.-L.; Engkvist, O.; Bjerrum, E. J. Datasets and their influence on the development of computer assisted synthesis planning tools in the pharmaceutical domain. Chemical science 2020, 11, 154-168.
[50] Thakkar, A.; Selmi, N.; Reymond, J.-L.; Engkvist, O.; Bjerrum, E. J. "Ring breaker": neural network driven synthesis prediction of the ring system chemical space. Journal of medicinal chemistry 2020, $63,8791-8808$.
[51] Yang, Z.; Li, L.; Wang, J.; Lin, K.; Azarnasab, E.; Ahmed, F.; Liu, Z.; Liu, C.; Zeng, M.; Wang, L. MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action. arXiv preprint arXiv:2303.11381 2023,
[52] Shen, Y.; Song, K.; Tan, X.; Li, D.; Lu, W.; Zhuang, Y. HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace. 2023.</p>
<p>[53] Karpas, E.; Abend, O.; Belinkov, Y.; Lenz, B.; Lieber, O.; Ratner, N.; Shoham, Y.; Bata, H.; Levine, Y.; Leyton-Brown, K., et al. MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning. arXiv preprint arXiv:2205.00445 2022,
[54] Boiko, D. A.; MacKnight, R.; Gomes, G. Emergent autonomous scientific research capabilities of large language models. arXiv preprint 2023,
[55] IBM RoboRXN I Science I IBM Research â€” research.ibm.com. https://research.ibm.com/ science/ibm-roborxn/, [Accessed 12-May-2023].
[56] Wittkopp, A.; Schreiner, P. R. Metal-Free, Noncovalent Catalysis of Diels-Alder Reactions by Neutral Hydrogen Bond Donors in Organic Solvents and in Water. Chemistry - A European Journal 2003, 9, 407-414.
[57] Schreiner, P. R.; Wittkopp, A. H-Bonding Additives Act Like Lewis Acid Catalysts. Organic Letters 2002, 4, 217-220, Publisher: American Chemical Society.
[58] Herrera, R. P.; Sgarzani, V.; Bernardi, L.; Ricci, A. Catalytic Enantioselective Friedel-Crafts Alkylation of Indoles with Nitroalkenes by Using a Simple Thiourea Organocatalyst. Angewandte Chemie International Edition 2005, 44, 6576-6579.
[59] Okino, T.; Hoashi, Y.; Takemoto, Y. Enantioselective Michael Reaction of Malonates to Nitroolefins Catalyzed by Bifunctional Organocatalysts. Journal of the American Chemical Society 2003, 125, 12672-12673, Publisher: American Chemical Society.
[60] Lowe, D. M. Extraction of chemical structures and reactions from the literature. Ph.D. thesis, University of Cambridge, 2012.
[61] Wu, Z.; Ramsundar, B.; Feinberg, E. N.; Gomes, J.; Geniesse, C.; Pappu, A. S.; Leswing, K.; Pande, V. MoleculeNet: a benchmark for molecular machine learning. Chemical science 2018, 9, $513-530$.
[62] Liu, Y.; Iter, D.; Xu, Y.; Wang, S.; Xu, R.; Zhu, C. GPTEval: NLG Evaluation using GPT-4 with Better Human Alignment. arXiv preprint arXiv:2303.16634 2023,
[63] Eloundou, T.; Manning, S.; Mishkin, P.; Rock, D. Gpts are gpts: An early look at the labor market impact potential of large language models. arXiv preprint arXiv:2303.10130 2023,
[64] Grzybowski, B. A.; Badowski, T.; Molga, K.; SzymkuÄ‡, S. Network search algorithms and scoring functions for advanced-level computerized synthesis planning. WIREs Computational Molecular Science 2023, 13, e1630.
[65] Thakkar, A.; Johansson, S.; Jorner, K.; Buttar, D.; Reymond, J.-L.; Engkvist, O. Artificial intelligence and automation in computer aided synthesis planning. Reaction chemistry \&amp; engineering 2021, 6, 27-51.
[66] Urbina, F.; Lentzos, F.; Invernizzi, C.; Ekins, S. Dual use of artificial-intelligence-powered drug discovery. Nature Machine Intelligence 2022, 4, 189-191.
[67] Urbina, F.; Lentzos, F.; Invernizzi, C.; Ekins, S. A teachable moment for dual-use. Nature machine intelligence 2022, 4, 607-607.
[68] Campbell, Q. L.; Herington, J.; White, A. D. Censoring chemical data to mitigate dual use risk. arXiv preprint arXiv:2304.10510 2023,
[69] Gao, L.; Schulman, J.; Hilton, J. Scaling Laws for Reward Model Overoptimization. arXiv preprint arXiv:2210.10760 2022,
[70] Radford, A.; Narasimhan, K.; Salimans, T.; Sutskever, I., et al. Improving language understanding by generative pre-training. 2018,
[71] Li, B.; Qi, P.; Liu, B.; Di, S.; Liu, J.; Pei, J.; Yi, J.; Zhou, B. Trustworthy AI: From Principles to Practices. ACM Computing Surveys 2021, 55, 1 - 46.</p>
<p>[72] Hocky, G. M.; White, A. D. Natural language processing models that automate programming will transform chemistry research and teaching. Digital Discovery 2022, 1, 79-83.
[73] Henderson, P.; Li, X.; Jurafsky, D.; Hashimoto, T.; Lemley, M. A.; Liang, P. Foundation Models and Fair Use. arXiv preprint arXiv:2303.15715 2023,
[74] Askell, A.; Brundage, M.; Hadfield, G. The Role of Cooperation in Responsible AI Development. 2019.
[75] Neufville, R. d.; Baum, S. D. Collective action on artificial intelligence: A primer and review. Technology in Society 2021, 66, 101649.
[76] Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.; Lacroix, T.; RoziÃ¨re, B.; Goyal, N.; Hambro, E.; Azhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; Lample, G. LLaMA: Open and Efficient Foundation Language Models. 2023.
[77] Chiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y.; Wu, Z.; Zhang, H.; Zheng, L.; Zhuang, S.; Zhuang, Y.; Gonzalez, J. E.; Stoica, I.; Xing, E. P. Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \% *$ ChatGPT Quality. 2023; https://lmsys.org/blog/2023-03-30-vicuna/.
[78] Mukherjee, S.; Mitra, A.; Jawahar, G.; Agarwal, S.; Palangi, H.; Awadallah, A. Orca: Progressive Learning from Complex Explanation Traces of GPT-4. 2023.
[79] Chase, H. LangChain. 2022; https://github.com/hwchase17/langchain.
[80] Press, O.; Zhang, M.; Min, S.; Schmidt, L.; Smith, N. A.; Lewis, M. Measuring and Narrowing the Compositionality Gap in Language Models. arXiv preprint arXiv:2210.03350 2022,
[81] SerpAPI, SerpAPI - Google Search Results API. 2023; https://serpapi.com/.
[82] Neelakantan, A.; Xu, T.; Puri, R.; Radford, A.; Han, J. M.; Tworek, J.; Yuan, Q.; Tezak, N.; Kim, J. W.; Hallacy, C., et al. Text and code embeddings by contrastive pre-training. arXiv preprint arXiv:2201.10005 2022,
[83] Johnson, J.; Douze, M.; JÃ©gou, H. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data 2019, 7, 535-547.
[84] ChemSpace, ChemSpace - Provider of Chemical Building Blocks, Fragment Libraries, and Screening Compounds. https://chem-space.com/, 2023.
[85] National Center for Biotechnology Information, PubChem. https://pubchem.ncbi.nlm.nih. gov/, 2023.
[86] Lowe, D. M.; Corbett, P. T.; Murray-Rust, P.; Glen, R. C. Chemical name to structure: OPSIN, an open source solution. 2011.
[87] Medina, J.; White, A. D. Bloom filters for molecules. arXiv preprint arXiv:2304.05386 2023,
[88] Irwin, J. J.; Tang, K. G.; Young, J.; Dandarchuluun, C.; Wong, B. R.; Khurelbaatar, M.; Moroz, Y. S.; Mayfield, J.; Sayle, R. A. ZINC20â€”a free ultralarge-scale chemical database for ligand discovery. Journal of chemical information and modeling 2020, 60, 6065-6073.
[89] Chemical Abstracts Service, CAS Registry Number. https://www.cas.org/content/ cas-registry, Accessed: April 2023.
[90] TT, T. An elementary mathematical theory of classification and prediction; 1958.
[91] Rogers, D.; Hahn, M. Extended-connectivity fingerprints. Journal of chemical information and modeling 2010, 50, 742-754.
[92] White, Andrew D, SynSpace. https://github.com/whitead/synspace, Accessed: April 2023; GitHub repository.
[93] Wellawatte, G. P.; Seshadri, A.; White, A. D. Model agnostic generation of counterfactual explanations for molecules. Chemical science 2022, 13, 3697-3705.</p>
<p>[94] MedChemComm. Accessed: 5 April 2023,
[95] Yang, Q.; Sresht, V.; Bolgar, P.; Hou, X.; Klug-McLeod, J. L.; Butler, C. R., et al. Molecular transformer unifies reaction prediction and retrosynthesis across pharma chemical space. Chemical communications 2019, 55, 12152-12155.
[96] Mcule, Purchasable Mcule. https://purchasable.mcule.com/, Accessed: April 2023.
[97] RDKit: Open-source cheminformatics. http://www.rdkit.org, [Online; accessed April 2023].
[98] Chemical Weapons Convention, Annex on Chemicals, B. Schedules of Chemicals. http://www. opcw.org, Accessed: May 2023.
[99] Group, A. Export Control List: Chemical Weapons Precursors. https://australiagroup.net/ en/, Accessed: May 2023.
[100] NextMove Software, NameRxn. https://www.nextmovesoftware.com/namerxn.html, last accessed 2020/11/07; Accessed: 5 April 2023.
[101] Carey, J. S.; Laffan, D.; Thomson, C.; Williams, M. T. Analysis of the reactions used for the preparation of drug candidate molecules. Organic \&amp; biomolecular chemistry 2006, 4, 2337-2347.
[102] Vaucher, A. C.; Schwaller, P.; Geluykens, J.; Nair, V. H.; Iuliano, A.; Laino, T. Inferring experimental procedures from text-based representations of chemical reactions. Nature Communications 2021, 12, 2573, Number: 1 Publisher: Nature Publishing Group.
[103] Edwards, C.; Lai, T.; Ros, K.; Honke, G.; Cho, K.; Ji, H. Translation between Molecules and Natural Language. 2022.
[104] Christofidellis, D.; Giannone, G.; Born, J.; Winther, O.; Laino, T.; Manica, M. Unifying Molecular and Textual Representations via Multi-task Language Modelling. arXiv preprint arXiv:2301.12586 2023,
[105] Azamfirei, R.; Kudchadkar, S. R.; Fackler, J. Large language models and the perils of their hallucinations. Critical Care 2023, 27, 1-2.
[106] Khapra, M. M.; Sai, A. B. A tutorial on evaluation metrics used in natural language generation. 2021, 15-19.
[107] Melis, G.; Dyer, C.; Blunsom, P. On the state of the art of evaluation in neural language models. arXiv preprint arXiv:1707.05589 2017,
[108] Flam-Shepherd, D.; Zhu, K.; Aspuru-Guzik, A. Language models can learn complex molecular distributions. Nature Communications 2022, 13, 3293.</p>
<h1>A Experimental procedures</h1>
<h2>A. 1 Insect repellent</h2>
<h2>Synthesis of N,N-Diethyl-m-toluamide (DEET)</h2>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>A 100 ml stainless-steel reactor inertized by vacuum and nitrogen flushing the reactor three times. To the reactor diethylamine ( $0.3 \mathrm{ml}, 4.1 \mathrm{mmol}$ ) and DCM ( 15 ml ) were added. A solution of 3-methylbenzoyl chloride ( $3.2 \mathrm{ml}, 3.2 \mathrm{mmol}, 1 \mathrm{M}$ in DCM) was added and the mixture was stirred at $25^{\circ} \mathrm{C}$ for 60 min . The reaction mixture was extracted with water $(15 \mathrm{ml})$ and DCM $(10 \mathrm{ml})$. The organic layer was collected and analyzed by taking a 0.3 ml sample. The sample was diluted with acetonitrile 100 times, filtered and injected into an HPLC/MS setup. MS (ES): m/z 192 [M+H] calculated, found $192.14 \mathrm{~m} / \mathrm{z}$.</p>
<h2>A. 2 Thiourea catalysts</h2>
<p>Synthesis of the Schreiners' catalyst:
1,3-Bis[3,5-bis(trifluoromethyl)phenyl]thiourea
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>A 100 ml stainless-steel reactor inertized by vacuum and nitrogen flushing the reactor three times. To this reactor a solution of 3,5-bis(trifluoromethyl)phenyl isothiocyanate ( $0.4 \mathrm{ml}, 4 \mathrm{mmol}, 1 \mathrm{M}$ in THF), and a solution of 3,5-bis(trifluoromethyl)aniline ( $0.3 \mathrm{ml}, 3 \mathrm{mmol}, 1 \mathrm{M}$ in THF) were added. The mixture was diluted with 14.3 ml of THF and stirred for 1 h at $60^{\circ} \mathrm{C}$. A 0.3 ml sample of the reaction mixture was diluted 10x with acetonitrile ( 2.7 ml ), filtered and injected into an HPLC/MS setup. MS (ES): m/z 501 [M+H] calculated, found $501.02 \mathrm{~m} / \mathrm{z}$.</p>
<h2>Synthesis of Takemoto catalyst:</h2>
<p>1-(3,5-Bis(trifluoromethyl)phenyl)-3-((1R,2R)-2-(dimethylamino)cyclohexyl)thiourea
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>A 100 mL stainless-steel reactor was inertized by vacuum and nitrogen flushing three times. To this reactor a solution of trans-N,N-dimethylcyclohexane-1,2-diamine ( $0.3 \mathrm{ml}, 3 \mathrm{mmol}, 1 \mathrm{M}$ in THF) was added and diluted with 14.7 ml of THF before adding 0.5 ml of a 3,5-bis(trifluoromethyl)phenyl isothiocyanate ( 0.5 ml , $5 \mathrm{mmol}, 1 \mathrm{M}$ in THF). The reaction mixture was stirred for 24 h at room temperature $\left(25^{\circ} \mathrm{C}\right)$. A 0.3 ml sample of the reaction mixture was diluted ( 10 x ) with acetonitrile ( 2.7 ml ) and filtered, before injecting into a HPLC/MS. MS(ES): m/z 413 [M+H] calculated, found $413.14 \mathrm{~m} / \mathrm{z}$.</p>
<h2>Synthesis of the Ricci's thiourea catalyst:</h2>
<p>(1-(3,5-Bis(trifluoromethyl)phenyl)-3-((1R,2S)-2-hydroxy-2,3-dihydro-1H-inden-1-yl)thiourea)
To a three times with nitrogen and vacuum inertized 100 ml stainless-steel reactor a glass ampule with (1R,2S)-1-amino-2-indanol ( $1.5 \mathrm{mmol}, 223.8 \mathrm{mg}$ ) was added. The reactor was pressurized, which causes the glass ampule to break then 15 ml of THF were added to the reactor. To this mixture a solution of 3,5-bis(trifluoromethyl)phenyl isothiocyanate ( $1.5 \mathrm{ml}, 1.5 \mathrm{mmol}, 1 \mathrm{M}$ in THF) was added. The reaction</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" />
mixture was stirred for 24 h at room temperature $\left(25^{\circ} \mathrm{C}\right)$. A sample $(0.3 \mathrm{ml})$ of the reaction mixture was diluted (10x) with acetonitrile and filtered, before injecting into an HPLC/MS. MS(ES): m/z 421 [M+H] calculated, found $421.08 \mathrm{~m} / \mathrm{z}$.</p>
<h1>A. 3 Chromophore Synthesis</h1>
<h2>Step 1: N-(4'-vinyl-[1,1'-biphenyl]-3-yl)methanesulfonamide</h2>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>To a round bottom flask 1-bromo-4-ethenylbenzene ( $170.2 \mathrm{mg}, 0.122 \mathrm{ml} 0.93 \mathrm{mmol}, 1 \mathrm{eq}$ ) was added followed by [3-(methanesulfonamido)phenyl]boronic acid (200mg, 0.93mmol, 1eq), SPhosPd G2 (70mg, $0.09 \mathrm{mmol}, 0.1 \mathrm{eq}$ ) and sodium carbonate ( $123 \mathrm{mg}, 1.63 \mathrm{mmol}, 1.25 \mathrm{eq})$. To this flask 8 ml acetonitrile and 2 ml of water was added and the mixture was for 2 h 20 min at $90^{\circ} \mathrm{C}$. After letting the mixture cool down to room temperature, 10 ml of water was added and the mixture was extracted twice with Ethyl acetate ( 2 x 15 ml ). The organic layers were combined and washed with 15 ml of brine. The organic layer was dried over Na2SO4. The mixture was concentrated and purified using a column chromatography (Silica column) with a gradient of n-hexane: Ethyl acetate from $10 \%$ to $50 \%$. The product N-(4'-vinyl-[1,1'-biphenyl]-3yl)methanesulfonamide was determined via MS(ESI) and NMR. HPLC-MS(ESI): R.T. $5.315 \mathrm{~min} .[\mathrm{M}+\mathrm{H}]$ calculated $274.3573 \mathrm{~m} / \mathrm{z}$, found $274.0901 \mathrm{~m} / \mathrm{z}[\mathrm{M}+\mathrm{NH} 4]$ calculated $291.3839 \mathrm{~m} / \mathrm{z}$, found $291.1171 \mathrm{~m} / \mathrm{z} 1 \mathrm{H}$ NMR ( 80 MHz , Chloroform-d) $\delta 7.64-7.22(\mathrm{~m}, 9 \mathrm{H}), 7.07-6.72(\mathrm{~m}, 1 \mathrm{H}), 5.84(\mathrm{~d}, \mathrm{~J}=17.5 \mathrm{~Hz}, 1 \mathrm{H}), 5.34$ $(\mathrm{d}, \mathrm{J}=10.8 \mathrm{~Hz}, 1 \mathrm{H}), 3.10(\mathrm{~s}, 3 \mathrm{H}) .13 \mathrm{C}$ NMR ( 20 MHz , Chloroform-d) $\delta 140.59,139.41,138.40,137.35$, $136.45,130.16,127.27,126.78,124.09,119.56,119.18,114.32,43.14$.</p>
<h2>Step 2: (E)-3-methyl-4-(2-(3'-(methylsulfonamido)-[1,1'-biphenyl]-4-yl)vinyl)benzoate</h2>
<p><img alt="img-10.jpeg" src="img-10.jpeg" />
$\mathrm{N}-(4^{\prime}$-vinyl-[1,1'-biphenyl]-3-yl)methanesulfonamide ( $60.1 \mathrm{mg}, 0.22 \mathrm{mmol}$, 1eq) was added to a round bottom flask and a mixture was formed by adding acetonitrile ( 2 ml ). Methyl 4-iodo-3-methyl-benzoate ( $66 \mathrm{mg}, 0.23 \mathrm{mmol}, 1.08 \mathrm{eq}$ ) was added, followed by an addition of palladium acetate ( $2.5 \mathrm{mg} 0.01 \mathrm{mmol}$, 0.05 eq ) and triethylamine ( $217.8 \mathrm{mg}, 0.3 \mathrm{ml}, 2.15 \mathrm{mmol} 9.78$ eq.). The mixture was refluxed for 4 h and cooled down. To the cool mixture 10 ml of ethyl acetate was added and extracted with 2 x 10 ml of 1 M aqueous HCL solution. The aqueous phase was washed with 10 ml ethyl acetate. The organic layers were combined washed with 20 ml of Brine followed by drying with NaSO 4 . The mixture was filtered and purified using column chromatography (Silica) with a gradient of n-hexane:ethyl acetate from 30:70 to 50:50. The product methyl (E)-3-methyl-4-(2-(3'-(methylsulfonamido)-[1,1'-biphenyl]-4-yl)vinyl)benzoate was determined via MS(ESI) and NMR. HPLC-MS(ESI): R.T. $6.031 \mathrm{~min} .[\mathrm{M}+\mathrm{H}]$ calculated $422.5159 \mathrm{~m} / \mathrm{z}$, found $422.1418 \mathrm{~m} / \mathrm{z}[\mathrm{M}+\mathrm{NH} 4]$ calculated $439.5425 \mathrm{~m} / \mathrm{z}$, found $439.1686 \mathrm{~m} / \mathrm{z}$ 1H NMR ( 80 MHz , Chloroform-d) $\delta 8.02-7.29(\mathrm{~m}, 13 \mathrm{H}), 6.55(\mathrm{~s}, 1 \mathrm{H}), 4.02(\mathrm{~s}, 3 \mathrm{H}), 3.16(\mathrm{~s}, 3 \mathrm{H}), 2.59$ $(\mathrm{s}, 3 \mathrm{H}) .13 \mathrm{C}$ NMR ( 20 MHz , Chloroform-d) $\delta 166.58,139.89,138.91,137.84,137.46,136.20,135.38$,$ 131.73,130.28,130.12,128.96,127.56,127.51,127.45,127.40,126.48,124.04,120.91,116.30,53.23$, $39.23,19.96$.</p>
<h1>B Human evaluation</h1>
<p>Human evaluation was carried out by a panel of four expert chemists. In order to facilitate their assessment of the models' performance, an evaluation sheet was prepared and provided. This sheet included the answers given by each model for every task. Whenever a molecular structure or reaction (IUPAC or SMILES notation) was mentioned in the text, it was converted to the corresponding graph depiction using the open-source IUPAC parsing tool OPSIN ${ }^{15}$. Preparing this sheet proved a challenge, as some responses from GPT-4 required significant human interpretation. An example response and its corresponding interpretation is shown below and in Figure 6.</p>
<p>To synthesize Takemoto's organocatalyst (1-[3,5-bis(trifluoromethyl)phenyl]-3[(1R,2R)-(-)-2-(dimethylamino)cyclohexyl]thiourea), we can follow a stepwise approach. Here is a possible synthetic route:</p>
<ol>
<li>Starting with (1R,2R)-(-)-2-(dimethylamino)cyclohexanol, protect the alcohol group using a suitable protecting group, such as a tert-butyldimethylsilyl (TBS) group. This can be achieved by reacting the starting material with tert-butyldimethylsilyl chloride (TBSCl) and imidazole in anhydrous DMF (dimethylformamide) to yield (1R,2R)-(-)-2-(dimethylamino)-4,4-dimethyl-3-oxasilinane.</li>
<li>Next, convert the amine group to an isothiocyanate group. This can be done by first converting the amine to a thiourea derivative using thiophosgene (CSCI2) in Nanhydrous THF (tetrahydrofuran) and then treating the resulting thiourea with a strong base, such as sodium hydride $(\mathrm{NaH})$, to yield $(1 \mathrm{R}, 2 \mathrm{R})$ -(-)-2-isothiocyanato-4,4-dimethyl-3-oxasilinane.</li>
<li>In a separate reaction, synthesize the 3,5-bis(trifluoromethyl)phenylamine starting from 3,5-bis(trifluoromethyl)aniline. Protect the amine group using a suitable protecting group, such as a Boc (tert-butoxycarbonyl) group. This can be achieved by reacting the starting material with di-tert-butyl dicarbonate (Boc2O) and a suitable base, such as triethylamine (Et3N), in anhydrous dichloromethane (DCM) to yield N-Boc-3,5-bis(trifluoromethyl)aniline.</li>
<li>Convert the protected aniline to the corresponding thiourea derivative by reacting it with carbon disulfide (CS2) and a suitable base, such as potassium tert-butoxide (t-BuOK), in anhydrous THF to yield N-Boc-3,5bis(trifluoromethyl)phenyl thiourea.</li>
<li>Now, couple the two protected thiourea derivatives from steps 2 and 4. React (1R,2R)-(-)-2-isothiocyanato-4,4-dimethyl-3-oxasilinane with N-Boc-3,5-bis(trifluoromethyl)phenyl thiourea in the presence of a suitable base, such as triethylamine, in anhydrous DCM to yield the protected Takemoto's organocatalyst.</li>
<li>Finally, deprotect the alcohol and amine groups. Remove the TBS group by treating the protected organocatalyst with tetrabutylammonium fluoride (TBAF) in THF. Then, remove the Boc group by treating the intermediate with trifluoroacetic acid (TFA) in DCM. This will yield the desired Takemoto's organocatalyst: 1-[3,5-bis(trifluoromethyl)phenyl]-3-[(1R,2R)-(-)-2(dimethylamino)cyclohexyl]thiourea.</li>
</ol>
<p>During the interpretation of these outputs, commonly found issues included inconsistencies between the given IUPAC name of a substance and the verbal description of the transformations leading to it. Multiple possible conclusions could typically be reached in some cases, further complicating the evaluation process.
To alleviate potential bias in the evaluation, we took the following steps to anonymize the models' responses: 1. Randomly shuffling the order of presentation of the models (i.e., for a given task, ChemCrow's answer shown before or after GPT's at random). 2. Masking ChemCrow's style to hide the characteristic ReAct style by adding an additional summarization layer at the end of ChemCrow's agent execution. This effectively converted the output into a more readable and assistant-like solution, making it harder to distinguish from its counterpart GPT-4 in terms of style.</p>
<h1>C Synthesis example: GPT-4 vs ChemCrow</h1>
<p>Hallucination in LLMs is an issue that ChemCrow seeks to tackle through the addition of expert tools. Figure 6 displays the results from GPT-4 and ChemCrow on the task of synthesizing Takemoto's organocatalyst, a bifunctional organocatalyst that enables enantioselective Michael reactions of malonates to nitroolefins ${ }^{59}$. The complete task is shown in Appendix G.14.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 6: Expert analysis of models' output. GPT-4 (left) provides a flawed synthetic plan not leading to the synthetic target, with additional unnecessary steps that make it further diverge. ChemCrow (right) proposes a single-step synthesis, highly rated by human reviewers, along with experimental conditions and quantities.</p>
<p>As shown, the synthetic plan proposed by ChemCrow is a simple disconnection that leads to a isothiocyanate and the chiral substituted cyclohexane to form the desired thiourea, providing experimental conditions like solvent, temperature and reaction times alongside. GPT-4's response proposes a long synthesis with a series of unnecessary protection/deprotection sequences, uses unnecesary condensations making the route diverge from the target, and proposes a disconnection that potentially risks the chiral center by using it to place the thioisocyanate. Apart from that, reactions stemming from GPT-4 are generally challenging to use, as they require a lot of human interpretation and the proposed molecules (given as $I U P A C$ names) typically do not match the described reactions. Regardless of this, EvaluatorGPT gives a higher grade to GPT-4, argumenting that the model "addresses stereochemistry and protecting group strategies. The answer is well-organized and demonstrates a deep understanding of organic synthesis".
This highlights a clear limitation of the LLM-powered evaluation in the realm of synthetic chemistry, as it relies heavily on how confident and fluent the response is, instead of how good the thought process is or how accurate the solutions are. Additionally it shows how human evaluation is still very much needed for the evaluation of these types of systems, specially in a fact-critical field like chemistry.</p>            </div>
        </div>

    </div>
</body>
</html>
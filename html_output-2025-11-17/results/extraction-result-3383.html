<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3383 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3383</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3383</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-77.html">extraction-schema-77</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <p><strong>Paper ID:</strong> paper-608a2b333fd8262e8c918f36c5700bafd3ea3cdd</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/608a2b333fd8262e8c918f36c5700bafd3ea3cdd" target="_blank">GeomVerse: A Systematic Evaluation of Large Models for Geometric Reasoning</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The empirical results obtained using the benchmark for state-of-the-art VLMs indicate that these models are not as capable in subjects like geometry (and, by generalization, other topics requiring similar reasoning) as suggested by previous benchmarks.</p>
                <p><strong>Paper Abstract:</strong> Large language models have shown impressive results for multi-hop mathematical reasoning when the input question is only textual. Many mathematical reasoning problems, however, contain both text and image. With the ever-increasing adoption of vision language models (VLMs), understanding their reasoning abilities for such problems is crucial. In this paper, we evaluate the reasoning capabilities of VLMs along various axes through the lens of geometry problems. We procedurally create a synthetic dataset of geometry questions with controllable difficulty levels along multiple axes, thus enabling a systematic evaluation. The empirical results obtained using our benchmark for state-of-the-art VLMs indicate that these models are not as capable in subjects like geometry (and, by generalization, other topics requiring similar reasoning) as suggested by previous benchmarks. This is made especially clear by the construction of our benchmark at various depth levels, since solving higher-depth problems requires long chains of reasoning rather than additional memorized knowledge. We release the dataset for further research in this area.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3383.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3383.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT4V</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 Vision (GPT-4V)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal variant of OpenAI's GPT-4 family that accepts visual inputs and generates text; used in the paper in zero-shot settings to answer text+image geometry problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal large transformer-based model (GPT-4 family) with vision capability that accepts images and text and produces free-form text outputs; the paper notes it appears to use zero-shot chain-of-thought internally.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>GeomVerse synthetic geometry problems (text+image)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Procedurally generated multi-hop geometry questions requiring spatial reasoning (lengths, angles, areas, perimeters) and chained geometric computations across shapes; problems have controllable depth (number of hops) and width (branching).</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Text + image (visual diagram containing shapes and numeric/annotated labels); the paper also has a text-only variant where all data is in text.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot (with no demonstrations). The paper notes GPT4V appears to use zero-shot CoT internally.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Qualitative analysis: GPT4V handled Depth-1 examples (single-hop geometric computations) in zero-shot but failed for deeper multi-hop instances. The authors evaluated a small random subset (10 examples per depth) for GPT4V and determined correctness manually; they observed GPT4V does not sustain multi-hop geometric deduction in this benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative: GPT4V solved Depth-1 examples in zero-shot on the small manually-reviewed subset; it failed on higher-depth (depth >=2) examples. Exact numeric accuracies are not reported beyond 'solves Depth 1 only' and that results were manually checked on 10 examples per depth.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Performs well only on single-hop (Depth-1) questions; fails at multi-hop geometric reasoning (depth >=2). Limited evaluation size for GPT4V in paper (subset of 10 per depth).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Compared qualitatively: GPT4V performed better than some VLM zero-shot baselines on Depth-1, but text-only LLM (PaLM 2 Large few-shot CoT) could solve a portion of depth-2 and depth-3 problems that GPT4V could not; human solvers outperformed all models by a large margin on multi-hop problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GeomVerse: A Systematic Evaluation of Large Models for Geometric Reasoning', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3383.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3383.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLI-55B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLI (55B parameter model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large vision-and-language model (PaLI family); the 55B variant was used in few-shot chain-of-thought experiments in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pali: A jointly-scaled multilingual language-image model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLI 55B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A joint vision-and-language transformer model from the PaLI family (multimodal), scaled to tens of billions of parameters; used here in few-shot chain-of-thought experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>55B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>GeomVerse synthetic geometry problems (text+image)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Same GeomVerse problems requiring spatial/geometric reasoning over diagrams and text, with controlled multi-hop depth and branching width.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Text + image (diagram plus text); the paper also experiments with text-only variants but PaLI here was evaluated as a VLM.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Few-shot with chain-of-thought (FS-CoT): 4 few-shot demonstrations including solutions (CoTs) were provided.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Empirical analysis: FS-CoT PaLI 55B showed near-zero accuracy on most depths in GeomVerse despite CoT demonstrations; authors interpret this as VLMs (including PaLI) being weak at multi-hop geometric reasoning out of the box.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative: 'almost zero on all depths' is reported for FS-CoT PaLI 55B (exact numeric values not provided in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Very low effectiveness with few-shot CoT on multi-hop geometry; failure modes include wrong proof planning, wrong formula selection, and numerical errors (see paper's failure-mode analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Underperforms text-only few-shot LLM (PaLM 2 Large FS-CoT) on multi-hop depths; far below human performance on the dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GeomVerse: A Systematic Evaluation of Large Models for Geometric Reasoning', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3383.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3383.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLI-5B FT/FT-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLI (5B parameter model) finetuned on GeomVerse</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller PaLI model (5B) finetuned on the GeomVerse training set to predict labels (FT) or produce full solutions/CoTs (FT-CoT); used to study the effect of finetuning and CoT-target finetuning on geometric reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLI 5B (finetuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLI 5B is a 5-billion-parameter vision-language transformer model; in this work it was finetuned on a 10k-example GeomVerse training set to directly predict answers (FT) or to generate chain-of-thought solutions (FT-CoT). Training hyperparameters reported: 10k steps, learning rate 0.0005, batch size 128, best checkpoint selected by validation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>5B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>GeomVerse synthetic geometry problems (text+image)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Procedurally generated geometry problems requiring chained application of geometric formulas; examples include shapes composed/removed (rectangle minus semicircle, composite shapes), angle computations, area/perimeter computations, and problems that require mapping text to diagram elements.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Text + image (diagram with annotations); paper also used a text-only variant for some experiments but finetuned PaLI was trained and evaluated on the image+text version.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Finetuning (FT) to directly predict labels; Finetuning to produce solutions/CoTs (FT-CoT). Also evaluated FT and FT-CoT models on in-distribution and out-of-distribution (new shapes) tests.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Detailed analysis in paper: FT-CoT substantially outperforms FT across depths, indicating benefit from training the model to produce intermediate reasoning steps. However, performance degrades monotonically with increasing depth (multi-hop length). Models were more robust to increases in width (branching) than to depth. Additional analyses: robustness to low-level visual changes (opacity, line width) but sensitivity to font size; distractors markedly reduce accuracy; FT-CoT models still hallucinate, misassign values, or pick wrong formulas in many failure cases. The paper also manually verified that when the final label is correct, the produced reasoning chain is often (but not always) correct.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative and relative metrics reported: FT-CoT is the best performing finetuned configuration across depths but shows large drops as depth increases; relaxed accuracy was used (prediction within ±3% of label). Exact per-depth numeric accuracies are shown in the paper's figures but not reproduced here as precise numbers; key reported behaviors: - FT-CoT >> FT (substantial improvement); - FT-CoT performance drops monotonically with depth; - generalization (OOD shapes) accuracy significantly lower than in-distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Major failure modes: wrong calculations (numerical errors), wrong formula selection, misunderstanding shapes (e.g., confusing sector vs triangle), wrong proof planning (wrong step order or disconnected steps), wrong assignment of values, hallucinated/non-existent values. FT-CoT improvements did not fully generalize to out-of-distribution shapes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Finetuned PaLI 5B (FT-CoT) outperforms few-shot VLMs and FT variants on in-distribution GeomVerse but remains far below human performance on more complex depths; text-only LLMs (few-shot PaLM 2 Large with CoT) sometimes solved deeper problems better than VLMs zero-shot, indicating LLMs may be stronger at pure multi-hop arithmetic/logic when visual parsing is not required.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GeomVerse: A Systematic Evaluation of Large Models for Geometric Reasoning', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3383.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3383.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM2-Large</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM 2 Large (instruction-tuned LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned large language model (text-only) used in the paper in few-shot CoT experiments to solve the text-only variant of GeomVerse problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2 Large (instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large text-only transformer language model (PaLM 2 family) that is instruction-tuned; used here in few-shot chain-of-thought prompting on the text-only version of GeomVerse problems.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>GeomVerse synthetic geometry problems (text-only variant)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Geometry problems presented entirely in text (all required numeric and structural information provided in text rather than in the image), requiring multi-hop mathematical and spatial reasoning (computations, application of geometric formulas and deductions).</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Text-only (problem description and numeric facts provided in text).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Few-shot with chain-of-thought demonstrations (FS-CoT) using 4 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>PaLM 2 Large FS-CoT solved a portion of Depth-2 and Depth-3 text-only problems (i.e., multi-hop geometry presented as text), suggesting strong multi-hop mathematical reasoning when visual interpretation is not required. When the model's final label was correct, its produced reasoning chain was mostly correct upon manual verification (high fidelity between correct answer and correct CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative: 'can solve a portion of Depth 2 and 3 problems' as reported; FS-CoT PaLM 2 Large outperformed the VLMs (without finetuning) on deeper text-only problems. Exact numeric accuracies are presented in paper figures but not enumerated in text.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>As a text-only model it relies on textual encoding of all required facts; it cannot directly parse diagrams. Failure modes include wrong proof planning and wrong formula selection when it does fail.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Outperformed VLMs in multi-hop text-only settings without finetuning; still underperforms human solvers on the hardest problems in the dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GeomVerse: A Systematic Evaluation of Large Models for Geometric Reasoning', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Pali: A jointly-scaled multilingual language-image model <em>(Rating: 2)</em></li>
                <li>GPT-4 technical report <em>(Rating: 2)</em></li>
                <li>Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning <em>(Rating: 2)</em></li>
                <li>GeoQA: A geometric question answering benchmark towards multimodal numerical reasoning <em>(Rating: 2)</em></li>
                <li>Unigeo: Unifying geometry logical reasoning via reformulating mathematical expression <em>(Rating: 2)</em></li>
                <li>A multi-modal neural geometric solver with textual clauses parsed from diagram <em>(Rating: 2)</em></li>
                <li>Clevr-math: A dataset for compositional language, visual and mathematical reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3383",
    "paper_id": "paper-608a2b333fd8262e8c918f36c5700bafd3ea3cdd",
    "extraction_schema_id": "extraction-schema-77",
    "extracted_data": [
        {
            "name_short": "GPT4V",
            "name_full": "GPT-4 Vision (GPT-4V)",
            "brief_description": "A multimodal variant of OpenAI's GPT-4 family that accepts visual inputs and generates text; used in the paper in zero-shot settings to answer text+image geometry problems.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4V",
            "model_description": "Multimodal large transformer-based model (GPT-4 family) with vision capability that accepts images and text and produces free-form text outputs; the paper notes it appears to use zero-shot chain-of-thought internally.",
            "model_size": null,
            "puzzle_name": "GeomVerse synthetic geometry problems (text+image)",
            "puzzle_description": "Procedurally generated multi-hop geometry questions requiring spatial reasoning (lengths, angles, areas, perimeters) and chained geometric computations across shapes; problems have controllable depth (number of hops) and width (branching).",
            "input_representation": "Text + image (visual diagram containing shapes and numeric/annotated labels); the paper also has a text-only variant where all data is in text.",
            "prompting_method": "Zero-shot (with no demonstrations). The paper notes GPT4V appears to use zero-shot CoT internally.",
            "spatial_reasoning_analysis": "Qualitative analysis: GPT4V handled Depth-1 examples (single-hop geometric computations) in zero-shot but failed for deeper multi-hop instances. The authors evaluated a small random subset (10 examples per depth) for GPT4V and determined correctness manually; they observed GPT4V does not sustain multi-hop geometric deduction in this benchmark.",
            "performance_metrics": "Qualitative: GPT4V solved Depth-1 examples in zero-shot on the small manually-reviewed subset; it failed on higher-depth (depth &gt;=2) examples. Exact numeric accuracies are not reported beyond 'solves Depth 1 only' and that results were manually checked on 10 examples per depth.",
            "limitations_or_failure_modes": "Performs well only on single-hop (Depth-1) questions; fails at multi-hop geometric reasoning (depth &gt;=2). Limited evaluation size for GPT4V in paper (subset of 10 per depth).",
            "comparison_to_other_models_or_humans": "Compared qualitatively: GPT4V performed better than some VLM zero-shot baselines on Depth-1, but text-only LLM (PaLM 2 Large few-shot CoT) could solve a portion of depth-2 and depth-3 problems that GPT4V could not; human solvers outperformed all models by a large margin on multi-hop problems.",
            "uuid": "e3383.0",
            "source_info": {
                "paper_title": "GeomVerse: A Systematic Evaluation of Large Models for Geometric Reasoning",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "PaLI-55B",
            "name_full": "PaLI (55B parameter model)",
            "brief_description": "A large vision-and-language model (PaLI family); the 55B variant was used in few-shot chain-of-thought experiments in the paper.",
            "citation_title": "Pali: A jointly-scaled multilingual language-image model",
            "mention_or_use": "use",
            "model_name": "PaLI 55B",
            "model_description": "A joint vision-and-language transformer model from the PaLI family (multimodal), scaled to tens of billions of parameters; used here in few-shot chain-of-thought experiments.",
            "model_size": "55B",
            "puzzle_name": "GeomVerse synthetic geometry problems (text+image)",
            "puzzle_description": "Same GeomVerse problems requiring spatial/geometric reasoning over diagrams and text, with controlled multi-hop depth and branching width.",
            "input_representation": "Text + image (diagram plus text); the paper also experiments with text-only variants but PaLI here was evaluated as a VLM.",
            "prompting_method": "Few-shot with chain-of-thought (FS-CoT): 4 few-shot demonstrations including solutions (CoTs) were provided.",
            "spatial_reasoning_analysis": "Empirical analysis: FS-CoT PaLI 55B showed near-zero accuracy on most depths in GeomVerse despite CoT demonstrations; authors interpret this as VLMs (including PaLI) being weak at multi-hop geometric reasoning out of the box.",
            "performance_metrics": "Qualitative: 'almost zero on all depths' is reported for FS-CoT PaLI 55B (exact numeric values not provided in the paper).",
            "limitations_or_failure_modes": "Very low effectiveness with few-shot CoT on multi-hop geometry; failure modes include wrong proof planning, wrong formula selection, and numerical errors (see paper's failure-mode analysis).",
            "comparison_to_other_models_or_humans": "Underperforms text-only few-shot LLM (PaLM 2 Large FS-CoT) on multi-hop depths; far below human performance on the dataset.",
            "uuid": "e3383.1",
            "source_info": {
                "paper_title": "GeomVerse: A Systematic Evaluation of Large Models for Geometric Reasoning",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "PaLI-5B FT/FT-CoT",
            "name_full": "PaLI (5B parameter model) finetuned on GeomVerse",
            "brief_description": "A smaller PaLI model (5B) finetuned on the GeomVerse training set to predict labels (FT) or produce full solutions/CoTs (FT-CoT); used to study the effect of finetuning and CoT-target finetuning on geometric reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLI 5B (finetuned)",
            "model_description": "PaLI 5B is a 5-billion-parameter vision-language transformer model; in this work it was finetuned on a 10k-example GeomVerse training set to directly predict answers (FT) or to generate chain-of-thought solutions (FT-CoT). Training hyperparameters reported: 10k steps, learning rate 0.0005, batch size 128, best checkpoint selected by validation.",
            "model_size": "5B",
            "puzzle_name": "GeomVerse synthetic geometry problems (text+image)",
            "puzzle_description": "Procedurally generated geometry problems requiring chained application of geometric formulas; examples include shapes composed/removed (rectangle minus semicircle, composite shapes), angle computations, area/perimeter computations, and problems that require mapping text to diagram elements.",
            "input_representation": "Text + image (diagram with annotations); paper also used a text-only variant for some experiments but finetuned PaLI was trained and evaluated on the image+text version.",
            "prompting_method": "Finetuning (FT) to directly predict labels; Finetuning to produce solutions/CoTs (FT-CoT). Also evaluated FT and FT-CoT models on in-distribution and out-of-distribution (new shapes) tests.",
            "spatial_reasoning_analysis": "Detailed analysis in paper: FT-CoT substantially outperforms FT across depths, indicating benefit from training the model to produce intermediate reasoning steps. However, performance degrades monotonically with increasing depth (multi-hop length). Models were more robust to increases in width (branching) than to depth. Additional analyses: robustness to low-level visual changes (opacity, line width) but sensitivity to font size; distractors markedly reduce accuracy; FT-CoT models still hallucinate, misassign values, or pick wrong formulas in many failure cases. The paper also manually verified that when the final label is correct, the produced reasoning chain is often (but not always) correct.",
            "performance_metrics": "Qualitative and relative metrics reported: FT-CoT is the best performing finetuned configuration across depths but shows large drops as depth increases; relaxed accuracy was used (prediction within ±3% of label). Exact per-depth numeric accuracies are shown in the paper's figures but not reproduced here as precise numbers; key reported behaviors: - FT-CoT &gt;&gt; FT (substantial improvement); - FT-CoT performance drops monotonically with depth; - generalization (OOD shapes) accuracy significantly lower than in-distribution.",
            "limitations_or_failure_modes": "Major failure modes: wrong calculations (numerical errors), wrong formula selection, misunderstanding shapes (e.g., confusing sector vs triangle), wrong proof planning (wrong step order or disconnected steps), wrong assignment of values, hallucinated/non-existent values. FT-CoT improvements did not fully generalize to out-of-distribution shapes.",
            "comparison_to_other_models_or_humans": "Finetuned PaLI 5B (FT-CoT) outperforms few-shot VLMs and FT variants on in-distribution GeomVerse but remains far below human performance on more complex depths; text-only LLMs (few-shot PaLM 2 Large with CoT) sometimes solved deeper problems better than VLMs zero-shot, indicating LLMs may be stronger at pure multi-hop arithmetic/logic when visual parsing is not required.",
            "uuid": "e3383.2",
            "source_info": {
                "paper_title": "GeomVerse: A Systematic Evaluation of Large Models for Geometric Reasoning",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "PaLM2-Large",
            "name_full": "PaLM 2 Large (instruction-tuned LLM)",
            "brief_description": "An instruction-tuned large language model (text-only) used in the paper in few-shot CoT experiments to solve the text-only variant of GeomVerse problems.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM 2 Large (instruction-tuned)",
            "model_description": "A large text-only transformer language model (PaLM 2 family) that is instruction-tuned; used here in few-shot chain-of-thought prompting on the text-only version of GeomVerse problems.",
            "model_size": null,
            "puzzle_name": "GeomVerse synthetic geometry problems (text-only variant)",
            "puzzle_description": "Geometry problems presented entirely in text (all required numeric and structural information provided in text rather than in the image), requiring multi-hop mathematical and spatial reasoning (computations, application of geometric formulas and deductions).",
            "input_representation": "Text-only (problem description and numeric facts provided in text).",
            "prompting_method": "Few-shot with chain-of-thought demonstrations (FS-CoT) using 4 examples.",
            "spatial_reasoning_analysis": "PaLM 2 Large FS-CoT solved a portion of Depth-2 and Depth-3 text-only problems (i.e., multi-hop geometry presented as text), suggesting strong multi-hop mathematical reasoning when visual interpretation is not required. When the model's final label was correct, its produced reasoning chain was mostly correct upon manual verification (high fidelity between correct answer and correct CoT).",
            "performance_metrics": "Qualitative: 'can solve a portion of Depth 2 and 3 problems' as reported; FS-CoT PaLM 2 Large outperformed the VLMs (without finetuning) on deeper text-only problems. Exact numeric accuracies are presented in paper figures but not enumerated in text.",
            "limitations_or_failure_modes": "As a text-only model it relies on textual encoding of all required facts; it cannot directly parse diagrams. Failure modes include wrong proof planning and wrong formula selection when it does fail.",
            "comparison_to_other_models_or_humans": "Outperformed VLMs in multi-hop text-only settings without finetuning; still underperforms human solvers on the hardest problems in the dataset.",
            "uuid": "e3383.3",
            "source_info": {
                "paper_title": "GeomVerse: A Systematic Evaluation of Large Models for Geometric Reasoning",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Pali: A jointly-scaled multilingual language-image model",
            "rating": 2
        },
        {
            "paper_title": "GPT-4 technical report",
            "rating": 2
        },
        {
            "paper_title": "Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning",
            "rating": 2
        },
        {
            "paper_title": "GeoQA: A geometric question answering benchmark towards multimodal numerical reasoning",
            "rating": 2
        },
        {
            "paper_title": "Unigeo: Unifying geometry logical reasoning via reformulating mathematical expression",
            "rating": 2
        },
        {
            "paper_title": "A multi-modal neural geometric solver with textual clauses parsed from diagram",
            "rating": 2
        },
        {
            "paper_title": "Clevr-math: A dataset for compositional language, visual and mathematical reasoning",
            "rating": 1
        }
    ],
    "cost": 0.013732249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>GeomVerse <br> A Systematic Evaluation of Large Models for Geometric Reasoning</h1>
<p>Mehran Kazemi ${ }^{1}$, Hamidreza Alvari ${ }^{1}$, Ankit Anand ${ }^{2}$, Jialin Wu ${ }^{1}$, Xi Chen ${ }^{1}$, Radu Soricut ${ }^{1}$<br>${ }^{1}$ Google Research, ${ }^{2}$ Google DeepMind<br>{mehrankazemi, hamidrz, anandank, jialinwu, chillxichen, rsoricut}@google.com</p>
<h4>Abstract</h4>
<p>Large language models have shown impressive results for multi-hop mathematical reasoning when the input question is only textual. Many mathematical reasoning problems, however, contain both text and image. With the everincreasing adoption of vision language models (VLMs), understanding their reasoning abilities for such problems is crucial. In this paper, we evaluate the reasoning capabilities of VLMs along various axes through the lens of geometry problems. We procedurally create a synthetic dataset of geometry questions with controllable difficulty levels along multiple axes, thus enabling a systematic evaluation. The empirical results obtained using our benchmark for state-of-the-art VLMs indicate that these models are not as capable in subjects like geometry (and, by generalization, other topics requiring similar reasoning) as suggested by previous benchmarks. This is made especially clear by the construction of our benchmark at various depth levels, since solving higher-depth problems requires long chains of reasoning rather than additional memorized knowledge. We release the dataset for further research in this area ${ }^{1}$.</p>
<h2>1 Introduction</h2>
<p>Multi-hop reasoning is a fundamental element in intelligence: it allows us to combine multiple pieces of information to answer questions or solve problems. While formal reasoning such as automated theorem proving (Robinson, 1965; Kovács and Voronkov, 2013; Schulz, 2002) has been a key focus in the AI literature, recent years have witnessed a great amount of progress in multi-hop reasoning with natural language thanks to the advances in pre-trained large language models (LLMs) (Wei et al., 2022; Nye et al., 2022; Kazemi et al., 2023b; Saparov et al., 2023; Yao et al., 2023; Pan et al., 2023). Among various types of multi-hop reasoning, mathematical reasoning has turned into a key</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Sample GeomVerseproblem. Question: If the ABEF shape is a rectangle where a semi-circle has been removed from one side of it, the perimeter of the ABEF shape is 34 [...] compute the degree of the DAB angle. Assume $\pi=3.14$. Round computations to 2 decimal places. Solution: The diameter of the semicircle in the ABEF shape is equal to the side of the rectangle with length 7 so the shape has two sides with equal but unknown lengths, one side with length 7, and one semi-circle arc with diameter 7. So the perimeter is $2 *$ UnknownSide $+7+\frac{7 \pi}{2}[$...] the length of the $A B$ side is $\frac{3649}{7}=8 .[$...] the final answer is 28.69.
focus domain for AI researchers (Lu et al., 2022; Lewkowycz et al., 2022) with many recent works targeting to solve open problems in mathematics (Fawzi et al., 2022; Davies et al., 2021). It is an appealing domain for AI research due to various reasons: it is a primitive skill that is essential for many tasks, it has an open-ended nature, and due to various challenges such as limited data it still remains a challenge for LLMs and modern AI systems. Recently, the International Math Olympiad (IMO) grand challenge (Selsam et al., 2020) was announced where the goal is to build an AI system that can win a gold medal in IMO, one of the most prestigious competitions. Not only research, with advancements in LLMs, many new applications and products are leveraging AI research for educa-</p>
<p>tion to build personalized tutors (Abdelghani et al., 2023; Khan, 2023). One of the key challenges so far has been to improve the performance of these systems in STEM subjects.</p>
<p>Due to the vast popularity of mathematical problem solving both from research and product perspectives, several datasets have been developed for measuring and improving the mathematical reasoning of LLMs (Cobbe et al., 2021; Ling et al., 2017; Hendrycks et al., 2021) and are widely adopted by the research community. While existing datasets mostly focus on textual problems, there are several bodies of mathematical problems that require both textual and visual understanding of the problem. Being one of the main school curriculum and having a high presence in many math competitions including IMO, geometry is a key domain in this space. With the fast pace in adoption of the visionlanguage models (VLMs) (Chen et al., 2022b; OpenAI, 2023) in various aforementioned applications, it is crucial to measure and improve their performance on such problems. Previous work has created a number of datasets with geometry questions based on high-school, college, or SAT exams, and developed specific models for this task. While evaluating VLMs on such datasets may provide a holistic understanding of the general capability of the models, such evaluation may provide little information about the specific areas of strengths and weaknesses of VLMs and hence provide little guidance on where research should focus.</p>
<p>In this paper we create GeomVerse, a dataset of synthetically generated geometry questions that require multi-hop mathematical reasoning over text and image. We bridge reasoning about geometry problems and logical reasoning, allowing us to measure model performances on reasoning factors that may go beyond geometry and may be present in many (mathematical) reasoning problems on text and image. In other words, GeomVerse allows for unveiling the reasoning ability of VLMs across several axes, by using geometry as a lens. Moreover, we also measure model performances on geometryspecific axes of difficulty. This enables a systematic evaluation of VLMs on this task. A sample generated question and its image and solution can be viewed in Figure 1.</p>
<p>Some of the main findings from our systematic evaluation on GeomVerse are summarized below. Firstly, through the unique property of GeomVerse that allows for constructing benchmarks at various depths, we find that current VLMs are not as capable in subjects like geometry as suggested by previous benchmarks, showing that they may still be immature for product applications such as AI tutoring. Importantly, since several of the difficulty axes we study are not specific to geometry, our results reveal a number of important failure modes as well as a significant gap in the reasoning capacity of state-of-the-art VLM that may go beyond geometry. Secondly, finetuning VLMs to produce the entire solution substantially improves their performance for in-distribution problems but that does not generalize to out-of-distribution problems. Thirdly, VLMs struggle more with increasing in depth rather than width of reasoning. Fourthly, VLMs are rather robust to the question and image representation. And finally, finetuning VLMs on a synthetic geometry dataset makes them perform better on real geometry questions.</p>
<h2>2 Related Work</h2>
<p>Our work is related to several research directions in the literature as summarized below.</p>
<p>Vision-Language Models (VLMs): Recent VLMs (Chen et al., 2022b; Allaway et al., 2022; Alayrac et al., 2022; Li et al., 2023; Wang et al., 2022; Chen et al., 2023) have demonstrated promising performance on a wide range of image and video tasks including captioning, question answering and visual reasoning. However, the capabilities of performing multi-modal multi-hop (mathematical) reasoning are less investigated. Because these VLMs are generative black-boxes, understanding how well they can comprehend and answer the multi-hop questions is a critical topic.</p>
<p>Multi-Hop Reasoning Datasets: There are a number of datasets available in the literature that require multi-hop logical (Tafjord et al., 2021; Kazemi et al., 2023a; Zhong et al., 2021) and mathematical (Cobbe et al., 2021; Ling et al., 2017; Hendrycks et al., 2021) reasoning over text. Lindström and Abraham (2022) create a version of the CLEVR dataset that requires basic mathematical reasoning over text and image. Previous work has also developed a number of geometric reasoning datasets (Seo et al., 2015; Lu et al., 2021; Chen et al., 2021, 2022a; Zhang et al., 2023) that require reasoning over both text and image. Table 1 provides an overview of the existing datasets and compares them along four axes: 1- requiring textual</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset $\rightarrow$ Feature $\downarrow$</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">4</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">6</th>
<th style="text-align: center;">7</th>
<th style="text-align: center;">8</th>
<th style="text-align: center;">9</th>
<th style="text-align: center;">10</th>
<th style="text-align: center;">11</th>
<th style="text-align: center;">12</th>
<th style="text-align: center;">13</th>
<th style="text-align: center;">14</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Textual Understanding</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Visual Understanding</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Mathematical Reasoning</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Automatic Difficulty control</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\sim$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 1: A comparison of GeomVerse with some of the recent and/or widely-used multi-hop (logical or mathematical) reasoning datasets. We use $\sim$ when a dataset contains a property to a limited extent.
understanding, 2- requiring visual understanding, 3- involving mathematical reasoning, and 4- automatic control of the difficulty level (thus allowing for a systematic evaluation).</p>
<p>Multi-Hop Reasoning Approaches: Some of the approaches for improving the multi-hop reasoning of LLMs and VLMs range from pre-training on relevant data (Hendrycks et al., 2021; Lewkowycz et al., 2022), finetuning with (Nye et al., 2022; Dalvi et al., 2021; Zelikman et al., 2022; Kazemi et al., 2023a) and without (Clark et al., 2021; Betz et al., 2021) explicitly generating the solution, incontext learning with solutions (Wei et al., 2022), decomposing the problem into smaller pieces and solving them separately (Zhou et al., 2023; Khot et al., 2023) and using LLMs/VLMs as tools within classical algorithms (Kazemi et al., 2023b; Creswell et al., 2023). In the realm of reasoning about geometry problems, existing work typically develops specialized models; measuring or improving the reasoning ability of general-purpose VLMs is less studied.</p>
<h2>3 The GeomVerse Dataset</h2>
<p>We start by describing multi-hop reasoning over logical theories and define some terminologies. Then, we explain how GeomVerse is created by making analogies to logical theories.</p>
<h3>3.1 Multi-Hop Logical Reasoning</h3>
<p>A logical theory consists of facts and rules. Consider the following theory as a running example:</p>
<p>$$
\text { Facts : }{a, b}
$$</p>
<p>$$
\text { Rules : }{a \Rightarrow c, a \wedge b \Rightarrow d, d \Rightarrow e}
$$</p>
<p>The theory contains two facts specifying $a$ and $b$ are true, and three rules specifying $a$ implies $c, a$ and $b$ imply $d$ and $d$ implies $e$. Starting from the facts, one can apply deduction on the set of facts and the rules to derive new facts and answer queries (e.g., we can query whether $e$ holds). We define the depth of a query as the number of hops of reasoning required to prove it, and the width of a query as the maximum number of branches in the proof of the query. For a given query, any fact or rule not necessarily in the proof of the query is referred to as a distractor. For example, if we query $a$ both the depth and width are 0 , if we query $c$ the depth is 1 and the width is also 1 , if we query $d$ the depth is 1 and the width is 2 , and if we query $e$ both the depth and the width are 2 . When we query $e$, the rule $a \Rightarrow c$ is a distractor. Note that queries with width 1 correspond to a chain of reasoning, whereas higher width queries correspond to a tree of reasoning.</p>
<h3>3.2 From Logical to Geometric Reasoning</h3>
<p>In geometry questions, typically the values of some of the elements (e.g., sides, angles, areas, etc.) are given to us as input and the other elements can be derived one by one by applying the rules and formulas of geometry. The elements whose values are given to us can be thought of as facts in logical theories, the geometry rules and formulas can be considered as the rules in logical theories, and the process of deriving the hidden values can be thought of as the deduction.</p>
<p>As an example, the solution to the problem in Figure 1 can be formulated in logical form as:</p>
<p>Facts : $\left{A_{A H I D}, A_{A B C D}, P_{A B E F}, L_{B E}\right}$
Rules : $\left{A_{A H I D} \Rightarrow A D, P_{A B E F}, L_{B E} \Rightarrow L_{A B}\right.$, $\left.A_{A B C D}, L_{A D}, L_{A B} \Rightarrow D_{D A B}\right}$
where $A_{x}, P_{x}, L_{x}$ and $D_{x}$ represent the area of a shape, perimeter of a shape, length of a side, and degree of an angle respectively. We note two key differences with logical reasoning: 1- unlike in deductive logical reasoning, the rules are not given to the model and the model has to use its own geometry knowledge (learned from pre-training or finetuning) to apply the right geometry formulae and derive new values, 2- in the case of geometry, applying rules involves computations.</p>
<h3>3.3 Creating the GeomVerse</h3>
<p>To create GeomVerse, we fix a set of 12 standard and non-standard shapes $\mathcal{S}$ as demonstrated in Figure 2 and gather a number of rules/formulas $\mathcal{F}<em _in="{in" _text="\text">{s}$ for each shape $s \in \mathcal{S}$ (e.g., the Pythagorean theorem) with a total of 68 formulas across all shapes. We further use supplementary and complementary angles as two special shapes with only a single formula each. For a formula $f$, we let $f</em>$ represent the element whose value can be computed based on the formula and the inputs (e.g., for the Pythagorean theorem, the two sides can be the input and the hypotenuse can be the output). Then, similar to several existing works on constructing multi-hop, textual logical reasoning datasets or textual stories (Kazemi et al., 2023a; Ye et al., 2022), we adopt a backward generation strategy where we start by generating a question, and then adding rules to increase the depth and width of reasoning to the desired amount.}}$ represent the input elements and $f_{\text {out }</p>
<p>Generating Examples with Depth 1: To generate an example with depth 1, we can simply sample a shape $s \in \mathcal{S}$ and formula $f \in \mathcal{F}_{s}$. Then, we let facts $=f^{(i n)}$, query $=f^{(\text {out })}$, and with the only required rule in the solution being rules $={f}$.</p>
<p>Increasing the Depth: Let $f_{1}$ be the formula we sampled for the depth 1 example and $f_{1}^{(i n)}$ and $f_{1}^{(\text {out })}$ be the inputs and output of $f_{1}$. To increase the depth to 2 , we select one of the elements $e$ in $f_{1}^{(i n)}$ and do not provide it in the facts. Instead, we sample a new shape $s_{2}$ and formula $f_{2}$ such that $f_{2}^{(\text {out })}$ has the same type as $e$ and we tie the values of $e$ and $f_{2}^{(\text {out } 2}$. For example, if $e$ is one of the sides of a triangle, then $s_{2}$ can be a square and $f_{2}$</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>can be the formula of deriving the side of a square from its area, where the square and the triangle share the same side. Then facts $=\left(f_{1}^{(i n)}-e\right) \cup$ $f_{2}^{(i n)}$, query $=f_{1}^{(\text {out })}$, and the required rules are rules $=\left{f_{1}, f_{2}\right}$ with $f_{2}$ providing the value for $e$ and then $f_{1}$ using this value to answer the query. The depth can be further increased in a similar way by appending a new shape and formula to one of the elements in $f_{2}^{(i n)}$.</p>
<p>Increasing the Width: Let $s$ and $f$ be the shape and formula we sampled at some depth for the construction of an example and $e_{1}$ and $e_{2}$ be two connectable elements (side or angle) in $f^{(i n)}$. We can include only $f^{(i n)}-\left{e_{1}, e_{2}\right}$ in the facts, and append new shapes and formulas as explained above so that the values for $e_{1}$ and $e_{2}$ can be derived.</p>
<p>Adding Distractors: Distractors can be added in a post processing step. Consider a Depth 2 (Width 1) example and suppose $e$ is the element that has to be computed in the first hop and be used in the second hop. If we provide the value of $e$ as input, then the model turns into a Depth 1 problem with a distracting shape and corresponding values. In Figure 1, for example, if we provide the value of the $A D$ side as input, then the square and its corresponding values can be considered as distractors.</p>
<p>The Overall Generation Algorithm: In Algorithm 1, we adopt the high-level idea of the GenerateTheory algorithm from Kazemi et al. (2023a) for recursively generating geometry problems (as opposed to logical theory problems) in a backward fashion. Initially, we select one element type $\tau$ to be asked for in the question (e.g., side, angle, area, perimeter, etc.), and a desired depth $d$. Then we call the BackwardGenerate function. If $d=0$, we sample a shape $s \in \mathcal{S}$ and formula $f \in \mathcal{F}<em s="s">{s}$ such that the type of the element in $f^{(\text {out })}$ is $\tau$, append the shape $s$ to the previous shapes on the shared element, assign random values to the elements in $f^{(i n)}$ and provide them as facts ${ }^{3}$. Otherwise, we sample a shape $s \in \mathcal{S}$ and formula $f \in \mathcal{F}</em>$ of the}$ such that 1) the type of the element in $f^{(\text {out })}$ is $\tau$ and 2) there is at least one connectable (side or angle) element in $f^{(i n)}$. Then, we select a subset $\mathcal{E</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The standard shapes (top row) and non-standard shapes (bottom row) used in our dataset.</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nx">BackwardGenerate</span>
<span class="nx">Input</span><span class="p">:</span><span class="w"> </span><span class="nx">Shared</span><span class="w"> </span><span class="nx">element</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">e</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">Shared</span><span class="w"> </span><span class="nx">element</span><span class="w"> </span><span class="k">type</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">tau</span><span class="err">\</span><span class="p">)</span>
<span class="nx">Depth</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">d</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">d</span><span class="p">}</span><span class="o">==</span><span class="mi">0</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="w">        </span><span class="nx">do</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">s</span><span class="p">}=</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">RandomSelect</span><span class="w"> </span><span class="err">\</span><span class="p">((</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">S</span><span class="p">})</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">f</span><span class="p">}=</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">RandomSelect</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">F</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">s</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="k">while</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">f</span><span class="o">^</span><span class="p">{(</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">out</span><span class="w"> </span><span class="p">})}</span><span class="err">\</span><span class="p">).</span><span class="k">type</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="o">!=</span><span class="err">\</span><span class="nx">tau</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nx">Append</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">s</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">other</span><span class="w"> </span><span class="nx">shapes</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">e</span><span class="err">\</span><span class="p">).</span>
<span class="w">        </span><span class="nx">Randomly</span><span class="w"> </span><span class="nx">assign</span><span class="w"> </span><span class="nx">values</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">f</span><span class="o">^</span><span class="p">{(</span><span class="nx">i</span><span class="w"> </span><span class="nx">n</span><span class="p">)}</span><span class="err">\</span><span class="p">).</span>
<span class="w">        </span><span class="nx">Provide</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">f</span><span class="o">^</span><span class="p">{(</span><span class="nx">i</span><span class="w"> </span><span class="nx">n</span><span class="p">)}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">values</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nx">facts</span><span class="p">.</span>
<span class="w">    </span><span class="k">else</span>
<span class="w">        </span><span class="nx">do</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">s</span><span class="p">}=</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">RandomSelect</span><span class="w"> </span><span class="err">\</span><span class="p">((</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">S</span><span class="p">})</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">f</span><span class="p">}=</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">RandomSelect</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">F</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">s</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">E</span><span class="p">}=</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">ConnectableElements</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">f</span><span class="o">^</span><span class="p">{(</span><span class="nx">i</span><span class="w"> </span><span class="nx">n</span><span class="p">)}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="k">while</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">f</span><span class="o">^</span><span class="p">{(</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">out</span><span class="w"> </span><span class="p">})}</span><span class="err">\</span><span class="p">).</span><span class="k">type</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="o">!=</span><span class="err">\</span><span class="nx">tau</span><span class="w"> </span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">O</span><span class="w"> </span><span class="nx">R</span><span class="p">}</span><span class="o">|</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">E</span><span class="p">}</span><span class="o">|=</span><span class="mi">0</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nx">Append</span><span class="w"> </span><span class="nx">s</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">other</span><span class="w"> </span><span class="nx">shapes</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">e</span><span class="err">\</span><span class="p">).</span>
<span class="w">        </span><span class="nx">Randomly</span><span class="w"> </span><span class="nx">assign</span><span class="w"> </span><span class="nx">values</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">f</span><span class="o">^</span><span class="p">{(</span><span class="nx">i</span><span class="w"> </span><span class="nx">n</span><span class="p">)}</span><span class="o">-</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">E</span><span class="p">}</span><span class="err">\</span><span class="p">).</span>
<span class="w">        </span><span class="nx">Provide</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">f</span><span class="o">^</span><span class="p">{(</span><span class="nx">i</span><span class="w"> </span><span class="nx">n</span><span class="p">)}</span><span class="o">-</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">E</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">values</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nx">facts</span><span class="p">.</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="nx">e_</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="p">,</span><span class="w"> </span><span class="nx">e_</span><span class="p">{</span><span class="nx">m</span><span class="p">}=</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">SampleElems</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">E</span><span class="p">},</span><span class="w"> </span><span class="nx">p_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">branch</span><span class="w"> </span><span class="p">}}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">e</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="nx">e_</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="p">,</span><span class="w"> </span><span class="nx">e_</span><span class="p">{</span><span class="nx">m</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">            </span><span class="nx">BackwardGenerate</span><span class="p">(</span><span class="nx">e</span><span class="p">,</span><span class="w"> </span><span class="nx">e</span><span class="p">.</span><span class="k">type</span><span class="p">,</span><span class="w"> </span><span class="nx">d</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>

<p>elements in $f^{(i n)}$ for expanding the number of hops. If $p_{\text {branch }}=0$, we only select one of the elements from $f^{(i n)}$ which introduces no branching and so no increase in width. Otherwise, with probability $p_{\text {branch }}$ we select a second element as well, which leads to a branching and so increases the width. We append the shape $s$ to the previous shapes on the shared element, assign random values to the elements in $f^{(i n)}-\mathcal{E}$ and provide them as facts. Then, for each element in $\mathcal{E}$, we recursively call the BackwardGenerate function to append new shapes such that these values can be derived. A visual example of the procedure in Algorithm 1 is provided in Appendix C.</p>
<p>Automatic Question and Solution Generation: We automatically produce a question that provides the facts as input and asks for $f^{(\text {out })}$ where $f$ is
the first formula used. We also keep track of the required rules (including shapes, formulas, and the shared elements) during the generation process (excluded from Algorithm 1 for brevity) and automatically produce a solution by applying deduction and computations on the rules and facts.</p>
<p>Text-Only vs. Text-Image: We create two versions of our problems. In one version, all the required information is given in the question and the image is not needed for answering the question (although the presence of the image can make it easier to understand the problem), and in the other version some information is given in the image and some in the question text so both the image and text of the question are required. We use the former to experiment with text-based LLMs and the latter to experiment with VLMs.</p>
<p>Quality Check: To ensure high quality for the questions, the solutions, the images, and the labels, we did two quality checks. Firstly, we generated all possible Depth 1 problems and manually verified their quality and correctness. Secondly, we asked 10 well-educated people to verify a total of 100 problems (from various depths and with various properties) and identify as many issues as possible with the questions, solutions, labels, or images. All the raised issues were then fixed, and the process was repeated with 100 new examples by a subset of the authors to ensure no issues remained.</p>
<p>Coverage: The connection between Algorithm 1 and logical reasoning helps specify what classes of geometry problems are covered by Algorithm 1. Specifically, Algorithm 1 can generate any geometry problem $\mathcal{P}$ containing a tree of shapes where each shape is connected to its parent shape via a single side or a single (vertical) angle, and where the solution can be found by finding the values of the shared elements bottom-up on the tree.</p>
<p>Further Considerations: While Algorithm 1 can generate problems with overlapping shapes, to ensure the quality of the generated examples remains high without any human involvement in the</p>
<p>generation process, we only accept the generated examples where the shapes are non-overlapping.</p>
<h2>4 Experiments</h2>
<p>We conduct experiments with two state-of-theart VLMs namely PaLI (Chen et al., 2022b) and GPT4V (OpenAI, 2023), as well as a state-of-theart LLM namely (instruction-tuned) PaLM 2 Large (Anil et al., 2023). We consider four settings: 1-zero-shot, 2- few-shot with chain-of-thought (CoT) prompting (Wei et al., 2022) (hereafter referred to as FS-CoT), where the CoT corresponds to the solution, 3- finetuning to directly predict the label (hereafter referred to as FT), and 4- finetuning to predict the solution/CoT (hereafter referred to as FT-CoT). We do the first experiment with GPT4V ${ }^{4}$ due to its high zero-shot performance, the second with PaLM 2 Large and PaLI 55B (the largest PaLI model), and the last two experiments with PaLI 5B to keep the amount of required computations manageable.</p>
<p>Following Methani et al. (2020) and Masry et al. (2022), we measure performance in terms of relaxed accuracy, where a prediction is considered correct if it is within $\delta$ percent of the golden label. We do this to accommodate for the slight variation in computations introduced due to the rounding strategy (e.g., due to the order of the computations). We empirically found $\delta=3$ to be appropriate so we consider a prediction $p$ correct if $0.97 <em>$ label $\leq p \leq 1.03 </em>$ label. We remove from our dataset any example where the difference between the label computed with and without rounding intermediate steps is more than $3 \%$.</p>
<p>In what follows, we provide results on various versions of our dataset with different properties. In each case, we generate 1000 examples randomly given the described parameters and report the results on those examples. We also generate a separate pool of train, validation, and fewshot examples for our experiments. The implementation details are presented in Appendix B.</p>
<h3>4.1 Performance as a Function of Depth</h3>
<p>Figure 3 represents the model results on examples with varying depths. Without finetuning, GPT4V can only solve Depth 1 examples, and the accuracy of the FS-CoT PaLI model is almost zero on all depths. In contrast, the text-only model can solve</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>a portion of the Depth 2 and 3 problems as well. While the presence of the image should make the problem easier to understand and solve, this results hints that LLMs may be stronger in mathematical and multi-hop reasoning compared to their counterpart VLMs. Moreover, while finetuning helps VLMs learn to do some reasoning, as the depth of reasoning increases the performance drops monotonically and quite significantly.</p>
<p>Notice that FT-CoT outperforms FT substantially for all depths. While such improvements have been previously observed for reasoning with textual inputs (Suzgun et al., 2022), this result shows the importance of showing CoT to VLMs as well. This result also hints at the quality of the automatic solutions in GeomVerse.</p>
<p>We also measured human performance on our dataset by having 4 well-educated (but not necessarily expert in geometry) people solve a total of 20 problems per depth. The results show a stark gap between the best model performances and the human performance; we also observe that our problems can be challenging to solve even for humans. The mistakes made by humans where due to various issues including wrong/forgotten degree to radians conversion, using wrong formulas, and making wrong assumptions.</p>
<p>Shape Generalization: We next measure how much the FT-CoT model (the best performing one across depths) can generalize to variations in the shapes. To this end, we finetune a model only on the following shapes: square, right triangle, trapezoid, semi-circle, rectangle plus equilateral triangle, rectangle minus semi-circle, and square minus circle. We then report the results separately for the test examples containing only these shapes (indistribution) vs examples containing at least one new shape (out-of-distribution). Notice that all the left-out shapes have a similar (but not exact) counterpart shape in the training. The results are reported in Figure 4. As it can be observed, the performance goes significantly down for the out-of-distribution case.</p>
<p>Our depth and generalization results combined show that VLMs struggle with solving multi-hop geometry questions and reveals a crucial gap in their reasoning capabilities.</p>
<p>Correct Label = Correct Reasoning? We next verify if the model produces a correct reasoning chain in the cases where it produces a correct final answer. Since re-using the reasoning chains pro-</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Model performances as a function of the depth of reasoning. Note: near-zero accuracies are not visible in the plot. *GPT4V results were obtained on a subset of randomly selected 10 examples per depth, and the correctness was determined manually.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Measuring the generalization ability with in-distribution and out-of-distribution problems.</p>
<p>duced by a model to further finetune it is becoming more prevalent (Zelikman et al., 2022; Huang et al., 2022; Magister et al., 2022), producing correct reasoning in the case of correct label is an important property of a model. To measure the reasoning accuracy, for the FS-CoT text-only and the FT-CoT models, we randomly selected up to 20 examples (upper-bounded by the number of correctly solved problems) from each of the depths where the model produced the exact label and verified manually if the produced reasoning chain is also correct. We also verified the examples for which the zero-shot model predicted the label correctly. For <em>Depth 1</em> examples, we observe that the reasoning chain is correct for the three models in all cases; for <em>Depth 2</em>, 20/20 have correct reasoning chains for the FS-CoT model and 19/20 have correct reasoning chains for the FT-CoT model, and for <em>Depth 3</em> examples, 8/9 examples have correct reasoning chains for the FS-CoT model and 16/20 for the FT-CoT model, with the errors mostly being on missed intermediate computations that were then replaced with correct numbers in later steps. This shows that the reasoning is mostly correct when the label is predicted correctly.</p>
<p>Due to the low performance of the zero-shot and</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Model performances as a function of width. Models seem to be less affected by increasing the width of the reasoning.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Model performance when distracting information is added to the question/image.</p>
<p>FS-CoT PaLI models, hereafter we only experiment with the FS-CoT Text-Only and finetuned PaLI models.</p>
<h3>4.2 Performance as a Function of Width</h3>
<p>We generate <em>Depth 2</em> examples (medium difficulty in terms of depth) with <em>p<sub>branch</sub></em> = 0, 0.5, and 1.0, and report the performances in Figure 5. We observe that while increasing the width negatively affects the performance in several cases (especially for the FS-CoT model) the amount of decrease is substantially lower compared to the depth experiments<sup>5</sup>.</p>
<p><sup>5</sup>Part of the reason for this observation could be because we have only 30/68 formulas that have more than 1 con-</p>
<p>Table 2: Top-5 failure modes of the models in the order of frequency.</p>
<table>
<thead>
<tr>
<th>Few-Shot Text-Only Model</th>
<th>FT-CoT VLM Model (OOD)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Wrong proof planning</td>
<td>Wrong calculations</td>
</tr>
<tr>
<td>Wrong formula</td>
<td>Misunderstanding shapes</td>
</tr>
<tr>
<td>Wrong calculations</td>
<td>Wrong formula</td>
</tr>
<tr>
<td>Wrong assignment of values</td>
<td>Wrong proof planning</td>
</tr>
<tr>
<td>Hallucinating values</td>
<td>Wrong value assignment</td>
</tr>
</tbody>
</table>
<p>The results hint at the ability of the models at learning to deal with higher width examples. This could be because the main added difficulty from higher width problems is that the model needs to solve more independent Depth 1 problems, on which they showed good performance according to Figure 3.</p>
<h3>4.3 Distractors</h3>
<p>We next measure how well models can deal with distracting information, a phenomenon which is common in real problems. We create a version of the Depth 2 problems where we provide the hidden value as input. This effectively turns the Depth 2 problem into a Depth 1 problem with some extra (distracting) shapes and values. The model performance is reported in Figure 6. Comparing Depth 1 results with and without distractors, the performance drops significantly for all models in presence of a distractor. Comparing Depth 1 with distractor and Depth 2 without distractor, while the text-only model has taken advantage of the value for the hidden element in some cases, for the finetuned VLM models the performance degrades to as low as that of the Depth 2 dataset.</p>
<h3>4.4 Failure Analysis</h3>
<p>To understand the main failure modes of the models, we manually verified 5 examples per depth for the FS-CoT text-only model and the FT-CoT model when tested on a combination of seen and unseen shapes. The main failure modes are presented in Table 2. Besides computation errors which have been previously observed as well for mathematical reasoning problems [lewkowycz2022revealing], we observe several other failure modes: 1- wrong proof planning (either wrong step order or disconnected steps), 2- wrong formulas (showing a gap in model knowledge), 3- misunderstanding shapes in the case of VLMs (e.g., confusing sector with triangle), 4- wrong value assignment (e.g., assigning the</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Model performances broken down by the question type.
value of a side to another side), and 5- hallucination (mostly hallucinating non-existent value). While proof planning is the most frequent failure mode of the text-only model, we notice that the FT-CoT model makes fewer planning errors.</p>
<h3>4.5 Finetuning on GeomVerse Helps Solve Real Geometry Problems</h3>
<p>We verify whether finetuning on GeomVerse helps improve performance on real geometry questions. Specifically, we use a established dataset named Geometry3k [lu2021geometry3k] which contains geometry problems from two high school textbooks across grades 6-12. We report model performance on this dataset in the following settings: 1- Fewshot PaLI model, 2- Finetuning on the train set of GeomVerse without CoT, and 3- Finetuned on the train set of GeomVerse with CoT. Note that this corresponds to an out-of-distribution evaluation due to the difference in the text style, image style, and the types of problems in Geometry3k vs GeomVerse. We also report the results of finetuning a model on the training set of Geometry3k as a reference point for the in-distribution finetuning results. We report the results in Figure 7. While the absolute model performance remains low in all cases (even in the case of in-distribution finetuning - partly due to the small model size), we see that finetuning on GeomVerse offers a boost in the performance of the base model, showing the merit of finetuning on GeomVerse. Moreover, we see that CoT finetuning offers more improvement compared to finetuning without CoT, thus showing the benefit of CoT finetuning even for out-of-distribution examples.</p>
<h3>4.6 Sensitivity to low-level visual features</h3>
<p>To measure how sensitive the VLM models are to the low-level visual features, we create separate</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Measuring model sensitivity to low-level features of the images.</p>
<p>test sets each varying in one low-level feature and measure the performance of the trained models on these new sets. Specifically, we experiment with changing the opacity of the shape colors, the line width of shape boundaries, and the font size of the texts on the images. The results are reported in Figure 8. We observe that the models are robust against opacity and line width, but not against font size changes.</p>
<h3>4.7 Other Variations</h3>
<p>While our dataset is focused around geometry problems, our experiments so far focus on general factors that may be present in many problems requiring reasoning on text and image (i.e. depth, width, distractors, generalization, and low-level visual features). In Appendix A, we experiment with various other axes of difficulty that are more specific to geometry problems (including shapes, source of information, image annotation, adding variablized inputs, and decomposing performance based on question type).</p>
<h2>5 Conclusion</h2>
<p>In this work, we procedurally generated a synthetic dataset of geometry reasoning questions that require multi-hop mathematical reasoning over both text and image. Through the lens of the generated geometry problems, we conducted a systematic analysis of various general and geometry-specific reasoning abilities of VLMs and found the gaps and strengths in their reasoning capabilities. Our work can be extended by adding more standard (e.g., polygons) and non-standard shapes, adding more formulas, and finding ways to generate problems that cover a larger class of geometry questions.</p>
<h2>References</h2>
<ul>
<li>Abdelghani et al. (2023) Rania Abdelghani, Yen-Hsiang Wang, Xingdi Yuan, Tong Wang, Pauline Lucas, Hélène Sauzéon, and Pierre-Yves Oudeyer. 2023. Gpt-3-driven pedagogical agents to train children's curious question-asking skills. <em>International Journal of Artificial Intelligence in Education</em>.</li>
<li>Alayrac et al. (2022) Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. 2022. Flamingo: a visual language model for few-shot learning. <em>Advances in Neural Information Processing Systems</em>, 35:23716–23736.</li>
<li>Allaway et al. (2022) Emily Allaway, Jena D Hwang, Chandra Bhagavatula, Kathleen McKeown, Doug Downey, and Yejin Choi. 2022. Penguins don't fly: Reasoning about generics through instantiations and exceptions. <em>arXiv preprint arXiv:2205.11658</em>.</li>
<li>Anil et al. (2023) Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. <em>arXiv preprint arXiv:2305.10403</em>.</li>
<li>Betz et al. (2021) Gregor Betz, Christian Voigt, and Kyle Richardson. 2021. Critical thinking for language models. In <em>Proceedings of the 14th International Conference on Computational Semantics (IWCS)</em>, pages 63–75, Groningen, The Netherlands (online). Association for Computational Linguistics.</li>
<li>Chen et al. (2022a) Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin, Chongyu Chen, and Xiaodan Liang. 2022a. Unigeo: Unifying geometry logical reasoning via reformulating mathematical expression. <em>arXiv preprint arXiv:2212.02746</em>.</li>
<li>Chen et al. (2021) Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric P Xing, and Liang Lin. 2021. Geoqa: A geometric question answering benchmark towards multimodal numerical reasoning. <em>arXiv preprint arXiv:2105.14517</em>.</li>
<li>Chen et al. (2023) Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, et al. 2023. Pali-x: On scaling up a multilingual vision and language model. <em>arXiv preprint arXiv:2305.18565</em>.</li>
<li>Chen et al. (2022b) Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. 2022b. Pali: A jointly-scaled multilingual language-image model. <em>arXiv preprint arXiv:2209.06794</em>.</li>
<li>Clark et al. (2021) Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2021. Transformers as soft reasoners over language. In <em>Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</em>, IJCAI'20.</li>
</ul>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Antonia Creswell, Murray Shanahan, and Irina Higgins. 2023. Selection-inference: Exploiting large language models for interpretable logical reasoning. In The Eleventh International Conference on Learning Representations.</p>
<p>Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pipatanangkura, and Peter Clark. 2021. Explaining answers with entailment trees. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7358-7370, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Alex Davies, Petar Veličković, Lars Buesing, Sam Blackwell, Daniel Zheng, Nenad Tomašev, Richard Tanburn, Peter Battaglia, Charles Blundell, András Juhász, Marc Lackenby, Geordie Williamson, Demis Hassabis, and Pushmeet Kohli. 2021. Advancing mathematics by guiding human intuition with AI. Nature, 600(7887):70-74.</p>
<p>Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Francisco J. R. Ruiz, Julian Schrittwieser, Grzegorz Swirszcz, David Silver, Demis Hassabis, and Pushmeet Kohli. 2022. Discovering faster matrix multiplication algorithms with reinforcement learning. Nature, 610:47-53.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1. Curran.</p>
<p>Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022. Large language models can self-improve. arXiv preprint arXiv:2210.11610.</p>
<p>Mehran Kazemi, Quan Yuan, Deepti Bhatia, Najoung Kim, Xin Xu, Vaiva Imbrasaite, and Deepak Ramachandran. 2023a. Boardgameqa: A dataset for natural language reasoning with contradictory information. In NeurIPS.</p>
<p>Seyed Mehran Kazemi, Najoung Kim, Deepti Bhatia, Xin Xu, and Deepak Ramachandran. 2023b. Lambada: Backward chaining for automated reasoning in natural language. In ACL.</p>
<p>Sal Khan. 2023. Khan Academy.
Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2023. Decomposed prompting: A modular approach for solving complex tasks. In The Eleventh</p>
<p>International Conference on Learning Representations.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213.</p>
<p>Laura Kovács and Andrei Voronkov. 2013. First-order theorem proving and vampire. In International Conference on Computer Aided Verification, pages 1-35. Springer.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. 2022. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:38433857.</p>
<p>Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597.</p>
<p>Adam Dahlgren Lindström and Savitha Sam Abraham. 2022. Clevr-math: A dataset for compositional language, visual and mathematical reasoning. arXiv preprint arXiv:2208.05358.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146.</p>
<p>Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. 2021. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv preprint arXiv:2105.04165.</p>
<p>Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang. 2022. A survey of deep learning for mathematical reasoning. arXiv preprint arXiv:2212.10535.</p>
<p>Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn. 2022. Teaching small language models to reason. arXiv preprint arXiv:2212.08410.</p>
<p>Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. 2022. Chartqa: A benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244.</p>
<p>Nitesh Methani, Pritha Ganguly, Mitesh M Khapra, and Pratyush Kumar. 2020. Plotqa: Reasoning over scientific plots. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages $1527-1536$.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. 2022. Show your work: Scratchpads for intermediate computation with language models. In Deep Learning for Code Workshop.</p>
<p>OpenAI. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.</p>
<p>Liangming Pan, Alon Albalak, Xinyi Wang, and William Yang Wang. 2023. Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. arXiv preprint arXiv:2305.12295.
J. A. Robinson. 1965. A machine-oriented logic based on the resolution principle. J. ACM, 12(1):23-41.</p>
<p>Abulhair Saparov, Richard Yuanzhe Pang, Vishakh Padmakumar, Nitish Joshi, Seyed Mehran Kazemi, Najoung Kim, and He He. 2023. Testing the general deductive reasoning capacity of large language models using ood examples. arxiv preprint arXiv:2305.15269.</p>
<p>Stephan Schulz. 2002. E-a brainiac theorem prover. AI Communications, 15(2, 3):111-126.</p>
<p>Daniel Selsam, Leonardo de Moura, Kevin Buzzard, Reid Barton, Percy Liang, Sarah Loos, and Freek Wiedijk. 2020. IMO Grand Challenge.</p>
<p>Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren Etzioni, and Clint Malcolm. 2015. Solving geometry problems: Combining text and diagram interpretation. In Proceedings of the 2015 conference on empirical methods in natural language processing, pages 14661476.</p>
<p>Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv:2210.09261.</p>
<p>Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021. ProofWriter: Generating implications, proofs, and abductive statements over natural language. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3621-3634, Online. Association for Computational Linguistics.</p>
<p>Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. 2022. Image as a foreign language: Beit pretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In</p>
<p>Advances in Neural Information Processing Systems, volume 35, pages 24824-24837. Curran Associates, Inc.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601.</p>
<p>Anbang Ye, Christopher Cui, Taiwei Shi, and Mark O Riedl. 2022. Neural story planning. arXiv preprint arXiv:2212.08718.</p>
<p>Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. STaR: Bootstrapping reasoning with reasoning. In Advances in Neural Information Processing Systems, volume 35, pages 15476-15488. Curran Associates, Inc.</p>
<p>Ming-Liang Zhang, Fei Yin, and Cheng-Lin Liu. 2023. A multi-modal neural geometric solver with textual clauses parsed from diagram. arXiv preprint arXiv:2302.11097.</p>
<p>Wanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo, Jiahai Wang, Jian Yin, Ming Zhou, and Nan Duan. 2021. AR-LSAT: Investigating analytical reasoning of text. arXiv preprint arXiv:2104.06598.</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, and Ed H. Chi. 2023. Least-to-most prompting enables complex reasoning in large language models. In The Eleventh International Conference on Learning Representations.</p>
<h2>A More Results: Other Axes of Difficulty</h2>
<p>Besides the experiments in the main text, we also consider a number of other axes of difficulty for a systematic evaluation. In what follows, we describe these axes and present the experimental results. In Section D, we provide samples corresponding to each of the axes of difficulty.</p>
<h2>A. 1 Standard vs Non-Standard Shapes</h2>
<p>In Figure 2, we outlined the standard and nonstandard shapes used in GeomVerse. Conceptually, it should be more difficult to solve problems involving non-standard shapes as they require more computations. We compare the performance of various models on problems that contain all shapes vs those that involve only standard shapes. To fix other axis of difficulty, we only consider depth 2 examples for this experiment where the problems are at a medium level of difficulty. The finetuned models are finetuned on all images in both cases. The results are in Figure 9. According to the results,</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Comparing model performance when using only standard shapes vs when using all shapes. Overall, we do not see a big drop in the performance.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Model performances as a function of providing more information in the text or on the image.
we observe that while the FS-CoT model performs better on the standard shapes, this is not the case after finetuning. This shows that finetuning can teach the models to effectively deal with non-standard (but in-distribution) shapes.</p>
<h2>A. 2 More Info in Text or on Image</h2>
<p>Some of the information can be provided either in the text of the question or on the image. For example, the degree of an angle can be provided in the image, or can be provided in the text. We generate examples where the information is given mostly in text and examples where it is given mostly on the image, and report model performances in Figure 10. For the FT model, we see that the former case results in lower accuracy which could be because in this case the model needs to first map those information to the elements in the image and then reason with them. FT-CoT almost closes the gap; this could be because the provided CoTs teach the model how to map information from text to image.</p>
<h2>A. 3 Image Annotation</h2>
<p>We consider two types of image annotation: 1- individual annotation: we refer to each side with a single lower-case letter, each angle with a Greek letter, and each shape with its (distinct) color, and 2- coordinate annotation: we assign upper-case
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: Model performances as a function of image annotation.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12: Model performances as a function of including variablized inputs in the question. The performance degrades as we include variables in the questions.
letters to the coordinates on the image and refer to sides with the letters on the two coordinates, to angles with the three coordinates, and to shapes with all their coordinates. We generate a test set with coordinate annotation and another with individual annotation and report model performances on these two sets in Figure 11. The two models show different behaviour with the FT model performing slightly better on the individual annotation case, but the FT-CoT model slightly performing better on the coordinate annotation case.</p>
<h2>A. 4 Variablized Inputs</h2>
<p>Instead of providing the exact values of the input elements (e.g., the $\alpha$ angle is 30 degrees), it is common in geometry questions to provide a variablized version of them (e.g., the $\alpha$ angle is $2 x+1$ ) in which case one needs to first infer the value of the variable based on the given information and then use that to infer the value of an element. As an example, we can either directly provide two of the angles of a triangle as input and ask for the third one, or we can provide variablized values for the three angles and ask for one of them. To generate variablized questions, when we use a formula $f$, instead of directly providing the values for $f^{(i n)}$ as input and expecting the model to apply the formula to derive the value of $f^{(\text {out })}$, we provide variablized values</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 13: Model performances broken down by the question type.</p>
<p>for (some of) the elements in $f^{(in)}$ and $f^{(out)}$ and expect the model to use the formula for deriving the value of the variable $x$ and use that to derive the numerical value of $f^{(out)}$. We selected 17/68 of our formulas for which a variablized version of the problem only requires solving an extra 1-d linear equation. We then conducted an experiment where, whenever one of the 17 rules was selected during generation, we provide a variablized version of it with probability $\rho$. Figure 12 demonstrates the results for $\rho = 0$, $\rho = 0.5$ (corresponding to level = medium) and $\rho = 1.0$ (corresponding to level = high). Based on the results, we observe that as we include variablized inputs, the performance of the models degrade, especially for the FT-CoT model. This shows VLMs (and also LLMs) struggle to work with variables when solving geometry problems.</p>
<h3>A.5 Decomposing by Question Type</h3>
<p>Our questions involve asking about the length of a side, the degree of an angle, or the area/perimeter of a shape. In Figure 13, we report model performances for each of these question types. We observe that the FS-CoT Text-Only model performs almost equally across all three types, with slight preference for angle and area/perimeter questions. For the FT model, questions about angles are substantially easier, followed by questions about side. We conjecture that part of the reason for the high performance of the FT model on angle questions might be because the degree of an angle can be estimated from the figure without actually solving the problem. This could be in part validated by the results of the FT-CoT model, where the jump in accuracy is substantially higher for side and area/perimeter questions. In the case of FT-CoT, we see that side questions are easier than the other two; this may in part be because these questions involve easier arithmetic operations (e.g., some of the angle questions require computing <em>arcsin</em> which might be difficult for a pre-trained model).</p>
<h3>B Implementation Details</h3>
<p>For our finetuning experiments, we first generated a training set containing 10k examples and a validation set containing 2k examples. For each of the examples in these two sets, the parameters corresponding to different axes of difficulty discussed in the paper were set randomly to allow for a diverse set of examples in the train and validation sets. We then removed the (few) examples whose solution was identical to one of the solutions in one of the examples in our test sets. The same train and validation sets were used for all of our test sets.</p>
<p>We finetuned our model for 10k steps with a learning rate of 0.0005 and a batch size of 128, measured the model performance on the validation set every 2000 steps, and reported the results on the test sets for the checkpoint achieving the best performance on the validation set.</p>
<p>For our fewshot experiments, we manually selected 4 examples from the training set and used those examples as fewshot demonstrations across all experiments. These 4 examples were selected to ensure many aspects of the test set are covered (e.g., to ensure there are examples at various depths, widths, with and without variables, with more information in text or on image, with different question types, etc.).</p>
<p><strong>Rounding Errors:</strong> Note that depending on how we round intermediate computations, the final answer can be slightly different. For example, consider the expression $$\frac{2.26<em>3.14}{4}$$. If we first multiply the numerator, round it and then divide by 4 and round again, we will get $$\frac{2.26</em>3.14}{4} = \frac{7.1}{4} = 1.78$$. However, if we first do both computations and then round at the end, we will get $$\frac{2.26*3.14}{4} = 1.77$$. For this reason, we reported relaxed accuracy in our experiments to account for the differences in the way we computed the final results and the way the model may compute it.</p>
<h3>C Sample Process for Algorithm 1</h3>
<p>In Figure 14, we provide a visual demonstration of the process in Algorithm 1 for generating an example with Depth 3. In Step 1, we select a shape from our set of shapes and then select one of the formulas. The shape selected in this example is a rectangle and let the selected formula be to compute the area of a rectangle given its height and width;</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 14: A visual demonstration of the process in Algorithm 1 for generating a example in Depth 3.</p>
<p>so $f_1^{(in)} = {L_{AC}, L_{CD}}$ and $f_1^{(out)} = {A_{ABCD}}$ where $L_{AC}$ and $L_{CD}$ represent the length of AC and CD and $A_{ABCD}$ represents the area of ABCD (note that we could also select $L_{BC}$ and $L_{AB}$ instead). We then select which element(s) from $f_1^{(in)}$ we will provide explicitly and which element(s) should be derived. Assume we decide to provide $L_{AC}$ explicitly and append other shapes to derive the value of $L_{CD}$. In this case, we assign a random value to $L_{AC}$ and provide it in the set of facts.</p>
<p>In Step 2, we need to select a shape where one of its sides is $CD$, and select a formula from which the length of this side can be derived. In the provided example, the selected shape is a right triangle and let us assume the selected formula is to compute a side of a right triangle given the hypotenuse and the opposite angle. So $f_2^{(in)} = {L_{CE}, D_{CED}}$ and $f_2^{(out)} = {L_{CD}}$, where $L_{CE}$ and $L_{CD}$ represent the lengths of the CE and CD sides and $D_{CED}$ represents the degree of the $CED$ angle. We then select which element(s) from $f_2^{(in)}$ we will provide explicitly and which element(s) should be derived. Assume both elements should be derived (corresponding to increasing the width of reasoning). So none of the elements will be added to the facts.</p>
<p>In Step 3, we need to select a shape where one of its sides is $CE$, and select a formula from which the length of this side can be derived. In the provided example, the selected shape is a semi-circle. Assume the formula is to compute the diameter of the semi-circle $L_{CE}$ given its perimeter $P_{SemiCircle}$. Since we want to generate Depth 3 examples, we add $P_{SemiCircle}$ to the set of facts.</p>
<p>In Step 4, we need to select a shape that can be connected to the $CED$ angle, such that $D_{CED}$ can be derived from that new shape. In the provided example, the selected shape is a supplementary angle, and the formula is that the sum of two supplementary angles is 180°. We provide $D_{DEF}$ in the facts so $D_{CED}$ can be derived based on that.</p>
<p>Putting it all together, we get the right-most shape in Figure 14. The facts include ${L_{AC}, P_{SemiCircle}, D_{DEF}}$ and the query is $A_{ABCD}$. Based on the rules we used, we can apply deduction to produce a solution as follows:</p>
<p>$$D_{DEF} \Rightarrow D_{DEC}$$</p>
<p>$$P_{SemiCircle} \Rightarrow L_{CE}$$</p>
<p>$$D_{DEC}, L_{CE} \Rightarrow L_{CD}$$</p>
<p>$$L_{CD}, L_{AC} \Rightarrow A_{ABCD}$$</p>
<p>To generate the question, we turn the facts (and the extra information needed to know such as the shapes, whether some angles are vertical or complementary, etc.) into a question using a template. We also provide the shape names in cases where it is necessary. For Figure 14, for example, the question will look like <em>If the length of the AC side of the ABDC rectangle is 10, CDE is a right triangle, the DEF angle is 120 degrees, the DEF and the DEC angles are complementary, and the perimeter of the semi-circle is 20, compute the area of the ABDC rectangle.</em></p>
<h3>D Samples from GeomVerse</h3>
<p>In this work, we experimented with several variations of GeomVerse. Here, we provide samples from these different variations to better illustrate how each test set looks like. The questions and solutions are provided in Tables 3 and 4 and the corresponding images are provided in Figure 15.</p>
<h3>E Limitations</h3>
<p>While GeomVerse covers a wide range of geometry questions, there are problems that cannot be produced using Algorithm 1 with our current set of shapes and formulas. The connection between</p>
<p>Algorithm 1 and logical reasoning makes evident the class of problems that cannot be represented by the algorithm. In particular, let $\mathcal{P}$ be the class of geometry problems containing a tree of shapes where each shape is connected to its parent shape via a single side or a single (vertical) angle, and where the solution can be found by finding the values of the shared elements bottom-up on the tree. Algorithm 1 cannot generate any geometry problem that is not in $\mathcal{P}$.</p>
<p>For example, let $A B C$ be a triangle, $D$ be a point on the $A C$ side dividing $A B C$ into two triangles $A B D$ and $A C D$, where some property of $A B C$ should be computed based on the properties of $A B D$ and $A C D$. This problem cannot be produced by Algorithm 1 as it does not correspond to a tree of connected shapes as described above. However, note that one can add such cases to our set of non-standard shapes in a similar way we added the other non-standard shapes.</p>
<p>Moreover, the problems in GeomVerse can be solved with a logical deduction procedure and may not require much creativity. For this reason, our evaluation should not be considered as measuring the creativity of the models in solving problems, but rather their ability in following a deduction procedure.</p>
<p><img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 15: Samples from our test sets.</p>
<p>Table 3: Questions and solutions corresponding to the images in Figure 15.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Image index</th>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">(a)</td>
<td style="text-align: center;">Compute the perimeter of the ABCD parallelogram.</td>
<td style="text-align: center;">The lengths of the AD and the AB sides of the ABCD parallelogram are 10 and 12, so the perimeter of the ABCD parallelogram is $2 *(10+12)=2 * 22=44$. Therefore the final answer is 44 .</td>
</tr>
<tr>
<td style="text-align: center;">(b)</td>
<td style="text-align: center;">If the degree of the BAC angle is $2 \mathrm{x}+50$, the degree of the BCA angle is $2 \mathrm{x}+35$ and the degree of the CBA angle is $5 \mathrm{x}+50$, compute the degree of the CBA angle.</td>
<td style="text-align: center;">The three degrees of the ABC triangle are $2 x+50$, $2 x+35$ and $5 x+50$. Therefore, $2 x+50+2 x+35+$ $5 x+50=180$, so $9 x+135=180$, so $9 x=45$, so $x=\frac{45}{5}=5$. The degree of the CBA angle $=$ $5 * 5+50=75$. Therefore the final answer is 75 .</td>
</tr>
<tr>
<td style="text-align: center;">(c)</td>
<td style="text-align: center;">If the BCDE shape is a combination of a rectangle and a semi-circle and the area of the BCDE shape is 102, compute the degree of the BCA angle. Assume $\pi=3.14$. Round computations to 2 decimal places.</td>
<td style="text-align: center;">The area of the BCDE shape is 102 and the length of the CD side is 6 , so (other side) $<em> 6+\frac{3.14 * 6^{2}}{6}=$ 102, so (other side) ${ }^{</em>} 6=102-\frac{3.14 * 6^{2}}{6}=102-$ $\frac{3.14 * 36}{6}=102-\frac{113.04}{6}=102-14.13=87.87$. Therefore, the length of the BC side is $\frac{87.87}{6}=14.65$. The length of the hypotenuse of the ABC triangle is 14.65 and the length of the side opposite to the BCA angle is 7 , so the BCA angle equals $\operatorname{asin}\left(\frac{7}{14.65}\right)=$ $\operatorname{asin}(0.48)=28.69$. Therefore the final answer is 28.69 .</td>
</tr>
<tr>
<td style="text-align: center;">(d)</td>
<td style="text-align: center;">If the length of the height of the ABCD trapezoid is 8 , the area of the blue semi-circle is 189.97 , the BCFGH shape is a combination of a rectangle and an equilateral triangle and the perimeter of the BCFGH shape is 42 , compute the area of the ABCD trapezoid. Assume $\pi=3.14$. Round computations to 2 decimal places.</td>
<td style="text-align: center;">The area of the blue semi-circle is 189.97 so the length of the AD diameter can be computed as $\sqrt{8 * \frac{189.97}{\pi}})=\sqrt{\frac{1519.75}{\pi}}=\sqrt{484.0}=22$. The side of the equilateral triangle in the BCFGH shape is equal to the side of the rectangle with length 8 so the shape has two sides with equal but unknown lengths, one with length 8 , and two triangle sides with length 8. The perimeter of the BCFGH shape is 42 so 2 * (unknown side) $+3 * 8=42$. So 2 * (unknown side) $=42-24=18$, and the length of the BC side is $\frac{18}{2}=9$. The lengths of the AD and the BC bases of the ABCD trapezoid are 22 and 9 and the height of the trapezoid is 8 , so the area of the trapezoid is $\frac{1}{2} *(22+9) * 8=\frac{1}{2} * 31 * 8=124$. Therefore the final answer is 124 .</td>
</tr>
<tr>
<td style="text-align: center;">(e)</td>
<td style="text-align: center;">If the perimeter of the ABE triangle is 42 , the BEFG shape is a rectangle where a semi-circle has been removed from one side of it and the perimeter of the BEFG shape is 62 , compute the perimeter of the ABCD rectangle. Assume $\pi=3.14$. Round computations to 2 decimal places.</td>
<td style="text-align: center;">The diameter of the semi-circle in the BEFG shape is equal to the side of the rectangle with length 10 so the shape has two sides with equal but unknown lengths, one side with length 10 , and one semi-circle arc with diameter 10. So the perimeter is $2 <em>$ UnknownSide + $10+\frac{10 * \pi}{6}$. So $2 </em>$ UnknownSide $+10+\frac{10 * 3.14}{6}=$ 62. So $2 <em>$ UnknownSide $=62-10-\frac{10 * 3.14}{6}=$ $62-10-\frac{31.4}{6}=62-10-15.7=36.3$. Therefore, the length of the BE side is $\frac{36.3}{6}=18.15$. The lengths of the AE and BE sides of the ABE triangle are 10 and 18.15 and the perimeter is 42 , so the lengths of the AB side equals $42-10-18.15=$ 13.85. The lengths of the AD and the AB sides of the ABCD rectangle are 15 and 13.85 , so the perimeter of the ABCD rectangle is $2 </em>(15+13.85)=2 * 28.85=$ 57.7. Therefore the final answer is 57.7 .</td>
</tr>
</tbody>
</table>
<p>Table 4: Continuing Table 3.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Image index</th>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">(f)</td>
<td style="text-align: center;">If the ABCD shape is a square where a circle has been removed from it, the AEFGH shape is a rectangle where an equilateral triangle has been removed from one side of it, the perimeter of the AEFGH shape is 48, the DEIJ shape is a rectangle where a semi-circle has been removed from one side of it and the area of the DEIJ shape is 78 , compute the area of the ABCD shape. Assume $\pi=3.14$. Round computations to 2 decimal places.</td>
<td style="text-align: center;">The side of the equilateral triangle in the AEFGH shape is equal to the side of the rectangle with length 9 and the shape has two sides with equal but unknown lengths, one with length 9 , and two triangle sides with length 9 . The perimeter of the shape is 48 so $2 <em>$ OtherSide $+3 * 9=48$. So $2 </em>$ OtherSide $=48-27=21$ and the length of the AE side is $\frac{21}{2}=10.5$. The area of the DEIJ shape is 78 and the length of the EI side is 6 , so OtherSide $<em> 6-\frac{3.14 * 6^{2}}{6}=78$, so OtherSide $</em> 6=78+\frac{3.14 * 6^{2}}{6}=78+\frac{3.14 * 36}{6}=$ $78+\frac{113.04}{6}=78+14.13=92.13$. Therefore, the length of the DE side is $92.13 / 6=15.35$. The lengths of the AE and DE sides of the ADE triangle are 10.5 and 15.35 , so the length of the hypotenuse (the AD side) is $\sqrt{10.5^{2}+15.35^{2}}=$ $\sqrt{110.25+235.62}=\sqrt{345.87}=18.6$. The length of the AD side of the ABCD shape is 18.6 , so its area is $18.6^{2}-\left(\frac{\pi}{4}\right) <em>\left(18.6^{2}\right)=345.96-0.79 </em>$ $345.96=345.96-273.31=72.65$. Therefore the final answer is 72.65 .</td>
</tr>
<tr>
<td style="text-align: center;">(g)</td>
<td style="text-align: center;">If the area of the ACD right triangle is 106, compute the area of the ABC right triangle. Round computations to 2 decimal places.</td>
<td style="text-align: center;">The length of the AD side in the ACD triangle is 14 and the area is 106 so the length of the AC side $=$ $\frac{106 * 2}{14}=\frac{212}{14}=15.14$. The lengths of the AC and AB sides of the ABC triangle are 15.14 and 15 , so the area of the triangle is $(15.14 * 15) / 2=227.1 / 2=$ 113.55. Therefore the final answer is 113.55 .</td>
</tr>
<tr>
<td style="text-align: center;">(h)</td>
<td style="text-align: center;">If the perimeter of the gray triangle is 44, the green shape is a combination of a rectangle and an equilateral triangle and the area of the green shape is 114 , compute the length of the side of the gray triangle marked with question mark. Round computations to 2 decimal places.</td>
<td style="text-align: center;">The area of the green shape is 114 and the length of one side of its rectangle is 6 , so (other side) $<em> 6+\frac{\pi^{2}}{4} </em>$ $6^{2}=114$, so (other side) $* 6=114-\frac{\pi^{2} 4}{4} * 6^{2}=$ $114-\frac{1.72}{2} * 36=114-0.43 * 36=114-15.48=$ 98.52. Therefore, the length of the side marked with letter "a" is $98.52 / 6=16.42$. The lengths of two sides of the gray triangle are 21 and 16.42 and the perimeter is 44 , so the lengths of the side marked with "?" equals $44-21-16.42=6.58$. Therefore the final answer is 6.58 .</td>
</tr>
<tr>
<td style="text-align: center;">(i)</td>
<td style="text-align: center;">If the perimeter of the ABC triangle is 33, the degree of the CAD angle is 75 , the area of the DAC sector is 157, the degree of the EBC angle is 75 and the area of the EBC sector is 56.52 , compute the length of the AB side of the ABC triangle. Assume $\pi=3.14$. Round computations to 2 decimal places.</td>
<td style="text-align: center;">The CAD angle of the DAC sector is 75 and the area is 157 so the AC radius can be computed as $=\sqrt{157 /((75 / 360) * \pi)}=\sqrt{157 /(0.21 * \pi)}=$ $\sqrt{157 / 0.66}=\sqrt{(237.88)}=15.42$. The EBC angle of the EBC sector is 75 and the area is 56.52 so the BC radius can be computed as $=$ $\sqrt{56.52 /((75 / 360) * \pi)}=\sqrt{56.52 /(0.21 * \pi)}=$ $\sqrt{56.52 / 0.66}=\sqrt{85.64}=9.25$. The lengths of the AC and BC sides of the ABC triangle are 15.42 and 9.25 and the perimeter is 33 , so the lengths of the AB side equals $33-15.42-9.25=8.33$. Therefore the final answer is 8.33 .</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>neztable elements in their inputs and so even in the case where $p_{\text {branch }}=1.0$, we still generate a number of examples that correspond to chains as opposed to trees.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ During random value assignment, we test multiple factors to ensure the assigned values are sensible (e.g., the sides of a right triangle are smaller than its hypotenuse) and re-assign values until these criteria are met. Sometimes, this becomes impossible due to some values that are derived from other hops; in these cases, we simply discard the example and generate another example from scratch.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
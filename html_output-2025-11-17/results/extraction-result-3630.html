<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3630 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3630</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3630</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-88.html">extraction-schema-88</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-268091326</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.19379v6.pdf" target="_blank">Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy</a></p>
                <p><strong>Paper Abstract:</strong> Human forecasting accuracy improves through the “wisdom of the crowd” effect, in which aggregated predictions tend to outperform individual ones. Past research suggests that individual large language models (LLMs) tend to underperform compared to human crowd aggregates. We simulate a wisdom of the crowd effect with LLMs. Specifically, we use an ensemble of 12 LLMs to make probabilistic predictions about 31 binary questions, comparing them with those made by 925 human forecasters in a 3-month tournament. We show that the LLM crowd outperforms a no-information benchmark and is statistically indistinguishable from the human crowd. We also observe human-like biases, such as the acquiescence bias. In another study, we find that LLM predictions (of GPT-4 and Claude 2) improve when exposed to the median human prediction, increasing accuracy by 17 to 28%. However, simply averaging human and machine forecasts yields more accurate results. Our findings suggest that LLM predictions can rival the human crowd’s forecasting accuracy through simple aggregation.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3630.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3630.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM Ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM Ensemble ("Wisdom of the Silicon Crowd")</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An aggregate forecast built by taking the median of probabilistic predictions from a diverse crowd of twelve large language models to predict real-world binary events; evaluated against human crowd forecasts from a Metaculus tournament.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM Ensemble (12 models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An ensemble composed of twelve heterogeneous LLMs (GPT-4, GPT-4 with Bing, Claude 2, GPT-3.5-Turbo-Instruct, Solar-0-70B, Llama-2-70B, PaLM 2 (Chat-Bison@002), Coral (Command), Mistral-7B-Instruct, Bard (PaLM 2), Falcon-180B, Qwen-7B-Chat) accessed via web interfaces using default generation parameters; models vary in architecture, size (7B to ~1.6T parameter estimates), proprietary vs open-source, and some have internet access.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Probabilistic forecasting of 31 real-time binary questions about geopolitical, economic, technological, and other real-world events (Metaculus tournament, Oct 2023–Jan 2024).</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Direct prompting via standardized 'superforecaster' prompts (requested a probability 0–100% with step-by-step rationale). Each model was queried three independent times within 48 hours of question opening; per-model medians were recorded when relevant, and the ensemble forecast for each question was the median of all non-missing model forecasts.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Real-world Metaculus forecasting tournament (31 binary questions; 925 human forecasters participating across the tournament). Human benchmark: publicly available median human forecast for each question (Metaculus).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Strictly proper scoring rule: Brier score (mean squared error of probabilistic forecasts). Reported ensemble mean Brier = 0.20 (SD=0.12) across 31 questions. Baseline (always predict 50%) Brier = 0.25. Ensemble median forecast distribution: median forecast 60%, mean forecast 57.35% (SD=20.93).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Ensemble significantly outperformed the no-information 50% baseline (t(30) = -2.35, p = 0.026; BH-adjusted p = 0.039). The ensemble's mean Brier (0.20) was not statistically different from the human crowd mean Brier (0.19, SD=0.19; t(60)=0.19, p=0.850), and equivalence testing (non-preregistered) within medium effect bounds found them equivalent within those bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Acquiescence bias (models skewed toward >50% predictions despite near-even event resolution), generally poor calibration and overconfidence of individual models and aggregate, missing forecasts for some models due to content restrictions or technical failures (109 forecasts missing), relatively small ensemble size (12 models), reliance on default web-interface settings (no API/tunable hyperparameter control), potential sensitivity to timing (human median captured at end-of-day to match model query times).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>Aggregation (median across diverse LLMs) yields forecasting accuracy that rivals aggregate human forecasts on the tested real-world questions, demonstrating a 'wisdom of the silicon crowd' effect; simple median aggregation sufficed to beat a 50% baseline and match human-crowd performance on these questions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3630.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3630.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4 (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art proprietary transformer-based LLM by OpenAI used both as an individual forecaster and in Study 2 as a frontier model that updates forecasts after exposure to human crowd predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based LLM (OpenAI GPT-4). Accessed via OpenAI web interface at default parameters (no API tuning). Evaluated both in single-model runs (Study 1) and in within-model updating experiments (Study 2).</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Same set of 31 binary real-time events from the Metaculus tournament; in Study 2, GPT-4 produced probability ranges (upper/lower bounds) and point midpoints, then updated after being shown the human median.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Direct standardized 'superforecaster' prompting and, in Study 2, a longer prompt invoking superforecasting principles plus a prediction-intervention prompt that provided the human median and asked for an update. Three independent runs per question in Study 1; in Study 2, three pre- and three post-intervention runs (range outputs treated by midpoint).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Metaculus forecasting tournament (31 binary questions). Benchmarks: 50% no-information baseline and median human forecasts from the tournament.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Brier score. Study 1 per-model mean Brier (Table 2) GPT-4 = 0.15 (SD=0.11). Study 2: pre-intervention average Brier = 0.17 (SD=0.13), post-intervention average Brier = 0.14 (SD=0.11), improvement p = 0.003. Simple average benchmark (machine+human) had a Brier ≈ 0.13, which was significantly better than GPT-4's updated forecasts (paired t-test t(92)=2.583, p=0.011).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>As an individual model, GPT-4 in Study 1 had better Brier than many individual models but did not beat the median-ensemble benchmark; in Study 2, GPT-4 significantly improved after exposure to human median but still underperformed a naive simple average of machine and human forecasts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Tendency toward acquiescence (predictions biased above 50%), imperfect calibration and overconfidence in some settings, updating mechanism benefited from human median but model's update procedure was less effective than simple averaging; results based on default web-interface settings without specialized calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>GPT-4 improves its probabilistic forecasting when given the human crowd median (Brier improvement from 0.17 to 0.14), and its magnitude of update is strongly correlated with initial deviation from the human median (r=0.88, p<0.001), indicating sensible updating behaviour but suboptimal performance relative to simple aggregation with humans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3630.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3630.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude 2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 2 (Anthropic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A frontier proprietary LLM from Anthropic evaluated as an individual forecaster in the ensemble and as a frontier model in Study 2 for updating forecasts after receiving human crowd medians.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude 2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Anthropic's Claude 2 transformer-based large language model accessed via the Anthropic web interface with default settings; used in both Study 1 (individual model forecasts) and Study 2 (pre/post human median updating).</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Probabilistic forecasts for the same 31 binary Metaculus questions; in Study 2 Claude 2 produced ranges and updated after exposure to the human median.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Direct prompting with the 'superforecaster' style prompt in Study 1; in Study 2 used longer superforecasting initial prompt and prediction-intervention prompt providing the human median, with three runs pre- and post-intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Metaculus forecasting tournament (31 binary questions); compared to 50% baseline and human median forecasts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Brier score. Study 1 per-model mean Brier (Table 2) Claude 2 = 0.21 (SD=0.16). Study 2: pre-intervention average Brier = 0.22 (SD=0.19), post-intervention average Brier = 0.15 (SD=0.14), p < 0.001. Simple average benchmark Brier ≈ 0.14 (paired test showed simple average was significantly better than Claude 2's updated forecasts, t(92)=3.530, p=0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Claude 2 improved significantly when exposed to human median forecasts (large Brier improvement). However, the updated Claude 2 forecasts still underperformed a naive average of machine and human medians.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Poor calibration and overconfidence common to models; updated forecasts narrower and systematically responsive to human medians but not as accurate as simple aggregation with human median; all experiments used default interface settings and no additional calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>Claude 2 substantially benefits from being provided with the human crowd median (Brier from 0.22 to 0.15), and updates correlate strongly with the initial deviation from the human median (r=0.87, p<0.001), demonstrating effective incorporation of human-derived information though not outperforming simple averaging.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3630.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3630.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human Crowd (Metaculus)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human Crowd Median from Metaculus Forecasting Tournament</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Median probabilistic forecasts aggregated from 925 human forecasters participating in a three-month Metaculus forecasting tournament; used as the human benchmark and as an informational intervention for LLM updating.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Human crowd median (Metaculus)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Median of publicly available human probabilistic predictions collected on the Metaculus platform for each binary question; human participants varied in engagement and could update over time.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>Same 31 real-time binary events; median human forecasts used both as a comparator (aggregate human performance) and as information provided to LLMs in Study 2.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Human forecasters on Metaculus made probabilistic (0–100%) forecasts and updates; study used the public median per question (captured end-of-day or after second day if collected on first day) as the human aggregate.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Metaculus tournament questions (31 binary items). Human crowd mean Brier = 0.19 (SD=0.19) across the question set.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Brier score (aggregate human mean Brier = 0.19). Also used as external information to LLMs which improved their Brier scores when exposed to it.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Human crowd median served as the gold-standard benchmark of crowd forecasting; LLM ensemble mean Brier (0.20) was statistically indistinguishable from human mean Brier (0.19) in preregistered tests, with equivalence tests supporting practical equivalence within medium effect bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Human access to the community prediction varied across questions (approx. half couldn't see community prediction initially), potential asynchronous updating relative to model queries, and human predictions themselves are subject to calibration issues though in Study 2 human medians used were collected after allowing humans to see community predictions and thus were more accurate.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>Human crowd medians not only served as a performance benchmark but also measurably improved LLM forecasts when provided as input: both GPT-4 and Claude 2 reduced Brier scores after exposure to the human median.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3630.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3630.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to estimate the probability or likelihood of future scientific discoveries or real-world events, including methods, results, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Individual LLMs Summary</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Individual Large Language Models (per-model performance summary)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Per-model forecasting performance across the twelve LLMs used in the ensemble, showing heterogeneity in accuracy and calibration (Table 2 & Table 3 in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various individual LLMs (GPT-4, GPT-4 w/ Bing, Bard, Falcon-180B, Claude 2, Solar-0-70B, PaLM 2, Mistral-7B, Qwen-7B-Chat, GPT-3.5-turbo-instruct, Llama-2-70B, Coral (Command))</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Diverse set of transformer-based LLMs from multiple providers and sizes; accessed via web interfaces with default generation parameters. Some had internet access enabled (GPT-4 w/ Bing, Bard, Coral).</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_task</strong></td>
                            <td>The same 31 binary forecasting questions from the Metaculus tournament.</td>
                        </tr>
                        <tr>
                            <td><strong>method_of_probability_estimation</strong></td>
                            <td>Direct standardized prompting; three independent responses per question per model; per-model medians computed and evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Metaculus 31-question tournament; human medians; 50% baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Per-model mean Brier scores (from Table 2): GPT-4 0.15, GPT-4 (with Bing) 0.16, Bard (PaLM 2) 0.19, Falcon-180B 0.21, Claude 2 0.21, Solar-0-70B 0.22, PaLM 2 (Chat-Bison@002) 0.23, Mistral-7B-Instruct 0.24, Qwen-7B-Chat 0.24, GPT-3.5-Turbo-Instruct 0.25, Llama-2-70B 0.25, Coral (Command) 0.38. Calibration indices (CI) vary, aggregate CI = 0.041; Coral notably worst CI = 0.212.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Significant differences across models found by ANOVA (F(12,354)=2.64, p=0.002); Coral underperformed many models and the human crowd after multiple-comparison adjustment, but most pairwise differences (other than those involving Coral) were not significant. Some frontier models (GPT-4, Claude 2) performed near or better than human median on average; ensemble median outperformed many single models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Heterogeneity in per-model performance and calibration; overconfidence and acquiescence bias common; some models refused or omitted answers for content reasons; default-interface querying limits reproducibility of internal sampling parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_findings</strong></td>
                            <td>Performance is heterogeneous across models with top-performing models (GPT-4, GPT-4 w/ Bing, some others) approaching human-crowd accuracy, while some models (e.g., Coral) underperform substantially; aggregation improves robustness and mitigates individual model biases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Approaching Human-Level Forecasting with Language Models <em>(Rating: 2)</em></li>
                <li>Large Language Model Prediction Capabilities: Evidence from a Real-World Forecasting Tournament <em>(Rating: 2)</em></li>
                <li>AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy <em>(Rating: 1)</em></li>
                <li>Large language models are zero-shot time series forecasters <em>(Rating: 1)</em></li>
                <li>Small steps to accuracy: Incremental belief updaters are better forecasters <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3630",
    "paper_id": "paper-268091326",
    "extraction_schema_id": "extraction-schema-88",
    "extracted_data": [
        {
            "name_short": "LLM Ensemble",
            "name_full": "LLM Ensemble (\"Wisdom of the Silicon Crowd\")",
            "brief_description": "An aggregate forecast built by taking the median of probabilistic predictions from a diverse crowd of twelve large language models to predict real-world binary events; evaluated against human crowd forecasts from a Metaculus tournament.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLM Ensemble (12 models)",
            "model_description": "An ensemble composed of twelve heterogeneous LLMs (GPT-4, GPT-4 with Bing, Claude 2, GPT-3.5-Turbo-Instruct, Solar-0-70B, Llama-2-70B, PaLM 2 (Chat-Bison@002), Coral (Command), Mistral-7B-Instruct, Bard (PaLM 2), Falcon-180B, Qwen-7B-Chat) accessed via web interfaces using default generation parameters; models vary in architecture, size (7B to ~1.6T parameter estimates), proprietary vs open-source, and some have internet access.",
            "prediction_task": "Probabilistic forecasting of 31 real-time binary questions about geopolitical, economic, technological, and other real-world events (Metaculus tournament, Oct 2023–Jan 2024).",
            "method_of_probability_estimation": "Direct prompting via standardized 'superforecaster' prompts (requested a probability 0–100% with step-by-step rationale). Each model was queried three independent times within 48 hours of question opening; per-model medians were recorded when relevant, and the ensemble forecast for each question was the median of all non-missing model forecasts.",
            "dataset_or_benchmark": "Real-world Metaculus forecasting tournament (31 binary questions; 925 human forecasters participating across the tournament). Human benchmark: publicly available median human forecast for each question (Metaculus).",
            "performance_metrics": "Strictly proper scoring rule: Brier score (mean squared error of probabilistic forecasts). Reported ensemble mean Brier = 0.20 (SD=0.12) across 31 questions. Baseline (always predict 50%) Brier = 0.25. Ensemble median forecast distribution: median forecast 60%, mean forecast 57.35% (SD=20.93).",
            "comparison_to_baselines": "Ensemble significantly outperformed the no-information 50% baseline (t(30) = -2.35, p = 0.026; BH-adjusted p = 0.039). The ensemble's mean Brier (0.20) was not statistically different from the human crowd mean Brier (0.19, SD=0.19; t(60)=0.19, p=0.850), and equivalence testing (non-preregistered) within medium effect bounds found them equivalent within those bounds.",
            "limitations_or_challenges": "Acquiescence bias (models skewed toward &gt;50% predictions despite near-even event resolution), generally poor calibration and overconfidence of individual models and aggregate, missing forecasts for some models due to content restrictions or technical failures (109 forecasts missing), relatively small ensemble size (12 models), reliance on default web-interface settings (no API/tunable hyperparameter control), potential sensitivity to timing (human median captured at end-of-day to match model query times).",
            "notable_findings": "Aggregation (median across diverse LLMs) yields forecasting accuracy that rivals aggregate human forecasts on the tested real-world questions, demonstrating a 'wisdom of the silicon crowd' effect; simple median aggregation sufficed to beat a 50% baseline and match human-crowd performance on these questions.",
            "uuid": "e3630.0",
            "source_info": {
                "paper_title": "Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4 (GPT-4)",
            "brief_description": "A state-of-the-art proprietary transformer-based LLM by OpenAI used both as an individual forecaster and in Study 2 as a frontier model that updates forecasts after exposure to human crowd predictions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Transformer-based LLM (OpenAI GPT-4). Accessed via OpenAI web interface at default parameters (no API tuning). Evaluated both in single-model runs (Study 1) and in within-model updating experiments (Study 2).",
            "prediction_task": "Same set of 31 binary real-time events from the Metaculus tournament; in Study 2, GPT-4 produced probability ranges (upper/lower bounds) and point midpoints, then updated after being shown the human median.",
            "method_of_probability_estimation": "Direct standardized 'superforecaster' prompting and, in Study 2, a longer prompt invoking superforecasting principles plus a prediction-intervention prompt that provided the human median and asked for an update. Three independent runs per question in Study 1; in Study 2, three pre- and three post-intervention runs (range outputs treated by midpoint).",
            "dataset_or_benchmark": "Metaculus forecasting tournament (31 binary questions). Benchmarks: 50% no-information baseline and median human forecasts from the tournament.",
            "performance_metrics": "Brier score. Study 1 per-model mean Brier (Table 2) GPT-4 = 0.15 (SD=0.11). Study 2: pre-intervention average Brier = 0.17 (SD=0.13), post-intervention average Brier = 0.14 (SD=0.11), improvement p = 0.003. Simple average benchmark (machine+human) had a Brier ≈ 0.13, which was significantly better than GPT-4's updated forecasts (paired t-test t(92)=2.583, p=0.011).",
            "comparison_to_baselines": "As an individual model, GPT-4 in Study 1 had better Brier than many individual models but did not beat the median-ensemble benchmark; in Study 2, GPT-4 significantly improved after exposure to human median but still underperformed a naive simple average of machine and human forecasts.",
            "limitations_or_challenges": "Tendency toward acquiescence (predictions biased above 50%), imperfect calibration and overconfidence in some settings, updating mechanism benefited from human median but model's update procedure was less effective than simple averaging; results based on default web-interface settings without specialized calibration.",
            "notable_findings": "GPT-4 improves its probabilistic forecasting when given the human crowd median (Brier improvement from 0.17 to 0.14), and its magnitude of update is strongly correlated with initial deviation from the human median (r=0.88, p&lt;0.001), indicating sensible updating behaviour but suboptimal performance relative to simple aggregation with humans.",
            "uuid": "e3630.1",
            "source_info": {
                "paper_title": "Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Claude 2",
            "name_full": "Claude 2 (Anthropic)",
            "brief_description": "A frontier proprietary LLM from Anthropic evaluated as an individual forecaster in the ensemble and as a frontier model in Study 2 for updating forecasts after receiving human crowd medians.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Claude 2",
            "model_description": "Anthropic's Claude 2 transformer-based large language model accessed via the Anthropic web interface with default settings; used in both Study 1 (individual model forecasts) and Study 2 (pre/post human median updating).",
            "prediction_task": "Probabilistic forecasts for the same 31 binary Metaculus questions; in Study 2 Claude 2 produced ranges and updated after exposure to the human median.",
            "method_of_probability_estimation": "Direct prompting with the 'superforecaster' style prompt in Study 1; in Study 2 used longer superforecasting initial prompt and prediction-intervention prompt providing the human median, with three runs pre- and post-intervention.",
            "dataset_or_benchmark": "Metaculus forecasting tournament (31 binary questions); compared to 50% baseline and human median forecasts.",
            "performance_metrics": "Brier score. Study 1 per-model mean Brier (Table 2) Claude 2 = 0.21 (SD=0.16). Study 2: pre-intervention average Brier = 0.22 (SD=0.19), post-intervention average Brier = 0.15 (SD=0.14), p &lt; 0.001. Simple average benchmark Brier ≈ 0.14 (paired test showed simple average was significantly better than Claude 2's updated forecasts, t(92)=3.530, p=0.001).",
            "comparison_to_baselines": "Claude 2 improved significantly when exposed to human median forecasts (large Brier improvement). However, the updated Claude 2 forecasts still underperformed a naive average of machine and human medians.",
            "limitations_or_challenges": "Poor calibration and overconfidence common to models; updated forecasts narrower and systematically responsive to human medians but not as accurate as simple aggregation with human median; all experiments used default interface settings and no additional calibration.",
            "notable_findings": "Claude 2 substantially benefits from being provided with the human crowd median (Brier from 0.22 to 0.15), and updates correlate strongly with the initial deviation from the human median (r=0.87, p&lt;0.001), demonstrating effective incorporation of human-derived information though not outperforming simple averaging.",
            "uuid": "e3630.2",
            "source_info": {
                "paper_title": "Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Human Crowd (Metaculus)",
            "name_full": "Human Crowd Median from Metaculus Forecasting Tournament",
            "brief_description": "Median probabilistic forecasts aggregated from 925 human forecasters participating in a three-month Metaculus forecasting tournament; used as the human benchmark and as an informational intervention for LLM updating.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Human crowd median (Metaculus)",
            "model_description": "Median of publicly available human probabilistic predictions collected on the Metaculus platform for each binary question; human participants varied in engagement and could update over time.",
            "prediction_task": "Same 31 real-time binary events; median human forecasts used both as a comparator (aggregate human performance) and as information provided to LLMs in Study 2.",
            "method_of_probability_estimation": "Human forecasters on Metaculus made probabilistic (0–100%) forecasts and updates; study used the public median per question (captured end-of-day or after second day if collected on first day) as the human aggregate.",
            "dataset_or_benchmark": "Metaculus tournament questions (31 binary items). Human crowd mean Brier = 0.19 (SD=0.19) across the question set.",
            "performance_metrics": "Brier score (aggregate human mean Brier = 0.19). Also used as external information to LLMs which improved their Brier scores when exposed to it.",
            "comparison_to_baselines": "Human crowd median served as the gold-standard benchmark of crowd forecasting; LLM ensemble mean Brier (0.20) was statistically indistinguishable from human mean Brier (0.19) in preregistered tests, with equivalence tests supporting practical equivalence within medium effect bounds.",
            "limitations_or_challenges": "Human access to the community prediction varied across questions (approx. half couldn't see community prediction initially), potential asynchronous updating relative to model queries, and human predictions themselves are subject to calibration issues though in Study 2 human medians used were collected after allowing humans to see community predictions and thus were more accurate.",
            "notable_findings": "Human crowd medians not only served as a performance benchmark but also measurably improved LLM forecasts when provided as input: both GPT-4 and Claude 2 reduced Brier scores after exposure to the human median.",
            "uuid": "e3630.3",
            "source_info": {
                "paper_title": "Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Individual LLMs Summary",
            "name_full": "Individual Large Language Models (per-model performance summary)",
            "brief_description": "Per-model forecasting performance across the twelve LLMs used in the ensemble, showing heterogeneity in accuracy and calibration (Table 2 & Table 3 in the paper).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various individual LLMs (GPT-4, GPT-4 w/ Bing, Bard, Falcon-180B, Claude 2, Solar-0-70B, PaLM 2, Mistral-7B, Qwen-7B-Chat, GPT-3.5-turbo-instruct, Llama-2-70B, Coral (Command))",
            "model_description": "Diverse set of transformer-based LLMs from multiple providers and sizes; accessed via web interfaces with default generation parameters. Some had internet access enabled (GPT-4 w/ Bing, Bard, Coral).",
            "prediction_task": "The same 31 binary forecasting questions from the Metaculus tournament.",
            "method_of_probability_estimation": "Direct standardized prompting; three independent responses per question per model; per-model medians computed and evaluated.",
            "dataset_or_benchmark": "Metaculus 31-question tournament; human medians; 50% baseline.",
            "performance_metrics": "Per-model mean Brier scores (from Table 2): GPT-4 0.15, GPT-4 (with Bing) 0.16, Bard (PaLM 2) 0.19, Falcon-180B 0.21, Claude 2 0.21, Solar-0-70B 0.22, PaLM 2 (Chat-Bison@002) 0.23, Mistral-7B-Instruct 0.24, Qwen-7B-Chat 0.24, GPT-3.5-Turbo-Instruct 0.25, Llama-2-70B 0.25, Coral (Command) 0.38. Calibration indices (CI) vary, aggregate CI = 0.041; Coral notably worst CI = 0.212.",
            "comparison_to_baselines": "Significant differences across models found by ANOVA (F(12,354)=2.64, p=0.002); Coral underperformed many models and the human crowd after multiple-comparison adjustment, but most pairwise differences (other than those involving Coral) were not significant. Some frontier models (GPT-4, Claude 2) performed near or better than human median on average; ensemble median outperformed many single models.",
            "limitations_or_challenges": "Heterogeneity in per-model performance and calibration; overconfidence and acquiescence bias common; some models refused or omitted answers for content reasons; default-interface querying limits reproducibility of internal sampling parameters.",
            "notable_findings": "Performance is heterogeneous across models with top-performing models (GPT-4, GPT-4 w/ Bing, some others) approaching human-crowd accuracy, while some models (e.g., Coral) underperform substantially; aggregation improves robustness and mitigates individual model biases.",
            "uuid": "e3630.4",
            "source_info": {
                "paper_title": "Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Approaching Human-Level Forecasting with Language Models",
            "rating": 2,
            "sanitized_title": "approaching_humanlevel_forecasting_with_language_models"
        },
        {
            "paper_title": "Large Language Model Prediction Capabilities: Evidence from a Real-World Forecasting Tournament",
            "rating": 2,
            "sanitized_title": "large_language_model_prediction_capabilities_evidence_from_a_realworld_forecasting_tournament"
        },
        {
            "paper_title": "AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy",
            "rating": 1,
            "sanitized_title": "aiaugmented_predictions_llm_assistants_improve_human_forecasting_accuracy"
        },
        {
            "paper_title": "Large language models are zero-shot time series forecasters",
            "rating": 1,
            "sanitized_title": "large_language_models_are_zeroshot_time_series_forecasters"
        },
        {
            "paper_title": "Small steps to accuracy: Incremental belief updaters are better forecasters",
            "rating": 1,
            "sanitized_title": "small_steps_to_accuracy_incremental_belief_updaters_are_better_forecasters"
        }
    ],
    "cost": 0.01372625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Rival Human Crowd Accuracy
22 Jul 2024</p>
<p>Philipp Schoenegger 
Peter S Park 
Philip E Tetlock </p>
<p>London School of Economics and Political Science Indre Tuminauskaite Independent Researcher</p>
<p>University of Pennsylvania</p>
<p>Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Rival Human Crowd Accuracy
22 Jul 202450507ECA0C27389DA6550D738FDF1C7FarXiv:2402.19379v6[cs.CY]
Human forecasting accuracy in practice relies on the 'wisdom of the crowd' effect, in which predictions about future events are significantly improved by aggregating across a crowd of individual forecasters.Past work on the forecasting ability of large language models (LLMs) suggests that frontier LLMs, as individual forecasters, underperform compared to the gold standard of a human crowd forecasting tournament aggregate.In Study 1, we expand this research by using an LLM ensemble approach consisting of a crowd of twelve LLMs.We compare the aggregated LLM predictions on 31 binary questions to that of a crowd of 925 human forecasters from a three-month forecasting tournament.Our preregistered main analysis shows that the LLM crowd outperforms a simple no-information benchmark and is not statistically different from the human crowd.In exploratory analyses, we find that these two approaches are equivalent with respect to medium-effect-size equivalence bounds.We also observe an acquiescence effect, with mean model predictions being significantly above 50%, despite an almost even split of positive and negative resolutions.Moreover, in Study 2, we test whether LLM predictions (of GPT-4 and Claude 2) can be improved by drawing on human cognitive output.We find that both models' forecasting accuracy benefits from exposure to the median human prediction as information, improving accuracy by between 17% and 28%: though this leads to less accurate predictions than simply averaging human and machine forecasts.Our results suggest that LLMs can achieve forecasting accuracy rivaling that of human crowd forecasting tournaments: via the simple, practically applicable method of forecast aggregation.This replicates the 'wisdom of the crowd' effect for LLMs, and opens up their use for a variety of applications throughout society.</p>
<p>Introduction</p>
<p>In the field of artificial intelligence (AI), the rapidly increasing capabilities of large language models (LLMs) have shown promise and even market-competitiveness in a rapidly increasing number of economically valuable and cognitively demanding tasks (Naveed et al. 2023;Sutton 2023).Stateof-the-art LLMs with billions of parameters, built on the Transformer architecture (Vaswani et al. 2017), are trained on a very large amount of internet text data (Shen et al. 2023b), before being fine-tuned.The LLMs are trained on this data to predict the next word or subword (token) when given an input string.This step of next-token prediction-when applied repeatedly-generates a sequence of tokens that form an output string coherently text-completing the input, often at a level of coherence previously thought to be only achievable by human cognition (Anthropic 2023;Gemini Team et al. 2023;OpenAI et al. 2023;Touvron et al. 2023) and at a high level of applicability to chat interfaces and various other settings.</p>
<p>This general training objective of next-token prediction, coupled with fine-tuning, also indirectly results in these LLMs displaying an array of specialized skills, which are often only emergently observed after the fact: in ways that were not-and for all practical purposes, likely could not have been-predicted before the first observation of the given capability (Wei et al. 2022).Such skills include but are not limited to marketing (Fraiwan and Khasawneh 2023), reading comprehension (Winter 2023), teaching (Fraiwan and Khasawneh 2023;Sallam et al. 2023), abstract object classification (Atari et al. 2023), cyberattacks (Heiding et al. 2023), robotics (Vemprala et al. 2023), social-science applications (Abdurahman et al. 2023;Park, Schoenegger, and Zhu 2024), medical analysis (Bubeck et al. 2023;Nori et al. 2023;Sallam et al. 2023), legal analysis (Bubeck et al. 2023;Katz et al. 2023), deception (Park et al. 2023), surgical knowledge (Beaulieu-Jones et al. 2024), and computer graphics assessment (Feng et al. 2024).</p>
<p>When evaluating the capabilities of a given AI system, the predominant traditional method is to measure how well an AI system performs at fixed benchmarks for specific tasks (Kistowski et al. 2015).The significant advancements achieved by transformer-based LLMs in these domains have rendered many previously established benchmarks obsolete (Laskar et al. 2023;Shen et al. 2023a), moving the metaphorical goalposts forward in the form of more challenging and comprehensive benchmarks (Alzahrani et al. 2024).It is plausible that a significant portion of the unprecedented successes that state-of-the-art LLMs have achieved on past task benchmarks is genuinely due to a deep understanding of the task-relevant cognitive skills achieved by the LLMs (Bubeck et al. 2023).Indeed, this argument is corroborated by the economic competitiveness-and even promises of economic superiority-that LLMs are achieving for an increasing array of human occupations (Sutton 2023), such as transcription (Peng et al. 2023), translation (Jiao et al. 2023), and programming (Bubeck et al. 2023).</p>
<p>However, it is also plausible that a significant portion of these successes on task benchmarks is due to a superficial memorization of the task's solutions: and shallow understanding of training-set patterns in general (Bender et al. 2021;Biderman et al. 2023;Carlini et al. 2023;Magar and Schwartz 2022).Distinguishing between deep understanding and shallow memorization is a complex challenge, and is central to accurate assessments of advanced reasoning capabilities in AI.This is akin to the examiner's problem of testing their student for deep understanding of the course material, even when many of the potential exam questions can be correctly answered by shallow memorization instead.In fact, just like the student can memorize the answers to exam questions if they see it beforehand, so too can an LLM if its training data contain the questions and answers used in the task benchmark.To resolve this ambiguity, one can exploit the testable presence or absence of the ability to generalize out-of-distribution: to apply learned knowledge beyond the settings represented in the training data (Arora and Goyal 2023).Such a test is arguably key to discerning deep understanding of the task at hand (Grove and Bretz 2012), but is difficult to design when aiming to assess broad LLM capabilities.</p>
<p>In contrast to task benchmarks, where questions and answers are fixed and potentially contained in an LLM's training data, there are contexts where this concern can be ruled out fully: for example, when predicting the future in real-world settings (Schoenegger and Park 2023;Schoenegger et al. 2024).This test stands out for its high external validity, in that the correct answer to a given real-world forecasting question cannot be in a given LLM's training set, as not even the human evaluator knows the answer at the time of evaluation.Moreover, the practice of forecasting is omnipresent in the cognitive tasks undertaken by humans, encompassing a wide range of applications from forecasting the trajectory of current events to setting long-term plans.The ubiquity of forecasting-especially in white-collar occupations where the increasing capabilities of LLMs are predicted to disrupt or even replace human professionals (Acemoglu 2023;Park and Tegmark 2023;Summers and Rattner 2023)-combined with the intrinsic external validity makes testing the forecasting capabilities of AI systems an ideal test for assessing the real-world applicability of LLMs.</p>
<p>One context where this can be tested directly are forecasting tournaments.These tournaments involve participants who make probabilistic predictions about future occurrences and are then evaluated and rewarded for their accuracy (Tetlock et al. 2014).Across a set of questions, prediction accuracy of these forecasts determines the reputational or monetary rewards, with more precise predictions yielding greater rewards, incentivising forecasters to research the questions and to provide wellinformed predictions.Based on the predictions of a crowd of forecasters, their aggregate is a gold-standard for human intelligence gathering.This effectiveness of the aggregate of competitive forecasting endeavors relies on the 'wisdom of the crowd' phenomenon, which is the effect that results in the collective accuracy of a set of predictions often surpasses the vast majority of individual judgments that make up the respective crowd.This concept is supported by extensive research across various fields such as prediction markets (Bassamboo, Cui, and Moreno 2018), political forecasting, and more, showing that the combined forecasts of many individuals tend to be remarkably precise (Da and Huang 2020;Lichtendahl Jr, Grushka-Cockayne, and Pfeifer 2013;Surowiecki 2004).This 'wisdom of the crowd' effect relies on independent and unbiased judgements, which achieves an error-cancellation effect (Budescu and Chen 2015) and thereby causes the aggregate to outperform randomly selected forecasts from parts of that crowd (Davis-Stober et al. 2014).As Budescu points out, this aggregation mechanism increases information and accounts for extremes (Budescu 2006), with the 'wisdom of the crowd' effect also holding in contexts of biased inputs (Koriat 2012) or when there are correlations among judgements (Davis-Stober et al. 2014), showing remarkable robustness.Moreover, there is a large literature on improvements of this aggregation process (Baron et al. 2014;Himmelstein, Budescu, and Han 2023;Himmelstein, Budescu, and Ho 2023), with a central take-away being that a simple median is a surprisingly powerful aggregation mechanism across contexts.</p>
<p>Past work has compared the prediction performance of frontier models against a human crowd.With respect to evaluating a single model, Schoenegger and Park (2023) found that the frontier model GPT-4 performed poorly when comparing its predictions to that of a crowd drawn from a forecasting tournament.In fact, GPT-4 did not even significantly outperform the no-information benchmark strategy of predicting 50% on every question.Also, the work of Halawi et al. (2024) has investigated the prediction capabilities of an LLM system, including a combination of news retrieval and reasoning systems.They replicated the finding of Schoenegger and Park (2023) that individual models show poor prediction accuracy, but also found that their optimised system approach aggregated human accuracy.This suggests that individual LLMs may have poor forecasting accuracy, but can produce accurate predictions if they are set in an advanced system.</p>
<p>A hypothesis worth probing is that the underperformance of individual LLMs in real-time forecasting may be, at least in part, due to not making use of the 'wisdom of the crowd' effect.It is reasonable that LLM forecast accuracy may be enhanced by aggregation, as crowd aggregates are known to result in better predictions even high-performing individuals.To test whether such a 'wisdom of the silicon crowd' effect exists, we simulate a crowd of diverse LLMs and draw questions from a real-world forecasting tournament, directly comparing the LLM crowd estimate to that of the human crowd, without introducing further additions like retrieval systems.</p>
<p>In Study 1, we test this LLM ensemble approach, aggregating twelve LLMs' forecasts into a collective crowd forecast, leveraging the diversity inherent in the different models' training data, parameters, and methodologies (such as idiosyncratic fine-tuning).We test whether this diversity improves machine forecast accuracy by reducing the impact of individual model biases and errors.We first test whether the LLM ensemble, unlike GPT-4 in the study of Schoenegger and Park (2023), will significantly outperform the no-information benchmark in a forecasting tournament.This benchmark provides a minimal benchmark of accuracy that is equivalent to guessing 50% on every question.</p>
<p>Null hypothesis 1, Study 1: The average of median LLM forecasts is neither statistically significantly more nor less accurate than the 50% baseline, H 01 : BLLM = 0.25.</p>
<p>We also conduct the stronger test of whether the LLM ensemble will significantly outperform the human crowd drawn from the real-world forecasting tournament.For both studies, we use a threemonth tournament run on the platform Metaculus as our human crowd comparison.This provides a more direct comparison of two aggregated forecasts and would present a result that had so far not been achieved.</p>
<p>Null hypothesis 2, Study 1: The average of median LLM forecasts is neither statistically significantly more nor less accurate than the average of median human forecasts, H 02 : µ LLM = µ Human .</p>
<p>Lastly for Study 1, we test for differences in forecasting accuracy between the twelve models.Some of these models are variations of each other, like GPT-4 and GPT-4 with Bing access, PaLM2 and PaLM2 in Bard, or Llama-2-70B and Solar-0-70B; while others differ on more fundamental grounds.Testing whether we find differences between models with different capabilities, endpoints, fine-tunings, sizes, etc. might provide further insight into which aspects help or hinder prediction accuracy.</p>
<p>Null hypothesis 3, Study 1: There are no statistically significant differences in the average accuracy across the different LLMs and humans, H 03 :
µ 1 = µ 2 = . . . = µ k .
In Study 2, we investigate the ability of two frontier models (GPT-4 and Claude 2) to integrate human intelligence into their forecasting updating processes.This contributes to work on the interactions between humans and AI.While previous work has focused on how AI can improve human predictions (Schoenegger et al. 2024), this study looks at the reverse; how human forecasts can improve LLM predictions.This is studied in a context where models update their forecasts in response to receiving the human crowd prediction.This investigation of updating behavior is grounded in the premise that access to external information, such as the median forecast of a human crowd, can serve as a valuable reference point for recalibrating predictions.This process builds on Bayesian principles (Ghirardato 2002;Park 2022;Savage 1972) where prior beliefs (in this case, initial forecasts) are adjusted in light of new evidence (the human crowd median) to produce updated posterior beliefs (revised forecasts).The interaction between human and machine intelligence in this context is of particular interest, as it exemplifies the potential synergies that can emerge from integrating the intuitive, experience-based judgments of humans with the data-processing capabilities of LLMs.</p>
<p>We first investigate whether for each of the two LLMs, its average forecast becomes more accurate after being presented with the human crowd's median forecast.This is the most straightforward test of whether human cognitive output in this setting can augment machine-generated forecasts, as measured by forecasting accuracy.</p>
<p>Null hypothesis 1, Study 2:</p>
<p>There is no statistically significant difference in the average accuracy of either LLM model before and after having been provided the human crowd median, H 01 : µ pre = µ post .</p>
<p>We next investigate the impact of human median forecasting exposure on the precision of LLM forecasts.Specifically, we investigate whether the prediction intervals become narrower, indicating increased confidence in the forecasts: an effect that would suggest that the human predictions-to which the LLMs have been exposed-have nontrivial information value.</p>
<p>Null hypothesis 2, Study 2: The size of the prediction intervals do not become narrower after exposure to the human crowd median, H 02 : ∆ range ≥ 0.</p>
<p>Finally, we investigate the relationship between the initial deviation of LLM forecasts from the human median and the magnitude of subsequent adjustments.This probes the extent to which larger discrepancies prompt more significant forecast revisions as would be expected.</p>
<p>Null hypothesis 3, Study 2:</p>
<p>The magnitude of LLM forecast adjustments is not correlated with the initial deviation of their forecasts from the human crowd median, H 03 : ρ = 0.</p>
<p>Both studies jointly provide the next step in LLM prediction capabilities research.Building on previous work (Schoenegger and Park 2023;Schoenegger et al. 2024), the present paper examines an LLM ensemble approach instead of a single model.Additionally, while other work (Schoenegger et al. 2024) has looked at how AI predictions can improve human accuracy, the present paper also tests the converse, thereby helping complete the picture of how humans and AI systems may interact in real-world contexts that require accurate forecasting.</p>
<p>Methods</p>
<p>All analyses were preregistered on the Open Science Framework1 .We clearly label all exploratory and non-preregistered analyses as such throughout the paper to indicate which tests were decided on after having seen the data.</p>
<p>Study 1</p>
<p>In Study 1, we collected data from a total of twelve diverse large language models to simulate the LLM crowd.Specifically, these twelve models were GPT-4, GPT-4 (with Bing), Claude 2, GPT3.5-Turbo-Instruct,Solar-0-70b, Llama-2-70b, PaLM 2 (Chat-Bison@002), Coral (Command), Mistral-7B-Instruct, Bard (PaLM 2), Falcon-180B, and Qwen-7B-Chat.We accessed each model through a web interface and did not query any models via their APIs to hold the query method constant, thus using default parameters (e.g., temperature) for all models.These web interfaces included company-specific interfaces like those offered for the models by OpenAI, Anthropic, Cohere, and Google, as well as interfaces provided by other third parties such as Poe, Huggingface, and Modelscope that provided access to the remaining models.We took this approach to maximise the number of models that we could reliably query throughout the study period that we collected data for while retaining heterogeneity of model specifications as our goal was to draw on a diverse set of models.Additionally, this also kept this study in the context of publicly available and easily accessible models.The final set of models includes frontier proprietary models (GPT-4, Claude 2) as well open-source models (e.g., Llama-2-70b, Mistral 7B-Instruct) from a variety of demographically diverse companies originating from China, France, United Arab Emirates, South Korea, Canada, and the United States.We also have a variety of models with internet access (e.g., GPT-4 with Bing, Bard, Coral) and a large diversity of model sizes, ranging from 7 billion parameters to an estimated 1.6 trillion.2For a full list of all models and their central specifications, see Table 1 below.In order to assess the prediction capabilities of these models, we drew on a set of forecasting questions that were asked in real time to a public forecasting tournament that ran from October 2023 to January 2024 on the platform Metaculus, where over the course of this tournament, 925 human forecasters provided at least one prediction.In this tournament, forecasters were able to sign up to Metaculus (Metaculus 2024) and predict on as many questions as they wanted.The questions posed ranged from conflict in the Middle East, interest rates, literary prizes, and English electoral politics to Indian air quality, cryptocurrency, consumer technology, and space travel.We focused exclusively on binary probabilistic forecasts, collecting a total of 31 questions.Each question included a question title, a background section detailing the context of the question being asked, and a resolution passage that spelled out in detail how the question will resolve.We drew on the same set of questions and used the publicly available human median predictions for each question as the human benchmark.For a full list of the questions, see Table 4 in the appendix.</p>
<p>For every probabilistic question, within 48 hours of the question opening, we queried each model three independent times and recorded their predictions at the default settings.We recorded both the quantitative forecast and the qualitative rationale for all entries.If a model was unresponsive because of a technical reason, we attempted to collect a forecast 24 hours after the first failed attempt.If a model failed to provide a forecast for non-technical reasons like model censorship/content restrictions after several attempts, we did not reattempt data collection and recorded the prediction as missing.</p>
<p>For each question, we prompted each model three times and recorded all predictions. 3For cases in which a model failed to provide a forecast for the second or third run after having provided a forecast before, we continued to query the model until all three forecasts were provided.</p>
<p>Our prompt that we used for all models included instructions on how to format the output as well as a number of prompting techniques that include instructing the model to respond as a superforecaster and to approach these questions step-by-step as is current best prompting practice.Each prompt also</p>
<p>Full Prompt</p>
<p>In this chat, you are a superforecaster that has a strong track record of accurate forecasts of the future.As an experienced forecaster, you evaluate past data and trends carefully and aim to predict future events as accurately as you can, even though you cannot know the answer.This means you put probabilities on outcomes that you are uncertain about (ranging from 0 to 100%).You aim to provide as accurate predictions as you can, ensuring that they are consistent with how you predict the future to be.You also outline your reasons for this forecasting.In your reasons, you will carefully consider the reasons for and against your probability estimate, you will make use of comparison classes of similar events and probabilities and take into account base rates and past events as well as other forecasts and predictions.In your reasons, you will also consider different perspectives.Once you have written your reasons, ensure that they directly inform your forecast.</p>
<p>Then, you will provide me with a number between 0 and 100 (up to 2 decimal places) that is your best prediction of the event.Take a deep breath and work on this problem step-by-step.</p>
<p>The question that you are forecasting as well as some background information and resolution details are below.Read them carefully before making your prediction.Background: Resolution: Question:</p>
<p>Figure 1: Full prompt for Study 1</p>
<p>For every set of machine forecasts, we also recorded the publicly available median human crowd prediction at the end of the day that the machine forecast was entered.If the prediction was entered on the first day, we collected the human crowd predictions at the end of the second day that the question was open to allow for higher participation rates.This was done to ensure a fair comparison of machine and human forecasts, as many LLMs can recall the current date, thus making timed forecasts of the nature studied here potentially sensitive to asynchronous queries while also introducing bias with respect to the human crowd.For roughly half the questions, the human forecasters were not able to see the human crowd forecast, though there is significant heterogeneity when the community predictions were made available to human forecasters.In 15 out of 31 questions, our data was collected prior to the revelation of the community prediction to the human forecasters.</p>
<p>For the human forecasts, we took the publicly available median forecast for each question.For the LLM ensemble approach, we computed the median from all non-missing forecasts on each question.We also computed the median forecast on each question for each model to enable cross-model comparisons.See Figure 2 for an overview of our LLM ensemble approach.</p>
<p>Study 2</p>
<p>In Study 2, we focused exclusively on two frontier models, GPT-4 and Claude 2. We used the same real-world forecasting tournament as in Study 1 as our study context, functioning as a source of questions and human forecasts.For Study 2, we employed a within-model research design that collected two forecasts (pre-and post-intervention) per model run for each question, with each question being posed three times at the standard temperature settings, resulting in six forecasts per model for each question.Our goal was to investigate LLM updating behaviour with respect to human cognitive output, i.e., whether and how LLMs take into account the human prediction estimates that forecasting tournament aggregates provide.We queried GPT-4 and Claude 2 via the OpenAI and Anthropic websites respectively.</p>
<p>We used a significantly longer and more elaborate set of prompts than in Study 1.The first prompt built on the '10 commandments of superforecasting' (Tetlock and Gardner 2016) as well as the literature on forecasting and updating, instructing models to carefully consider distinguishing different degrees of doubt, strike the correct balance between under-and overconfidence, and break difficult problems into sub-problems that are easier to solve, among other instructions.The second prompt, the intervention,  informed the model of the respective human crowd's median forecast and asked it to update, if necessary, and to outline the reasons for the update (if any).For a full text of both prompts, see Figure 3 and Figure 4.</p>
<p>For both prompts, we collected forecasts not as point estimates but as probability ranges between 0% and 100% with two decimal point specificity.For further analysis, we treat the midpoint of this range as the point estimate and the provided predictions as upper and lower estimates.The human crowd median that is provided to the models is collected within 48 hours of the community prediction being revealed to allow human forecasters to learn about it and update their forecasts accordingly, generally leading to more well-calibrated predictions.Because of the time difference, the human forecasts are more accurate than those used in Study 1.</p>
<p>Results</p>
<p>Study 1</p>
<p>We collected a total of 1007 individual forecasts over the 31 questions from twelve LLMs that make up the ensemble.For 109 forecasts that we did not collect, this was due to technical problems with the model or interface at the time of forecast collection (in the case of Falcon-180B and PaLM 2),or because other models selectively chose not to answer certain questions, presumably due to their content restriction policies (this was the case for Coral (Command) and Qwen-7B-Chat).We also recorded some missing forecasts for Bard, which was due to the fact that the underlying model powering the interface was changed to Gemini Pro.To ensure consistency and allow comparisons between the different contexts of PaLM 2, we stopped collecting data at this point.</p>
<p>Across all models and questions, we observe a minimum raw forecast value of 0.1% and a maximum raw forecast value of 99.5%, with a median forecast of 60%.This indicates that the LLM models are more likely to make predictions above the 50% mid-point, with the mean forecast value of the crowd M=57.35 (SD=20.93)being significantly above the 50% mark, t(1006)=86.20,p&lt;0.001.Importantly, the total question set resolved close to evenly, with 14/31 questions resolving positively.This imbalance thus suggests that LLM predictions generally favour positive resolutions above and beyond the appropriate empirical expectation, with just over 45% of questions resolving positively.</p>
<p>Initial Prompt</p>
<p>In this chat, you are a superforecaster who has a strong track record of accurate forecasting.You evaluate past data and trends carefully for potential clues to future events, while recognising that the past is an imperfect guide to the future so you will need to put probabilities on possible future outcomes (ranging from 0 to 100%).Your specific goal is to maximize the accuracy of these probability judgments by minimising the Brier scores that your probability judgments receive once future outcomes are known.Brier scores have two key components: calibration (across all questions you answer, the probability estimates you assign to possible future outcomes should correspond as closely as possible to the objective frequency with which outcomes occur) and resolution (across all questions, aim to assign higher probabilities to events that occur than to events that do not occur).You outline your reasons for each forecast: list the strongest evidence and arguments for making lower or higher estimates and explain how you balance the evidence to make your own forecast.You begin this analytic process by looking for reference or comparison classes of similar events and grounding your initial estimates in base rates of occurrence (how often do events of this sort occur in situations that look like the present one?).You then adjust that initial estimate in response to the latest news and distinctive features of the present situation, recognising the need for flexible adjustments but also the risks of over-adjusting and excessive volatility.Superforecasting requires weighing the risks of opposing errors: e.g., of failing to learn from useful historical patterns vs. over-relying on misleading patterns.In this process of error balancing, you draw on the 10 commandments of superforecasting (Tetlock &amp; Gardner, 2015) as well as on other peer-reviewed research on superforecasting:</p>
<ol>
<li>Triage 2. Break seemingly intractable problems into tractable sub-problems 3. Strike the right balance between inside and outside views 4. Strike the right balance between under-and overreacting to evidence 5. Look for the clashing causal forces at work in each problem 6. Strive to distinguish as many degrees of doubt as the problem permits but no more 7. Strike the right balance between under-and overconfidence, between prudence and decisiveness 8. Look for the errors behind your mistakes but beware of rearview-mirror hindsight biases 9. Bring out the best in others and let others bring out the best in you 10. Master the error-balancing bicycle Once you have written your reasons, ensure that they directly inform you forecast.Then, you will provide me with your forecast that is a range between two numbers, each between between 0 and 100 (up to 2 decimal places) that is your best range of prediction of the event.Output your prediction as "My Prediction: Between XX.XX% and YY.YY%".Take a deep breath and work on this problem step-by-step.The question that you are forecasting as well as some background information and resolution criteria are below.Read them carefully before making your prediction.Background: Resolution Criteria: Question: Such a bias towards more positive predictions may be a function of the machine-equivalent of acquiescence bias (Costello and Roodenburg 2015), where human responders tend to favour the positive/agreement option irrespective of question content (Hinz et al. 2007).See Figure 5 for a scatter plot of all model forecasts across all questions that shows heterogeneity between models of forecast distribution, ranges, and acquiescence bias.</li>
</ol>
<p>Prediction Intervention</p>
<p>You have made your forecast based on careful reasoning and analysis.Now consider the following new piece of information: The median crowd prediction in the forecasting tournament where this question was posed was XXX%.Please adjust your reasoning and forecast based on this information, as you deem appropriate.The large research literature on the "wisdom of the crowd" suggests it is difficult for any single forecaster to out-predict crowd medians or averages.But there are occasions when the crowd has proven to be wrong.In considering whether/how much to revise your earlier forecast, keep in mind the theme of error-balancing: the need to balance the risk of giving too little weight to the crowd judgment vs. the risk of over-relying on the crowd.Please explain how you balanced these risks.Please also make this prediction be in the same format as before: "My Prediction: Between XX.XX% and YY.YY%".In order to assess forecasting accuracy, we use the strictly proper scoring rule (Gneiting and Raftery 2007) of Brier scores (Brier 1950).It is a metric for assessing the accuracy of probabilistic predictions by taking the mean squared difference between the forecasted probability and the actual outcome.It is defined mathematically as Brier Score = (
f i − o i ) 2
where f i is the forecasted probability for the instance, and o i is the actual outcome, which can be 0 or 1.A lower Brier score indicates higher accuracy, with 0 being the perfect accuracy score.A score of 0.250 represents a typical benchmark that would be arrived at if all predictions were 50%.</p>
<p>Testing our first hypothesis as preregistered, we investigate whether the LLM crowd can outperform the simple baseline of assigning a 50% prediction on every question, a baseline that GPT-4 was unable to beat in previous work (Schoenegger and Park 2023).To arrive at our LLM median forecast for this and further analysis using this aggregate, we calculate the median LLM forecast across all models for every question.We then take these medians and average them across all questions.We then take this average and compare it a Brier score of 0.25 (the result of predicting 50% on all questions).We are able to reject our null hypothesis, with the LLM crowd, M=0.20 (SD=0.12),being significantly more accurate than the benchmark, t(30) = -2.35,p = 0.026.This is first evidence that crowd-aggregated LLM forecasts can improve upon basic benchmarks.</p>
<p>Next, we compare the LLM crowd performance to that of the human crowd for our second hypothesis, directly putting the two crowd-aggregation mechanisms head-to-head.To do this, we use the same LLM crowd average as before (taking the median LLM prediction on each question and averaging up the Brier scores across questions).We compare this to the average of median human predictions on the same questions.In our preregistered analysis, we fail to find statistically significant differences between the LLM crowd's mean Brier score of M=0.20 (SD=0.12)and that of the human crowd, M=0.19 (SD=0.19),t(60) = 0.19, p = 0.850.This result only enables us to directly conclude that the LLM crowd is neither more nor less accurate than the human crowd in the question set studied here.To provide some evidence in favour of the equivalence of these two approaches, we conduct a non-preregistered equivalence test with the conventional medium effect size of Cohen's d=0.5 as equivalence bounds (Cohen 2013), which allows us to test whether the effect is zero or less than a 0.081 change in Brier scores.For these equivalence bounds, we find that the LLM crowd and the human crowd are equally accurate, with both tests for the lower bound, t(60)=2.16,p=0.017 and the upper bound, t(60)=-1.78,p=0.040, being statistically significant.This provides evidence that the LLM crowd is as accurate as the human crowd within these bounds, though note that the bounds are quite wide.For our third null hypothesis, we compare the forecasting accuracy of each model (and the human crowd) against each other to find potential effects of internet access (GPT-4 vs. GPT-4 with Bing) or access points (Bard with PaLM2 vs. PaLM2).Using an analysis of variance, we find significant aggregate differences, F(12, 354)=2.64,p=0.002, leading us to reject our third null hypothesis.Using Tukey HSD to adjust for multiple comparisons in the post-hoc pair-wise tests, we find that Coral (Command) underperforms a set of models (e.g., Claude 2, GPT-4) as well as the human crowd.However, we fail to find statistically significant effects between any other pairs not involving Coral (Command), thus being unable to provide evidence in favour or against potential effects of internet access, access points, or fine-tuning on prediction accuracy.See Table 2 for average Brier scores for each model.For all three hypotheses, we implemented the Benjamini-Hochberg (BH) procedure to adjust the p-values obtained from multiple hypothesis tests.This method was selected to control the False Discovery Rate (FDR) and thereby reduce the risk of Type I errors.The original p-values for null hypotheses 1, 2, and 3 were 0.026, 0.850, and 0.002, respectively.These p-values were first sorted in ascending order and then ranked accordingly.The adjusted p-values were computed using the Benjamini-Hochberg procedure, which calculates the adjusted p-value for the i-th hypothesis as min 1, pi×m i , where p i is the i-th p-value in the sorted list, m is the total number of hypotheses tested, and i is the rank of the p-value.The results show that the adjusted p-values for the hypotheses were 0.039 for the first hypothesis (original p=0.026), 0.850 for the second hypothesis (original p=0.850), and 0.006 for the third hypothesis (original p=0.002).These results indicate that our rejections of the first and third null hypothesis remain robust after adjusting for multiple comparisons.</p>
<p>In non-preregistered analyses, we conduct calibration analyses using the Murphy Decomposition (Mandel and Barnes 2014;Siegert 2017) to provide data on how well calibrated the LLM models are in this context, i.e., how reliably their probability estimates match the fraction of real outcomes.In Figure 8, calibration curves for each model and their aggregate are plotted against the ideal 45-degree dotted line.This dotted line represents perfect calibration, where predicted probabilities match observed frequencies.Deviations from this line indicate calibration errors: curves above the line suggest underconfidence (predicting events as less likely than they actually are), while those below indicate overconfidence (predicting events as more likely than they actually are).Figure 8 visually represents how closely the models' predictions align with actual outcomes.We also calculate the Calibration Index (CI), which quantifies this deviation, with lower values indicating better calibration.CI is calculated using the formula:
CI = 1 N K k=1 N k (f k − o k ) 2
where N is the total number of forecasts, K the number of bins, N k the number of forecasts in bin k, f k the mean forecast probability in bin k, and o k the observed relative frequency in bin k.This weights each bin's contribution to the Calibration Index (CI) by the number of forecasts it contains.This approach ensures that bins with more forecasts, which provide a more statistically reliable estimate of forecasting accuracy, have a proportionately greater impact on the overall CI.Our results demonstrate poor calibration of most models and overconfidence of the aggregate, suggesting that models overpredict outcomes compared to their actual rate of occurrence, see Figure 8.This is in line with the finding that we find a acquiescence bias of LLMs on a question set where less than half of questions resolve positively.We also find generally poor calibration across all models.However, there are substantial differences in the CI scores, with some models having substantially better calibration than others, see Table 3.This suggests that a further line of research may build upon improving calibration of models in an attempt to improve machine prediction capabilities and reliability further.For Study 2, we collected a total of 186 primary forecasts and 186 updated forecasts from both frontier models (GPT-4 and Claude 2) over the 31 binary questions studied.Neither model refused to provide a forecast or failed to respond to our querying.</p>
<p>First, we test whether exposure to the human crowd median improves model accuracy.We are able to reject the first null hypothesis of Study 2 for both models: For GPT-4, there is a statistically significant difference in Brier Scores before and after exposure to the human median, with an average Brier score for the primary forecast of 0.17 (SD: 0.13) and an updated score of 0.14 (SD: 0.11), p = 0.003.For Claude 2, we also find a statistically significant difference in Brier Scores before and after exposure to the human median, improving from 0.22 (SD: 0.19) to 0.15 (SD: 0.14), p &lt; 0.001.This suggests that the provision of human cognition in the form of crowd forecasts can improve model prediction capabilities.</p>
<p>We also find that, testing our second hypothesis, the size of the prediction interval narrows after exposure to human crowd predictions that lie within the probability range provided by the model, as would be predicted by theory: The prediction intervals for GPT-4 become significantly narrower after exposure to the human median, ranging from an average interval size of 17.75 (SD: 5.66)to 14.22 (SD: 5.97), p &lt; 0.001.The prediction intervals for Claude 2 also become significantly narrower after exposure to the human median forecast, narrowing from 11.67 (SD: 4.201 to 8.28 (SD: 3.63), p &lt;0.001.This suggests that the models appropriately reduce their prediction uncertainty when the human forecast is already included in the LLM's, incorporating this additional information and adjusting their uncertainty.See Figure 9 for a graphical illustration of LLM forecasts for either model before and after exposure to the human forecasts.</p>
<p>Lastly, with respect to our third hypothesis, we analyse whether LLMs' updates are proportional to the distance between their point forecast and that of the human benchmark.We are able to reject our null hypothesis for both models, finding significant correlation between the initial deviation and the magnitude of forecast adjustment for GPT-4, r=0.88, p &lt; 0.001 as well as for Claude 2 r=0.87, p &lt; 0.001.This suggests that models move their predictions roughly in accordance with how large the difference between their prediction and the human median is.</p>
<p>As in Study 1, we use the Benjamini-Hochberg procedure for controlling multiple comparisons, given our three hypotheses each tested for each model, resulting in six tests.The original p-values were [0.001, 0.001, 0.001, 0.001, 0.001, 0.003].After applying the Benjamini-Hochberg adjustment, the p-values were [0.006, 0.006, 0.006, 0.006, 0.006, 0.003], all of which were below the 0.05 FDR threshold.This indicates that, post-adjustment, the results from all tests remained statistically significant.</p>
<p>We also conduct the following exploratory analysis.Instead of comparing the LLM forecast after having been exposed to the human median to the LLM forecast before this exposure as preregistered, we compare this updated prediction to a simple average of the machine and human predictions as a naive benchmark using straightforward aggregation.This allows us to test whether the improvements the models make are due to understanding of the need to appropriately update or simply as an agreement-focused response.We find in paired t-tests that for both GPT-4 at a Brier score of 0.13, t(92) = 2.583, p = .011,and Claude 2 at a Brier score of 0.14, t(92) = 3.530, p = .001,their updated forecasts are significantly less accurate than a simple average between the machine and the human median forecasts.This suggests that the updating itself is directionally correct but fails to improve upon a simple benchmark.</p>
<p>Discussion</p>
<p>Our results show that LLM prediction capabilities can rival the gold standard of the human crowd tournament method, if they themselves draw on what we call the 'wisdom of the silicon crowd.'Previous results on single models (Halawi et al. 2024;Schoenegger and Park 2023) showed that LLMs not only underperformed compared to a human crowd in a probabilistic forecasting context, but also failed to clear simple benchmarks; while others (Abolghasemi, Ganbold, and Rotaru 2023) failed to find evidence in favour of the LLMs outperforming humans in the context of time-series forecasting.4However, taking into account more sophisticated systems built on top of LLMs, such as combined retrieval and reasoning systems (Halawi et al. 2024), human-level prediction accuracy may already be considered matched in some aspects.We propose that the capabilities jump in moving from single frontier models to crowds of simple models in the same probabilistic forecasting context is a benefit that can be exploited in a variety of real-world contexts, as this aggregation approach remains simple to implement and does not require additions like that of news retrieval on each question.Our finding opens the door for simple, practically applicable steps like forecast aggregation to increase current AI models' forecasting ability-to predict future events in politics, economics, technology, and other real-world subjects-to a level on par with the human crowd.This opens up a lot of directly applied work, given that LLM prediction capabilities can inform decision-makers and businesses in circumstances where accurate probabilistic forecasts are difficult or expensive to acquire.Furthermore, since both our finding and the finding of Halawi et al. (2024) suggest that placing individual LLMs in advanced systems can increase their forecasting ability to a market-competitive level, it is natural to expect LLM predictions to be more widely applied across society in the near future.</p>
<p>Importantly, our finding holds despite the presence of an acquiescence bias (Costello and Roodenburg 2015; Hinz et al. 2007) in model predictions, in that our models' predictions are more likely to be above 50%, despite the resolution rate of all questions being almost even.This suggests that the 'wisdom of crowds' effect using median as our aggregation is able to counteract even this acquiescence bias that is present in the majority of individual models, a robustness feature of the 'wisdom of crowds' mechanism (Koriat 2012).In our aggregation results, we also find that only three of the twelve models outperform the model median, which is also in line with standard accounts of wisdom of crowds.This overall suggests that the 'wisdom of the crowd' effect-in addition to applying to the human context-also applies to the silicon context.The literature on the size of the crowd needed to produce reliable 'wisdom of the crowd' effects is not very well established, though a central finding is that increasing crowd size does lead to better performance (Walter, Kölle, and Collmar 2022).As such, a natural next step in this line of research is to expand the set of models queried from the twelve we used to a substantially higher number.</p>
<p>However, there also do remain substantial areas of improvement for machine predictions in probabilistic forecasting.Most directly, both the aggregate and the individual models were badly calibrated, with most models showing overconfidence, i.e., they assign higher probabilities to outcomes than is warranted by the empirical facts.Improving calibration is central to providing reliable predictions over the long run (Buizza 2018), and our results of acquiescence bias suggest that this may be an actionable area for future work.Additionally, as the distance between the end of the training data and the forecasted period grows, forecasts may become less accurate as necessary background knowledge is no longer readily available to the model.Moreover, our study could draw on a well-curated set of questions from the forecasting tournament.Applications in contexts where neutral background information and question details are not available further reduce performance.</p>
<p>Our results from Study 2 contribute to the literature on human-AI interactions (Kim et al. 2024;Yang et al. 2024).While previous work in the context of forecasting has looked at how LLMs can augment humans in improving prediction accuracy (Schoenegger et al. 2024), this paper provides evidence for the reverse.Specifically, our results show that machine predictions can be improved substantially by the provision of human cognition output drawn from forecasting tournaments.This finding suggests at first glance that LLM reasoning is already advanced enough to properly exploit the informational value provided by human cognition output.However, our exploratory analyses find that this process is substantially less effective than simply averaging the two estimates, suggesting that aggregation methods based on the reasoning capabilities of frontier models (in this case, GPT-4 and Claude 2) still underperform simple aggregations.</p>
<p>On the other hand, our findings that both frontier models (GPT-4 and Claude 2) respond as expected in their forecast updates-reducing their uncertainty when the human estimate lies within their prediction intervals, and updating in relation to the distance between their own point estimate and the human forecasts-match past theory and results pertaining to human forecasters (Atanasov et al. 2020).This overall suggests that the ability of these models to reason and act as expected-by past theory and results pertaining to human forecasters-depend on the type of task and benchmark applied.While this is not a massively strict test of their reasoning abilities-as alternative explanations of model behaviour being explained by simple expectation fulfilling remain-it does provide some evidence in favour of it.</p>
<p>Importantly, both studies reported in this paper test LLM capabilities in a context where it is not possible that any of the answers used to resolve the questions were part of the training data, as we queried the models in real-time alongside the human tournament.Because all question answers were unknown at the time of data collection-even to the study authors-this provides an ideal evaluation criterion for LLM capabilities: one at which our LLM ensemble approach excelled.Our findings provide evidence of LLMs' advanced reasoning capabilities, and does so in a robust way such that many of the challenges that may be raised with respect to traditional benchmarks do not apply.</p>
<p>In conclusion, the present paper is among the first to show that current LLMs are able to provide a human-crowd-competitive level of accurate forecasting about future real-world events.In order to do so, it is sufficient to apply the simple, practically applicable method of forecast aggregation: manifesting as the LLM ensemble approach in the so-called silicon setting.This replicates the human forecasting tournament's 'wisdom of the crowd' effect for LLMs: a phenomenon we call the 'wisdom of the silicon crowd.'Our finding opens up a number of areas for further research as well as practical applications, since the LLM ensemble approach is substantially cheaper and faster than data collection from human forecasters.Future research can aim to combine the ensemble approach with model and scaffolding progress, which may potentially result in even stronger capability gains in the domain of forecasting.</p>
<p>Figure 2 :
2
Figure 2: LLM Ensemble Mechanism Overview</p>
<p>Figure 3 :
3
Figure 3: Initial prompt for Study 2</p>
<p>Figure 4 :Figure 5 :
45
Figure 4: Prediction intervention prompt for Study 2</p>
<p>Figure 6 :
6
Figure6: KDE of the LLM and human crowd forecasts (averaged median scores over all questions).Vertical dotted black line represents the 50% baseline.</p>
<p>Figure 7 :
7
Figure 7: Raincloud plots of Brier scores for each LLM model as well as the human crowd.</p>
<p>Figure 8 :
8
Figure 8: Calibration plot for all LLM models as well as the aggregate (bolded)</p>
<p>Figure 9 :
9
Figure9: LLM forecasts for and Claude 2 (right) before and after exposure to the human forecast.Colours distinguish first forecasts above, below, or within 20 percentage points of the human median forecast.Highlighted changes and intervals are of the respective median forecast within that group.</p>
<p>Table 1 :
1
Model Details
ModelCompanyInternetOpen SourceHosting PlatformCountry ofAccessCompanyGPT-4OpenAINoNoOpenAIUnited StatesGPT-4 BingOpenAIYesNoOpenAIUnited StatesClaude 2AnthropicNoNoAnthropicUnited StatesGPT-3.5-Turbo-OpenAINoNoOpenAIUnited StatesInstructSolar-0-70BUpstageNoYesPoeSouth KoreaLlama-2-70BMetaNoYesPoeUnited StatesPaLM 2GoogleNoNoPoeUnited States(Chat-Bison@002)Coral (Command)CohereYesNoCohereCanadaMistral-7B-InstructMistralNoYesPoeFranceBard (PaLM 2)GoogleYesNoGoogleUnited StatesFalcon 180BTechnologyNoNoHuggingfaceUnited ArabInnovationEmiratesInstituteQwen-7B-ChatAlibaba CloudNoYesModelscopeChina</p>
<p>Table 2 :
2
Average Brier Score for Each Model
ModelAccuracy SDGPT-40.150.11GPT-4 (with Bing)0.160.11Bard (PaLM 2)0.190.17Falcon-180B0.210.13Claude 20.210.16Solar-0-70B0.220.16PaLM 2 (Chat-Bison@002)0.230.15Mistral-7B-Instruct0.240.16Qwen-7B-Chat0.240.17GPT3.5-Turbo-Instruct0.250.20Llama-2-70B0.250.16Coral (Command)0.380.40Human0.190.19</p>
<p>Table 3 :
3
Calibration index values for all LLM models.
ModelCalibration IndexFalcon-180B0.027Qwen-7B-Chat0.055PaLM 2 (Chat-Bison@002)0.068Bard (PaLM 2)0.071Llama-2-70B0.071GPT-40.075Mistral-7B-Instruct0.080Solar-0-70B0.081Claude 20.082GPT-4 (with Bing)0.088GPT3.5-Turbo-Instruct0.106Coral (Command)0.212Aggregate0.0413.2 Study 2
https://osf.io/sb6mw/?view_only=395ab8faccba419c91f5f12dcaf97ce6
We monitored updates to the original models at the web interfaces and responded as follows to changes: In response to the release of GPT-4-Turbo, from Nov 6, we queried the 'Classic' model instead. For the upgrade to Claude 2.1, we did not switch the query method from Nov 21 . When Bard switched, at least in part, to Gemini Pro from PaLM 2, we ceased data collection of this model via the Bard interface from Dec 6.
If a model only responded with 'Yes' or 'No' as their prediction, we coded this as 99% and 1% respectively, though we note that this happened in less than 1% of cases across models.
For more applications of LLMs in time-series forecasting see additional work(Cholakov and Kolev 2021;Gruver et al. 2024;Jin et al. 2023) 
AcknowledgementsWe are grateful to Lawrence Phillips and Peter Mühelbacher for helping us discover and correct a coding error in the non-preregistered equivalence test pertaining to the second null hypothesis of Study 1.QuestionsWill a nearly continuous human chain stretch across the length of the Forth and Clyde Canal on 14 October 2023?Will Hamas lose control of Gaza before 2024?Will Yahya Sinwar cease to act as Hamas Chief in the Gaza Strip before 2024?Will Israel carry out and explicitly acknowledge a deadly attack on Iran before 2024?Will the Conservatives hold on to their seat in the Mid Bedfordshire by-election?Will it be determined that Israel was responsible for the attack on the Al-Ahli Baptist Hospital in Gaza City before 2024?Will the Federal Funds Rate be raised before December 14, 2023?Will Peter Bone MP be suspended from Parliament in 2023?Will George Weah win re-election in the 2023 Liberian General Election?Will India request that another Canadian diplomat be recalled before 2024?Will New Delhi experience a "Very Unhealthy" or worse air quality index on at least four of the seven days for the week starting October 29?Will the MONUSCO UN peacekeeping mission to the Democratic Republic of the Congo be extended with a military personnel ceiling above 11,000 before January 1, 2024?Will OpenAI report having ≥99% uptime for ChatGPT and the OpenAI API in December 2023?Will the November 2023 Israel-Hamas humanitarian pause be extended?Will a majority of voters approve Venezuela's referendum on incorporating Guayana Esequiba into Venezuela?Will any additional Republican candidates for president drop out before 2024?Will there be a white Christmas in at least 4 of these 9 large European cities in 2023?Will the US Supreme Court issue a decision on hearing the case about presidential immunity before January 1, 2024?Before 2024, will it be announced that either of the Harvard or MIT presidents will vacate their positions?Will a major shipping company announce that they are resuming shipments through the Red Sea before 2024?Will the ban on imports of Apple watches with blood oxygen sensors take effect before December 27, 2023?Will there be a US military combat death in the Red Sea before 2024?Will NASA re-establish communications with Voyager 1 before 1 Jan 2024?
Perils and Opportunities in Using Large Language Models in Psychological Research. Suhaib Abdurahman, PsyArXiv. 2023</p>
<p>Humans vs Large Language Models: Judgmental Forecasting in an Era of Advanced AI. Mahdi Abolghasemi, Odkhishig Ganbold, Kristian Rotaru, arXiv:2312.069412023arXiv preprint</p>
<p>Harms of AI. Daron Acemoglu, 10.1093/oxfordhb/9780197579329.013.65The Oxford Handbook of AI Governance. Oxford University Press2023</p>
<p>Norah Alzahrani, arXiv:2402.01781[cs.CL]When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards. 2024</p>
<p>Anthropic, Model Card and Evaluations for Claude Models. 2023</p>
<p>A Theory for Emergence of Complex Skills in Language Models. Sanjeev Arora, Anirudh Goyal, arXiv:2307.159362023arXiv preprint</p>
<p>Small steps to accuracy: Incremental belief updaters are better forecasters. Pavel Atanasov, Proceedings of the 21st ACM Conference on Economics and Computation. the 21st ACM Conference on Economics and Computation2020</p>
<p>Which humans. Mohammad Atari, PsyArXiv. 2023</p>
<p>Two reasons to make aggregated probability forecasts more extreme. Jonathan Baron, Decision Analysis. 1122014</p>
<p>Wisdom of crowds: Forecasting using prediction markets. Achal Bassamboo, Ruomeng Cui, Antonio Moreno, 2018Kellogg School of Management, Northwestern UniversityTech. rep</p>
<p>Evaluating capabilities of large language models: Performance of GPT-4 on surgical knowledge assessments. Beaulieu-Jones, R Brendin, Surgery. 2024</p>
<p>On the Dangers of Stochastic Parrots: Can Language Models be too Big?. Emily M Bender, 10.1145/3442188.3445922Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. FAccT '21. Virtual Event. the 2021 ACM Conference on Fairness, Accountability, and Transparency. FAccT '21. Virtual EventCanadaAssociation for Computing Machinery2021</p>
<p>Stella Biderman, arXiv:2304.11158[cs.CL]Emergent and Predictable Memorization in Large Language Models. 2023</p>
<p>Verification of forecasts expressed in terms of probability. Glenn W Brier, Monthly weather review. 781950</p>
<p>Sébastien Bubeck, arXiv:2303.12712[cs.CL]Sparks of Artificial General Intelligence: Early Experiments with GPT-4. 2023</p>
<p>Identifying expertise to extract the wisdom of crowds. Chen Budescu, Management science. 6122015</p>
<p>Confidence in aggregation of opinions from multiple sources. David V Budescu, Information Sampling and Adaptive Cognition. Klaus Fiedler, Peter Juslin, Cambridge, UKCambridge University Press2006</p>
<p>Ensemble forecasting and the need for calibration. Roberto Buizza, Statistical postprocessing of ensemble forecasts. Elsevier2018</p>
<p>Quantifying Memorization Across Neural Language Models. Nicholas Carlini, The Eleventh International Conference on Learning Representations, ICLR 2023. Kigali, Rwanda2023. May 1-5, 2023</p>
<p>Applying attention in next-frame and time series forecasting. Radostin Cholakov, Todor Kolev, arXivpreprintarXiv:2108.082242021Transformers predicting the future</p>
<p>Acquiescence response bias-Yeasaying and higher education. Jacob Cohen, The Educational and Developmental Psychologist. Costello, Shane and John Roodenburg3222013. 2015Academic pressStatistical power analysis for the behavioral sciences</p>
<p>Harnessing the wisdom of crowds. Zhi Da, Xing Huang, Management Science. 662020</p>
<p>More Than Meets the AI: Evaluating the performance of GPT-4 on Computer Graphics assessment questions. Davis-Stober, P Clintin, Proceedings of the 26th Australasian Computing Education Conference. the 26th Australasian Computing Education Conference2014. 2024When is a crowd wise?</p>
<p>Mohammad Fraiwan, Natheer Khasawneh, arXiv:2305.00237[cs.CY]A Review of ChatGPT Applications in Education, Marketing, Software Engineering, and Healthcare: Benefits, Drawbacks, and Research Directions. 2023</p>
<p>Gemini Team, arXiv:2312.11805[cs.CL]Gemini: A Family of Highly Capable Multimodal Models. 2023</p>
<p>Revisiting Savage in a conditional world. Paolo Ghirardato, Economic Theory. 202002</p>
<p>Strictly proper scoring rules, prediction, and estimation. Tilmann Gneiting, Adrian E Raftery, Journal of the American statistical Association. 1022007</p>
<p>A Continuum of Learning: From Rote Memorization to Meaningful Learning in Organic Chemistry. Nathaniel P Grove, Stacey Lowery, Bretz , Chemistry Education Research and Practice. 1332012</p>
<p>Large language models are zero-shot time series forecasters. Nate Gruver, Advances in Neural Information Processing Systems. 202436</p>
<p>Danny Halawi, arXiv:2402.18563[cs.LG]Approaching Human-Level Forecasting with Language Models. 2024</p>
<p>Devising and detecting phishing: Large language models vs. smaller human models. Fredrik Heiding, arXiv:2308.122872023arXiv preprint</p>
<p>The wisdom of timely crowds. Himmelstein, David V Michael, Yoonjin Budescu, Han, 2023Springer International PublishingJudgment in predictive analytics</p>
<p>The wisdom of many in few: Finding individuals who are as wise as the crowd. Himmelstein, David V Michael, Elizabeth H Budescu, Ho, Journal of Experimental Psychology: General. 2023</p>
<p>The acquiescence effect in responding to a questionnaire. Andreas Hinz, GMS Psycho-Social Medicine. 42007</p>
<p>Wenxiang Jiao, arXiv:2301.08745[cs.CL]Is ChatGPT a Good Translator? Yes with GPT-4 as the Engine. 2023</p>
<p>Time-llm: Time series forecasting by reprogramming large language models. Ming Jin, arXiv:2310.017282023arXiv preprint</p>
<p>GPT-4 Passes the Bar Exam. Daniel Katz, Martin, 2023SSRN</p>
<p>Human-AI Collaboration in Large Language Model-Assisted Brain MRI Differential Diagnosis: A Usability Study. Su Kim, Hwan, medRxiv. 2024</p>
<p>How to build a benchmark. Jóakim V Kistowski, Proceedings of the 6th ACM/SPEC international conference on performance engineering. the 6th ACM/SPEC international conference on performance engineering2015</p>
<p>When are two heads better than one and why. Asher Koriat, Science. 3362012</p>
<p>Md Laskar, Tahmid Rahman, arXiv:2305.18486[cs.CL]A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. 2023</p>
<p>The wisdom of competitive crowds. Lichtendahl Jr, C Kenneth, Phillip E Yael Grushka-Cockayne, Pfeifer, Operations Research. 6162013</p>
<p>Data Contamination: From Memorization to Exploitation. Inbal Magar, Roy Schwartz, 10.18653/v1/2022.acl-short.18Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational LinguisticsMay 20222Short Papers)</p>
<p>Accuracy of forecasts in strategic intelligence. David R Mandel, Alan Barnes, 111.30Proceedings of the National Academy of Sciences. 2014</p>
<p>Metaculus, Metaculus. 2024</p>
<p>Humza Naveed, arXiv:2307.06435[cs.CL]A Comprehensive Overview of Large Language Models. 2023</p>
<p>Harsha Nori, arXiv:2303.13375[cs.CL]Capabilities of GPT-4 on Medical Challenge Problems. 2023</p>
<p>. Openai, arXiv:2303.08774[cs.CL]2023GPT-4 Technical Report</p>
<p>The evolution of cognitive biases in human learning. Peter S Park, Journal of Theoretical Biology. 5411110312022</p>
<p>Diminished diversity-of-thought in a standard large language model. Peter S Park, Philipp Schoenegger, Chongyang Zhu, Behavior Research Methods. 2024</p>
<p>Peter S Park, Max Tegmark, arXiv:2310.06009[cs.CY]Divide-and-Conquer Dynamics in AI-Driven Disempowerment. 2023</p>
<p>Peter S Park, arXiv:2308.14752[cs.CY]AI Deception: A Survey of Examples, Risks, and Potential Solutions. 2023</p>
<p>Reproducing Whisper-Style Training Using An Open-Source Toolkit And Publicly Available Data. Yifan Peng, 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU. IEEE2023</p>
<p>ChatGPT applications in medical, dental, pharmacy, and public health education: A descriptive study highlighting the advantages and limitations. Malik Sallam, Narra J 3. 12023</p>
<p>The Foundations of Statistics. Leonard J Savage, 1972Dover PublicationsNew York</p>
<p>Large Language Model Prediction Capabilities: Evidence from a Real-World Forecasting Tournament. Philipp Schoenegger, Peter S Park, arXivpreprintarXiv:2310.130142023</p>
<p>Philipp Schoenegger, 10.48550/arXiv.2402.07862arXiv:2402.07862AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy. 2024arXiv preprint</p>
<p>Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization. Chenhui Shen, arXiv:2305.13091[cs.CL]2023a</p>
<p>SlimPajama-DC: Understanding Data Combinations for LLM Training. Zhiqiang Shen, arXiv:2309.108182023barXiv preprint</p>
<p>Simplifying and generalising Murphy's Brier score decomposition. Stefan Siegert, Quarterly Journal of the Royal Meteorological Society. 1432017</p>
<p>Larry Summers on who could be replaced by AI. Lawrence H Summers, Steve Rattner, 2023Interviewed by Bloomberg TV's David Westin</p>
<p>The Wisdom of Crowds: Why the Many Are Smarter Than the Few and How Collective Wisdom Shapes Business, Economies, Societies and Nations. James Surowiecki, 2004LittleLondon; Brown</p>
<p>AI succession [Youtube video of talk. Rich Sutton, 2023</p>
<p>World Artificial Intelligence Conference in Shanghai. </p>
<p>Superforecasting: The Art and Science of Prediction. Philip E Tetlock, Dan Gardner, 2016Random House</p>
<p>Forecasting Tournaments: Tools for Increasing Transparency and Improving the Quality of Debate. Philip E Tetlock, arXiv:2307.09288[cs.CL]Llama 2: Open Foundation and Fine-Tuned Chat Models. HugoTouvron et al2014. 202323</p>
<p>Attention is All You Need. Ashish Vaswani, Advances in Neural Information Processing Systems. 201730</p>
<p>Chatgpt for robotics: Design principles and model abilities. Sai Vemprala, Microsoft Auton. Syst. Robot. Res. 2202023</p>
<p>Measuring the Wisdom of the Crowd: How Many is Enough?. Walter, Michael Volker, David Kölle, Collmar, PFG-Journal of Photogrammetry, Remote Sensing and Geoinformation Science. 9032022</p>
<p>Emergent abilities of large language models. Jason Wei, arXiv:2206.076822022arXiv preprint</p>
<p>Can ChatGPT Pass High School Exams on English Language Comprehension?. Joost C F Winter, De, International Journal of Artificial Intelligence in Education. 1560- 42922023</p>
<p>Human-AI Interactions in the Communication Era: Autophagy Makes Large Models Achieving Local Optima. Shu Yang, arXivpreprintarXiv:2402.112712024</p>            </div>
        </div>

    </div>
</body>
</html>
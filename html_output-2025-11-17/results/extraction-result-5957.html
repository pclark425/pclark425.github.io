<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5957 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5957</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5957</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-120.html">extraction-schema-120</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-267627257</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.06894v1.pdf" target="_blank">GenTranslate: Large Language Models are Generative Multilingual Speech and Machine Translators</a></p>
                <p><strong>Paper Abstract:</strong> Recent advances in large language models (LLMs) have stepped forward the development of multilingual speech and machine translation by its reduced representation errors and incorporated external knowledge. However, both translation tasks typically utilize beam search decoding and top-1 hypothesis selection for inference. These techniques struggle to fully exploit the rich information in the diverse N -best hypotheses, making them less optimal for translation tasks that require a single, high-quality output sequence. In this paper, we propose a new generative paradigm for translation tasks, namely “GenTranslate”, which builds upon LLMs to generate better results from the diverse translation versions in N -best list. Leveraging the rich linguistic knowledge and strong reasoning abilities of LLMs, our new paradigm can integrate the rich information in N -best candidates to generate a higher-quality translation result. Furthermore, to support LLM finetuning, we build and release a HypoTranslate dataset that contains over 592K hypotheses-translation pairs in 11 languages. Experiments on various speech and machine translation benchmarks ( e.g. , FLEURS, CoVoST-2, WMT) demonstrate that our Gen-Translate significantly outperforms the state-of-the-art model 1 .</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5957",
    "paper_id": "paper-267627257",
    "extraction_schema_id": "extraction-schema-120",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.004859499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>GenTranslate: Large Language Models are Generative Multilingual Speech and Machine Translators</p>
<p>Yuchen Hu 
Nanyang Technological University</p>
<p>Chen Chen 
Chao-Han Huck 
Nanyang Technological University</p>
<p>Georgia Institute of Technology</p>
<p>Ruizhe Li 
University of Aberdeen</p>
<p>Dong Zhang 
Fudan University</p>
<p>Zhehuai Chen 
Eng Siong Chng 
Nanyang Technological University</p>
<p>GenTranslate: Large Language Models are Generative Multilingual Speech and Machine Translators
96D2364A9E4F6F02E7FF568C0D310218
Recent advances in large language models (LLMs) have stepped forward the development of multilingual speech and machine translation by its reduced representation errors and incorporated external knowledge.However, both translation tasks typically utilize beam search decoding and top-1 hypothesis selection for inference.These techniques struggle to fully exploit the rich information in the diverse N -best hypotheses, making them less optimal for translation tasks that require a single, high-quality output sequence.In this paper, we propose a new generative paradigm for translation tasks, namely "GenTranslate", which builds upon LLMs to generate better results from the diverse translation versions in N -best list.Leveraging the rich linguistic knowledge and strong reasoning abilities of LLMs, our new paradigm can integrate the rich information in N -best candidates to generate a higher-quality translation result.Furthermore, to support LLM finetuning, we build and release a HypoTranslate dataset that contains over 592K hypotheses-translation pairs in 11 languages.Experiments on various speech and machine translation benchmarks (e.g., FLEURS, CoVoST-2, WMT) demonstrate that our Gen-Translate significantly outperforms the state-ofthe-art model 1 .</p>
<p>Introduction</p>
<p>Recent advances in large language models (LLMs) have attracted a surge of research interest due to their strong abilities in logical reasoning and language generation (OpenAI, 2022(OpenAI, , 2023;;Touvron et al., 2023a,b).These models have achieved surprisingly wide-ranging success across various natural language processing (NLP) tasks (Brown et al., 2020;Wang et al., 2022;Wei et al., 2022a,b;Ouyang et al., 2022).In the realm of NLP, the translation tasks, which encompasses speech and machine translation (ST &amp; MT), hold significant practical importance for global communication.Similar to other NLP tasks, translation tasks also gain a notable progress thanks to the recent advancement of LLMs (Zhang et al., 2023a;Lyu et al., 2023).In the domain of speech translation, Whisper (Radford et al., 2023) demonstrates superior performance by collecting 680K-hour data for web-scale model training.Au-dioPaLM2 (Rubenstein et al., 2023) integrates both text-and speech-based language models into a unified architecture to process and generate text and speech, thereby augmenting speech translation performance to a great extent.On the other hand, LLMs also show remarkable ability in machine translation.NLLB (Costa-jussà et al., 2022) is the first to extend LLMs' linguistic capability to over 200 languages.BigTranslate (Yang et al., 2023b) is finetuned on LLaMA (Touvron et al., 2023a) with multilingual instruction tuning, which achieves comparable performance to ChatGPT (OpenAI, 2022) and Google Translate.Most recent work arXiv:2402.06894v2[cs.CL] 16 May 2024</p>
<p>proposes SeamlessM4T (Barrault et al., 2023a), a foundational multilingual and multitask model that can translate across speech and text, which achieves the state-of-the-art on both ST and MT tasks on various public datasets.</p>
<p>Despite the superior performance, most existing translation models employ the typical beam search algorithm for inference and select the top-1 hypothesis as final output (see Fig. 1 (a)), following that in automatic speech recognition (ASR) (Tsunoo et al., 2021).However, this strategy discards the 2 to N -best hypotheses that could be advantageous to the generation of ground-truth translation.As illustrated in Fig. 2, the discarded 2 to N -best hypotheses contain abundant semantic information that is the key to composite the ground-truth utterance, while the 1-best hypothesis lacks this part of information.As a result, the typical top-1 hypothesis selection is sub-optimal to the translation tasks that require a single informative and high-quality output sequence (Li et al., 2022;Xiao et al., 2022).</p>
<p>Inspired by the recent works on LLMs-enhanced ASR (Ma et al., 2023b;Chen et al., 2023c;Yang et al., 2023a;Radhakrishnan et al., 2023), we propose a new generative paradigm for translation tasks, namely GenTranslate (see Fig. 1 (b)).Leveraging the rich linguistic knowledge and strong reasoning ability of LLMs, our paradigm integrates the diverse translation versions in the N -best list from foundation model to generate a higher-quality translation result.Furthermore, in order to support LLM finetuning, we also build and release a Hypo-Translate dataset that contains over 592K pairs of N -best hypotheses and ground-truth translation in 11 languages.Experimental evidence on various ST and MT benchmarks (e.g., FLEURS, CoVoST-2, WMT) demonstrate that our proposed GenTranslate significantly outperforms the state-of-the-art model with efficient LLM finetuning.</p>
<p>Our contributions are summarized as follows:</p>
<p>• We propose GenTranslate, a new generative paradigm for translation tasks that leverages LLMs to generate higher-quality translation results from the diverse N -best hypotheses decoded from foundation translation model.</p>
<p>• We release a HypoTranslate dataset to support LLM finetuning, which contains over 592K pairs of N -best hypotheses and ground-truth translation in 11 languages.</p>
<p>• Experiments on various ST and MT bench-  (n=1,2,3) in ST 1-best hypothesis (green), 2 to N -best hypotheses (blue), and the ground-truth translation (orange), where the text embeddings are extracted using SBERT (Reimers and Gurevych, 2019).It indicates that the 2 to N -best hypotheses contain richer information than 1-best for generating ground-truth translation.</p>
<p>marks show that our GenTranslate significantly outperforms the state-of-the-art model.</p>
<p>2 Related Work</p>
<p>Large Language Models</p>
<p>There is recently a surge of research interests in Transformer-based large language models, such as ChatGPT (OpenAI, 2022), GPT-4 (OpenAI, 2023) and LLaMA (Touvron et al., 2023a,b).Benefiting from the giant model size and oceans of training data, LLMs can understand better the linguistic structures and semantic meanings behind raw text, which thus shows remarkable performance on a wide range of natural language processing (NLP) tasks (Brown et al., 2020;Wei et al., 2022a;Ouyang et al., 2022).Thereafter, with techniques like incontext learning (Xie et al., 2021) and efficient finetuning (Hu et al., 2021;Yang et al., 2021b), LLMs further show powerful ability on downstream generative and reasoning tasks (Lampinen et al., 2022;Yang et al., 2023a;Hu et al., 2023i;Zhang et al., 2023b).Our proposed GenTranslate is exactly inspired by the promising generative ability of LLMs.</p>
<p>Speech and Machine Translation</p>
<p>The advancement of LLMs has notably enhanced the capabilities of translation tasks.In the domain of speech translation (Liu et al., 2021), Whisper (Radford et al., 2023) demonstrates commendable effectiveness, leveraging extensive web-scale data.AudioPaLM2 (Rubenstein et al., 2023)  thereby augmenting the speech translation performance.In the context of machine translation, NLLB (Costa-jussà et al., 2022), a model finetuned on LLMs, extends its linguistic range to over 200 languages.Additionally, BigTranslate (Yang et al., 2023b) utilizes instruction tuning to enhance the translation capabilities of LLMs.The most recent innovation, SeamlessM4T (Barrault et al., 2023a), represents a highly-unified model capable of fluid translation between speech and text, setting new benchmarks in both ST and MT tasks.However, it is noteworthy that the majority of these methodologies rely on beam search decoding (Yang et al., 2021a;Hu et al., 2023a) and top-1 hypothesis selection for inference.How to leverage N -best hypotheses to deliver better translation result remains to be an open question.</p>
<p>LLMs-Enhanced ASR</p>
<p>Recent works investigate LLMs to enhance the ASR output by error correction (Ma et al., 2023a;Chen et al., 2023cChen et al., ,b, 2024;;Hu et al., 2024), which serves as a post-processing technique to improve the recognition result (Leng et al., 2021).In particular, they leverage LLM finetuning (Zhang et al., 2023b) and in-context learning (Wang et al., 2023) to correct the wrongly recognized tokens in hypotheses by second-pass reasoning, which achieves promising improvement.Inspired by them, in this work we leverage LLMs to integrate the diverse translation versions in N -best list to generate a informative and higher-quality translation result.</p>
<p>Methodology</p>
<p>In this section, we introduce the proposed method.First, we describe the latest foundational translation model, SeamlessM4T, which we employ for beam search decoding and hypotheses generation ( §3.1).</p>
<p>Then, we introduce our LLMs-based GenTranslate paradigm by N -best hypotheses integration ( §3.2).Finally, we present the details of our released Hypo-Translate dataset for GenTranslate training ( §3.3).</p>
<p>Foundational Translation Model: SeamlessM4T</p>
<p>Recent work (Barrault et al., 2023a,b) proposes SeamlessM4T2 (Massively Multilingual &amp; Multimodal Machine Translation), a single Transformerbased (Vaswani et al., 2017) model that supports speech-to-speech translation, speech-to-text translation, text-to-speech translation, text-to-text translation, and automatic speech recognition for up to 100 languages.During development process, it is firstly pre-trained on 1 million hours of speech data by self-supervised learning, and it is then finetuned on a 406K-hour multimodal corpus of automatically aligned speech translations named Seam-lessAlign.Experiments show that SeamlessM4T yields superior performance on all of the five supported tasks.In particular, it has achieved the stateof-the-art on both ST and MT tasks in terms of BLEU score on various public benchmarks.</p>
<p>Considering its effectiveness, generality and popularity, we employ SeamlessM4T as the foundation model for both speech and machine translation in our system, as depicted in the left part of Fig. 3. Given an input speech S src or text T src in source language (e.g., German), SeamlessM4T translates it into target language (e.g., English) text by beam search decoding, which generates N -best hypotheses list
T tgt N = {T tgt 1 , T tgt 2 , • • • , T tgt N }.</p>
<p>GenTranslate</p>
<p>Overall Framework</p>
<p>To solve the information loss in typical top-1 hypothesis selection, we leverage LLMs to generate a final translation result based on the decoded Nbest hypotheses.Since each candidate in N -best list represents one unique version of translation for source language input, our GenTranslate can integrate their rich information to generate a higherquality translation result, thanks to the strong linguistic and reasoning ability of LLMs.This new generative paradigm can be formulated as:
T tgt = M GT (T tgt N , I), (1)
where I is a proper instruction for LLM prompting.</p>
<p>The goal of GenTranslate is to learn a mapping M GT from N -best hypotheses to the true translation.Following typical sequence-to-sequence learning strategy, we employ the ground-truth translation T tgt* as supervision signal and optimize the LLM to learn M GT in an auto-regressive manner.</p>
<p>The cross-entropy-based training loss is defined as:
L GT = L l=1 − log P θ (t tgt<em> l |t tgt</em> l−1 , • • • , t tgt<em> 1 ; T tgt N , I),
(2) where t tgt</em> l is the l-th token of T tgt* , L denotes the sequence length, and θ denotes the learnable parameters in LLM (i.e., adapter).</p>
<p>Efficient LLM Finetuning</p>
<p>Considering the giant scale of LLMs, we adopt the popular efficient finetuning strategy, LLaMA-Adapter (Zhang et al., 2023b), which is comparable to LoRA tuning ( §4.3.4).As shown in Fig. 3 (right), it inserts a set of learnable adaptation prompts into the top-L of total H Transformer layers in a pretrained LLM to learn high-level semantics.Denote the prompt for l-th layer as P l ∈ R U ×D , where U is prompt length and D is embedding size.</p>
<p>Assume we gain M tokens including instruction and already generated response, i.e., T l ∈ R M ×D , now we aim to predict the (M + 1)-th token as response.The learnable adaptation prompt is concatenated with T l as prefix, i.e., [P l ; T l ] ∈ R (U +M )×D , which provides learned instruction knowledge to guide the subsequent response generation.</p>
<p>Furthermore, considering the prompt P l is randomly initialized and thus could disturb the LLM tuning at early training stage, a zero-initialized attention mechanism is devised to mitigate such disturbance.Denote the current M -th token as T (M ) l ∈ R 1×D , in attention there are three projection layers to generate query, key and value:
Q l = Linear q (T (M ) l ), K l = Linear k ([P l ; T l ]), V l = Linear v ([P l ; T l ]),(3)
Then the attention score is calculated as +M ) , which captures the correlation between current token and the history tokens as well as prompts to predict the next token.Therefore, it can be split into two parts accordingly:
A l = Q l • K l / √ D ∈ R 1×(UA l = [A P l ; A T l ] T ,(4)
where
A P l ∈ R U ×1
is the attention score of U adaptation prompts and A T l ∈ R M ×1 is that of M history tokens.Since the adaptation prompts are randomly initialized, their attention scores may cast disturbance on next-token prediction at early training stage.To this end, a learnable gating factor g l with zero initialization is introduced to adaptively control the weight of prompt in attention:
A g l = [g l • softmax(A P l ); softmax(A T l )] T ,(5)
Finally, the attention output of l-th Transformer layer is obtained with a linear projection:
O (M ) l = Linear o (A g l • V l ) ∈ R 1×D ,(6)
It is then employed to predict the next token
T (M +1) l
as response.The zero-initialization mechanism yields an effective trade-off between the pretrained knowledge of LLM and the learned instructional knowledge through adaptation prompt.</p>
<p>HypoTranslate Dataset</p>
<p>In order to support the LLM finetuning for Gen-Translate, we release a HypoTranslate dataset that contains over 592K pairs of N -best hypotheses and ground-truth translation in 11 languages.In particular, we use the state-of-the-art SeamlessM4T-Large as foundation translation model to decode N -best hypotheses from input speech by beam search algorithm, where the beam size N is set to 5. Specifically, for ST task we investigate two popular pipelines in literature, i.e., end-to-end ST and cascaded ASR+MT.Thanks to the universal ability of SeamlessM4T on ST, ASR and MT tasks, we only need one model to build above two pipelines.</p>
<p>To build HypoTranslate dataset, we select several public ST and MT corpora in both X→En and 38.9 37.0 39.7 29.0 27.7 34.1 37.7 33.9 28.9 21.7 42.3 23.7 34.0 24.9 24.4 31.9GenTranslate (ours) 39.9 39.4 41.6 32.8 31.2 35.9 40.6 34.9 32.1 22.8 45.0 24.1 36.9 27.4 25.7 34.0 SeamlessM4T-V2 (ASR+MT) (2023b) † 39.2 36.8 39.1 29.4 26.7 33.9 35.7 32.9 29.3 22.5 43.2 25.4 34.8 29.7 25.9 32 En→X language directions.For speech translation, we select FLEURS (Conneau et al., 2023), CoVoST-2 (Wang et al., 2020), and MuST-C (Di Gangi et al., 2019).For machine translation, we select FLO-RES (Costa-jussà et al., 2022), WMT'16 (Bojar et al., 2016), WMT'19 (Barrault et al., 2019), and WMT'20 (Loïc et al., 2020)  with inserted prompts.The prompt length U is set to 10.More details are provided in §B.1.</p>
<p>Training Details</p>
<p>The batch size is set to 4, with accumulation iterations set to 8 (i.e., real batch size is 32).We train 2 epochs with AdamW optimizer (Loshchilov and Hutter, 2018), with learning rate initialized to 1e −2 and then linearly decrease to 1e −5 during training.Following the speech translation literature, we also investigate cascaded ASR+MT methods for evaluation.We can observe from Table 1 that, with the same SeamlessM4T-Large backbone, cascaded system outperforms end-to-end system by 4.8 BLEU score, which is consistent with previous findings (Xu et al., 2023a).Latest SeamlessM4T-Large-V2 further improves V1 model, and our Gen-Translate shows significant and consistent gains of performance over theses two backbones.</p>
<p>Comparison with the</p>
<p>Table 2 presents the X→En ST results on more language directions of CoVoST-2 dataset, where we introduce more latest baselines for comprehensive comparison.In end-to-end methods, SeamlessM4T-Large achieves a good 34.5 BLEU score though underperforms the state-of-the-art AudioPaLM28 .In comparison, our GenTranslate achieves a promis-  (Yang et al., 2023b) 38.2 ALMA-7b (Xu et al., 2023b) 40.6</p>
<p>Table 7: Performance of ASR+GenTranslate system on FLEURS De→En ST test set.As shown in Fig. 4, it first uses ASR to produce German N -best hypotheses, and then leverages LLMs to generate the English translation from them.Different LLMs are investigated here.</p>
<p>ing improvement over SeamlessM4T.Similar phenomenon can be observed in cascaded systems, where SeamlessM4T significantly outperforms the competitive baselines that combine state-of-theart ASR and MT models, and our GenTranslate moves one step forward with 1.8 BLEU improvement.Similar improvements can be observed on SeamlessM4T-Large-V2 backbone.English (En)→X.For comprehensive evaluation, we also present En→X ST results on three datasets in Table 3. SeamlessM4T (both Large and Large-V2) achieves excellent performance on En→X ST tasks under both end-to-end and cascaded systems.In comparison, our proposed GenTranslate achieves significant performance improvements (∼3 BLEU score) in various language directions.Since En→X translation tasks produce non-English N -best hypotheses for LLM integration, such performance gains indicates the excellent multilingual abilities of LLMs (i.e., LLaMA-2).</p>
<p>Machine Translation</p>
<p>X→English (En).This system engages LLMs into the translation process by combining it with the N -best integration process.</p>
<p>SeamlessM4T-Large.Based on that, our GenTranslate achieves the state-of-the-art with consistent gains on all language directions except Ja→En.English (En)→X.Table 5 presents the En→X MT results on WMT test sets.Similar to previous results, we observe much higher BLEU scores of NLLB-3.3bthan ALMA-13b and BigTranslate.SeamlessM4T-Large surpasses NLLB-3.3bby large-scale multitask training.The proposed GenTranslate achieves the state-of-the-arts on all language directions with a gain of 2.4 BLEU score.</p>
<p>Please note that SeamlessM4T-Large-V2 underperforms V1 on selected MT datasets, but our Gen-Translate achieves consistent gains on both of them.In summary, we observe consistent improvements of GenTranslate over various baselines (i.e., SeamlessM4T, Whisper, etc.), various tasks (i.e., ST and MT), various test data (i.e., FLEURS, WMT, etc.), and various language directions (i.e., X→En and En→X).Therefore, the effectiveness and generality of our approach are well verified.</p>
<p>Ablation Study</p>
<p>Effect of Different LLMs</p>
<p>According to Table 3 and 5 two latest multilingual LLMs for comparison, i.e., BigTranslate and ALMA-13b.Table 6 shows that both of them perform worse than LLaMA-2-13b for ST and MT tasks.One explanation is, Big-Translate and ALMA-13b are finetuned on MT task that requires cross-lingual ability, while the En→X GenTranslate mainly requires strong monolingual ability of language X, such mismatch may explain why MT finetuning fails to enhance GenTranslate.</p>
<p>Role of LLMs in GenTranslate</p>
<p>To further investigate the role of LLMs in our Gen-Translate, we build an ASR+GenTranslate system for ST task as shown in Fig. 4. Take De→En as an example, we first send the German speech input into ASR to produce N -best transcriptions, which are then fed by LLMs to generate English translation.In other words, LLMs are assigned N -best integration and translation tasks at the same time.As shown in Table 7, among the three evaluated LLMs, ALMA-7b achieves the best performance thanks to its MT finetuning during development, but it still underperforms the best cascaded method (40.6 vs. 41.6).We can conclude from such observations that 1) LLaMA-2 provides reasonable translation ability and it can be further improved via MT task finetuning (i.e., ALMA).2) In this study, LLM underperforms SeamlessM4T in translation task, but it shows remarkable ability in N -best integration.Therefore, future work may focus on how to better engage LLMs into the translation part.</p>
<p>Effect of N -best List Size</p>
<p>GenTranslate relies on powerful LLMs and informative N -best hypotheses to generate higherquality translation output.Therefore, the amount of information in N -best hypotheses could be a key factor of GenTranslate's performance.We can observe from Table 10 that with the increase of N, the performance of GenTranslate first improves and then drops, where the best choice ranges from 5 to 10.We believe that small N results in insufficient information for generation of ground-truth translation, while too large N leads to information redundancy and thus increases the miscorrection and hallucination.In this work, we set N to 5 for the best trade-off between efficiency and quality.</p>
<p>LLaMA-Adapter vs. LLaMA-LoRA</p>
<p>Apart from LLaMA-Adapter, low-rank adaptation (LoRA) (Hu et al., 2021;Yu et al., 2023)  TV reports show that white smoke is escaping from the plant.28.6 TV reports show that white smoke is escaping from the facility.</p>
<p>12.2 Television reports show that white smoke is escaping from the plant.</p>
<p>34.2 Television reports show that white smoke is escaping from the facility.</p>
<p>19.2 TV reports show that white smoke escapes from the plant.</p>
<p>31.7</p>
<p>GenTranslate (ours) Television reports show white smoke coming out of the plant.</p>
<p>58.8</p>
<p>Ground-truth Translation Television reports show white smoke coming from the plant.</p>
<p>-</p>
<p>Table 11: Case study of GenTranslate.The test sample is selected from the FLEURS De→En ST test set.</p>
<p>Analysis</p>
<p>Effect of Language Family</p>
<p>Table 9 analyzes the effect of language family using the X→En ST results.The source language X is grouped into two categories depending on whether it belongs to Indo-European family (English is also Indo-European language).First, we observe better results of SeamlessM4T when X belongs to Indo-European family, indicating that translation within same family is easier than across different families.Then, we also observe larger BLEU improvement of GenTranslate over baseline when X is Indo-European language (2.6 vs. 1.3).The reason could be, within-family translation produces N -best hypotheses with higher quality and richer information, which is beneficial to GenTranslate.</p>
<p>Case Study</p>
<p>Table 11 shows a case study where GenTranslate outperforms the 1-best hypothesis by a large margin.We may speculate two key points about its working mechanism, where it first extract the word "Television" from 3 rd /4 th hypotheses to replace "TV" and then reason out the word "coming" that does not exist in N -best list.Therefore, our paradigm may not only integrate the N -best sentences for better result, but also improve the translation quality by itself.Another non-English case study is in Appendix C.1.</p>
<p>Visualizations of GenTranslate Output</p>
<p>Fig. 5 visualizes the n-gram tokens in GenTranslate output, which contains sufficient semantic information to match the ground-truth translation.</p>
<p>In comparison, the 1-best hypothesis lacks such information to produce high-quality translation output, which highlights the contribution of N -best hypotheses in GenTranslate paradigm (see Fig. 2).</p>
<p>Conclusion</p>
<p>In this paper, we propose a generative paradigm for translation tasks, namely GenTranslate, which leverages LLMs to integrate the diverse candidates in the decoded N -best list and generate a higherquality translation result.Furthermore, we release a HypoTranslate dataset to support LLM finetuning, which contains over 592K hypotheses-translation pairs in 11 languages.Experimental evidence on various speech and machine translation benchmarks shows that our GenTranslate significantly outperforms the state-of-the-art model.</p>
<p>Limitations</p>
<p>There are two limitations existed in this work.First, the contribution of LLMs in our GenTranslate paradigm focuses on N -best hypotheses integration, while the translation part is actually done by SeamlessM4T model.Experiment results in</p>
<p>A HypoTranslate Dataset Details</p>
<p>In this section, we introduce the details of our proposed HypoTranslate dataset.We first introduce the speech and machine translation corpora that we utilize to build HypoTranslate in §A.1 and §A.2.Then, we present the dataset statistics in §A.3.</p>
<p>A.1 Speech Translation Corpus Selection</p>
<p>For speech translation task, we select three popular and public datasets that cover multiple languages:</p>
<p>FLEURS9 (Conneau et al., 2023): Few-shot Learning Evaluation of Universal Representations of Speech (FLEURS) benchmark provides an n-way parallel speech dataset in 102 languages built on top of the machine translation FLORES-101 benchmark (Goyal et al., 2022), with approximately 12 hours of speech supervision per language.In this work, we select 15 X→En and 6 En→X language directions of speech translation data for evaluation.</p>
<p>CoVoST-210 (Wang et al., 2020): CoVoST-2 is a popular multilingual speech translation corpus based on Common Voice (Ardila et al., 2019) that consists of 2,880 hours speech data recorded from 78K speakers.In this work, we select 15 X→En and 3 En→X language directions for evaluation.Specifically, for En→X language directions, we randomly select 1,000 testing samples from the original test split for higher evaluation efficiency.</p>
<p>MuST-C11 (Di Gangi et al., 2019): MuST-C is a multilingual speech translation corpus whose size and quality facilitate the training of end-to-end systems for spoken language translation from English into 15 languages.In this work, we select 3 En→X language directions for evaluation.</p>
<p>A.2 Machine Translation Corpus Selection</p>
<p>For machine translation task, we select two popular and public datasets that cover multiple languages:  (Barrault et al., 2019), Ja→En and Zh→En directions from WMT'2015 (Loïc et al., 2020) for evaluation, and corresponding newdev data is used for validation.The training data is obtained from ParaCrawl-V916 (Bañón et al., 2020) and JParaCrawl17 (Morishita et al., 2020) datasets.
FLORES 12 (</p>
<p>A.3 Statistics</p>
<p>After performing beam search decoding on the selected speech and machine translation corpora introduced above, we collect over 592K pairs of N -best hypotheses and ground-truth translation to build the HypoTranslate dataset.The statistics are illustrated in Table 15 and 17, which present the number of hypotheses-translation pairs and the average utterance length.We plan to release the Hy-poTranslate dataset to public upon publication and open the development venue for more data.</p>
<p>B Experimental Setup Details B.1 Model Setups</p>
<p>We select two latest foundation LLMs for evaluation, including LLaMA-2-7b (Touvron et al., 2023b) and LLaMA-2-13b (Touvron et al., 2023b).</p>
<p>In addition, in order to evaluate the multilingual ability of LLMs for GenTranslate with non-Englishtarget directions, we also select two latest finetuned LLMs on MT task, including BigTranslate (Yang et al., 2023b) and ALMA-13b (Xu et al., 2023b).Table 12 compares their main configurations.For efficient LLM finetuning, we follow the default settings of LLaMA-Adapter18 (Zhang et al., 2023b).</p>
<p>B.2 Inference Setups</p>
<p>In the response generation during inference stage, we set a temperature of 0.2 and top-1 sampling, i.e., greedy search.We have observed over-confidence phenomenon in our experiments (i.e., output probability distribution for decision is close to one-hot), which results in similar performance with different k for top-k sampling.Therefore, we select top-1 sampling for higher decoding efficiency.</p>
<p>B.3 Translation Baselines</p>
<p>To comprehensively evaluate our GenTranslate model, we selected some of the latest and most advanced baselines in speech and machine translation for comparison.We will introduce these in the following subsections.</p>
<p>B.3.1 Speech Translation</p>
<p>XLS-R19 (Babu et al., 2021): XLS-R is a largescale model for cross-lingual speech representation learning based on Wav2vec 2.0 (Baevski et al., 2020).They train models with up to 2B parameters on 500K hours of publicly available speech audio in 128 languages, which achieves superior performance on a wide range of multilingual speech processing tasks, including speech translation, speech recognition and language identification.</p>
<p>Whisper20 (Radford et al., 2023): Whisper is a large-scale automatic speech recognition (ASR) system (Chen et al., 2022a(Chen et al., ,b, 2023d,a;,a;Hu et al., 2022Zhu et al., 2023Zhu et al., , 2024) ) trained on 680K hours of multilingual and multitask supervised data collected from the web, which shows excellent robustness to accents, background noise and technical language.Moreover, it enables transcription in multiple languages, as well as translation from those languages into English.for the abundant parallel data that traditional translation models usually depend on, which includes two stages: initial finetuning on monolingual data followed by subsequent finetuning on a small set of high-quality parallel data.Built based on LLaMA-2, it has achieved significant improvement over prior works across multiple translation directions.</p>
<p>C Supplementary Experiment Results</p>
<p>C.1 Supplementary Case Study</p>
<p>C.2 BLEU vs. chrF++</p>
<p>We report translation performance in terms of the BLEU score (Papineni et al., 2002) in most experiments of this work.For more comprehensive evaluation, Table 16 presents both BLEU and chrF++ scores (Popović, 2017;Barrault et al., 2023a)</p>
<p>FoundationFigure 1 :
1
Figure 1: Illustration of (a) Typical seq2seq translation with beam search decoding and top-1 hypothesis selection, (b) our "GenTranslate" with LLM integration.</p>
<p>Figure 2
2
Figure2: t-SNE visualization of the n-gram tokens (n=1,2,3) in ST 1-best hypothesis (green), 2 to N -best hypotheses (blue), and the ground-truth translation (orange), where the text embeddings are extracted using SBERT(Reimers and Gurevych, 2019).It indicates that the 2 to N -best hypotheses contain richer information than 1-best for generating ground-truth translation.</p>
<p>Figure 3 :
3
Figure 3: Left: Overview of the GenTranslate paradigm (e.g., De→En).Right: Details of efficient LLM finetuning.</p>
<p>Figure 5
5
Figure 5: t-SNE visualization of n-grams in 1-best hypothesis (green), ground-truth translation (orange) and GenTranslate output (purple).It's an extension of Fig. 2.</p>
<p>Table 1 :
1
(Popović, 2017)ion results on FLEURS X→En test sets in terms of BLEU score, where more results on chrF++ metric(Popović, 2017)are in Table16.We use bold to denote surpassing SeamlessM4T baseline, and use underline to denote the state-of-the-art.The baseline methods are introduced in §B.3.*denotes reported by original paper, or else it denotes reproduced by ourselves (same for Table2 to 5).† denotes the most latest baseline 3 .
.3</p>
<p>Table 2 :
2
Speech translation results on CoVoST-2 X→En test sets in terms of BLEU score.Remarks follow Table 1.</p>
<p>Table 3 :
3
Speech translation results on FLEURS, CoVoST-2, and MuST-C En→X test sets in terms of BLEU score.We use bold to highlight surpassing SeamlessM4T baseline, and use underline to highlight the state-of-the-art performance.The baseline methods are introduced in §B.3, and all of their results are reproduced by ourselves.
corpora. As a result,we obtain over 592K hypotheses-translation pairsin 11 languages. The details of dataset statistics arepresented in  §A.3 and Table 15, 17.Since the hypotheses-translation data pairs in Hy-poTranslate dataset are monolingual, we can alsouse ASR dataset to benefit GenTranslate training,especially for low-resource language pairs. Rele-vant studies are illustrated in  §4.3.2 and Table 7.Our best result was obtained by first performingtranslation with SeamlessM4T and then integratingthe N -best candidates using LLMs.</p>
<p>Table 4 :
4
Machine translation results on FLORES X→En test sets in terms of BLEU score.Remarks follow Table3.
En→XWMT'16 RoWMT'19 Cs LtWMT'20 Ja ZhAvg.ALMA-13b (2023b)6.26.10.33.5 11.3 5.5BigTranslate (2023b)21.419.0 8.77.3 29.0 17.1NLLB-3.3b (2022)31.025.3 16.0 15.2 26.9 22.9SeamlessM4T-Large32.726.0 17.2 17.0 27.2 24.0GenTranslate (ours)33.527.2 19.4 21.4 30.7 26.4SeamlessM4T-Large-V232.225.2 16.2 15.2 28.7 23.5GenTranslate-V2 (ours)33.226.6 18.2 19.3 31.6 25.8</p>
<p>Table 5 :
5
Machine translation results on WMT'16,19,20 En→X test sets in BLEU.Remarks follow Table 3.</p>
<p>Table 6 :
6
Large (2023a) 24.6 44.6 25.4 41.9 34.1 18.824.035.1 26.0 32.7 26.0 17.2 17.0 27.2 24.0 GenTranslate with BigTranslate (2023b) 25.3 44.2 25.5 40.8 34.0 5.2 23.5 42.6 23.8 31.3 24.9 15.8 13.9 27.9 22.8 ALMA-13b (2023b) 24.9 43.5 25.1 40.6 33.5 19.2 29.3 43.9 30.8 31.1 25.5 17.7 17.3 26.8 23.7 LLaMA-2-13b (2023b) 26.8 45.0 26.6 43.1 35.4 21.8 30.5 43.3 31.9 33.5 27.2 19.4 21.4 30.7 26.4Effect of different multilingual LLMs on GenTranslate, in terms of the speech translation results on FLEURS and CoVoST-2 En→X test sets, as well as the machine translation results on WMT En→X test sets.
En→XEsFrFLEURS ItPtAvg.FaCoVoST-2 Ja Zh Avg. RoCsWMT It JaZh Avg.SeamlessM4T-De→EnBLEU ScoreEnd-to-end ST MethodsSeamlessM4T (ST) (Barrault et al., 2023a)35.8SeamlessM4T (ST) + GenTranslate39.2Cascaded ASR+MT MethodsSeamlessM4T (ASR+MT) (Barrault et al., 2023a)39.7SeamlessM4T (ASR+MT) + GenTranslate41.6ASR+GenTranslate MethodSeamlessM4T (ASR) + GenTranslate withLLaMA-2-7b (Touvron et al., 2023b)36.8BigTranslate</p>
<p>Table 8 :
8
, LLaMA-2 has shown excellent multilingual ability.To further investigate the role of this ability in GenTranslate, we select Comparison between LLaMA-Adapter and LLaMA-LoRA for efficient LLM finetuning in our GenTranslate, in terms of the speech translation results on FLEURS X→En test sets.
X→EnArCyDeElEsFaFrHiItJaPtTaUkViZh Avg.SeamlessM4T (ASR+MT) 38.9 37.0 39.7 29.0 27.7 34.1 37.7 33.9 28.9 21.7 42.3 23.7 34.0 24.9 24.4 31.9GenTranslate withLLaMA-Adapter39.9 39.4 41.6 32.8 31.2 35.9 40.6 34.9 32.1 22.8 45.0 24.1 36.9 27.4 25.7 34.0LLaMA-LoRA40.2 39.3 41.8 32.8 31.6 36.0 40.6 35.2 32.4 22.5 45.1 24.1 36.7 27.1 26.0 34.1X→EnFaHiItEsIndo-European Fr Pt CyDeElUk Avg. Arnon-Indo-European Vi Ja Ta Zh Avg.SeamlessM4T (ASR+MT) 34.1 33.9 28.9 27.7 37.7 42.3 37.0 39.7 29.0 34.0 34.4 38.9 24.9 21.7 23.7 24.4 26.7GenTranslate (ours)35.9 34.9 32.1 31.2 40.6 45.0 39.4 41.6 32.8 36.9 37.0 39.9 27.4 22.8 24.1 25.7 28.0∆ BLEU1.81.03.23.52.92.72.41.93.82.92.61.02.51.10.41.41.3</p>
<p>Table 9 :
9
Effect of language family on our proposed GenTranslate.We report speech translation results on FLEURS X→En test sets in this study.For simplicity, we split all the languages into two families, i.e., Indo-European (same as English) and non-Indo-European, and more detailed information are presented in Table14.
X→EnArDeEsFrPtZh Avg.SeamlessM4T-Large 32.8 35.8 25.0 33.1 38.9 19.8 30.9GenTranslate with131.3 35.4 26.9 35.2 41.5 19.3 31.6334.2 38.9 29.5 36.4 42.8 21.3 33.9N =5 834.6 39.2 29.8 37.0 43.0 21.7 34.2 34.8 39.9 29.4 36.9 43.0 21.5 34.31035.3 39.8 29.4 36.6 43.2 21.6 34.31534.9 39.5 29.6 36.4 42.8 21.6 34.1
Table 10: Effect of N -best list size on GenTranslate (default N=5), in terms of ST results on FLEURS X→En.</p>
<p>Table7also indicate that LLMs are good at Nbest hypotheses integration and SeamlessM4T is good at translation.Therefore, our future work could focus on how to better engage LLMs into the translation part to further improve the translation quality.Another limitation is about the latest second version of SeamlessM4T released by Meta, which indicates a stronger baseline for GenTranslate.In fact, our experiments had already been done on SeamlessM4T-Large before Meta released the latest SeamlessM4T-Large-V2 on November 30th, 2023.For comprehensive evaluation, we also rerun our main experiments on this latest V2 backbone, and our GenTranslate has shown similar effectiveness on it (highlighted in gray in Table1 to 5).For brevity, we prefer to leave the ablation study and analyses on SeamlessM4T-Large backbone only, as our GenTranslate paradigm has shown similar effectiveness and patterns on V1 and V2 backbones.
Ethics StatementThis work does not pose any ethical issues. All thedata used in this paper are publicly available andare used under following licenses: Creative Com-mons BY 4.0 License, Creative Commons CC0License, Creative Commons BY-NC-ND 4.0 Li-cense, and Creative Commons BY-SA 4.0 License.</p>
<p>Table 12 :
12
Comparison between main configurations of different popular LLMs.
LLMLLaMA-2-7b LLaMA-2-13b BigTranslate ALMA-13bNumber of Transformer Layers H32404040Number of Attention Heads N head32404040Embedding Size D4,0965,1205,1205,120Block Size B4,0964,0964,0964,096Vocabulary Size V32,00032,00053,61332,000</p>
<p>Table 13 :
13
Supplementary case study.The test sample is selected from the FLEURS En→Zh ST test set.
AudioPaLM2 (Rubenstein et al., 2023): Au-dioPaLM2 fuses text-based and speech-based lan-</p>
<p>Table 14 :
14
(Babu et al., 2021)amily and sub-grouping information(Babu et al., 2021)of FLEURS datasets.</p>
<p>Table 15 :
15
HypoTranslate dataset (ST part) statistics in terms of the number of hypotheses-translation pairs and average length of ground-truth utterance in different language directions.ASR+MT) 62.7 60.0 63.8 55.0 56.0 58.7 62.4 58.8 57.0 47.9 65.9 49.8 59.2 50.5 51.5 57.3 GenTranslate (ours) 63.1 61.2 64.9 57.0 57.1 59.7 64.0 59.1 58.0 47.6 67.2 49.7 60.8 51.6 52.0 58.2
on</p>
<p>Table 16 :
16
Speech translation results on FLEURS X→En test sets in terms of chrF++ score.
Data SourceSource / Target Language XTrain # Pairs Length # Pairs Length # Pairs Length Dev. TestArabic (Ar)2,06220.429519.81,01221.6German (De)2,92620.736320.11,01221.6Greek (El)3,14820.927120.51,01221.6Spanish (Es)2,73220.840820.51,01221.6FLORES (Costa-jussà et al., 2022)Persian (Fa)3,03220.936920.11,01221.6(X→En)French (Fr)3,11920.828919.91,01221.6Italian (It)2,97020.639120.21,01221.6Japanese (Ja)2,24120.226619.61,01221.6Ukrainian (Uk)2,74120.832520.31,01221.6Chinese (Zh)3,17821.040920.61,01221.6Czech (Cs)15,00012.32,98315.81,99718.8WMT'{16,19,20} (En→X)Japanese (Ja) Lithuanian (Lt) Romanian (Ro)15,000 15,000 15,00049.8 12.0 16.71,998 2,000 1,99953.1 16.5 22.61,000 998 1,99959.8 16.6 21.7Chinese (Zh)15,00035.61,99747.81,41860.7Total103,14924.014,36327.517,53226.3</p>
<p>Table 17 :
17
HypoTranslate dataset (MT part) statistics in terms of the number of hypotheses-translation pairs and average length of ground-truth utterance in different language directions.</p>
<p>This work is open sourced at: https://github.com/Y UCHEN005/GenTranslate
https://github.com/facebookresearch/seamless _communication
Our experiments are mainly conducted on SeamlessM4T-Large as they had already been done before Meta released the latest SeamlessM4T-Large-V2 on November 30th, 2023. For comprehensive evaluation, we rerun the main experiments on V2, which demonstrate similar effectiveness of our paradigm.
4 Experiments4.1 Setup 4.1.1 Model Selection LLMs. We select the popular LLaMA-2(Touvron et al., 2023b) for our paradigm. Specifically, we employ LLaMA-2-7b 4 for English-target directions (X→En) and LLaMA-2-13b for non-English-target directions (En→X), as LLaMA-2 shows superior ability on English language while less-optimal on other languages. In addition, for En→X we also try some latest multilingual LLMs like BigTrans-
late 5(Yang et al., 2023b) and
ALMA 6(Xu et al., 2023b) that are finetuned on LLaMA-13b. Adapter. We follow the default settings of LLaMA-Adapter(Zhang et al., 2023b). The number of tunable Transformer layers L is set to H −1, which means all layers except the first one are tunable 4 https://huggingface.co/meta-llama/Llama-2-7 b-hf 5 https://huggingface.co/James-WYang/BigTransl ate 6 https://huggingface.co/haoranxu/ALMA-13B
Latest SeamlessM4T-Large-V2 achieves significant gains over V1, based on which the proposed GenTranslate also shows similar effectiveness in our study.
We speculate it could be attributed to the train-test domain mismatch because SeamlessM4T-Large outperforms Au-dioPaLM2 by a large margin on FLEURS dataset in Table1.
https://huggingface.co/datasets/google/fleurs
https://github.com/facebookresearch/covost
https://mt.fbk.eu/must-c-releases/
https://huggingface.co/datasets/facebook/flo res
https://www.statmt.org/wmt16/translation-tas k.html
https://www.statmt.org/wmt19/translation-tas k.html
https://www.statmt.org/wmt20/translation-tas k.html
https://paracrawl.eu/
https://www.kecl.ntt.co.jp/icl/lirg/jparacra wl/
https://github.com/Lightning-AI/lit-gpt/blob/ main/lit_gpt/adapter.py
https://huggingface.co/models?other=xls_r
https://github.com/openai/whisper
https://github.com/nethermanpro/ComSL
https://huggingface.co/facebook/nllb-200-3.3 B
Persian (Fa) Indo-European Indo-Iranian Hindi (Hi) Indo-European Indo-Iranian Italian (It) Indo-European Indo-Iranian Spanish (Es) Indo-European Italic French (Fr) Indo-European Italic Portuguese (Pt) Indo-European Italic Welsh (Cy) Indo-European Celtic English (En) Indo-European Germantic German (De) Indo-European Germantic Greek (El) Indo-European Greek Ukranian (Uk) Indo-European Balto-Slavic Arabic (Ar) Afro-Asiatic Semitic Vietnamese (Vi) Austro-Asiatic Mon-Khmer Japanese (Ja) Japonic -Tamil (Ta) Dravidian Dravidian Chinese (Zh) Sino-Tibetan Chinese
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, arXiv:2305.10403Palm 2 technical report. 2023arXiv preprint</p>
<p>Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis M Tyers, Gregor Weber, arXiv:1912.06670Common voice: A massivelymultilingual speech corpus. 2019arXiv preprint</p>
<p>Xls-r: Self-supervised cross-lingual speech representation learning at scale. Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Yatharth Patrick Von Platen, Juan Saraf, Pino, arXiv:2111.092962021arXiv preprint</p>
<p>Abdelrahman Mohamed, and Michael Auli. 2020. wav2vec 2.0: A framework for self-supervised learning of speech representations. Alexei Baevski, Yuhao Zhou, Advances in neural information processing systems. 33</p>
<p>Paracrawl: Web-scale acquisition of parallel corpora. Marta Bañón, Pinzhen Chen, Barry Haddow, Kenneth Heafield, Hieu Hoang, Miquel Esplà-Gomis, Mikel Forcada, Amir Kamran, Faheem Kirefu, Philipp Koehn, 2020Association for Computational Linguistics</p>
<p>. Loıc Barrault, Ondrej Bojar, Marta R Costa-Jussa, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, Shervin Malmasi, et al. 2019. Findings of the 2019 conference on machine translation. Proceedings of WMT</p>
<p>Loïc Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, arXiv:2308.11596Seamlessm4t-massively multilingual &amp; multimodal machine translation. 2023aarXiv preprint</p>
<p>Loïc Barrault, Yu-An Chung, Mariano Coria Meglioli, David Dale, Ning Dong, Mark Duppenthaler, Paul-Ambroise Duquenne, Brian Ellis, Hady Elsahar, Justin Haaheim, John Hoffman, Seamless: Multilingual expressive and streaming speech translation. 2023b. 2023</p>
<p>Findings of the 2016 conference on machine translation (wmt16). Ondrej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, First conference on machine translation. Association for Computational Linguistics2016</p>
<p>Audiolm: a language modeling approach to audio generation. Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, IEEE/ACM Transactions on Audio. 2023Speech, and Language Processing</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Noise-robust speech recognition with 10 minutes unparalleled in-domain data. Chen Chen, Nana Hou, Yuchen Hu, Shashank Shirol, Eng Siong, Chng , ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE2022a</p>
<p>Selfcritical sequence training for automatic speech recognition. Chen Chen, Yuchen Hu, Nana Hou, Xiaofeng Qi, Heqing Zou, Eng Siong, Chng , ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE2022b</p>
<p>Metric-oriented speech enhancement using diffusion probabilistic model. Chen Chen, Yuchen Hu, Weiwei Weng, Eng Siong, Chng , ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE2023a</p>
<p>Generative error correction for codeswitching speech recognition using large language models. Chen Chen, Yuchen Hu, Chao-Han Huck Yang, Hexin Liu, Sabato Marco Siniscalchi, Eng Siong, Chng , arXiv:2310.130132023barXiv preprint</p>
<p>Hyporadise: An open baseline for generative speech recognition with large language models. Chen Chen, Yuchen Hu, Chao-Han Huck Yang, Sabato Marco Siniscalchi, Pin-Yu Chen, Ensiong Chng, Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023c</p>
<p>Leveraging modality-specific representations for audio-visual speech recognition via reinforcement learning. Chen Chen, Yuchen Hu, Qiang Zhang, Heqing Zou, Beier Zhu, Eng Siong, Chng , Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2023d37</p>
<p>It's never too late: Fusing acoustic information into large language models for automatic speech recognition. Chen Chen, Ruizhe Li, Yuchen Hu, Sabato Marco Siniscalchi, Pin-Yu Chen, Ensiong Chng, Chao-Han Huck, Yang , arXiv:2402.054572024arXiv preprint</p>
<p>Fleurs: Few-shot learning evaluation of universal representations of speech. Alexis Conneau, Min Ma, 2022 IEEE Spoken Language Technology Workshop (SLT). IEEE2023</p>
<p>James Marta R Costa-Jussà, Onur Cross, Maha Çelebi, Kenneth Elbayad, Kevin Heafield, Elahe Heffernan, Janice Kalbassi, Daniel Lam, Jean Licht, Maillard, arXiv:2207.04672No language left behind: Scaling human-centered machine translation. 2022arXiv preprint</p>
<p>Must-c: a multilingual speech translation corpus. Di Mattia, Roldano Gangi, Luisa Cattoni, Matteo Bentivogli, Marco Negri, Turchi, Proc. NAACL. NAACLAssociation for Computational Linguistics2019</p>
<p>The flores-101 evaluation benchmark for low-resource and multilingual machine translation. Naman Goyal, Cynthia Gao, Transactions of the Association for Computational Linguistics. 102022</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>Improving multilingual and code-switching asr using large language model generated text. Ke Hu, Tara N Sainath, 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE2023a</p>
<p>Gradient remedy for multitask learning in end-to-end noise-robust speech recognition. Yuchen Hu, Chen Chen, Ruizhe Li, Qiushi Zhu, Eng Siong, Chng , arXiv:2302.113622023barXiv preprint</p>
<p>Yuchen Hu, Chen Chen, Ruizhe Li, Qiushi Zhu, Eng Siong, Chng , arXiv:2307.08029Noise-aware speech enhancement using diffusion probabilistic model. 2023carXiv preprint</p>
<p>MIR-GAN: Refining framelevel modality-invariant representations with adversarial network for audio-visual speech recognition. Yuchen Hu, Chen Chen, Ruizhe Li, Heqing Zou, Eng Siong, Chng , Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsACL2023d1</p>
<p>Large language models are efficient learners of noise-robust speech recognition. Yuchen Hu, Chen Chen, Chao-Han Huck Yang, Ruizhe Li, Chao Zhang, Pin-Yu Chen, Ensiong Chng, arXiv:2401.104462024arXiv preprint</p>
<p>Wav2code: Restore clean speech representations via codebook lookup for noise-robust asr. Yuchen Hu, Chen Chen, Qiushi Zhu, Eng Siong, Chng , Speech, and Language Processing. 2023e</p>
<p>Interactive feature fusion for end-to-end noise-robust speech recognition. Yuchen Hu, Nana Hou, Chen Chen, Eng Siong, Chng , ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE2022</p>
<p>Dual-path style learning for end-toend noise-robust speech recognition. Yuchen Hu, Nana Hou, Chen Chen, Eng Siong, Chng , Proc. Interspeech. Interspeech2023f</p>
<p>Hearing lips in noise: Universal viseme-phoneme mapping and transfer for robust audio-visual speech recognition. Yuchen Hu, Ruizhe Li, Chen Chen, Chengwei Qin, Qiu-Shi Zhu, Eng Siong, Chng , Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsACL2023g1</p>
<p>Cross-modal global interaction and local alignment for audiovisual speech recognition. Yuchen Hu, Ruizhe Li, Chen Chen, Heqing Zou, Qiushi Zhu, Eng Siong, Chng , Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-23. the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-23IJ-CAI Organization2023h</p>
<p>Llm-adapters: An adapter family for parameter-efficient finetuning of large language models. Zhiqiang Hu, Yihuai Lan, arXiv:2304.019332023iarXiv preprint</p>
<p>Can language models learn from explanations in context?. Ishita Andrew K Lampinen, Dasgupta, C Y Stephanie, Kory Chan, Michael Henry Matthewson, Antonia Tessler, James L Creswell, Jane X Mcclelland, Felix Wang, Hill, arXiv:2204.023292022arXiv preprint</p>
<p>Chenyang Le, Yao Qian, Long Zhou, Shujie Liu, Michael Zeng, Xuedong Huang, arXiv:2305.14838Comsl: A composite speech-language model for end-toend speech-to-text translation. 2023arXiv preprint</p>
<p>Fastcorrect: Fast error correction with edit alignment for automatic speech recognition. Yichong Leng, Xu Tan, Linchen Zhu, Jin Xu, Renqian Luo, Linquan Liu, Tao Qin, Xiangyang Li, Edward Lin, Tie-Yan Liu, Advances in Neural Information Processing Systems. 202134</p>
<p>Diffusionlm improves controllable text generation. Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang, Tatsunori B Hashimoto, Advances in Neural Information Processing Systems. 202235</p>
<p>Dan Liu, Mengge Du, Xiaoxi Li, Yuchen Hu, Lirong Dai, arXiv:2107.00279The ustc-nelslip systems for simultaneous speech translation task at iwslt 2021. 2021arXiv preprint</p>
<p>Findings of the 2020 conference on machine translation (wmt20). Barrault Loïc, Biesialska Magdalena, Bojar Ondřej, Federmann Christian, Graham Yvette, Grundkiewicz Roman, Haddow Barry, Huck Matthias, Proceedings of the Fifth Conference on Machine Translation. the Fifth Conference on Machine TranslationAssociation for Computational Linguistics2020</p>
<p>Decoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, International Conference on Learning Representations. 2018</p>
<p>New trends in machine translation using large language models: Case examples with chatgpt. Chenyang Lyu, Jitao Xu, Longyue Wang, arXiv:2305.011812023arXiv preprint</p>
<p>N-best t5: Robust asr error correction using multiple input hypotheses and constrained decoding space. Rao Ma, J F Mark, Kate Gales, Mengjie Knill, Qian, arXiv:2303.004562023aarXiv preprint</p>
<p>Can generative large language models perform asr error correction?. Rao Ma, Mengjie Qian, Potsawee Manakul, Mark Gales, Kate Knill, arXiv:2307.041722023barXiv preprint</p>
<p>JParaCrawl: A large scale web-based English-Japanese parallel corpus. Makoto Morishita, Jun Suzuki, Masaaki Nagata, Proceedings of The 12th Language Resources and Evaluation Conference. The 12th Language Resources and Evaluation ConferenceMarseille, FranceEuropean Language Resources Association2020</p>
<p>Introducing chatgpt. arXiv:2303.08774OpenAI Blog. OpenAI. 2023. Gpt-4 technical report. 2022OpenAIarXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>chrf++: words helping character n-grams. Maja Popović, Proceedings of the second conference on machine translation. the second conference on machine translation2017</p>
<p>Robust speech recognition via large-scale weak supervision. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey, Ilya Sutskever, International Conference on Machine Learning. PMLR2023</p>
<p>Whispering llama: A cross-modal generative error correction framework for speech recognition. Srijith Radhakrishnan, Chao-Han, Sumeer Yang, Khan, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Nils Reimers, Iryna Gurevych, arXiv:1908.10084Sentence-bert: Sentence embeddings using siamese bert-networks. 2019arXiv preprint</p>
<p>Chulayuth Paul K Rubenstein, Asawaroengchai, Dung Duc, Nguyen, arXiv:2306.12925Audiopalm: A large language model that can speak and listen. 2023arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Izacard, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023aarXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, arXiv:2307.092882023barXiv preprint</p>
<p>Streaming transformer asr with blockwise synchronous beam search. Emiru Tsunoo, Yosuke Kashiwagi, Shinji Watanabe, 2021 IEEE Spoken Language Technology Workshop (SLT). IEEE2021</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. 201730</p>
<p>Covost 2 and massively multilingual speech-to-text translation. Changhan Wang, Anne Wu, Juan Pino, arXiv:2007.103102020arXiv preprint</p>
<p>Can whisper perform speech-based incontext learning. Siyin Wang, Chao-Han Huck Yang, Ji Wu, Chao Zhang, arXiv:2309.070812023arXiv preprint</p>
<p>What language model architecture and pretraining objective works best for zero-shot generalization?. Thomas Wang, Adam Roberts, International Conference on Machine Learning. PMLR2022</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022aarXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022b35</p>
<p>Bitiimt: A bilingual text-infilling method for interactive machine translation. Yanling Xiao, Lemao Liu, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>An explanation of in-context learning as implicit bayesian inference. Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma, arXiv:2111.020802021arXiv preprint</p>
<p>Chen Xu, Rong Ye, Qianqian Dong, Chengqi Zhao, Tom Ko, Mingxuan Wang, Tong Xiao, Jingbo Zhu, arXiv:2306.11646Recent advances in direct speech-to-text translation. 2023aarXiv preprint</p>
<p>A paradigm shift in machine translation: Boosting translation performance of large language models. Haoran Xu, Young , Jin Kim, Amr Sharaf, Hany Hassan Awadalla, arXiv:2309.116742023barXiv preprint</p>
<p>Generative speech recognition error correction with large language models and task-activating prompting. Chao-Han Huck, Yile Yang, Yi-Chieh Gu, Shalini Liu, Ivan Ghosh, Andreas Bulyko, Stolcke, 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE2023a</p>
<p>Multitask language modeling for improving speech recognition of rare words. Chao-Han Huck, Linda Yang, Liu, 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE2021a</p>
<p>Voice2series: Reprogramming acoustic models for time series classification. Chao-Han Huck, Yun-Yun Yang, Pin-Yu Tsai, Chen, International Conference on Machine Learning. PMLR2021b</p>
<p>Wen Yang, Chong Li, Jiajun Zhang, Chengqing Zong, arXiv:2305.18098Bigtrans: Augmenting large language models with multilingual translation capability over 100 languages. 2023barXiv preprint</p>
<p>Low-rank adaptation of large language model rescoring for parameter-efficient speech recognition. Yu Yu, Chao-Han Huck Yang, Jari Kolehmainen, arXiv:2301.070692023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). Barry Haddow, Alexandra Birch, IEEE2023. 2023aarXiv preprintPrompting large language model for machine translation: A case study</p>
<p>Llamaadapter: Efficient fine-tuning of language models with zero-init attention. Renrui Zhang, Jiaming Han, arXiv:2303.161992023barXiv preprint</p>
<p>Robust data2vec: Noise-robust speech representation learning for asr by combining regression and improved contrastive learning. Qiu-Shi Zhu, Long Zhou, Jie Zhang, Shu-Jie Liu, Yu-Chen Hu, Li-Rong Dai, ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE2023</p>
<p>Multichannel av-wav2vec2: A framework for learning multichannel multi-modal speech representation. Qiushi Zhu, Jie Zhang, Yu Gu, Yuchen Hu, Lirong Dai, arXiv:2401.034682024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
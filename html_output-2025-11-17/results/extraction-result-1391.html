<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1391 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1391</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1391</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-3ebdd3db0dd91069fa0cd31cbf8308b60b1b565e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3ebdd3db0dd91069fa0cd31cbf8308b60b1b565e" target="_blank">Planning with Diffusion for Flexible Behavior Synthesis</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> This paper considers what it would look like to fold as much of the trajectory optimization pipeline as possible into the modeling problem, such that sampling from the model and planning with it become nearly identical.</p>
                <p><strong>Paper Abstract:</strong> Model-based reinforcement learning methods often use learning only for the purpose of estimating an approximate dynamics model, offloading the rest of the decision-making work to classical trajectory optimizers. While conceptually simple, this combination has a number of empirical shortcomings, suggesting that learned models may not be well-suited to standard trajectory optimization. In this paper, we consider what it would look like to fold as much of the trajectory optimization pipeline as possible into the modeling problem, such that sampling from the model and planning with it become nearly identical. The core of our technical approach lies in a diffusion probabilistic model that plans by iteratively denoising trajectories. We show how classifier-guided sampling and image inpainting can be reinterpreted as coherent planning strategies, explore the unusual and useful properties of diffusion-based planning methods, and demonstrate the effectiveness of our framework in control settings that emphasize long-horizon decision-making and test-time flexibility.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1391.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1391.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Diffuser</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Diffuser (trajectory-level diffusion probabilistic model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A trajectory-level denoising diffusion probabilistic model that generates full state-action trajectories non-autoregressively (as a 2D states x time / actions x time array) and performs planning by guided sampling/inpainting of trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Diffuser</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A diffusion probabilistic generative model over entire trajectories τ represented as a 2-D array [s_0 ... s_T; a_0 ... a_T]. The reverse denoising process p_θ(τ^{i-1} | τ^{i}) is parameterized by a U-Net-like temporal-convolutional architecture (temporal residual blocks, group norm, Mish activations). The model is trained to predict noise ε (simplified diffusion objective) and thus parameterizes a learned denoising / score field used to iteratively refine trajectory samples. Planning is performed by sampling from p_θ(τ) perturbed by h(τ), implemented via classifier-guided sampling (gradients of a learned return predictor J_φ) or inpainting-style conditioning for goal constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>trajectory-level diffusion generative world model (non-autoregressive neural simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Continuous control / reinforcement learning domains: Maze2D (long-horizon sparse-reward navigation), block stacking (robotic manipulation), D4RL locomotion benchmarks (HalfCheetah, Hopper, Walker2d).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Training objective: MSE between true diffusion noise ε and predicted ε_θ(τ^i, i) (simplified diffusion loss). Practical fidelity assessed by: task returns / benchmark scores (Maze2D returns/scores, block-stacking task success scores, D4RL normalized scores) and qualitative measures (visualizations of denoising process and stitched subsequences).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reported task performance used as proxy for model fidelity: Maze2D single-task average score 119.5; Maze2D Multi-task average 129.4; Block-stacking average 54.4 (score 100 = perfect stack); D4RL locomotion average 77.5 (mean over specified datasets). No single-step MSE or explicit next-state prediction error numbers reported.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Primarily a black-box deep generative model (U-Net conv nets), but has interpretable properties: the iterative denoising trajectory generation can be visualized (reverse diffusion process visualizations), and temporal compositionality (ability to stitch in-distribution subsequences) is demonstrated empirically. Internal latent factors are not claimed to be semantically disentangled.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of reverse denoising steps and generated trajectories, experiments demonstrating temporal compositionality (stitching subsequences) and inpainting visualizations; architectural transparency (2-D trajectory array representation and temporal receptive field) used to explain behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Training: models trained for 500k steps with Adam (lr=4e-5), batch size 32. Architecture: U-Net with 6 residual temporal blocks (exact parameter count not reported). Inference: iterative denoising with N diffusion steps (N=20 for locomotion tasks, N=100 for block-stacking in experiments). Planning latency is dominated by the number of denoising steps; warm-starting (re-inject noise into previous plan and denoise fewer steps) can reduce runtime substantially (authors report being able to reduce denoising steps to ~1/10 with modest performance loss).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Not computationally cheaper than classical single-step dynamics models at inference due to iterative generation; however, in benchmarks (Maze2D, block stacking) Diffuser achieved superior task performance compared to model-free baselines and outperformed MPPI using ground-truth dynamics in long-horizon sparse-reward Maze2D, indicating better task efficiency per generated plan despite higher per-plan compute. When used as a drop-in dynamics model with conventional trajectory optimizers (e.g., MPPI) performance was poor (no better than random), indicating that computational efficiency gains are not realized by treating Diffuser as a traditional dynamics model.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Strong empirical task performance: Maze2D U-Maze 113.9 ± 3.1 (single-task), Medium 121.5 ± 2.7, Large 123.0 ± 6.4; Multi2D averages in the 127–132 range. Block stacking: unconditional 58.7 ± 2.5, conditional 45.6 ± 3.1, rearrangement 58.9 ± 3.4 (average 54.4). D4RL locomotion: various per-environment scores reported; overall average 77.5 (comparable or better than many baselines but not uniformly best).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Designing the model to generate whole trajectories (rather than single-step predictions) trades single-step predictive accuracy for trajectory-level fidelity and planning utility: this leads to improved long-horizon planning under sparse rewards and test-time flexibility (goal conditioning and composing reward guides) because sampled trajectories reflect learned feasible behaviors. Authors report that high-quality trajectory generation (even if not superior single-step accuracy) translates to improved planning performance in long-horizon tasks; conversely, using Diffuser purely as a dynamics model in standard trajectory optimizers did not yield good control, highlighting coupling between model form and planner.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Key trade-offs identified: (1) Fidelity vs compute: iterative denoising gives high-quality trajectory samples but is computationally expensive per plan (many diffusion steps). Warm-starting reduces steps but may slightly lower performance. (2) Non-autoregressive global coherence vs Markovian simplicity: predicting all timesteps concurrently enables temporal compositionality and non-greedy planning but loses temporal causality and requires architectural design (temporal locality via receptive field) to build coherence. (3) Model-agnostic rewards: Diffuser prioritizes task-agnostic dynamics/behavior prior (reward-agnostic training) enabling reuse across tasks but requires lightweight guides at test time. (4) Using Diffuser as a conventional dynamics model is ineffective (poor performance), indicating that some efficiency/compatibility trade-offs exist with classical planners.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Predict whole trajectories non-autoregressively; represent trajectories as 2-D arrays combining states and actions per timestep; use temporal-convolutional U-Net with small receptive fields per denoising step to build global coherence through many steps; train to predict diffusion noise ε (simplified denoising objective); separate dynamics prior p_θ(τ) from auxiliary perturbation h(τ) for reward or constraints; use a learned return predictor J_φ whose gradients guide sampling; choose diffusion hyperparameters (covariances follow cosine schedule), typical N values per domain (20 or 100), guide scale α (0.1 default, tuned per task).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared empirically to model-free baselines (CQL, IQL, BCQ, BC), return-conditioned sequence models (Decision Transformer), and other model-based approaches (Trajectory Transformer, MPPI, MOPO, MOReL, MBOP): Diffuser substantially outperformed model-free baselines in Maze2D and block-stacking (especially on long-horizon and multi-task settings), was competitive on D4RL locomotion (average comparable to strong baselines), and outperformed MPPI with ground-truth dynamics on hard long-horizon Maze2D tasks. However, integrating Diffuser with classical trajectory optimizers performed poorly, indicating that Diffuser's planning utility arises from its coupled sampling-guidance paradigm rather than from raw predictive accuracy as a drop-in dynamics model.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommendations and empirical settings: train the model to optimize trajectory-level denoising objective (MSE on predicted noise) rather than single-step error; use temporal-local convolutional architecture (small receptive field per denoising step) and many denoising steps to build global coherence; separate dynamics prior from reward via guides so a single model can be reused across tasks; tune guide scale α and number of diffusion steps N per domain (authors used N=20 for locomotion, N=100 for block stacking; α=0.1 typically); use warm-starting (partial forward diffusion of previous plan then fewer denoising steps) to trade off runtime and performance. The paper argues an "optimal" world model for their setting emphasizes trajectory-level generation fidelity and planner-model coupling rather than single-step predictive accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>All specifics (scores, N, training steps, hyperparameters) are taken directly from the paper. Exact parameter counts and wall-clock training durations / GPU counts are not reported in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Planning with Diffusion for Flexible Behavior Synthesis', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1391.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1391.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Trajectory Transformer (TT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Trajectory Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive Transformer-based sequence model over state-action-return trajectories used for offline planning and control by modeling next-step distributions or sequence likelihoods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Trajectory Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Trajectory Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive Transformer sequence model trained on trajectories to model behavior and support planning via sampling or likelihood-based selection (cited as a baseline and compared in D4RL experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer-based autoregressive world model / sequence model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Offline reinforcement learning (D4RL locomotion benchmarks and other sequence modeling for control).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Not specified in this paper (but evaluated via task returns when used as a policy/planner).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Task scores are reported in the D4RL comparison table (TT had strong per-task scores on some locomotion datasets), but exact predictive fidelity metrics (e.g., MSE) are not given here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Used as a baseline; Diffuser is compared against TT in D4RL results (Diffuser comparable on average but differences depend on task). Exact compute comparisons are not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Per-task D4RL scores shown in Table 2 (TT scores reported from Janner et al. 2021), but detailed runtime/compute trade-offs vs Diffuser are not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Included as a strong baseline for offline RL; no in-paper analysis beyond empirical comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Not analyzed in this paper beyond comparative results.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Not detailed here beyond being an autoregressive Transformer sequence model (reference cited).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared empirically in D4RL table; Diffuser achieves comparable average performance but with different planning-generation trade-offs (Diffuser emphasizes non-autoregressive trajectory sampling and guided denoising).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Planning with Diffusion for Flexible Behavior Synthesis', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1391.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1391.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Decision Transformer (DT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decision Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequence modeling approach that conditions on desired returns to produce actions from trajectories using Transformer architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Decision transformer: Reinforcement learning via sequence modeling</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Decision Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based return-conditioned sequence model that maps conditioning (past states, actions, desired return) to actions, used as a baseline in offline RL comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer-based return-conditioned sequence model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Offline RL (D4RL locomotion benchmarks and other sequence modeling tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Not specified in this paper; evaluated via task returns in benchmark tables.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>DT scores are listed in the D4RL comparison table (from Chen et al. 2021b); no next-state predictive fidelity numbers are provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Used as a baseline; comparison limited to task scores.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Included in D4RL comparison table; Diffuser performs comparably on some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Not analyzed beyond empirical comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Not discussed beyond citation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared in tables; Diffuser differs in generating full trajectories non-autoregressively and using guided diffusion for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Planning with Diffusion for Flexible Behavior Synthesis', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1391.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1391.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Autoregressive / single-step dynamics models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Autoregressive single-step learned dynamics models (general)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard learned dynamics models that predict the next state given current state and action, typically used as surrogates for environment dynamics plugged into classical trajectory optimizers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Autoregressive single-step dynamics models</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Models (deterministic or probabilistic) that predict s_{t+1} = f_θ(s_t, a_t), trained with supervised losses on next-state prediction and used with trajectory optimization (MPPI, CEM, shooting methods).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>explicit learned dynamics / stepwise predictive world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Model-based reinforcement learning and control (various benchmarks).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Typically measured by single-step prediction error (MSE) or multi-step rollout error; in this paper the authors emphasize single-step error but note compounding rollout errors.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No quantitative fidelity numbers provided in this paper; authors state single-step models can suffer from compounding rollout errors and myopic failure modes in long-horizon planning.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Varies by architecture; not specifically discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Generally cheaper per-step inference than iterative diffusion sampling; used with fast trajectory optimizers (random shooting, CEM) enabling low-latency planning. Exact numbers not given.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Authors note these models are often more compatible with classical planners (low per-step compute) but can be exploited by optimizers producing adversarial trajectories; MPPI with ground-truth dynamics performed poorly in long-horizon Maze2D compared to Diffuser despite access to true dynamics, highlighting task difficulty rather than pure compute.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>When used with standard planners, performance can degrade in long-horizon sparse-reward tasks due to compounding errors and adversarial exploitation; specific benchmark numbers are not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Paper argues that single-step fidelity does not necessarily translate to useful long-horizon planning utility; Diffuser's trajectory-level modeling is designed to better serve long-horizon planning despite not being an explicit single-step dynamics surrogate.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Single-step models: lower inference cost per step, easier integration with classical planners, but suffer compounding errors and can be exploited by optimizers; Diffuser: higher per-plan compute but better long-horizon and compositional planning utility.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Typically autoregressive next-state prediction; Markovian inductive bias. Not a focus of novel design in this paper beyond critique.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Contrasted with Diffuser: Diffuser focuses on trajectory-level generation and planning-as-sampling, whereas single-step models emphasize stepwise predictive accuracy and compatibility with classical planners.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests that for long-horizon planning under sparse rewards, a model designed for trajectory-level generation (like Diffuser) may be preferable to single-step predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Planning with Diffusion for Flexible Behavior Synthesis', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1391.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1391.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Diffusion models (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Denoising diffusion probabilistic models (DDPM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generative models that learn to reverse a gradual noising process via iterative denoising transitions, parameterized by neural networks, and usable for flexible conditional sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Denoising diffusion probabilistic models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Diffusion probabilistic models</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Probabilistic generative models formulating data generation as a reverse denoising Markov chain p_θ(τ^{i-1}|τ^{i}), trained with variational bounds / simplified noise-prediction losses; used here specialized to trajectories rather than images.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>generative diffusion model / score-based generative model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Originally image/audio generation; in this work adapted to trajectory generation for control.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Trained via noise-prediction MSE; fidelity commonly assessed by sample quality measures (not numerically detailed here for trajectories).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not specified numerically for trajectories beyond resulting task scores when guided for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Iterative sampling affords inspectable denoising steps (visualizable).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualizations of denoising process and classifier-guided sampling analogies (image inpainting) used to explain conditional sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Iterative sampling cost proportional to number of diffusion timesteps N; authors used N between 20 and 100 depending on task and discuss warm-starting to reduce cost.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Iterative nature is more expensive than single-step predictors per sample, but supports flexible conditioning and compositionality not readily available in single-step models.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>When specialized to trajectories (Diffuser), yields strong planning performance on selected benchmarks; general diffusion model performance metrics not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Diffusion's iterative, gradient-based sampling lends itself to conditioning with auxiliary guides (rewards, constraints), enabling compositional, flexible planning.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Expressive conditional sampling and compositionality vs higher inference cost due to many denoising steps.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Authors adopt cosine noise schedule (Nichol & Dhariwal 2021), simplified ε prediction objective, and temporal-convolutional U-Net architecture specialized to trajectory arrays.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared conceptually and empirically to autoregressive/stepwise models and Transformers; diffusion-based planning provides flexible conditioning and multi-task reuse but at higher per-plan compute cost.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Authors recommend choosing N and guide scale α per domain and using warm-starting when runtime constrained.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Planning with Diffusion for Flexible Behavior Synthesis', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1391.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1391.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Non-autoregressive trajectory models (Lambert et al. 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Non-autoregressive trajectory-level dynamics models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work studying non-autoregressive trajectory-level models for long-horizon prediction; cited as related work motivating trajectory-level modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning accurate long-term dynamics for model-based reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Non-autoregressive trajectory-level dynamics models</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Models that predict multiple future timesteps concurrently rather than autoregressively; specifics are in the cited work (Lambert et al., 2020).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>non-autoregressive trajectory predictors</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Long-horizon dynamics prediction for model-based RL.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Mentioned as related; authors position Diffuser as an alternative instantiation with diffusion-based sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Cited to show prior interest in non-autoregressive trajectory modeling for long-term prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Diffuser is a diffusion-based, non-autoregressive trajectory model; Lambert et al. offered other non-autoregressive approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Planning with Diffusion for Flexible Behavior Synthesis', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1391.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1391.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Energy-based models (EBMs)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Energy-Based Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Models that associate an (unnormalized) energy to configurations and sample or optimize under that energy; cited as a line of related work for planning and generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Energy-Based Models (EBMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Implicit generative models that define energies over state/action sequences and can be used for planning via sampling or optimization; referenced as prior work connecting denoising diffusion to score-based/EBM methods.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>energy-based world model / implicit generative model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Model-based planning and generation (general).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Sampling from EBMs typically requires iterative methods (MCMC-like) similar to diffusion; not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Mentioned conceptually as related to diffusion/score-based methods; no direct empirical comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Cited as conceptual relatives to diffusion models; diffusion planning leverages similar iterative gradient-based sampling benefits (flexible conditioning).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Not elaborated beyond conceptual relation to diffusion.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Diffusion models are connected to EBMs via score matching and provide an explicit denoising chain facilitating conditional sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Planning with Diffusion for Flexible Behavior Synthesis', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1391.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1391.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Latent-space world models (e.g., Dreamer / TransDreamer)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Latent-space learned world models (e.g., Dreamer family / TransDreamer references)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Models that learn compact latent representations and dynamics in latent space for planning/policy learning; cited among related model-based approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>TransDreamer: Reinforcement learning with transformer world models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Latent-space world models (Dreamer / TransDreamer etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encode observations into latent variables and learn latent dynamics (often recurrent or transformer-based), enabling planning or policy learning in a compressed latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Model-based RL, planning from pixels and other high-dimensional observations.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Typically measured by latent predictive likelihoods or downstream policy returns; not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Mentioned as an alternative approach in related work; no empirical comparison here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Cited as part of the landscape of world models; Diffuser differs by modeling full trajectories directly rather than encoding latent dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Not specifically analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Positioned as one category of model-based approaches; Diffuser emphasizes trajectory-level generative modeling rather than latent autoregressive dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Planning with Diffusion for Flexible Behavior Synthesis', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Denoising diffusion probabilistic models <em>(Rating: 2)</em></li>
                <li>Improved denoising diffusion probabilistic models <em>(Rating: 2)</em></li>
                <li>Trajectory Transformer <em>(Rating: 2)</em></li>
                <li>Decision transformer: Reinforcement learning via sequence modeling <em>(Rating: 2)</em></li>
                <li>Learning accurate long-term dynamics for model-based reinforcement learning <em>(Rating: 2)</em></li>
                <li>Model based planning with energy based models <em>(Rating: 1)</em></li>
                <li>Recurrent world models facilitate policy evolution <em>(Rating: 1)</em></li>
                <li>Model based reinforcement learning for atari <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1391",
    "paper_id": "paper-3ebdd3db0dd91069fa0cd31cbf8308b60b1b565e",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "Diffuser",
            "name_full": "Diffuser (trajectory-level diffusion probabilistic model)",
            "brief_description": "A trajectory-level denoising diffusion probabilistic model that generates full state-action trajectories non-autoregressively (as a 2D states x time / actions x time array) and performs planning by guided sampling/inpainting of trajectories.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Diffuser",
            "model_description": "A diffusion probabilistic generative model over entire trajectories τ represented as a 2-D array [s_0 ... s_T; a_0 ... a_T]. The reverse denoising process p_θ(τ^{i-1} | τ^{i}) is parameterized by a U-Net-like temporal-convolutional architecture (temporal residual blocks, group norm, Mish activations). The model is trained to predict noise ε (simplified diffusion objective) and thus parameterizes a learned denoising / score field used to iteratively refine trajectory samples. Planning is performed by sampling from p_θ(τ) perturbed by h(τ), implemented via classifier-guided sampling (gradients of a learned return predictor J_φ) or inpainting-style conditioning for goal constraints.",
            "model_type": "trajectory-level diffusion generative world model (non-autoregressive neural simulator)",
            "task_domain": "Continuous control / reinforcement learning domains: Maze2D (long-horizon sparse-reward navigation), block stacking (robotic manipulation), D4RL locomotion benchmarks (HalfCheetah, Hopper, Walker2d).",
            "fidelity_metric": "Training objective: MSE between true diffusion noise ε and predicted ε_θ(τ^i, i) (simplified diffusion loss). Practical fidelity assessed by: task returns / benchmark scores (Maze2D returns/scores, block-stacking task success scores, D4RL normalized scores) and qualitative measures (visualizations of denoising process and stitched subsequences).",
            "fidelity_performance": "Reported task performance used as proxy for model fidelity: Maze2D single-task average score 119.5; Maze2D Multi-task average 129.4; Block-stacking average 54.4 (score 100 = perfect stack); D4RL locomotion average 77.5 (mean over specified datasets). No single-step MSE or explicit next-state prediction error numbers reported.",
            "interpretability_assessment": "Primarily a black-box deep generative model (U-Net conv nets), but has interpretable properties: the iterative denoising trajectory generation can be visualized (reverse diffusion process visualizations), and temporal compositionality (ability to stitch in-distribution subsequences) is demonstrated empirically. Internal latent factors are not claimed to be semantically disentangled.",
            "interpretability_method": "Visualization of reverse denoising steps and generated trajectories, experiments demonstrating temporal compositionality (stitching subsequences) and inpainting visualizations; architectural transparency (2-D trajectory array representation and temporal receptive field) used to explain behavior.",
            "computational_cost": "Training: models trained for 500k steps with Adam (lr=4e-5), batch size 32. Architecture: U-Net with 6 residual temporal blocks (exact parameter count not reported). Inference: iterative denoising with N diffusion steps (N=20 for locomotion tasks, N=100 for block-stacking in experiments). Planning latency is dominated by the number of denoising steps; warm-starting (re-inject noise into previous plan and denoise fewer steps) can reduce runtime substantially (authors report being able to reduce denoising steps to ~1/10 with modest performance loss).",
            "efficiency_comparison": "Not computationally cheaper than classical single-step dynamics models at inference due to iterative generation; however, in benchmarks (Maze2D, block stacking) Diffuser achieved superior task performance compared to model-free baselines and outperformed MPPI using ground-truth dynamics in long-horizon sparse-reward Maze2D, indicating better task efficiency per generated plan despite higher per-plan compute. When used as a drop-in dynamics model with conventional trajectory optimizers (e.g., MPPI) performance was poor (no better than random), indicating that computational efficiency gains are not realized by treating Diffuser as a traditional dynamics model.",
            "task_performance": "Strong empirical task performance: Maze2D U-Maze 113.9 ± 3.1 (single-task), Medium 121.5 ± 2.7, Large 123.0 ± 6.4; Multi2D averages in the 127–132 range. Block stacking: unconditional 58.7 ± 2.5, conditional 45.6 ± 3.1, rearrangement 58.9 ± 3.4 (average 54.4). D4RL locomotion: various per-environment scores reported; overall average 77.5 (comparable or better than many baselines but not uniformly best).",
            "task_utility_analysis": "Designing the model to generate whole trajectories (rather than single-step predictions) trades single-step predictive accuracy for trajectory-level fidelity and planning utility: this leads to improved long-horizon planning under sparse rewards and test-time flexibility (goal conditioning and composing reward guides) because sampled trajectories reflect learned feasible behaviors. Authors report that high-quality trajectory generation (even if not superior single-step accuracy) translates to improved planning performance in long-horizon tasks; conversely, using Diffuser purely as a dynamics model in standard trajectory optimizers did not yield good control, highlighting coupling between model form and planner.",
            "tradeoffs_observed": "Key trade-offs identified: (1) Fidelity vs compute: iterative denoising gives high-quality trajectory samples but is computationally expensive per plan (many diffusion steps). Warm-starting reduces steps but may slightly lower performance. (2) Non-autoregressive global coherence vs Markovian simplicity: predicting all timesteps concurrently enables temporal compositionality and non-greedy planning but loses temporal causality and requires architectural design (temporal locality via receptive field) to build coherence. (3) Model-agnostic rewards: Diffuser prioritizes task-agnostic dynamics/behavior prior (reward-agnostic training) enabling reuse across tasks but requires lightweight guides at test time. (4) Using Diffuser as a conventional dynamics model is ineffective (poor performance), indicating that some efficiency/compatibility trade-offs exist with classical planners.",
            "design_choices": "Predict whole trajectories non-autoregressively; represent trajectories as 2-D arrays combining states and actions per timestep; use temporal-convolutional U-Net with small receptive fields per denoising step to build global coherence through many steps; train to predict diffusion noise ε (simplified denoising objective); separate dynamics prior p_θ(τ) from auxiliary perturbation h(τ) for reward or constraints; use a learned return predictor J_φ whose gradients guide sampling; choose diffusion hyperparameters (covariances follow cosine schedule), typical N values per domain (20 or 100), guide scale α (0.1 default, tuned per task).",
            "comparison_to_alternatives": "Compared empirically to model-free baselines (CQL, IQL, BCQ, BC), return-conditioned sequence models (Decision Transformer), and other model-based approaches (Trajectory Transformer, MPPI, MOPO, MOReL, MBOP): Diffuser substantially outperformed model-free baselines in Maze2D and block-stacking (especially on long-horizon and multi-task settings), was competitive on D4RL locomotion (average comparable to strong baselines), and outperformed MPPI with ground-truth dynamics on hard long-horizon Maze2D tasks. However, integrating Diffuser with classical trajectory optimizers performed poorly, indicating that Diffuser's planning utility arises from its coupled sampling-guidance paradigm rather than from raw predictive accuracy as a drop-in dynamics model.",
            "optimal_configuration": "Paper recommendations and empirical settings: train the model to optimize trajectory-level denoising objective (MSE on predicted noise) rather than single-step error; use temporal-local convolutional architecture (small receptive field per denoising step) and many denoising steps to build global coherence; separate dynamics prior from reward via guides so a single model can be reused across tasks; tune guide scale α and number of diffusion steps N per domain (authors used N=20 for locomotion, N=100 for block stacking; α=0.1 typically); use warm-starting (partial forward diffusion of previous plan then fewer denoising steps) to trade off runtime and performance. The paper argues an \"optimal\" world model for their setting emphasizes trajectory-level generation fidelity and planner-model coupling rather than single-step predictive accuracy.",
            "notes": "All specifics (scores, N, training steps, hyperparameters) are taken directly from the paper. Exact parameter counts and wall-clock training durations / GPU counts are not reported in the text.",
            "uuid": "e1391.0",
            "source_info": {
                "paper_title": "Planning with Diffusion for Flexible Behavior Synthesis",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Trajectory Transformer (TT)",
            "name_full": "Trajectory Transformer",
            "brief_description": "An autoregressive Transformer-based sequence model over state-action-return trajectories used for offline planning and control by modeling next-step distributions or sequence likelihoods.",
            "citation_title": "Trajectory Transformer",
            "mention_or_use": "mention",
            "model_name": "Trajectory Transformer",
            "model_description": "Autoregressive Transformer sequence model trained on trajectories to model behavior and support planning via sampling or likelihood-based selection (cited as a baseline and compared in D4RL experiments).",
            "model_type": "transformer-based autoregressive world model / sequence model",
            "task_domain": "Offline reinforcement learning (D4RL locomotion benchmarks and other sequence modeling for control).",
            "fidelity_metric": "Not specified in this paper (but evaluated via task returns when used as a policy/planner).",
            "fidelity_performance": "Task scores are reported in the D4RL comparison table (TT had strong per-task scores on some locomotion datasets), but exact predictive fidelity metrics (e.g., MSE) are not given here.",
            "interpretability_assessment": "Not discussed in this paper.",
            "interpretability_method": "Not discussed in this paper.",
            "computational_cost": "Not discussed in this paper.",
            "efficiency_comparison": "Used as a baseline; Diffuser is compared against TT in D4RL results (Diffuser comparable on average but differences depend on task). Exact compute comparisons are not reported.",
            "task_performance": "Per-task D4RL scores shown in Table 2 (TT scores reported from Janner et al. 2021), but detailed runtime/compute trade-offs vs Diffuser are not provided here.",
            "task_utility_analysis": "Included as a strong baseline for offline RL; no in-paper analysis beyond empirical comparison.",
            "tradeoffs_observed": "Not analyzed in this paper beyond comparative results.",
            "design_choices": "Not detailed here beyond being an autoregressive Transformer sequence model (reference cited).",
            "comparison_to_alternatives": "Compared empirically in D4RL table; Diffuser achieves comparable average performance but with different planning-generation trade-offs (Diffuser emphasizes non-autoregressive trajectory sampling and guided denoising).",
            "optimal_configuration": "Not discussed in this paper.",
            "uuid": "e1391.1",
            "source_info": {
                "paper_title": "Planning with Diffusion for Flexible Behavior Synthesis",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Decision Transformer (DT)",
            "name_full": "Decision Transformer",
            "brief_description": "A sequence modeling approach that conditions on desired returns to produce actions from trajectories using Transformer architectures.",
            "citation_title": "Decision transformer: Reinforcement learning via sequence modeling",
            "mention_or_use": "mention",
            "model_name": "Decision Transformer",
            "model_description": "Transformer-based return-conditioned sequence model that maps conditioning (past states, actions, desired return) to actions, used as a baseline in offline RL comparisons.",
            "model_type": "transformer-based return-conditioned sequence model",
            "task_domain": "Offline RL (D4RL locomotion benchmarks and other sequence modeling tasks).",
            "fidelity_metric": "Not specified in this paper; evaluated via task returns in benchmark tables.",
            "fidelity_performance": "DT scores are listed in the D4RL comparison table (from Chen et al. 2021b); no next-state predictive fidelity numbers are provided here.",
            "interpretability_assessment": "Not discussed in this paper.",
            "interpretability_method": "Not discussed in this paper.",
            "computational_cost": "Not discussed in this paper.",
            "efficiency_comparison": "Used as a baseline; comparison limited to task scores.",
            "task_performance": "Included in D4RL comparison table; Diffuser performs comparably on some tasks.",
            "task_utility_analysis": "Not analyzed beyond empirical comparison.",
            "tradeoffs_observed": "Not discussed here.",
            "design_choices": "Not discussed beyond citation.",
            "comparison_to_alternatives": "Compared in tables; Diffuser differs in generating full trajectories non-autoregressively and using guided diffusion for planning.",
            "optimal_configuration": "Not discussed in this paper.",
            "uuid": "e1391.2",
            "source_info": {
                "paper_title": "Planning with Diffusion for Flexible Behavior Synthesis",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Autoregressive / single-step dynamics models",
            "name_full": "Autoregressive single-step learned dynamics models (general)",
            "brief_description": "Standard learned dynamics models that predict the next state given current state and action, typically used as surrogates for environment dynamics plugged into classical trajectory optimizers.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Autoregressive single-step dynamics models",
            "model_description": "Models (deterministic or probabilistic) that predict s_{t+1} = f_θ(s_t, a_t), trained with supervised losses on next-state prediction and used with trajectory optimization (MPPI, CEM, shooting methods).",
            "model_type": "explicit learned dynamics / stepwise predictive world model",
            "task_domain": "Model-based reinforcement learning and control (various benchmarks).",
            "fidelity_metric": "Typically measured by single-step prediction error (MSE) or multi-step rollout error; in this paper the authors emphasize single-step error but note compounding rollout errors.",
            "fidelity_performance": "No quantitative fidelity numbers provided in this paper; authors state single-step models can suffer from compounding rollout errors and myopic failure modes in long-horizon planning.",
            "interpretability_assessment": "Varies by architecture; not specifically discussed here.",
            "interpretability_method": "Not discussed in this paper.",
            "computational_cost": "Generally cheaper per-step inference than iterative diffusion sampling; used with fast trajectory optimizers (random shooting, CEM) enabling low-latency planning. Exact numbers not given.",
            "efficiency_comparison": "Authors note these models are often more compatible with classical planners (low per-step compute) but can be exploited by optimizers producing adversarial trajectories; MPPI with ground-truth dynamics performed poorly in long-horizon Maze2D compared to Diffuser despite access to true dynamics, highlighting task difficulty rather than pure compute.",
            "task_performance": "When used with standard planners, performance can degrade in long-horizon sparse-reward tasks due to compounding errors and adversarial exploitation; specific benchmark numbers are not provided here.",
            "task_utility_analysis": "Paper argues that single-step fidelity does not necessarily translate to useful long-horizon planning utility; Diffuser's trajectory-level modeling is designed to better serve long-horizon planning despite not being an explicit single-step dynamics surrogate.",
            "tradeoffs_observed": "Single-step models: lower inference cost per step, easier integration with classical planners, but suffer compounding errors and can be exploited by optimizers; Diffuser: higher per-plan compute but better long-horizon and compositional planning utility.",
            "design_choices": "Typically autoregressive next-state prediction; Markovian inductive bias. Not a focus of novel design in this paper beyond critique.",
            "comparison_to_alternatives": "Contrasted with Diffuser: Diffuser focuses on trajectory-level generation and planning-as-sampling, whereas single-step models emphasize stepwise predictive accuracy and compatibility with classical planners.",
            "optimal_configuration": "Paper suggests that for long-horizon planning under sparse rewards, a model designed for trajectory-level generation (like Diffuser) may be preferable to single-step predictors.",
            "uuid": "e1391.3",
            "source_info": {
                "paper_title": "Planning with Diffusion for Flexible Behavior Synthesis",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Diffusion models (general)",
            "name_full": "Denoising diffusion probabilistic models (DDPM)",
            "brief_description": "Generative models that learn to reverse a gradual noising process via iterative denoising transitions, parameterized by neural networks, and usable for flexible conditional sampling.",
            "citation_title": "Denoising diffusion probabilistic models",
            "mention_or_use": "mention",
            "model_name": "Diffusion probabilistic models",
            "model_description": "Probabilistic generative models formulating data generation as a reverse denoising Markov chain p_θ(τ^{i-1}|τ^{i}), trained with variational bounds / simplified noise-prediction losses; used here specialized to trajectories rather than images.",
            "model_type": "generative diffusion model / score-based generative model",
            "task_domain": "Originally image/audio generation; in this work adapted to trajectory generation for control.",
            "fidelity_metric": "Trained via noise-prediction MSE; fidelity commonly assessed by sample quality measures (not numerically detailed here for trajectories).",
            "fidelity_performance": "Not specified numerically for trajectories beyond resulting task scores when guided for planning.",
            "interpretability_assessment": "Iterative sampling affords inspectable denoising steps (visualizable).",
            "interpretability_method": "Visualizations of denoising process and classifier-guided sampling analogies (image inpainting) used to explain conditional sampling.",
            "computational_cost": "Iterative sampling cost proportional to number of diffusion timesteps N; authors used N between 20 and 100 depending on task and discuss warm-starting to reduce cost.",
            "efficiency_comparison": "Iterative nature is more expensive than single-step predictors per sample, but supports flexible conditioning and compositionality not readily available in single-step models.",
            "task_performance": "When specialized to trajectories (Diffuser), yields strong planning performance on selected benchmarks; general diffusion model performance metrics not provided here.",
            "task_utility_analysis": "Diffusion's iterative, gradient-based sampling lends itself to conditioning with auxiliary guides (rewards, constraints), enabling compositional, flexible planning.",
            "tradeoffs_observed": "Expressive conditional sampling and compositionality vs higher inference cost due to many denoising steps.",
            "design_choices": "Authors adopt cosine noise schedule (Nichol & Dhariwal 2021), simplified ε prediction objective, and temporal-convolutional U-Net architecture specialized to trajectory arrays.",
            "comparison_to_alternatives": "Compared conceptually and empirically to autoregressive/stepwise models and Transformers; diffusion-based planning provides flexible conditioning and multi-task reuse but at higher per-plan compute cost.",
            "optimal_configuration": "Authors recommend choosing N and guide scale α per domain and using warm-starting when runtime constrained.",
            "uuid": "e1391.4",
            "source_info": {
                "paper_title": "Planning with Diffusion for Flexible Behavior Synthesis",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Non-autoregressive trajectory models (Lambert et al. 2020)",
            "name_full": "Non-autoregressive trajectory-level dynamics models",
            "brief_description": "Prior work studying non-autoregressive trajectory-level models for long-horizon prediction; cited as related work motivating trajectory-level modeling.",
            "citation_title": "Learning accurate long-term dynamics for model-based reinforcement learning",
            "mention_or_use": "mention",
            "model_name": "Non-autoregressive trajectory-level dynamics models",
            "model_description": "Models that predict multiple future timesteps concurrently rather than autoregressively; specifics are in the cited work (Lambert et al., 2020).",
            "model_type": "non-autoregressive trajectory predictors",
            "task_domain": "Long-horizon dynamics prediction for model-based RL.",
            "fidelity_metric": "Not specified in this paper.",
            "fidelity_performance": "Not specified in this paper.",
            "interpretability_assessment": "Not discussed in this paper.",
            "interpretability_method": "Not discussed in this paper.",
            "computational_cost": "Not discussed in this paper.",
            "efficiency_comparison": "Mentioned as related; authors position Diffuser as an alternative instantiation with diffusion-based sampling.",
            "task_performance": "Not discussed in this paper.",
            "task_utility_analysis": "Cited to show prior interest in non-autoregressive trajectory modeling for long-term prediction.",
            "tradeoffs_observed": "Not discussed here.",
            "design_choices": "Not detailed here.",
            "comparison_to_alternatives": "Diffuser is a diffusion-based, non-autoregressive trajectory model; Lambert et al. offered other non-autoregressive approaches.",
            "optimal_configuration": "Not discussed in this paper.",
            "uuid": "e1391.5",
            "source_info": {
                "paper_title": "Planning with Diffusion for Flexible Behavior Synthesis",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Energy-based models (EBMs)",
            "name_full": "Energy-Based Models",
            "brief_description": "Models that associate an (unnormalized) energy to configurations and sample or optimize under that energy; cited as a line of related work for planning and generation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Energy-Based Models (EBMs)",
            "model_description": "Implicit generative models that define energies over state/action sequences and can be used for planning via sampling or optimization; referenced as prior work connecting denoising diffusion to score-based/EBM methods.",
            "model_type": "energy-based world model / implicit generative model",
            "task_domain": "Model-based planning and generation (general).",
            "fidelity_metric": "Not specified here.",
            "fidelity_performance": "Not specified here.",
            "interpretability_assessment": "Not discussed in this paper.",
            "interpretability_method": "Not discussed here.",
            "computational_cost": "Sampling from EBMs typically requires iterative methods (MCMC-like) similar to diffusion; not quantified here.",
            "efficiency_comparison": "Mentioned conceptually as related to diffusion/score-based methods; no direct empirical comparison provided.",
            "task_performance": "Not reported in this paper.",
            "task_utility_analysis": "Cited as conceptual relatives to diffusion models; diffusion planning leverages similar iterative gradient-based sampling benefits (flexible conditioning).",
            "tradeoffs_observed": "Not elaborated beyond conceptual relation to diffusion.",
            "design_choices": "Not detailed here.",
            "comparison_to_alternatives": "Diffusion models are connected to EBMs via score matching and provide an explicit denoising chain facilitating conditional sampling.",
            "optimal_configuration": "Not discussed here.",
            "uuid": "e1391.6",
            "source_info": {
                "paper_title": "Planning with Diffusion for Flexible Behavior Synthesis",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Latent-space world models (e.g., Dreamer / TransDreamer)",
            "name_full": "Latent-space learned world models (e.g., Dreamer family / TransDreamer references)",
            "brief_description": "Models that learn compact latent representations and dynamics in latent space for planning/policy learning; cited among related model-based approaches.",
            "citation_title": "TransDreamer: Reinforcement learning with transformer world models",
            "mention_or_use": "mention",
            "model_name": "Latent-space world models (Dreamer / TransDreamer etc.)",
            "model_description": "Encode observations into latent variables and learn latent dynamics (often recurrent or transformer-based), enabling planning or policy learning in a compressed latent space.",
            "model_type": "latent world model",
            "task_domain": "Model-based RL, planning from pixels and other high-dimensional observations.",
            "fidelity_metric": "Typically measured by latent predictive likelihoods or downstream policy returns; not specified in this paper.",
            "fidelity_performance": "Not provided in this paper.",
            "interpretability_assessment": "Not discussed in this paper.",
            "interpretability_method": "Not discussed in this paper.",
            "computational_cost": "Not reported here.",
            "efficiency_comparison": "Mentioned as an alternative approach in related work; no empirical comparison here.",
            "task_performance": "Not reported here.",
            "task_utility_analysis": "Cited as part of the landscape of world models; Diffuser differs by modeling full trajectories directly rather than encoding latent dynamics.",
            "tradeoffs_observed": "Not specifically analyzed here.",
            "design_choices": "Not detailed here.",
            "comparison_to_alternatives": "Positioned as one category of model-based approaches; Diffuser emphasizes trajectory-level generative modeling rather than latent autoregressive dynamics.",
            "optimal_configuration": "Not discussed in this paper.",
            "uuid": "e1391.7",
            "source_info": {
                "paper_title": "Planning with Diffusion for Flexible Behavior Synthesis",
                "publication_date_yy_mm": "2022-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Denoising diffusion probabilistic models",
            "rating": 2
        },
        {
            "paper_title": "Improved denoising diffusion probabilistic models",
            "rating": 2
        },
        {
            "paper_title": "Trajectory Transformer",
            "rating": 2
        },
        {
            "paper_title": "Decision transformer: Reinforcement learning via sequence modeling",
            "rating": 2
        },
        {
            "paper_title": "Learning accurate long-term dynamics for model-based reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Model based planning with energy based models",
            "rating": 1
        },
        {
            "paper_title": "Recurrent world models facilitate policy evolution",
            "rating": 1
        },
        {
            "paper_title": "Model based reinforcement learning for atari",
            "rating": 1
        }
    ],
    "cost": 0.018342999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Planning with Diffusion for Flexible Behavior Synthesis</h1>
<p>Michael Janner<em> ${ }^{</em> 1}$ Yilun Du*2 Joshua B. Tenenbaum ${ }^{2}$ Sergey Levine ${ }^{1}$</p>
<h4>Abstract</h4>
<p>Model-based reinforcement learning methods often use learning only for the purpose of estimating an approximate dynamics model, offloading the rest of the decision-making work to classical trajectory optimizers. While conceptually simple, this combination has a number of empirical shortcomings, suggesting that learned models may not be well-suited to standard trajectory optimization. In this paper, we consider what it would look like to fold as much of the trajectory optimization pipeline as possible into the modeling problem, such that sampling from the model and planning with it become nearly identical. The core of our technical approach lies in a diffusion probabilistic model that plans by iteratively denoising trajectories. We show how classifier-guided sampling and image inpainting can be reinterpreted as coherent planning strategies, explore the unusual and useful properties of diffusion-based planning methods, and demonstrate the effectiveness of our framework in control settings that emphasize longhorizon decision-making and test-time flexibility.</p>
<h2>1 Introduction</h2>
<p>Planning with a learned model is a conceptually simple framework for reinforcement learning and data-driven decision-making. Its appeal comes from employing learning techniques only where they are the most mature and effective: for the approximation of unknown environment dynamics in what amounts to a supervised learning problem. Afterwards, the learned model may be plugged into classical trajectory optimization routines (Tassa et al., 2012; Posa et al., 2014; Kelly, 2017), which are similarly wellunderstood in their original context.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Diffuser is a diffusion probabilistic model that plans by iteratively refining trajectories.</p>
<p>However, this combination rarely works as described. Because powerful trajectory optimizers exploit learned models, plans generated by this procedure often look more like adversarial examples than optimal trajectories (Talvitie, 2014; Ke et al., 2018). As a result, contemporary modelbased reinforcement learning algorithms often inherit more from model-free methods, such as value functions and policy gradients (Wang et al., 2019), than from the trajectory optimization toolbox. Those methods that do rely on online planning tend to use simple gradient-free trajectory optimization routines like random shooting (Nagabandi et al., 2018) or the cross-entropy method (Botev et al., 2013; Chua et al., 2018) to avoid the aforementioned issues.</p>
<p>In this work, we propose an alternative approach to datadriven trajectory optimization. The core idea is to train a model that is directly amenable to trajectory optimization, in the sense that sampling from the model and planning with it become nearly identical. This goal requires a shift in how the model is designed. Because learned dynamics models are normally meant to be proxies for environment dynamics, improvements are often achieved by structuring the model according to the underlying causal process (Bapst et al., 2019). Instead, we consider how to design a model in line with the planning problem in which it will be used. For example, because the model will ultimately be used for planning, action distributions are just as important as state dynamics and long-horizon accuracy is more important than single-step error. On the other hand, the model should remain agnostic to reward function so that it may be used</p>
<p>Code and visualizations of the learned denoising process are available at diffusion-planning. github.io.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Diffuser samples plans by iteratively denoising twodimensional arrays consisting of a variable number of state-action pairs. A small receptive field constrains the model to only enforce local consistency during a single denoising step. By composing many denoising steps together, local consistency can drive global coherence of a sampled plan. An optional guide function $\mathcal{J}$ can be used to bias plans toward those optimizing a test-time objective or satisfying a set of constraints.
in multiple tasks, including those unseen during training. Finally, the model should be designed so that its plans, and not just its predictions, improve with experience and are resistant to the myopic failure modes of standard shootingbased planning algorithms.</p>
<p>We instantiate this idea as a trajectory-level diffusion probabilistic model (Sohl-Dickstein et al., 2015; Ho et al., 2020) called Diffuser, visualized in Figure 2. Whereas standard model-based planning techniques predict forward in time autoregressively, Diffuser predicts all timesteps of a plan simultaneously. The iterative sampling process of diffusion models leads to flexible conditioning, allowing for auxiliary guides to modify the sampling procedure to recover trajectories with high return or satisfying a set of constraints. This formulation of data-driven trajectory optimization has several appealing properties:</p>
<p>Long-horizon scalability Diffuser is trained for the accuracy of its generated trajectories rather than its singlestep error, so it does not suffer from the compounding rollout errors of single-step dynamics models and scales more gracefully with respect to long planning horizon.</p>
<p>Task compositionality Reward functions provide auxiliary gradients to be used while sampling a plan, allowing for a straightforward way of planning by composing multiple rewards simultaneously by adding together their gradients.</p>
<p>Temporal compositionality Diffuser generates globally coherent trajectories by iteratively improving local consistency, allowing it to generalize to novel trajectories by stitching together in-distribution subsequences.</p>
<p>Effective non-greedy planning By blurring the line between model and planner, the training procedure that improves the model's predictions also has the effect of improving its planning capabilities. This design yields a learned planner that can solve the types of long-horizon, sparse-reward problems that prove difficult for many conventional planning methods.</p>
<p>The core contribution of this work is a denoising diffusion model designed for trajectory data and an associated probabilistic framework for behavior synthesis. While unconventional compared to the types of models routinely used in deep model-based reinforcement learning, we demonstrate that Diffuser has a number of useful properties and is particularly effective in offline control settings that require long-horizon reasoning and test-time flexibility.</p>
<h2>2 Background</h2>
<p>Our approach to planning is a learning-based analogue of past work in behavioral synthesis using trajectory optimization (Witkin \&amp; Kass, 1988; Tassa et al., 2012). In this section, we provide a brief background on the problem setting considered by trajectory optimization and the class of generative models we employ for that problem.</p>
<h3>2.1 Problem Setting</h3>
<p>Consider a system governed by the discrete-time dynamics $\mathbf{s}<em t="t">{t+1}=\boldsymbol{f}\left(\mathbf{s}</em>}, \mathbf{a<em t="t">{t}\right)$ at state $\mathbf{s}</em>}$ given an action $\mathbf{a<em 0:="0:" T="T">{t}$. Trajectory optimization refers to finding a sequence of actions $\mathbf{a}</em>}^{*}$ that maximizes (or minimizes) an objective $\mathcal{J}$ factorized over per-timestep rewards (or costs) $r\left(\mathbf{s<em t="t">{t}, \mathbf{a}</em>\right)$ :</p>
<p>$$
\mathbf{a}<em 0:="0:" T="T">{0: T}^{*}=\underset{\mathbf{a}</em>}}{\arg \max } \mathcal{J}\left(\mathbf{s<em 0:="0:" T="T">{0}, \mathbf{a}</em>}\right)=\underset{\mathbf{a<em t="0">{0: T}}{\arg \max } \sum</em>}^{T} r\left(\mathbf{s<em t="t">{t}, \mathbf{a}</em>\right)
$$</p>
<p>where $T$ is the planning horizon. We use the abbreviation $\boldsymbol{\tau}=\left(\mathbf{s}<em 0="0">{0}, \mathbf{a}</em>}, \mathbf{s<em 1="1">{1}, \mathbf{a}</em>}, \ldots, \mathbf{s<em T="T">{T}, \mathbf{a}</em>)$ to denote the objective value of that trajectory.}\right)$ to refer to a trajectory of interleaved states and actions and $\mathcal{J}(\boldsymbol{\tau</p>
<h3>2.2 Diffusion Probabilistic Models</h3>
<p>Diffusion probabilistic models (Sohl-Dickstein et al., 2015; Ho et al., 2020) pose the data-generating process as an iterative denoising procedure $p_{\theta}\left(\boldsymbol{\tau}^{i-1} \mid \boldsymbol{\tau}^{i}\right)$. This denoising is the reverse of a forward diffusion process $q\left(\boldsymbol{\tau}^{i} \mid \boldsymbol{\tau}^{i-1}\right)$ that slowly corrupts the structure in data by adding noise. The data distribution induced by the model is given by:</p>
<p>$$
p_{\theta}\left(\boldsymbol{\tau}^{0}\right)=\int p\left(\boldsymbol{\tau}^{N}\right) \prod_{i=1}^{N} p_{\theta}\left(\boldsymbol{\tau}^{i-1} \mid \boldsymbol{\tau}^{i}\right) \mathrm{d} \boldsymbol{\tau}^{1: N}
$$</p>
<p>where $p\left(\boldsymbol{\tau}^{N}\right)$ is a standard Gaussian prior and $\boldsymbol{\tau}^{0}$ denotes (noiseless) data. Parameters $\theta$ are optimized by minimizing</p>
<p>a variational bound on the negative log likelihood of the reverse process: $\theta^{*}=\arg \min <em _boldsymbol_tau="\boldsymbol{\tau">{\theta}-\mathbb{E}</em>\right)\right]$. The reverse process is often parameterized as Gaussian with fixed timestep-dependent covariances:}^{0}}\left[\log p_{\theta}\left(\boldsymbol{\tau}^{0</p>
<p>$$
p_{\theta}\left(\boldsymbol{\tau}^{i-1} \mid \boldsymbol{\tau}^{i}\right)=\mathcal{N}\left(\boldsymbol{\tau}^{i-1} \mid \mu_{\theta}\left(\boldsymbol{\tau}^{i}, i\right), \Sigma^{i}\right)
$$</p>
<p>The forward process $q\left(\boldsymbol{\tau}^{i} \mid \boldsymbol{\tau}^{i-1}\right)$ is typically prespecified.
Notation. There are two "times" at play in this work: that of the diffusion process and that of the planning problem. We use superscripts ( $i$ when unspecified) to denote diffusion timestep and subscripts ( $t$ when unspecified) to denote planning timestep. For example, $\mathbf{s}<em _mathbf_a="\mathbf{a">{t}^{0}$ refers to the $t^{\text {th }}$ state in a noiseless trajectory. When it is unambiguous from context, superscripts of noiseless quantities are omitted: $\boldsymbol{\tau}=\boldsymbol{\tau}^{0}$. We overload notation slightly by referring to the $t^{\text {th }}$ state (or action) in a trajectory $\boldsymbol{\tau}$ as $\boldsymbol{\tau}</em><em _mathbf_a="\mathbf{a">{t}}$ (or $\boldsymbol{\tau}</em>$ ).}_{t}</p>
<h2>3 Planning with Diffusion</h2>
<p>A major obstacle to using trajectory optimization techniques is that they require knowledge of the environment dynamics $\boldsymbol{f}$. Most learning-based methods attempt to overcome this obstacle by training an approximate dynamics model and plugging it in to a conventional planning routine. However, learned models are often poorly suited to the types of planning algorithms designed with ground-truth models in mind, leading to planners that exploit learned models by finding adversarial examples.</p>
<p>We propose a tighter coupling between modeling and planning. Instead of using a learned model in the context of a classical planner, we subsume as much of the planning process as possible into the generative modeling framework, such that planning becomes nearly identical to sampling. We do this using a diffusion model of trajectories, $p_{\theta}(\boldsymbol{\tau})$. The iterative denoising process of a diffusion model lends itself to flexible conditioning by way of sampling from perturbed distributions of the form:</p>
<p>$$
\tilde{p}<em _theta="\theta">{\theta}(\boldsymbol{\tau}) \propto p</em>)
$$}(\boldsymbol{\tau}) h(\boldsymbol{\tau</p>
<p>The function $h(\boldsymbol{\tau})$ can contain information about prior evidence (such as an observation history), desired outcomes (such as a goal to reach), or general functions to optimize (such as rewards or costs). Performing inference in this perturbed distribution can be seen as a probabilistic analogue to the trajectory optimization problem posed in Section 2.1, as it requires finding trajectories that are both physically realistic under $p_{\theta}(\boldsymbol{\tau})$ and high-reward (or constraint-satisfying) under $h(\boldsymbol{\tau})$. Because the dynamics information is separated from the perturbation distribution $h(\boldsymbol{\tau})$, a single diffusion model $p_{\theta}(\boldsymbol{\tau})$ may be reused for multiple tasks in the same environment.</p>
<p>In this section, we describe Diffuser, a diffusion model designed for learned trajectory optimization. We then discuss two specific instantiations of planning with Diffuser, realized as reinforcement learning counterparts to classifierguided sampling and image inpainting.</p>
<h3>3.1 A Generative Model for Trajectory Planning</h3>
<p>Temporal ordering. Blurring the line between sampling from a trajectory model and planning with it yields an unusual constraint: we can no longer predict states autoregressively in temporal order. Consider the goalconditioned inference $p\left(\mathbf{s}<em 0="0">{1} \mid \mathbf{s}</em>}, \mathbf{s<em 1="1">{T}\right)$; the next state $\mathbf{s}</em>$ Because we cannot use a temporal autoregressive ordering, we design Diffuser to predict all timesteps of a plan concurrently.}$ depends on a future state as well as a prior one. This example is an instance of a more general principle: while dynamics prediction is causal, in the sense that the present is determined by the past, decision-making and control can be anti-causal, in the sense that decisions in the present are conditional on the future. ${ }^{1</p>
<p>Temporal locality. Despite not being autoregressive or Markovian, Diffuser features a relaxed form of temporal locality. In Figure 2, we depict a dependency graph for a diffusion model consisting of a single temporal convolution. The receptive field of a given prediction only consists of nearby timesteps, both in the past and the future. As a result, each step of the denoising process can only make predictions based on local consistency of the trajectory. By composing many of these denoising steps together, however, local consistency can drive global coherence.</p>
<p>Trajectory representation. Diffuser is a model of trajectories designed for planning, meaning that the effectiveness of the controller derived from the model is just as important as the quality of the state predictions. As a result, states and actions in a trajectory are predicted jointly; for the purposes of prediction the actions are simply additional dimensions of the state. Specifically, we represent inputs (and outputs) of Diffuser as a two-dimensional array:</p>
<p>$$
\boldsymbol{\tau}=\left[\begin{array}{llll}
\mathbf{s}<em 1="1">{0} &amp; \mathbf{s}</em>} &amp; \cdots &amp; \mathbf{s<em 0="0">{T} \
\mathbf{a}</em>} &amp; \mathbf{a<em T="T">{1} &amp; \cdots &amp; \mathbf{a}</em>
\end{array}\right]
$$</p>
<p>with one column per timestep of the planning horizon.
Architecture. We now have the ingredients needed to specify a Diffuser architecture: (1) an entire trajectory should be predicted non-autoregressively, (2) each step of the denoising process should be temporally local,</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">Guided</span><span class="w"> </span><span class="n">Diffusion</span><span class="w"> </span><span class="n">Planning</span>
<span class="n">Require</span><span class="w"> </span><span class="n">Diffuser</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mu_</span><span class="p">{</span>\<span class="n">theta</span><span class="p">}</span>\<span class="p">),</span><span class="w"> </span><span class="n">guide</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">J</span><span class="p">}</span>\<span class="p">),</span><span class="w"> </span><span class="n">scale</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">alpha</span>\<span class="p">),</span><span class="w"> </span><span class="n">covariances</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">Sigma</span><span class="o">^</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="p">)</span>
<span class="k">while</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">done</span><span class="w"> </span><span class="n">do</span>
<span class="w">    </span><span class="n">Observe</span><span class="w"> </span><span class="n">state</span><span class="w"> </span><span class="n">s</span><span class="p">;</span><span class="w"> </span><span class="n">initialize</span><span class="w"> </span><span class="n">plan</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">boldsymbol</span><span class="p">{</span>\<span class="n">tau</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">N</span><span class="p">}</span><span class="w"> </span>\<span class="n">sim</span><span class="w"> </span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">N</span><span class="p">}(</span>\<span class="n">mathbf</span><span class="p">{</span><span class="mi">0</span><span class="p">},</span><span class="w"> </span>\<span class="n">boldsymbol</span><span class="p">{</span><span class="n">I</span><span class="p">})</span>\<span class="p">)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span>\<span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="n">N</span><span class="p">,</span><span class="w"> </span>\<span class="n">ldots</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span><span class="o">//</span><span class="w"> </span><span class="n">parameters</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">reverse</span><span class="w"> </span><span class="n">transition</span>
<span class="w">        </span>\<span class="p">(</span>\<span class="n">mu</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">mu_</span><span class="p">{</span>\<span class="n">theta</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span>\<span class="n">boldsymbol</span><span class="p">{</span>\<span class="n">tau</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">        </span><span class="o">//</span><span class="w"> </span><span class="n">guide</span><span class="w"> </span><span class="n">using</span><span class="w"> </span><span class="n">gradients</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="k">return</span>
<span class="w">        </span>\<span class="p">(</span>\<span class="n">boldsymbol</span><span class="p">{</span>\<span class="n">tau</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span><span class="w"> </span>\<span class="n">sim</span><span class="w"> </span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">N</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span>\<span class="n">mu</span><span class="o">+</span>\<span class="n">alpha</span><span class="w"> </span>\<span class="n">Sigma</span><span class="w"> </span>\<span class="n">nabla</span><span class="w"> </span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">J</span><span class="p">}(</span>\<span class="n">mu</span><span class="p">),</span><span class="w"> </span>\<span class="n">Sigma</span><span class="o">^</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
<span class="w">        </span><span class="o">//</span><span class="w"> </span><span class="n">constrain</span><span class="w"> </span><span class="n">first</span><span class="w"> </span><span class="n">state</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">plan</span>
<span class="w">        </span>\<span class="p">(</span>\<span class="n">boldsymbol</span><span class="p">{</span>\<span class="n">tau</span><span class="p">}</span><span class="n">_</span><span class="p">{</span>\<span class="n">mathbf</span><span class="p">{</span><span class="n">z</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="mi">0</span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">boldsymbol</span><span class="p">{</span><span class="n">s</span><span class="p">}</span>\<span class="p">)</span>
<span class="w">    </span><span class="n">Execute</span><span class="w"> </span><span class="n">first</span><span class="w"> </span><span class="n">action</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">plan</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">boldsymbol</span><span class="p">{</span>\<span class="n">tau</span><span class="p">}</span><span class="n">_</span><span class="p">{</span>\<span class="n">mathbf</span><span class="p">{</span><span class="n">a</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="mi">0</span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="mi">0</span><span class="p">}</span>\<span class="p">)</span>
</code></pre></div>

<p>and (3) the trajectory representation should allow for equivariance along one dimension (the planning horizon) but not the other (the state and action features). We satisfy these criteria with a model consisting of repeated (temporal) convolutional residual blocks. The overall architecture resembles the types of U-Nets that have found success in image-based diffusion models, but with two-dimensional spatial convolutions replaced by onedimensional temporal convolutions (Figure A1). Because the model is fully convolutional, the horizon of the predictions is determined not by the model architecture, but by the input dimensionality; it can change dynamically during planning if desired.</p>
<p>Training. We use Diffuser to parameterize a learned gradient $\epsilon_{\theta}\left(\boldsymbol{\tau}^{i}, i\right)$ of the trajectory denoising process, from which the mean $\mu_{\theta}$ can be solved in closed form (Ho et al., 2020). We use the simplified objective for training the $\epsilon$ model, given by:</p>
<p>$$
\mathcal{L}(\theta)=\mathbb{E}<em _theta="\theta">{i, \epsilon, \boldsymbol{\tau}^{0}}\left[\left|\epsilon-\epsilon</em>\right]
$$}\left(\boldsymbol{\tau}^{i}, i\right)\right|^{2</p>
<p>in which $i \sim \mathcal{U}{1,2, \ldots, N}$ is the diffusion timestep, $\epsilon \sim \mathcal{N}(\mathbf{0}, \boldsymbol{I})$ is the noise target, and $\boldsymbol{\tau}^{i}$ is the trajectory $\boldsymbol{\tau}^{0}$ corrupted with noise $\epsilon$. Reverse process covariances $\Sigma^{i}$ follow the cosine schedule of Nichol \&amp; Dhariwal (2021).</p>
<h3>3.2 Reinforcement Learning as Guided Sampling</h3>
<p>In order to solve reinforcement learning problems with Diffuser, we must introduce a notion of reward. We appeal to the control-as-inference graphical model (Levine, 2018) to do so. Let $\mathcal{O}<em t="t">{t}$ be a binary random variable denoting the optimality of timestep $t$ of a trajectory, with $p\left(\mathcal{O}</em>}=1\right)=\exp \left(r\left(\mathbf{s<em t="t">{t}, \mathbf{a}</em>\right)$ in Equation 1:}\right)\right)$. We can sample from the set of optimal trajectories by setting $h(\boldsymbol{\tau})=p\left(\mathcal{O}_{1: T} \mid \boldsymbol{\tau</p>
<p>$$
\tilde{p}<em 1:="1:" T="T">{\theta}(\boldsymbol{\tau})=p\left(\boldsymbol{\tau} \mid \mathcal{O}</em>\right)
$$}=1\right) \propto p(\boldsymbol{\tau}) p\left(\mathcal{O}_{1: T}=1 \mid \boldsymbol{\tau</p>
<p>We have exchanged the reinforcement learning problem for one of conditional sampling. Thankfully, there has been
much prior work on conditional sampling with diffusion models. While it is intractable to sample from this distribution exactly, when $p\left(\mathcal{O}_{1: T} \mid \boldsymbol{\tau}^{i}\right)$ is sufficiently smooth, the reverse diffusion process transitions can be approximated as Gaussian (Sohl-Dickstein et al., 2015):</p>
<p>$$
p_{\theta}\left(\boldsymbol{\tau}^{i-1} \mid \boldsymbol{\tau}^{i}, \mathcal{O}_{1: T}\right) \approx \mathcal{N}\left(\boldsymbol{\tau}^{i-1} ; \mu+\Sigma g, \Sigma\right)
$$</p>
<p>where $\mu, \Sigma$ are the parameters of the original reverse process transition $p_{\theta}\left(\boldsymbol{\tau}^{i-1} \mid \boldsymbol{\tau}^{i}\right)$ and</p>
<p>$$
\begin{aligned}
g &amp; =\left.\nabla_{\boldsymbol{\tau}} \log p\left(\mathcal{O}<em _boldsymbol_tau="\boldsymbol{\tau">{1: T} \mid \boldsymbol{\tau}\right)\right|</em> \
&amp; =\left.\sum_{t=0}^{T} \nabla_{\mathbf{s}}=\mu<em t="t">{t}, \mathbf{a}</em>}} r\left(\mathbf{s<em t="t">{t}, \mathbf{a}</em>\right)\right|<em t="t">{\left(\mathbf{s}</em>}, \mathbf{a<em t="t">{t}\right)=\mu</em>(\mu)
\end{aligned}
$$}}=\nabla \mathcal{J</p>
<p>This relation provides a straightforward translation between classifier-guided sampling, used to generate classconditional images (Dhariwal \&amp; Nichol, 2021), and the reinforcement learning problem setting. We first train a diffusion model $p_{\theta}(\boldsymbol{\tau})$ on the states and actions of all available trajectory data. We then train a separate model $\mathcal{J}<em _phi="\phi">{\phi}$ to predict the cumulative rewards of trajectory samples $\boldsymbol{\tau}^{i}$. The gradients of $\mathcal{J}</em>=1\right)$ may be executed in the environment, after which the planning procedure begins again in a standard receding-horizon control loop. Pseudocode for the guided planning method is given in Algorithm 1.}$ are used to guide the trajectory sampling procedure by modifying the means $\mu$ of the reverse process according to Equation 3. The first action of a sampled trajectory $\boldsymbol{\tau} \sim p\left(\boldsymbol{\tau} \mid \mathcal{O}_{1: T</p>
<h3>3.3 Goal-Conditioned RL as Inpainting</h3>
<p>Some planning problems are more naturally posed as constraint satisfaction than reward maximization. In these settings, the objective is to produce any feasible trajectory that satisfies a set of constraints, such as terminating at a goal location. Appealing to the two-dimensional array representation of trajectories described by Equation 2, this setting can be translated into an inpainting problem, in which state and action constraints act analogously to observed pixels in an image (Sohl-Dickstein et al., 2015). All unobserved locations in the array must be filled in by the diffusion model in a manner consistent with the observed constraints.</p>
<p>The perturbation function required for this task is a Dirac delta for observed values and constant elsewhere. Concretely, if $\mathbf{c}_{t}$ is state constraint at timestep $t$, then</p>
<p>$$
h(\boldsymbol{\tau})=\delta_{\mathbf{c}<em 0="0">{t}}\left(\mathbf{s}</em>}, \mathbf{a<em T="T">{0}, \ldots, \mathbf{s}</em>}, \mathbf{a<em t="t">{T}\right)=\left{\begin{array}{cl}
+\infty &amp; \text { if } \mathbf{c}</em> \
0 &amp; \text { otherwise }
\end{array}\right.
$$}=\mathbf{s}_{t</p>
<p>The definition for action constraints is identical. In practice, this may be implemented by sampling from</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. (Properties of diffusion planners) (a) Learned long-horizon planning: Diffuser's learned planning procedure does not suffer from the myopic failure modes common to shooting algorithms and is able to plan over long horizons with sparse reward. (b) Temporal compositionality: Even though the model is not Markovian, it generates trajectories via iterated refinements to local consistency. As a result, it exhibits the types of generalization usually associated with Markovian models, with the ability to stitch together snippets of trajectories from the training data to generate novel plan. (c) Variable-length plans: Despite being a trajectory-level model, Diffuser's planning horizon is not determined by its architecture. The horizon can be updated after training by changing the dimensionality of the input noise. (d) Task compositionality: Diffuser can be composed with new reward functions to plan for tasks unseen during training. In all subfigures, denotes a starting state and denotes a goal state.
the unperturbed reverse process $\boldsymbol{\tau}^{i-1} \sim p_{\theta}\left(\boldsymbol{\tau}^{i-1} \mid \boldsymbol{\tau}^{i}\right)$ and replacing the sampled values with conditioning values $\mathbf{c}_{t}$ after all diffusion timesteps $i \in{0,1, \ldots, N}$.</p>
<p>Even reward maximization problems require conditioningby-inpainting because all sampled trajectories should begin at the current state. This conditioning is described by line 10 in Algorithm 1.</p>
<h2>4 Properties of Diffusion Planners</h2>
<p>We discuss a number of Diffuser's important properties, focusing on those that are are either distinct from standard dynamics models or unusual for non-autoregressive trajectory prediction.</p>
<p>Learned long-horizon planning. Single-step models are typically used as proxies for ground-truth environment dynamics $\boldsymbol{f}$, and as such are not tied to any planning algorithm in particular. In contrast, the planning routine in Algorithm 1 is closely tied to the specific affordances of diffusion models. Because our planning method is nearly identical to sampling (with the only difference being guidance by a perturbation function $h(\boldsymbol{\tau})$ ), Diffuser's effectiveness as a long-horizon predictor directly translates to effective long-horizon planning. We demonstrate the benefits of learned planning in a goal-reaching setting in Figure 3a, showing that Diffuser is able to generate feasible trajectories in the types of sparse reward settings where shooting-based approaches are known to struggle. We
explore a more quantitative version of this problem setting in Section 5.1.</p>
<p>Temporal compositionality. Single-step models are often motivated using the Markov property, allowing them to compose in-distribution transitions to generalize to outof-distribution trajectories. Because Diffuser generates globally coherent trajectories by iteratively improving local consistency (Section 3.1), it can also stitch together familiar subsequences in novel ways. In Figure 3b, we train Diffuser on trajectories that only travel in a straight line, and show that it can generalize to v-shaped trajectories by composing trajectories at their point of intersection.</p>
<p>Variable-length plans. Because our model is fully convolutional in the horizon dimension of its prediction, its planning horizon is not specified by architectural choices. Instead, it is determined by the size of the input noise $\boldsymbol{\tau}^{N} \sim$ $\mathcal{N}(\mathbf{0}, \mathbf{I})$ that initializes the denoising process, allowing for variable-length plans (Figure 3c).</p>
<p>Task compositionality. While Diffuser contains information about both environment dynamics and behaviors, it is independent of reward function. Because the model acts as a prior over possible futures, planning can be guided by comparatively lightweight perturbation functions $h(\boldsymbol{\tau})$ (or even combinations of multiple perturbations) corresponding to different rewards. We demonstrate this by planning for a new reward function unseen during training of the diffusion model (Figure 3d).</p>
<table>
<thead>
<tr>
<th>Environment</th>
<th></th>
<th>MPPI</th>
<th>CQL</th>
<th>IQL</th>
<th>Diffuser</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maze2D</td>
<td>U-Maze</td>
<td>33.2</td>
<td>5.7</td>
<td>47.4</td>
<td>$\mathbf{1 1 3 . 9} \pm 3.1$</td>
</tr>
<tr>
<td>Maze2D</td>
<td>Medium</td>
<td>10.2</td>
<td>5.0</td>
<td>34.9</td>
<td>$\mathbf{1 2 1 . 5} \pm 2.7$</td>
</tr>
<tr>
<td>Maze2D</td>
<td>Large</td>
<td>5.1</td>
<td>12.5</td>
<td>58.6</td>
<td>$\mathbf{1 2 3 . 0} \pm 6.4$</td>
</tr>
<tr>
<td>Single-task Average</td>
<td></td>
<td>16.2</td>
<td>7.7</td>
<td>47.0</td>
<td>$\mathbf{1 1 9 . 5}$</td>
</tr>
<tr>
<td>Multi2D</td>
<td>U-Maze</td>
<td>41.2</td>
<td>-</td>
<td>24.8</td>
<td>$\mathbf{1 2 8 . 9} \pm 1.8$</td>
</tr>
<tr>
<td>Multi2D</td>
<td>Medium</td>
<td>15.4</td>
<td>-</td>
<td>12.1</td>
<td>$\mathbf{1 2 7 . 2} \pm 3.4$</td>
</tr>
<tr>
<td>Multi2D</td>
<td>Large</td>
<td>8.0</td>
<td>-</td>
<td>13.9</td>
<td>$\mathbf{1 3 2 . 1} \pm 5.8$</td>
</tr>
<tr>
<td>Multi-task Average</td>
<td></td>
<td>21.5</td>
<td>-</td>
<td>16.9</td>
<td>$\mathbf{1 2 9 . 4}$</td>
</tr>
</tbody>
</table>
<p>Table 1. (Long-horizon planning) The performance of Diffuser and prior model-free algorithms in the Maze2D environment, which tests long-horizon planning due to its sparse reward structure. The Multi2D setting refers to a multi-task variant with goal locations resampled at the beginning of every episode. Diffuser substantially outperforms prior approaches in both settings. Appendix A details the sources for the scores of the baseline algorithms.</p>
<h2>5 Experimental Evaluation</h2>
<p>The focus of our experiments is to evaluate Diffuser on the capabilities we would like from a data-driven planner. In particular, we evaluate (1) the ability to plan over long horizons without manual reward shaping, (2) the ability to generalize to new configurations of goals unseen during training, and (3) the ability to recover an effective controller from heterogeneous data of varying quality. We conclude by studying practical runtime considerations of diffusion-based planning, including the most effective ways of speeding up the planning procedure while suffering minimally in terms of performance.</p>
<h3>5.1 Long Horizon Multi-Task Planning</h3>
<p>We evaluate long-horizon planning in the Maze2D environments (Fu et al., 2020), which require traversing to a goal location where a reward of 1 is given. No reward shaping is provided at any other location. Because it can take hundreds of steps to reach the goal location, even the best model-free algorithms struggle to adequately perform credit assignment and reliably reach the goal (Table 1).</p>
<p>We plan with Diffuser using the inpainting strategy to condition on a start and goal location. (The goal location is also available to the model-free methods; it is identifiable by being the only state in the dataset with non-zero reward.) We then use the sampled trajectory as an open-loop plan. Diffuser achieves scores over 100 in all maze sizes, indicating that it outperforms a reference expert policy. We visualize the reverse diffusion process generating Diffuser’s plans in Figure 4.</p>
<p>While the training data in Maze2D is undirected - consisting of a controller navigating to and from randomly selected</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. (Planning as inpainting) Plans are generated in the Maze2D environment by sampling trajectories consistent with a specified start $\boldsymbol{\otimes}$ and goal $\boldsymbol{\otimes}$ condition. The remaining states are "inpainted" by the denoising process.
locations - the evaluation is single-task in that the goal is always the same. In order to test multi-task flexibility, we modify the environment to randomize the goal location at the beginning of each episode. This setting is denoted as Multi2D in Table 1. Diffuser is naturally a multi-task planner; we do not need to retrain the model from the single-task experiments and simply change the conditioning goal. As a result, Diffuser performs as well in the multitask setting as in the single-task setting. In contrast, there is a substantial performance drop of the best model-free algorithm in the single-task setting (IQL; Kostrikov et al. 2022) when adapted to the multi-task setting. Details of our multi-task IQL with hindsight experience relabeling (Andrychowicz et al., 2017) are provided in Appendix A. MPPI uses the ground-truth dynamics; its poor performance compared to the learned planning algorithm of Diffuser highlights the difficulty posed by long-horizon planning even when there are no prediction inaccuracies.</p>
<h3>5.2 Test-time Flexibility</h3>
<p>In order to evaluate the ability to generalize to new test-time goals, we construct a suite of block stacking tasks with three settings: (1) Unconditional Stacking, for which the task is to build a block tower as tall as possible; (2) Conditional Stacking, for which the task is to construct a block tower with a specified order of blocks, and (3) Rearrangement, for which the task is to match a set of reference blocks’ locations in a novel arrangement. We train all methods on 10000 trajectories from demonstrations generated by PDDLStream (Garrett et al., 2020); rewards are equal to one upon successful stack placements and zero otherwise. These block stacking are challenging diagnostics of test-</p>
<table>
<thead>
<tr>
<th>Environment</th>
<th>BCQ</th>
<th>CQL</th>
<th>Diffuser</th>
</tr>
</thead>
<tbody>
<tr>
<td>Unconditional Stacking</td>
<td>0.0</td>
<td>24.4</td>
<td>$\mathbf{58.7}\pm 2.5$</td>
</tr>
<tr>
<td>Conditional Stacking</td>
<td>0.0</td>
<td>0.0</td>
<td>$\mathbf{45.6}\pm 3.1$</td>
</tr>
<tr>
<td>Rearrangement</td>
<td>0.0</td>
<td>0.0</td>
<td>$\mathbf{58.9}\pm 3.4$</td>
</tr>
<tr>
<td>Average</td>
<td>0.0</td>
<td>8.1</td>
<td>$\mathbf{54.4}$</td>
</tr>
</tbody>
</table>
<p>Table 3: (Test-time flexibility) Performance of BCQ, CQL, and Diffuser on block stacking tasks. A score of 100 corresponds to a perfectly executed stack; 0 is that of a random policy.
time flexibility; in the course of executing a partial stack for a randomized goal, a controller will venture into novel states not included in the training configuration.</p>
<p>We use one trained Diffuser for all block-stacking tasks, only modifying the perturbation function $h(\boldsymbol{\tau})$ between settings. In the Unconditional Stacking task, we directly sample from the unperturbed denoising process $p_{\theta}(\boldsymbol{\tau})$ to emulate the PDDLStream controller. In the Conditional Stacking and Rearrangement tasks, we compose two perturbation functions $h(\boldsymbol{\tau})$ to bias the sampled trajectories: the first maximizes the likelihood of the trajectory’s final state matching the goal configuration, and the second enforces a contact constraint between the end effector and a cube during stacking motions. (See Appendix B for details.)</p>
<p>We compare with two prior model-free offline reinforcement learning algorithms: BCQ <em>(Fujimoto et al., 2019)</em> and CQL <em>(Kumar et al., 2020)</em>, training standard variants for Unconditional Stacking and goal-conditioned variants for Conditional Stacking and Rearrangement. (Baseline details are provided in Appendix A.) Quantitative results are given in Table 3, in which a score of 100 corresponds to a perfect execution of the task. Diffuser substantially outperforms both prior methods, with the conditional settings requiring flexible behavior generation proving especially difficult for the model-free algorithms. A visual depiction of an execution by Diffuser is provided in Figure 5.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: (Block stacking) A block stacking sequence executed by Diffuser. This task is best illustrated by videos viewable at diffusion-planning.github.io.</p>
<h3>5.3 Offline Reinforcement Learning</h3>
<p>Finally, we evaluate the capacity to recover an effective single-task controller from heterogeneous data of varying quality using the D4RL offline locomotion suite <em>(Fu et al., 2020)</em>. We guide the trajectories generated by Diffuser toward high-reward regions using the sampling procedure described in Section 3.2 and condition the trajectories on the current state using the inpainting procedure described in Section 3.3. The reward predictor $\mathcal{J}_{\phi}$ is trained on the same trajectories as the diffusion model.</p>
<p>We compare to a variety of prior algorithms spanning other approaches to data-driven control, including the model-free reinforcement learning algorithms CQL <em>(Kumar et al., 2020)</em> and IQL <em>(Kostrikov et al., 2022)</em>; returnconditioning approaches like Decision Transformer (DT; [Chen et al. 2021b]); and model-based reinforcement learning approaches including Trajectory Transformer (TT; [Janner et al. 2021]), MOPO <em>(Yu et al., 2020)</em>, MOReL <em>(Kidambi et al., 2020)</em>, and MBOP <em>(Argenson &amp; Dulac-Arnold, 2021)</em>. In the single-task setting, Diffuser performance comparably to prior algorithms: better than the model-based MOReL and MBOP and return-conditioning DT, but worse than</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Environment</th>
<th>BC</th>
<th>CQL</th>
<th>IQL</th>
<th>DT</th>
<th>TT</th>
<th>MOPO</th>
<th>MOReL</th>
<th>MBOP</th>
<th>Diffuser</th>
</tr>
</thead>
<tbody>
<tr>
<td>Medium-Expert</td>
<td>HalfCheetah</td>
<td>55.2</td>
<td>91.6</td>
<td>86.7</td>
<td>86.8</td>
<td>95.0</td>
<td>63.3</td>
<td>53.3</td>
<td>$\mathbf{1 0 5 . 9}$</td>
<td>$88.9 \pm 0.3$</td>
</tr>
<tr>
<td>Medium-Expert</td>
<td>Hopper</td>
<td>52.5</td>
<td>$\mathbf{1 0 5 . 4}$</td>
<td>91.5</td>
<td>$\mathbf{1 0 7 . 6}$</td>
<td>$\mathbf{1 1 0 . 0}$</td>
<td>23.7</td>
<td>$\mathbf{1 0 8 . 7}$</td>
<td>55.1</td>
<td>$103.3 \pm 1.3$</td>
</tr>
<tr>
<td>Medium-Expert</td>
<td>Walker2d</td>
<td>$\mathbf{1 0 7 . 5}$</td>
<td>$\mathbf{1 0 8 . 8}$</td>
<td>$\mathbf{1 0 9 . 6}$</td>
<td>$\mathbf{1 0 8 . 1}$</td>
<td>101.9</td>
<td>44.6</td>
<td>95.6</td>
<td>70.2</td>
<td>$\mathbf{1 0 6 . 9} \pm 0.2$</td>
</tr>
<tr>
<td>Medium</td>
<td>HalfCheetah</td>
<td>42.6</td>
<td>44.0</td>
<td>$\mathbf{4 7 . 4}$</td>
<td>42.6</td>
<td>$\mathbf{4 6 . 9}$</td>
<td>42.3</td>
<td>42.1</td>
<td>44.6</td>
<td>$42.8 \pm 0.3$</td>
</tr>
<tr>
<td>Medium</td>
<td>Hopper</td>
<td>52.9</td>
<td>58.5</td>
<td>66.3</td>
<td>67.6</td>
<td>61.1</td>
<td>28.0</td>
<td>$\mathbf{9 5 . 4}$</td>
<td>48.8</td>
<td>$74.3 \pm 1.4$</td>
</tr>
<tr>
<td>Medium</td>
<td>Walker2d</td>
<td>75.3</td>
<td>72.5</td>
<td>$\mathbf{7 8 . 3}$</td>
<td>74.0</td>
<td>$\mathbf{7 9 . 0}$</td>
<td>17.8</td>
<td>$\mathbf{7 7 . 8}$</td>
<td>41.0</td>
<td>$\mathbf{7 9 . 6} \pm 0.55$</td>
</tr>
<tr>
<td>Medium-Replay</td>
<td>HalfCheetah</td>
<td>36.6</td>
<td>45.5</td>
<td>44.2</td>
<td>36.6</td>
<td>41.9</td>
<td>$\mathbf{5 3 . 1}$</td>
<td>40.2</td>
<td>42.3</td>
<td>$37.7 \pm 0.5$</td>
</tr>
<tr>
<td>Medium-Replay</td>
<td>Hopper</td>
<td>18.1</td>
<td>$\mathbf{9 5 . 0}$</td>
<td>$\mathbf{9 4 . 7}$</td>
<td>82.7</td>
<td>$\mathbf{9 1 . 5}$</td>
<td>67.5</td>
<td>$\mathbf{9 3 . 6}$</td>
<td>12.4</td>
<td>$\mathbf{9 3 . 6} \pm 0.4$</td>
</tr>
<tr>
<td>Medium-Replay</td>
<td>Walker2d</td>
<td>26.0</td>
<td>77.2</td>
<td>73.9</td>
<td>66.6</td>
<td>$\mathbf{8 2 . 6}$</td>
<td>39.0</td>
<td>49.8</td>
<td>9.7</td>
<td>$70.6 \pm 1.6$</td>
</tr>
<tr>
<td>Average</td>
<td></td>
<td>51.9</td>
<td>$\mathbf{7 7 . 6}$</td>
<td>$\mathbf{7 7 . 0}$</td>
<td>74.7</td>
<td>$\mathbf{7 8 . 9}$</td>
<td>42.1</td>
<td>72.9</td>
<td>47.8</td>
<td>$\mathbf{7 7 . 5}$</td>
</tr>
</tbody>
</table>
<p>Table 2: (Offline reinforcement learning) The performance of Diffuser and a variety of prior algorithms on the D4RL locomotion benchmark <em>(Fu et al., 2020)</em>. Results for Diffuser correspond to the mean and standard error over 150 planning seeds. We detail the sources for the performance of prior methods in Appendix A.3. Following <em>Kostrikov et al. (2022)</em>, we emphasize in bold scores within 5 percent of the maximum per task ( $\geq 0.95 \cdot \max$ ).</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. (Guided sampling) Diffuser generates all timesteps of a plan concurrently, instead of autoregressively, through the denoising process.
the best offline techniques designed specifically for singletask performance. We also investigated a variant using Diffuser as a dynamics model in conventional trajectory optimizers such as MPPI (Williams et al., 2015), but found that this combination performed no better than random, suggesting that the effectiveness of Diffuser stems from coupled modeling and planning, and not from improved open-loop predictive accuracy.</p>
<h3>5.4 Warm-Starting Diffusion for Faster Planning</h3>
<p>A limitation of Diffuser is that individual plans are slow to generate (due to iterative generation). Naïvely, as we execute plans open loop, a new plan must be regenerated at each step of execution. To improve execution speed of Diffuser, we may further reuse previously generated plans to warm-start generations of subsequent plans.</p>
<p>To warm-start planning, we may run a limited number of forward diffusion steps from a previously generated plan and then run a corresponding number of denoising steps from this partially noised trajectory to regenerate an updated plan. In Figure 7, we illustrate the trade-off between performance and runtime budget as we vary the underlying number of denoising steps used to regenerate each a new plan from 2 to 100. We find that we may reduce the planning budget of our approach markedly with only modest drop in performance.</p>
<h2>6 Related Work</h2>
<p>Advances in deep generative modeling have recently made inroads into model-based reinforcement learning, with multiple lines of work exploring dynamics models parameterized as convolutional U-networks (Kaiser et al., 2020), stochastic recurrent networks (Ke et al., 2018;</p>
<p>Performance versus runtime
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. (Fast Planning) Performance of Diffuser on Walker2d Medium-Expert when varying the number of diffusion steps to warm-start planning. Performance suffers only minimally even when using one-tenth the number of diffusion steps, as long as plans are initialized from the previous timestep's plan.</p>
<p>Hafner et al., 2021a; Ha \&amp; Schmidhuber, 2018), vectorquantized autoencoders (Hafner et al., 2021b; Ozair et al., 2021), neural ODEs (Du et al., 2020a), normalizing flows (Rhinehart et al., 2020; Janner et al., 2020), generative adversarial networks (Eysenbach et al., 2021), energy-based models ( $E B M s$; Du et al. 2019), graph neural networks (Sanchez-Gonzalez et al., 2018), neural radiance fields ( Li et al., 2021), and Transformers (Janner et al., 2021; Chen et al., 2021a). Further, Lambert et al. 2020 have studied non-autoregressive trajectory-level dynamics models for long-horizon prediction. These investigations generally assume an abstraction barrier between the model and planner. Specifically, the role of learning is relegated to approximating environment dynamics; once learning is complete the model may be inserted into any of a variety of planning (Botev et al., 2013; Williams et al., 2015) or policy optimization (Sutton, 1990; Wang et al., 2019) algorithms because the form of the planner does not depend strongly on the form of the model. Our goal is to break this abstraction barrier by designing a model and planning algorithm that are trained alongside one another, resulting in a non-autoregressive trajectory-level model for which sampling and planning are nearly identical.
A number of parallel lines of work have studied how to break the abstraction barrier between model learning and planning in different ways. Approaches include training an autoregressive latent-space model for reward prediction (Tamar et al., 2016; Oh et al., 2017; Schrittwieser et al., 2019); weighing model training objectives by state values (Farahmand et al., 2017); and applying collocation techniques to learned single-step energies (Du et al., 2019; Rybkin et al., 2021). In contrast, our method plans by modeling and generating all timesteps of a trajectory</p>
<p>concurrently, instead of autoregressively, and conditioning the sampled trajectories with auxiliary guidance functions.</p>
<p>Diffusion models have emerged as a promising class of generative model that formulates the data-generating process as an iterative denoising procedure (Sohl-Dickstein et al., 2015; Ho et al., 2020). The denoising procedure can be seen as parameterizing the gradients of the data distribution (Song \&amp; Ermon, 2019), connecting diffusion models to score matching (Hyvärinen, 2005) and EBMs (LeCun et al., 2006; Du \&amp; Mordatch, 2019; Nijkamp et al., 2019; Grathwohl et al., 2020). Iterative, gradientbased sampling lends itself towards flexible conditioning (Dhariwal \&amp; Nichol, 2021) and compositionality (Du et al., 2020b), which we use to recover effective behaviors from heterogeneous datasets and plan for reward functions unseen during training. While diffusion models have been developed for the generation of images (Song et al., 2021), waveforms (Chen et al., 2021c), 3D shapes (Zhou et al., 2021), and text (Austin et al., 2021), to the best of our knowledge they have not previously been used in the context of reinforcement learning or decision-making.</p>
<h2>7 Conclusion</h2>
<p>We have presented Diffuser, a denoising diffusion model for trajectory data. Planning with Diffuser is almost identical to sampling from it, differing only in the addition of auxiliary perturbation functions that serve to guide samples. The learned diffusion-based planning procedure has a number of useful properties, including graceful handling of sparse rewards, the ability to plan for new rewards without retraining, and a temporal compositionality that allows it to produce out-of-distribution trajectories by stitching together in-distribution subsequences. Our results point to a new class of diffusion-based planning procedures for deep modelbased reinforcement learning.</p>
<h2>Code References</h2>
<p>We used the following open-source libraries for this work: NumPy (Harris et al., 2020), PyTorch (Paszke et al., 2019), and Diffusion Models in PyTorch (Wang, 2020).</p>
<h2>Acknowledgements</h2>
<p>We thank Ajay Jain for feedback on an early draft and Leslie Kaelbling, Tomás Lozano-Pérez, Jascha Sohl-Dickstein, Ben Eysenbach, Amy Zhang, Colin Li, and Toru Lin for helpful discussions. This work was partially supported by computational resource donations from Microsoft. M.J. is supported by fellowships from the National Science Foundation and the Open Philanthropy Project. Y.D. is supported by a fellowship from the National Science Foundation.</p>
<h2>References</h2>
<p>Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P., McGrew, B., Tobin, J., Abbeel, P., and Zaremba, W. Hindsight experience replay. In Advances in Neural Information Processing Systems. 2017.</p>
<p>Argenson, A. and Dulac-Arnold, G. Model-based offline planning. In International Conference on Learning Representations, 2021.</p>
<p>Austin, J., Johnson, D. D., Ho, J., Tarlow, D., and van den Berg, R. Structured denoising diffusion models in discrete state-spaces. In Advances in Neural Information Processing Systems, 2021.</p>
<p>Bapst, V., Sanchez-Gonzalez, A., Doersch, C., Stachenfeld, K. L., Kohli, P., Battaglia, P. W., and Hamrick, J. B. Structured agents for physical construction. In International Conference on Machine Learning, 2019.</p>
<p>Botev, Z. I., Kroese, D. P., Rubinstein, R. Y., and L'Ecuyer, P. The cross-entropy method for optimization. In Handbook of Statistics, volume 31, chapter 3. 2013.</p>
<p>Chen, C., Yoon, J., Wu, Y.-F., and Ahn, S. TransDreamer: Reinforcement learning with transformer world models, 2021a.</p>
<p>Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A., and Mordatch, I. Decision transformer: Reinforcement learning via sequence modeling. In Advances in Neural Information Processing Systems, 2021b.</p>
<p>Chen, N., Zhang, Y., Zen, H., Weiss, R. J., Norouzi, M., and Chan, W. Wavegrad: Estimating gradients for waveform generation. In International Conference on Learning Representations, 2021c.</p>
<p>Chua, K., Calandra, R., McAllister, R., and Levine, S. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. In Advances in Neural Information Processing Systems. 2018.</p>
<p>Dhariwal, P. and Nichol, A. Q. Diffusion models beat GANs on image synthesis. In Advances in Neural Information Processing Systems, 2021.</p>
<p>Du, J., Futoma, J., and Doshi-Velez, F. Modelbased reinforcement learning for semi-markov decision processes with neural odes. In Advances in Neural Information Processing Systems, 2020a.</p>
<p>Du, Y. and Mordatch, I. Implicit generation and generalization in energy-based models. In Advances in Neural Information Processing Systems, 2019.</p>
<p>Du, Y., Lin, T., and Mordatch, I. Model based planning with energy based models. In Conference on Robot Learning, 2019.</p>
<p>Du, Y., Li, S., and Mordatch, I. Compositional visual generation with energy based models. In Advances in Neural Information Processing Systems, 2020b.</p>
<p>Eysenbach, B., Khazatsky, A., Levine, S., and Salakhutdinov, R. Mismatched no more: Joint modelpolicy optimization for model-based rl. arXiv preprint arXiv:2110.02758, 2021.</p>
<p>Farahmand, A.-M., Barreto, A., and Nikovski, D. Valueaware loss function for model-based reinforcement learning. In International Conference on Artificial Intelligence and Statistics, 2017.</p>
<p>Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S. D4RL: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.</p>
<p>Fujimoto, S., Meger, D., and Precup, D. Off-policy deep reinforcement learning without exploration. In International Conference on Machine Learning, 2019.</p>
<p>Garrett, C. R., Lozano-Pérez, T., and Kaelbling, L. P. Pddlstream: Integrating symbolic planners and blackbox samplers via optimistic adaptive planning. In International Conference on Automated Planning and Scheduling, 2020.</p>
<p>Grathwohl, W., Wang, K.-C., Jacobsen, J.-H., Duvenaud, D., and Zemel, R. Learning the stein discrepancy for training and evaluating energy-based models without sampling. In International Conference on Machine Learning, 2020.</p>
<p>Ha, D. and Schmidhuber, J. Recurrent world models facilitate policy evolution. In Advances in Neural Information Processing Systems, 2018.</p>
<p>Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and Davidson, J. Learning latent dynamics for planning from pixels. In International Conference on Machine Learning, 2021a.</p>
<p>Hafner, D., Lillicrap, T. P., Norouzi, M., and Ba, J. Mastering atari with discrete world models. In International Conference on Learning Representations, 2021b.</p>
<p>Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van Kerkwijk, M. H., Brett, M., Haldane, A., del Río, J. F., Wiebe, M., Peterson, P., Gérard-Marchant, P., Sheppard, K., Reddy, T., Weckesser, W., Abbasi, H., Gohlke, C., and Oliphant, T. E. Array programming with NumPy. Nature, 585(7825):357-362, 2020.</p>
<p>Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, 2020.</p>
<p>Hyvärinen, A. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 2005.</p>
<p>Janner, M., Mordatch, I., and Levine, S. $\gamma$-models: Generative temporal difference learning for infinitehorizon prediction. In Advances in Neural Information Processing Systems, 2020.</p>
<p>Janner, M., Li, Q., and Levine, S. Offline reinforcement learning as one big sequence modeling problem. In Advances in Neural Information Processing Systems, 2021.</p>
<p>Kaiser, L., Babaeizadeh, M., Miłos, P., Osiński, B., Campbell, R. H., Czechowski, K., Erhan, D., Finn, C., Kozakowski, P., Levine, S., Mohiuddin, A., Sepassi, R., Tucker, G., and Michalewski, H. Model based reinforcement learning for atari. In International Conference on Learning Representations, 2020.</p>
<p>Ke, N. R., Singh, A., Touati, A., Goyal, A., Bengio, Y., Parikh, D., and Batra, D. Modeling the long term future in model-based reinforcement learning. In International Conference on Learning Representations, 2018.</p>
<p>Kelly, M. An introduction to trajectory optimization: How to do your own direct collocation. SIAM Review, 59(4): 849-904, 2017.</p>
<p>Kidambi, R., Rajeswaran, A., Netrapalli, P., and Joachims, T. MOReL: Model-based offline reinforcement learning. In Advances in Neural Information Processing Systems, 2020.</p>
<p>Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.</p>
<p>Kostrikov, I., Nair, A., and Levine, S. Offline reinforcement learning with implicit Q-learning. In International Conference on Learning Representations, 2022.</p>
<p>Kumar, A., Zhou, A., Tucker, G., and Levine, S. Conservative Q-learning for offline reinforcement learning. In Advances in Neural Information Processing Systems, 2020.</p>
<p>Lambert, N. O., Wilcox, A., Zhang, H., Pister, K. S., and Calandra, R. Learning accurate long-term dynamics for model-based reinforcement learning. arXiv preprint arXiv:2012.09156, 2020.</p>
<p>LeCun, Y., Chopra, S., Hadsell, R., Huang, F. J., and et al. A tutorial on energy-based learning. In Predicting Structured Data. MIT Press, 2006.</p>
<p>Levine, S. Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv preprint arXiv:1805.00909, 2018.</p>
<p>Li, Y., Li, S., Sitzmann, V., Agrawal, P., and Torralba, A. 3d neural scene representations for visuomotor control. In Conference on Robot Learning, 2021.</p>
<p>Misra, D. Mish: A self regularized non-monotonic neural activation function. In British Machine Vision Conference, 2019.</p>
<p>Nagabandi, A., Kahn, G., S. Fearing, R., and Levine, S. Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. In International Conference on Robotics and Automation, 2018.</p>
<p>Nichol, A. Q. and Dhariwal, P. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning, 2021.</p>
<p>Nijkamp, E., Hill, M., Zhu, S.-C., and Wu, Y. N. Learning non-convergent non-persistent short-run MCMC toward energy-based model. In Advances in Neural Information Processing Systems, 2019.</p>
<p>Oh, J., Singh, S., and Lee, H. Value prediction network. In Advances in Neural Information Processing Systems, 2017.</p>
<p>Ozair, S., Li, Y., Razavi, A., Antonoglou, I., Van Den Oord, A., and Vinyals, O. Vector quantized models for planning. In International Conference on Machine Learning, 2021.</p>
<p>Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems. 2019.</p>
<p>Posa, M., Cantu, C., and Tedrake, R. A direct method for trajectory optimization of rigid bodies through contact. The International Journal of Robotics Research, 2014.</p>
<p>Rhinehart, N., McAllister, R., and Levine, S. Deep imitative models for flexible inference, planning, and control. In International Conference on Learning Representations, 2020.</p>
<p>Rybkin, O., Zhu, C., Nagabandi, A., Daniilidis, K., Mordatch, I., and Levine, S. Model-based reinforcement
learning via latent-space collocation. In International Conference on Machine Learning, pp. 9190-9201. PMLR, 2021.</p>
<p>Sanchez-Gonzalez, A., Heess, N., Springenberg, J. T., Merel, J., Riedmiller, M., Hadsell, R., and Battaglia, P. Graph networks as learnable physics engines for inference and control. In International Conference on Machine Learning, 2018.</p>
<p>Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel, T., Lillicrap, T., and Silver, D. Mastering atari, go, chess and shogi by planning with a learned model. arXiv preprint arXiv:1911.08265, 2019.</p>
<p>Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, 2015.</p>
<p>Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021.</p>
<p>Song, Y. and Ermon, S. Generative modeling by estimating gradients of the data distribution. In Advances in Neural Information Processing Systems, 2019.</p>
<p>Sutton, R. S. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In International Conference on Machine Learning, 1990.</p>
<p>Talvitie, E. Model regularization for stable sample rollouts. In Conference on Uncertainty in Artificial Intelligence, 2014.</p>
<p>Tamar, A., Wu, Y., Thomas, G., Levine, S., and Abbeel, P. Value iteration networks. In Advances in Neural Information Processing Systems. 2016.</p>
<p>Tassa, Y., Erez, T., and Todorov, E. Synthesis and stabilization of complex behaviors through online trajectory optimization. In International Conference on Intelligent Robots and Systems, 2012.</p>
<p>Wang, P. Implementation of denoising diffusion probabilistic models in pytorch, 2020. URL https://github.com/lucidrains/ denoising-diffusion-pytorch.</p>
<p>Wang, T., Bao, X., Clavera, I., Hoang, J., Wen, Y., Langlois, E., Zhang, S., Zhang, G., Abbeel, P., and Ba, J. Benchmarking model-based reinforcement learning. arXiv preprint arXiv:1907.02057, 2019.</p>
<p>Williams, G., Aldrich, A., and Theodorou, E. Model predictive path integral control using covariance variable importance sampling. arXiv preprint arXiv:1509.01149, 2015.</p>
<p>Witkin, A. and Kass, M. Spacetime constraints. ACM Siggraph Computer Graphics, 1988.</p>
<p>Wu, Y. and He, K. Group normalization. In European Conference on Computer Vision, 2018.</p>
<p>Yu, T., Thomas, G., Yu, L., Ermon, S., Zou, J., Levine, S., Finn, C., and Ma, T. MOPO: Model-based offline policy optimization. In Advances in Neural Information Processing Systems, 2020.</p>
<p>Zhou, L., Du, Y., and Wu, J. 3D shape generation and completion through point-voxel diffusion. In International Conference on Computer Vision, 2021.</p>
<h2>Appendix A Baseline details and sources</h2>
<p>In this section, we provide details about baselines we ran ourselves. For scores of baselines previously evaluated on standardized tasks, we provide the source of the listed score.</p>
<h3>A.1 Maze2D experiments</h3>
<p>Single-task. The performance of CQL and IQL on the standard Maze2D environments is reported in the D4RL whitepaper (Fu et al., 2020) in Table 2.</p>
<p>We ran IQL using the offical implementation from the authors:
github.com/ikostrikov/implicit_q_learning.
We tuned over two hyperparameters:</p>
<ol>
<li>temperature $\in[3,10]$</li>
<li>expectile $\in[0.65,0.95]$</li>
</ol>
<p>Multi-task. We only evaluated IQL on the Multi2D environments because it is the strongest baseline in the single-task Maze2D environments by a sizeable margin. To adapt IQL to the multi-task setting, we modified the $Q$ functions, value function, and policy to be goal-conditioned. To select goals during training, we employed a strategy based on hindsight experience replay, in which we sampled a goal from among those states encountered in the future of a trajectory. For a training backup $\left(\mathbf{s}<em t="t">{t}, \mathbf{a}</em>\right)$, we sampled goals according to a geometric distribution over the future}, \mathbf{s}_{t+1</p>
<p>$$
\Delta \sim \operatorname{Geom}(1-\gamma) \quad \mathbf{g}=\mathbf{s}_{t+\Delta}
$$</p>
<p>recalculated rewards based on the sampled goal, and conditioned all relevant models on the goal during updating. During testing, we conditioned the policy on the groundtruth goal.</p>
<p>We tuned over the same IQL parameters as in the single-task setting.</p>
<h3>A.2 Block stacking experiments</h3>
<p>Single-task. We ran CQL using the following implementation
https://github.com/young-geng/cql.
and used default hyperparameters in the code. We ran BCQ using the author's original implementation
https://github.com/sfujim/BCQ.
For BCQ, we tuned over two hyperparameters:</p>
<ol>
<li>discount factor $\in[0.9,0.999]$</li>
<li>tau $\in[0.001,0.01]$</li>
</ol>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure A1. Diffuser has a U-Net architecture with residual blocks consisting of temporal convolutions, group normalization, and Mish nonlinearities.</p>
<p>Multi-task. To evaluate BCQ and CQL in the multitask setting, we modified the $Q$-functions, value function and policy to be goal-conditioned. We trained using goal relabeling as in the Multi2D environments. We tuned over the same hyperparameters described in the single-task block stacking experiments.</p>
<h2>A. 3 Offline Locomotion</h2>
<p>The scores for BC, CQL, IQL, and AWAC are from Table 1 in Kostrikov et al. (2022). The scores for DT are from Table 2 in Chen et al. (2021b). The scores for TT are from Table 1 in Janner et al. (2021). The scores for MOReL are from Table 2 in Kidambi et al. (2020). The scores for MBOP are from Table 1 in Argenson \&amp; Dulac-Arnold (2021).</p>
<h2>Appendix B Test-time Flexibility</h2>
<p>To guide Diffuser to stack blocks in specified configurations, we used two separate perturbation functions $h(\boldsymbol{\tau})$ to specify a given stack of block A on top of block B, which we detail below.</p>
<p>Final State Matching To enforce a final state consisting of block A on top of block B, we trained a perturbation function $h_{\text {match }}(\boldsymbol{\tau})$ as a per-timestep classifier determining whether a a state $\boldsymbol{s}$ exhibits a stack of block A on top of block B. We train the classifier on the demonstration data as the diffusion model.</p>
<p>Contact Constraint To guide the Kuka arm to stack block A on top of block B, we construct a perturbation function $h_{\text {contact }}(\boldsymbol{\tau})=\sum_{i=0}^{64}-1 *\left|\boldsymbol{\tau}<em i="i">{c</em>}}-1\right|^{2}$, where $\boldsymbol{\tau<em i="i">{c</em>}}$ corresponds to the underlying dimension in state $\boldsymbol{\tau<em i="i">{s</em>$ that specifies the presence or absence of contact between the Kuka arm and block A. We apply the contact constraint between the Kuka arm and block A for the first 64 timesteps in a trajectory, corresponding to initial contact with block A in a plan.}</p>
<h2>Appendix C Implementation Details</h2>
<p>In this section we describe the architecture and record hyperparameters.</p>
<ol>
<li>The architecture of Diffuser (Figure A1) consists of a U-Net structure with 6 repeated residual blocks. Each block consisted of two temporal convolutions, each followed by group norm (Wu \&amp; He, 2018), and a final Mish nonlinearity (Misra, 2019). Timestep embeddings are produced by a single fully-connected layer and added to the activations of the first temporal convolution within each block.</li>
<li>We train the model using the Adam optimizer (Kingma $\&amp; \mathrm{Ba}, 2015$ ) with a learning rate of $4 \mathrm{e}-05$ and batch size of 32 . We train the models for 500 k steps.</li>
<li>The return predictor $\mathcal{J}$ has the structure of the first half of the U-Net used for the diffusion model, with a final linear layer to produce a scalar output.</li>
<li>We use a planning horizon $T$ of 32 in all locomotion tasks, 128 for block-stacking, 128 in Maze2D / Multi2D U-Maze, 265 in Maze2D / Multi2D Medium, and 384 in Maze2D / Multi2D Large.</li>
<li>We found that we could reduce the planning horizon for many tasks, but that the guide scale would need to be lowered (e.g., to 0.001 for a horizon of 4 in the halfcheetah tasks) to accommodate. The configuration file in the open-source code demonstrates how to run with a modified scale and horizon.</li>
<li>We use $N=20$ diffusion steps for locomotion tasks and $N=100$ for block-stacking.</li>
<li>We use a guide scale of $\alpha=0.1$ for all tasks except hopper-medium-expert, in which we use a smaller scale of 0.0001 .</li>
<li>We used a discount factor of 0.997 for the return prediction $\mathcal{J}_{\phi}$, though found that above $\gamma=0.99$ planning was fairly insensitive to changes in discount factor.</li>
<li>We found that control performance was not substantially affected by the choice of predicting noise $\epsilon$ versus uncorrupted data $\boldsymbol{\tau}^{0}$ with the diffusion model.</li>
</ol>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ In general reinforcement learning contexts, conditioning on the future emerges from the assumption of future optimality for the purpose of writing a dynamic programming recursion. Concretely, this appears as the future optimality variables $\mathcal{O}<em t="t">{t: T}$ in the action distribution $\log p\left(\mathbf{a}</em>} \mid \mathbf{s<em T="T" t:="t:">{t}, \mathcal{O}</em>\right)$ (Levine, 2018).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
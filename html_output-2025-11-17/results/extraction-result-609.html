<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-609 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-609</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-609</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-3cf13e7e55c833276f90ba7a1023667698e39784</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3cf13e7e55c833276f90ba7a1023667698e39784" target="_blank">Squib: Reproducibility in Computational Linguistics: Are We Willing to Share?</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Computational Logic</p>
                <p><strong>Paper TL;DR:</strong> The findings show that even though the situation appears to have improved comparing 2016 to 2016, empiricism in computational linguistics still largely remains a matter of faith, Nevertheless, the future is somewhat optimistic about the future.</p>
                <p><strong>Paper Abstract:</strong> This study focuses on an essential precondition for reproducibility in computational linguistics: the willingness of authors to share relevant source code and data. Ten years after Ted Pedersen’s influential “Last Words” contribution in Computational Linguistics, we investigate to what extent researchers in computational linguistics are willing and able to share their data and code. We surveyed all 395 full papers presented at the 2011 and 2016 ACL Annual Meetings, and identified whether links to data and code were provided. If working links were not provided, authors were requested to provide this information. Although data were often available, code was shared less often. When working links to code or data were not provided in the paper, authors provided the code in about one third of cases. For a selection of ten papers, we attempted to reproduce the results using the provided data and code. We were able to reproduce the results approximately for six papers. For only a single paper did we obtain the exact same results. Our findings show that even though the situation appears to have improved comparing 2016 to 2011, empiricism in computational linguistics still largely remains a matter of faith. Nevertheless, we are somewhat optimistic about the future. Ensuring reproducibility is not only important for the field as a whole, but also seems worthwhile for individual researchers: The median citation count for studies with working links to the source code is higher.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e609.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e609.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reproducibility Assessment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reproducibility assessment of computational linguistics experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper defines reproducibility as exact re-creation of reported results using the same data and methods, and reports an empirical assessment by attempting to reproduce results for a sample of papers and by measuring availability of code/data across two ACL years (2011, 2016).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reproducibility in Computational Linguistics: Are We Willing to Share?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computational linguistics / natural language processing</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>empirical assessment of availability and reproducibility: (a) measure whether papers provide working links to data and source code, (b) attempt to reproduce results for a random sample of 10 papers using provided data/code within an 8-hour limit per study</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Documented sources discussed include undocumented experimental details (pre-processing, experimental set-up), software/toolkit version differences, system variations, randomness in procedures (random seeds), small sample sizes creating large per-subset variation, long training times limiting validation, proprietary tools preventing sharing, lost or inaccessible artifacts, and differences in runtime environments.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Replication success recorded as exact match of reported numeric results (exact reproduction) and a relaxed criterion (allowing small deviations up to ~2 percentage points and excluding small-sample artifacts); also measured proportion of papers with working links to data/code and proportion that supplied materials after request.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Exact reproduction achieved for 1 of 10 selected studies (10%). Using a relaxed tolerance (ignoring one missing value and small-sample deviations and allowing up to 2 percentage points difference) up to 6 of 10 studies were considered successfully reproduced (60%). Availability statistics: working data links in paper: 64.1% (2011) and 78.9% (2016); working code links in paper: 18.6% (2011) and 36.2% (2016). After contacting authors, overall availability rose to data: 76% (2011) and 86% (2016); code: 33% (2011) and 59% (2016).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Lack of detailed documentation for pre-processing and experimental setup, software/toolkit version changes, randomness (unstated seeds), missing or proprietary code/data, lost artifacts, insufficient reporting of system variations, long training times preventing full validation, and small sample sizes causing large metric variance for subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Proposed measures: provide virtual images (e.g., Docker) with software, source code, and data; use reproducibility platforms (e.g., CodaLab worksheets); supply working software and data as part of publication; include detailed experimental documentation and version information.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>One reproduction attempt per selected paper (10 papers total); each attempt limited to up to 8 human-hours (CPU time allowed but human time limited).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Exact reproducibility is rare (1/10); allowing small tolerances increases apparent reproducibility (up to 6/10). Code sharing improved between 2011 and 2016 but many authors either cannot or do not supply code when requested (only ~18–36% of requested code supplied in earlier/later year samples).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Squib: Reproducibility in Computational Linguistics: Are We Willing to Share?', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e609.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e609.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sources of variability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Concrete sources of variability affecting reproducibility in CL experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper lists and documents multiple concrete factors that produce variability in reproduced results, based on literature and the authors' reproduction attempts and correspondence with original authors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reproducibility in Computational Linguistics: Are We Willing to Share?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computational linguistics / NLP experiments</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>identification and cataloging of factors causing variability in experimental outcomes and reproduction attempts</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Pre-processing differences, experimental set-up details, software/toolkit version differences (e.g., MALLET versions), undocumented system variations, random seeds and stochasticity in algorithms, small sample sizes (leading to high variance on subsets), lost/inaccessible code or data, proprietary components preventing re-use, and runtime/environment differences (e.g., dependencies/compilers).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Qualitative cataloging and illustrative numeric examples from reproduction attempts (see 'variability_results' entry for observed numeric deviations).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Same as variability_sources; emphasizes that many influencing details are not properly documented in publications and that even available code/data may not suffice due to versioning and undocumented preprocessing.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Document preprocessing and environment, track and report software versions, provide code and exact configuration, and share virtualized environments (Docker) or reproducibility worksheets (CodaLab).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A wide range of practical factors—especially software/version differences and undocumented preprocessing—can cause substantial divergence between original and reproduced results, and these are common in CL publications.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Squib: Reproducibility in Computational Linguistics: Are We Willing to Share?', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e609.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e609.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Observed numeric variability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Empirical numeric variability observed during reproduction attempts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper reports concrete numeric deviations between original and reproduced results that illustrate sources of variability, including toolkit-version-induced drops and large deviations on small subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reproducibility in Computational Linguistics: Are We Willing to Share?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computational linguistics / NLP reproducibility</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>attempted reproduction of reported performance metrics from selected ACL papers and reporting of numeric deviations</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Toolkit/version differences (MALLET), small-sample variability (per-verb subsets), possible undocumented preprocessing/settings.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Examples: He, Lin, and Alani (2011) reported 94.98% but reproduced 94.8% using same MALLET version; using a newer MALLET version decreased performance to 83.3% (version effect). Gao et al. (2016) reproduced two accuracy scores exactly, two deviated by 0.7 and 1.1 percentage points; per-verb subsets showed deviations up to 16.7 percentage points due to small sample sizes (subset sizes 6–58). Nicolai & Kondrak (2016) reproduced two values reported (98.5 and 82.3) as 94.8 and 80.8 within an 8-hour run limit. Coavoux & Crabbé (2016) reproduced most values exactly except one missing value out of ten.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Direct numeric comparison of reported vs. reproduced accuracy/performance percentages; counts of exact matches vs. toleranced matches.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Exact numeric match: 1/10 papers. Allowing small deviations (<=2 percentage points) and ignoring small-sample artifacts: 6/10 considered reproduced.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Version-induced performance differences can be large (e.g., ~11.7 percentage points drop from 94.98% to 83.3%); small-sample subsets can yield extremely large deviations (up to 16.7 percentage points).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Use exact software versioning and containerized environments; provide balanced/adequately sized evaluation subsets or report uncertainty for small subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Single reproduction run per reported metric/value (per paper), often limited by an 8-hour human time cap; not multiple-run statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Concrete numeric examples show that toolkit/version mismatches and small sample sizes can cause large and practically important deviations (several to >10 percentage points), so exact reproduction often fails without preserving environment and processing details.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Squib: Reproducibility in Computational Linguistics: Are We Willing to Share?', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e609.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e609.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mitigation recommendations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Practical mitigation methods to improve reproducibility in CL experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper recommends concrete interventions to improve reproducibility, notably sharing working code/data, providing virtualized execution environments (Docker), and using reproducibility platforms (CodaLab), and it links code availability to higher citation counts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reproducibility in Computational Linguistics: Are We Willing to Share?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computational linguistics / research reproducibility practice</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>evaluation of code/data sharing practices and formulation of recommendations to improve reproducibility</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>N/A (this entry focuses on mitigation methods rather than sources)</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Indirect assessment via (a) empirical reproduction success rates under current practices and (b) citation analysis comparing papers with and without code links.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Papers providing a link to source code had higher citation counts (2016: mean citations m_sc=27 vs m_no-sc=15, significant p=0.04; medians 8 vs 6 with p=0.005). Authors suggest Docker/CodaLab and mandatory accompanying software to improve reproducibility, but effectiveness not quantitatively tested in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Practical obstacles to adopting mitigations include proprietary code or data, lost artifacts, authors' inability to share, and legacy projects lacking preserved environments.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Provide working software and data with publications, include virtual images/containers (Docker) with environment and dependencies, use reproducibility platforms (CodaLab worksheets), document preprocessing and software versions, and enforce availability as part of publication acceptance.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Providing code/data and containerized environments is recommended; papers that provide code links tend to be cited more, suggesting an incentive for authors to share, but the paper does not experimentally validate mitigation effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Squib: Reproducibility in Computational Linguistics: Are We Willing to Share?', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Offspring from reproduction problems: What replication failure teaches us <em>(Rating: 2)</em></li>
                <li>Repeatability and benefaction in computer systems research <em>(Rating: 2)</em></li>
                <li>Empiricism is not a matter of faith <em>(Rating: 1)</em></li>
                <li>Learning dependency-based compositional semantics <em>(Rating: 1)</em></li>
                <li>A quantitative study of data in the NLP community <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-609",
    "paper_id": "paper-3cf13e7e55c833276f90ba7a1023667698e39784",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "Reproducibility Assessment",
            "name_full": "Reproducibility assessment of computational linguistics experiments",
            "brief_description": "The paper defines reproducibility as exact re-creation of reported results using the same data and methods, and reports an empirical assessment by attempting to reproduce results for a sample of papers and by measuring availability of code/data across two ACL years (2011, 2016).",
            "citation_title": "Reproducibility in Computational Linguistics: Are We Willing to Share?",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "computational linguistics / natural language processing",
            "experimental_task": "empirical assessment of availability and reproducibility: (a) measure whether papers provide working links to data and source code, (b) attempt to reproduce results for a random sample of 10 papers using provided data/code within an 8-hour limit per study",
            "variability_sources": "Documented sources discussed include undocumented experimental details (pre-processing, experimental set-up), software/toolkit version differences, system variations, randomness in procedures (random seeds), small sample sizes creating large per-subset variation, long training times limiting validation, proprietary tools preventing sharing, lost or inaccessible artifacts, and differences in runtime environments.",
            "variability_measured": false,
            "variability_metrics": null,
            "variability_results": null,
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Replication success recorded as exact match of reported numeric results (exact reproduction) and a relaxed criterion (allowing small deviations up to ~2 percentage points and excluding small-sample artifacts); also measured proportion of papers with working links to data/code and proportion that supplied materials after request.",
            "reproducibility_results": "Exact reproduction achieved for 1 of 10 selected studies (10%). Using a relaxed tolerance (ignoring one missing value and small-sample deviations and allowing up to 2 percentage points difference) up to 6 of 10 studies were considered successfully reproduced (60%). Availability statistics: working data links in paper: 64.1% (2011) and 78.9% (2016); working code links in paper: 18.6% (2011) and 36.2% (2016). After contacting authors, overall availability rose to data: 76% (2011) and 86% (2016); code: 33% (2011) and 59% (2016).",
            "reproducibility_challenges": "Lack of detailed documentation for pre-processing and experimental setup, software/toolkit version changes, randomness (unstated seeds), missing or proprietary code/data, lost artifacts, insufficient reporting of system variations, long training times preventing full validation, and small sample sizes causing large metric variance for subsets.",
            "mitigation_methods": "Proposed measures: provide virtual images (e.g., Docker) with software, source code, and data; use reproducibility platforms (e.g., CodaLab worksheets); supply working software and data as part of publication; include detailed experimental documentation and version information.",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": false,
            "number_of_runs": "One reproduction attempt per selected paper (10 papers total); each attempt limited to up to 8 human-hours (CPU time allowed but human time limited).",
            "key_findings": "Exact reproducibility is rare (1/10); allowing small tolerances increases apparent reproducibility (up to 6/10). Code sharing improved between 2011 and 2016 but many authors either cannot or do not supply code when requested (only ~18–36% of requested code supplied in earlier/later year samples).",
            "uuid": "e609.0",
            "source_info": {
                "paper_title": "Squib: Reproducibility in Computational Linguistics: Are We Willing to Share?",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "Sources of variability",
            "name_full": "Concrete sources of variability affecting reproducibility in CL experiments",
            "brief_description": "The paper lists and documents multiple concrete factors that produce variability in reproduced results, based on literature and the authors' reproduction attempts and correspondence with original authors.",
            "citation_title": "Reproducibility in Computational Linguistics: Are We Willing to Share?",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "computational linguistics / NLP experiments",
            "experimental_task": "identification and cataloging of factors causing variability in experimental outcomes and reproduction attempts",
            "variability_sources": "Pre-processing differences, experimental set-up details, software/toolkit version differences (e.g., MALLET versions), undocumented system variations, random seeds and stochasticity in algorithms, small sample sizes (leading to high variance on subsets), lost/inaccessible code or data, proprietary components preventing re-use, and runtime/environment differences (e.g., dependencies/compilers).",
            "variability_measured": false,
            "variability_metrics": null,
            "variability_results": null,
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Qualitative cataloging and illustrative numeric examples from reproduction attempts (see 'variability_results' entry for observed numeric deviations).",
            "reproducibility_results": null,
            "reproducibility_challenges": "Same as variability_sources; emphasizes that many influencing details are not properly documented in publications and that even available code/data may not suffice due to versioning and undocumented preprocessing.",
            "mitigation_methods": "Document preprocessing and environment, track and report software versions, provide code and exact configuration, and share virtualized environments (Docker) or reproducibility worksheets (CodaLab).",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": false,
            "number_of_runs": null,
            "key_findings": "A wide range of practical factors—especially software/version differences and undocumented preprocessing—can cause substantial divergence between original and reproduced results, and these are common in CL publications.",
            "uuid": "e609.1",
            "source_info": {
                "paper_title": "Squib: Reproducibility in Computational Linguistics: Are We Willing to Share?",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "Observed numeric variability",
            "name_full": "Empirical numeric variability observed during reproduction attempts",
            "brief_description": "The paper reports concrete numeric deviations between original and reproduced results that illustrate sources of variability, including toolkit-version-induced drops and large deviations on small subsets.",
            "citation_title": "Reproducibility in Computational Linguistics: Are We Willing to Share?",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "computational linguistics / NLP reproducibility",
            "experimental_task": "attempted reproduction of reported performance metrics from selected ACL papers and reporting of numeric deviations",
            "variability_sources": "Toolkit/version differences (MALLET), small-sample variability (per-verb subsets), possible undocumented preprocessing/settings.",
            "variability_measured": false,
            "variability_metrics": null,
            "variability_results": "Examples: He, Lin, and Alani (2011) reported 94.98% but reproduced 94.8% using same MALLET version; using a newer MALLET version decreased performance to 83.3% (version effect). Gao et al. (2016) reproduced two accuracy scores exactly, two deviated by 0.7 and 1.1 percentage points; per-verb subsets showed deviations up to 16.7 percentage points due to small sample sizes (subset sizes 6–58). Nicolai & Kondrak (2016) reproduced two values reported (98.5 and 82.3) as 94.8 and 80.8 within an 8-hour run limit. Coavoux & Crabbé (2016) reproduced most values exactly except one missing value out of ten.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Direct numeric comparison of reported vs. reproduced accuracy/performance percentages; counts of exact matches vs. toleranced matches.",
            "reproducibility_results": "Exact numeric match: 1/10 papers. Allowing small deviations (&lt;=2 percentage points) and ignoring small-sample artifacts: 6/10 considered reproduced.",
            "reproducibility_challenges": "Version-induced performance differences can be large (e.g., ~11.7 percentage points drop from 94.98% to 83.3%); small-sample subsets can yield extremely large deviations (up to 16.7 percentage points).",
            "mitigation_methods": "Use exact software versioning and containerized environments; provide balanced/adequately sized evaluation subsets or report uncertainty for small subsets.",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": false,
            "number_of_runs": "Single reproduction run per reported metric/value (per paper), often limited by an 8-hour human time cap; not multiple-run statistics.",
            "key_findings": "Concrete numeric examples show that toolkit/version mismatches and small sample sizes can cause large and practically important deviations (several to &gt;10 percentage points), so exact reproduction often fails without preserving environment and processing details.",
            "uuid": "e609.2",
            "source_info": {
                "paper_title": "Squib: Reproducibility in Computational Linguistics: Are We Willing to Share?",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "Mitigation recommendations",
            "name_full": "Practical mitigation methods to improve reproducibility in CL experiments",
            "brief_description": "The paper recommends concrete interventions to improve reproducibility, notably sharing working code/data, providing virtualized execution environments (Docker), and using reproducibility platforms (CodaLab), and it links code availability to higher citation counts.",
            "citation_title": "Reproducibility in Computational Linguistics: Are We Willing to Share?",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "computational linguistics / research reproducibility practice",
            "experimental_task": "evaluation of code/data sharing practices and formulation of recommendations to improve reproducibility",
            "variability_sources": "N/A (this entry focuses on mitigation methods rather than sources)",
            "variability_measured": false,
            "variability_metrics": null,
            "variability_results": null,
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Indirect assessment via (a) empirical reproduction success rates under current practices and (b) citation analysis comparing papers with and without code links.",
            "reproducibility_results": "Papers providing a link to source code had higher citation counts (2016: mean citations m_sc=27 vs m_no-sc=15, significant p=0.04; medians 8 vs 6 with p=0.005). Authors suggest Docker/CodaLab and mandatory accompanying software to improve reproducibility, but effectiveness not quantitatively tested in this study.",
            "reproducibility_challenges": "Practical obstacles to adopting mitigations include proprietary code or data, lost artifacts, authors' inability to share, and legacy projects lacking preserved environments.",
            "mitigation_methods": "Provide working software and data with publications, include virtual images/containers (Docker) with environment and dependencies, use reproducibility platforms (CodaLab worksheets), document preprocessing and software versions, and enforce availability as part of publication acceptance.",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": false,
            "number_of_runs": null,
            "key_findings": "Providing code/data and containerized environments is recommended; papers that provide code links tend to be cited more, suggesting an incentive for authors to share, but the paper does not experimentally validate mitigation effectiveness.",
            "uuid": "e609.3",
            "source_info": {
                "paper_title": "Squib: Reproducibility in Computational Linguistics: Are We Willing to Share?",
                "publication_date_yy_mm": "2018-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Offspring from reproduction problems: What replication failure teaches us",
            "rating": 2
        },
        {
            "paper_title": "Repeatability and benefaction in computer systems research",
            "rating": 2
        },
        {
            "paper_title": "Empiricism is not a matter of faith",
            "rating": 1
        },
        {
            "paper_title": "Learning dependency-based compositional semantics",
            "rating": 1
        },
        {
            "paper_title": "A quantitative study of data in the NLP community",
            "rating": 1
        }
    ],
    "cost": 0.00966825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Reproducibility in Computational Linguistics: Are We Willing to Share?</h1>
<p>Martijn Wieling<br>University of Groningen<br>Center for Language and<br>Cognition Groningen<br>wieling@gmail.com<br>Josine Rawee<br>Master's student<br>University of Groningen<br>Center for Language and<br>Cognition Groningen<br>josine@rawee.nl<br>Gertjan van Noord<br>University of Groningen<br>Center for Language and<br>Cognition Groningen<br>g.j.m.van.noord@rug.nl</p>
<p>This study focuses on an essential precondition for reproducibility in computational linguistics: the willingness of authors to share relevant source code and data. Ten years after Ted Pedersen's influential "Last Words" contribution in Computational Linguistics, we investigate to what extent researchers in computational linguistics are willing and able to share their data and code. We surveyed all 395 full papers presented at the 2011 and 2016 ACL Annual Meetings, and identified whether links to data and code were provided. If working links were not provided, authors were requested to provide this information. Although data were often available, code was shared less often. When working links to code or data were not provided in the paper, authors provided the code in about one third of cases. For a selection of ten papers, we attempted to reproduce the results using the provided data and code. We were able to reproduce the results approximately for six papers. For only a single paper did we obtain the exact same results. Our findings show that even though the situation appears to have improved comparing 2016 to 2011, empiricism in computational linguistics still largely remains a matter of faith. Nevertheless, we are somewhat optimistic about the future. Ensuring reproducibility is not only important for the field as a whole, but also seems worthwhile for individual researchers: The median citation count for studies with working links to the source code is higher.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>1. Introduction</h1>
<p>Reproducibility ${ }^{1}$ of experimental research results has become an important topic in the scientific debate across many disciplines. There now even is a Wikipedia page on the topic entitled "Replication Crisis," ${ }^{2}$ with a description of some of the most worrying results and links to the relevant studies. In a survey conducted by Nature in 2016, more than half of over 1,500 participating scientists claim that there is a "significant reproducibility crisis." ${ }^{3}$</p>
<p>For computational linguistics, one might initially be optimistic about reproducibility, given that we mostly work with relatively "static" data sets and computer programs-rather than, for instance, with human participants or chemical substances. Yet, Pedersen (2008) points out in a very recognizable "Last Words" contribution in Computational Linguistics that it is often impossible to obtain the relevent data and software. Our study, ten years later, investigates whether this basic prerequisite for reproducibility is now in a better state.</p>
<p>Reproducing the outcome of an experiment is often difficult because there are many details that influence the outcome, and more often than not those details are not properly documented. Observations about reproducibility difficulties have been made frequently in the past. Bikel (2004), for instance, attempted to reproduce the parsing results of Collins (1999) but initially did not obtain nearly the same results. Bikel then continued to show that implementing Collins' model using only the published details caused an $11 \%$ increase in relative error over Collins' own published results.</p>
<p>Fokkens et al. (2013) report on two failed reproduction efforts. Their results indicate that even if data and code are available, reproduction is far from trivial, and they provide a careful analysis of why reproduction is difficult. They show that many details (including pre-processing, the experimental set-up, versioning, system output, and system variations) are important in reproducing the exact results of published research. In most cases, such details are not documented in the publication, nor elsewhere. Their results are the more striking because one of the co-authors of that study was the original author of the paper documenting the experiments that the authors set out to reproduce.</p>
<p>It is clear, therefore, that in computational linguistics reproducibility cannot be taken for granted either-as is also illustrated by recent initiatives, such as the IJCAI workshop on replicability and reproduciblity in NLP in 2015, the set-up of a dedicated LREC workshop series "4Real" with workshops in 2016 and 2018, and the introduction of a special section of Language Resources and Evaluation (Branco et al. 2017).</p>
<p>Our study extends the study of Mieskes (2017). She investigated how often studies published at various computational linguistics conferences provided a link to the data. She found that about $40 \%$ of the papers collected new data or changed existing data. Only in about $65 \%$ of these papers was a link to the data provided. A total of $18 \%$ of these links did not appear to work.</p>
<p>In our study, we focus on another essential precondition for reproduction, namely, the availability of the underlying source code. We evaluate how often data and source code are shared. We did not only follow up on links given in the papers, but we contacted authors of papers by e-mail with requests for their data and code as well. In</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>addition, we investigate to what extent we are able to reproduce results of ten studies for which we were able to obtain the relevant data and software. Our study is related to the study of Collberg, Proebsting, and Warren (2015), who investigated the frequency with which they could obtain the source code and data for publications in ACM conferences and journals, and whether the received code could be compiled. They found that only in about one third of the cases were they able to obtain and build the code without any special effort.</p>
<p>Importantly, we also evaluate (a rough indication of) the impact of each study via the citation counts of each study. Specifically, we assess whether there are observable differences in impact when comparing papers whose authors share their code directly (i.e., via a link in the paper) versus those that do not. Because we establish that papers that provide links to the code are typically somewhat more often cited than papers that do not, we hope to provide researchers in computational linguistics with additional motivation to make their source code available.</p>
<h1>2. Methods</h1>
<h3>2.1 Obtaining Data and Source Code</h3>
<p>The goal of this study is to assess the availability of the underlying data and source code of computational linguistics studies that were presented at two ACL conferences. We selected all full papers from the 2011 and 2016 ACL Annual Meetings (in Portland and Berlin), enabling us to compare the willingness and ability to share data for older (i.e., over 6 years ago at the time our study was conducted) versus more recent studies (i.e., about 1 year ago at the time our study was conducted).</p>
<p>Our procedure was as follows. For all 2011 and 2016 ACL full papers, we manually assessed whether data and/or software (i.e., source code) was used, modified, or created. For each paper, we subsequently registered whether links to data and/or the software were made available. ${ }^{4}$ If data and/or source code were used and not made available, we contacted the first author of the study with a request for the data and/or source code (depending on what was missing).</p>
<p>Given that we wanted to obtain a realistic estimate of the number of authors who were willing to provide their data and/or source code, we constructed the e-mail text (included in the supplementary material, see Section 4) in such a way that the recipients had the impression that their specific study would be reproduced. Although this is not completely fair to the authors (since we only reproduced a small sample), simply asking them about their willingness to provide the data and/or source code without actually asking them to send the files would have resulted in overly optimistic results. ${ }^{5}$ In addition, we explicitly indicated in the e-mail that one of the senders was a past president of the ACL. Given that the request for source code and/or data came from an established member of the ACL community, it is likely that our request was not dismissed easily. We will return to this point in the Discussion.</p>
<p>The first e-mail was sent on 9 September 2017 to the first author of each study for which data and/or source code was not available. If the e-mail address (extracted from the paper) no longer existed, we tried to obtain the current e-mail address via a Google</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>search. In the very few cases where this did not work, we sent the e-mail to another author of the paper. If the author did not send the data and/or source code (nor replied that it was not possible to send the requested information), we sent a second and final e-mail on 24 October 2017. In contrast to the first e-mail, this second e-mail was sent to all authors of the paper, and the deadline for sending the information was extended to 19 November 2017.</p>
<p>A slightly different procedure was used for those authors who provided links in the paper to their source code and/or data that were no longer accessible. In that case we immediately contacted all authors with a request (similar to the other e-mail) to send us the updated link to the data and/or source code within two weeks. As these authors had already made this information available earlier, we only sent a single e-mail and no reminder.</p>
<p>Finally, for each of the 395 papers in this study, we obtained citation counts from Google Scholar on 10 March 2018.</p>
<h1>2.2 Reproducing Results from Selected Studies</h1>
<p>After having obtained the underlying data and/or code, we attempted to reproduce the results of a random selection of five studies from 2011 (Nakov and Ng 2011; He, Lin, and Alani 2011; Sauper, Haghighi, and Barzilay 2011; Liang, Jordan, and Klein 2011; Branavan, Silver, and Barzilay 2011) and a random ${ }^{6}$ selection of five studies from 2016 (Coavoux and Crabbé 2016; Gao et al. 2016; Hu et al. 2016; Nicolai and Kondrak 2016; Tian, Okazaki, and Inui 2016) for which the data and source code was provided, either through links in the paper, or to us after our request.</p>
<p>Our approach to reproduce these results was as follows: We used the information provided in the paper and accompanying the source code to reproduce the results. If we were not able to run the source code, or if our results deviated from the results of the authors, we contacted the authors to see if they were able to help. Note that this should only be seen as a minimal reproduction effort: We limited the amount of human (not CPU) time spent on reproducing each study to a total of 8 hours. The results obtained within this time limit were compared with the original results of the aforementioned studies. The second author (a Language and Communication Technologies Erasmus Mundus Master student) conducted the replication using a regular laptop.</p>
<h2>3. Results</h2>
<h3>3.1 Availability of Data and/or Source Code</h3>
<p>The distribution of the links that were available and the responses of the authors we contacted is shown in Table 1. Whereas most of the data were already provided or uniquely specified in the paper (i.e., links worked in $64 \%$ and $79 \%$ of cases for 2011 and 2016, respectively), this was not the same for the source code (provided in $19 \%$ and $36 \%$ of cases, respectively). After having contacted the authors, and including that data and source code as well (i.e., providing the updated link or sending the data and/or source code), these percentages increased to $76 \%$ and $86 \%$ for the data availability, and $33 \%$ and $59 \%$ for the source code availability. When contacting the authors, the</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1
Distribution of data and code availability in both 2011 and 2016.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">2011: data</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">2016: data</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">2011: code</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">2016: code</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Data / code available</td>
<td style="text-align: center;">116</td>
<td style="text-align: center;">75.8\%</td>
<td style="text-align: center;">196</td>
<td style="text-align: center;">86.3\%</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">$33.1 \%$</td>
<td style="text-align: center;">131</td>
<td style="text-align: center;">59.3\%</td>
</tr>
<tr>
<td style="text-align: center;">- working link in paper</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">$64.1 \%$</td>
<td style="text-align: center;">179</td>
<td style="text-align: center;">78.9\%</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">$18.6 \%$</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">$36.2 \%$</td>
</tr>
<tr>
<td style="text-align: center;">- link sent</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">$7.2 \%$</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">$6.6 \%$</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">$11.7 \%$</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">$22.6 \%$</td>
</tr>
<tr>
<td style="text-align: center;">- repaired link sent</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">$4.6 \%$</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$0.9 \%$</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$2.8 \%$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$0.5 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Data / code unavailable</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">24.2\%</td>
<td style="text-align: center;">31</td>
<td style="text-align: center;">13.7\%</td>
<td style="text-align: center;">97</td>
<td style="text-align: center;">66.9\%</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">40.7\%</td>
</tr>
<tr>
<td style="text-align: center;">- sharing impossible</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">$12.4 \%$</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">$6.2 \%$</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">$31.7 \%$</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">$19.0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">- no reply</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">$11.1 \%$</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">$5.3 \%$</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">$29.7 \%$</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">$14.5 \%$</td>
</tr>
<tr>
<td style="text-align: center;">- good intentions</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$0.0 \%$</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$0.9 \%$</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">$3.4 \%$</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">$5.4 \%$</td>
</tr>
<tr>
<td style="text-align: center;">- link down</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$0.7 \%$</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$1.3 \%$</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$2.0 \%$</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$1.8 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">153</td>
<td style="text-align: center;">100\%</td>
<td style="text-align: center;">227</td>
<td style="text-align: center;">100\%</td>
<td style="text-align: center;">145</td>
<td style="text-align: center;">100\%</td>
<td style="text-align: center;">221</td>
<td style="text-align: center;">100\%</td>
</tr>
<tr>
<td style="text-align: center;">No data/code used</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">19</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">10</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Total nr. of papers</td>
<td style="text-align: center;">164</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">231</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">164</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">231</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>most frequent response type was that sharing was impossible due to (for example,) having moved to another institute or company and not having access to the data, being prohibited from sharing source code that used proprietary company tools, or having lost the data or source code. The second-most frequent type we observed was the absence of action. In those cases, we did not receive any reply to our e-mails. The thirdmost frequent response type was authors with good intentions, who replied that they were going to send the requested data and / or code, but did not end up doing so. In only a very few cases ( $1-2 \%$ ), the link to the source code and / or data was not provided anew, if they were initially present in the paper and no longer working. The total percentage of available data and / or source code is informative, but another important measure is how often the source code and / or data were provided when it had to be requested (i.e., the sum of the sent and repaired link sent frequencies in the appropriate column in Table 1 as a proportion of the sum of these two frequencies and the number of papers in the corresponding column for which data or code was unavailable). Unfortunately, these percentages are rather low, with $32.7 \%$ for requested 2011 data, $35.4 \%$ for requested 2016 data, $17.8 \%$ for requested 2011 source code, and $36.2 \%$ for requested 2016 source code. In sum, if data and / or source code were not referenced through a link to a repository in the paper, authors will most likely not (be able to) supply this information.</p>
<p>Nevertheless, there is a clear improvement between 2011 and 2016. The number of papers containing a working link to source code almost doubled. Of course, the improvement can be explained at least partly by observing that it is much easier to share recent data and source code, rather than older data and code from 5 years ago.</p>
<p>Subsequently, another important question is, if we get access to the data and/or code, how likely is it that the results reported therein are reproducible? The following subsection attempts to provide a tentative answer to this question.</p>
<h1>3.2 Reproducibility of Selected Studies</h1>
<p>For the 2011 papers we selected, we were only able to reproduce the results of a single study (Liang, Jordan, and Klein 2011) perfectly (time invested: 4 hours). For the study of He, Lin, and Alani (2011), we were able to reproduce the results almost (but not</p>
<p>quite) perfectly: Their reported performance was $94.98 \%$, whereas the performance we obtained was $94.8 \%$ (using the same version of the underlying MAchine Learning for LanguagE Toolkit the authors used). Interestingly, the performance was reduced to $83.3 \%$ when the most recent version of the MALLET toolkit was used (which was also noted by the authors). We were not able to reproduce any results for the three remaining 2011 studies we selected (Branavan, Silver, and Barzilay 2011; Nakov and Ng 2011; Sauper, Haghighi, and Barzilay 2011).</p>
<p>The results are better for the 2016 papers we selected. For the paper of Coavoux and Crabbé (2016), we were able to reproduce (within 5.5 hours) most of the results exactly as reported. There was only a single value out of ten that we were not able to compute. For the study of Gao et al. (2016), the reproduction results (obtained within 2 hours) were also similar. On the basis of all data, two accuracy scores out of four were identical, and the other two deviated by 0.7 and 1.1 points. The results for the corresponding baseline deviated by 0.3 and 1.0 points. The results regarding individual verbs (i.e., subsets of all data) were much more variable, with deviations of up to 16.7 points. However, this was caused by the smaller sample sizes (ranging from 6 to 58). For the study of Hu et al. (2016) we obtained results (within 3.5 hours) that were (almost) identical to those reported in the paper (i.e., 88.8 and 89.1 versus 88.8 and 89.3 reported in the paper). The models used by Nicolai and Kondrak (2016) took a long time to train and for this reason we were only able to validate two accuracy values (out of nine). Both values we obtained (taking 8 hours to compute) were similar, but not identical to those reported in the paper (reported performance: 98.5 and 82.3 , our performance: 94.8 and 80.8). Finally, we were able to reproduce (within 3.5 hours) most of the results reported by Tian, Okazaki, and Inui (2016). Four out of six performance values were reproduced exactly, the remaining two performance values we checked only differed slightly ( 0.41 and $81.2 \%$ compared to the reproduced values of 0.42 and $81.1 \%$, respectively).</p>
<p>In sum, we were only able to reproduce the identical results of a single study (Liang, Jordan, and Klein 2011). Of course some variability may be expected, due to (for example) randomness in the procedure. If we are a bit more flexible and ignore the single value we were not able to compute during the reproduction of Coavoux and Crabbé (2016) and the small sample results of Gao et al. (2016), and also ignore deviations for reproduced results of up to 2 percentage points, then two 2011 studies (Liang, Jordan, and Klein 2011; He, Lin, and Alani 2011) and four 2016 studies (Coavoux and Crabbé 2016; Gao et al. 2016; Hu et al. 2016; Tian, Okazaki, and Inui 2016) were reproduced successfully.</p>
<h1>3.3 Citation Analysis</h1>
<p>To see if there is a tangible benefit for authors to share the source code underlying their paper, we contrasted the number of citations for the papers that provided the code through a link in the paper to those that did not. Comparing the citation counts for the papers from 2011 showed a non-significant ( $p&gt;0.05$ ) higher mean citation count for the studies that did not provide the source code compared with those that did provide the source code: $\mathrm{t}(117.74)=-0.78, \mathrm{p}=0.44, m_{\mathrm{sc}}=71, m_{\mathrm{no}-\mathrm{sc}}=84$. Note that the higher mean for the studies that did not provide the link to the code is caused by 12 highly cited papers. Excluding these outliers (and the single outlier from the 2011 papers that did provide a link to the code) yields the opposite pattern, with a significant higher mean citation count for the 2011 papers providing the source code than those that did not: $\mathrm{t}(52.19)=2.13, \mathrm{p}=0.04, m_{\mathrm{sc}}=62, m_{\mathrm{no}-\mathrm{sc}}=44$. For 2016, we observe a significant difference, with a higher citation count for the papers providing the source</p>
<p>code than those that did not: $\mathrm{t}(115.12)=2.1, \mathrm{p}=0.04, m_{\mathrm{sc}}=27, m_{\text {no-sc }}=15$. Excluding the outliers ( 9 papers providing the source code, 15 papers that did not provide the source code) strengthened this effect: $\mathrm{t}(94.68=3.7, \mathrm{p}&lt;0.001, m_{\mathrm{sc}}=14.3, m_{\text {no-sc }}=7.3)$. Papers providing the source code had a mean citation count almost double that of the papers that did not provide the source code.</p>
<p>Even though the t-test is highly robust to deviations from normality (Zar 1999, pages 127-129), we also analyzed the results using (quasi-)Poisson regression. This supplementary analysis supported the findings resulting from the t-test: when analyzing all data including outliers, the difference between the 2011 papers providing a link to the underlying source code versus those which did not was not significant ( $\mathrm{p}=0.58$ ). For the 2016 papers, the difference was significant ( $\mathrm{p}=0.01$ ). When excluding the outliers, the differences were significant for both 2011 and 2016 (all p's $&lt;0.04$ ).</p>
<p>Given that citation counts are highly skewed, we also compared the medians (that are influenced less by outliers). For 2011, the median citation count for the papers that provided a link to the source code was 60 , whereas it was only 30 for those that did not provide a link to the source code underlying the paper. Despite the large difference in medians, this difference was not significant (Mann-Whitney U test: $\mathrm{p}=0.15$ ). For the papers published in 2016, the median citation count for the papers providing a link to the source code was 8 , whereas it was 6 for those which did not. As with the t-test, this difference was significant (Mann-Whitney $U$ test: $p=0.005$ ). When excluding the outliers, the median group differences were significant for both years (all p's $&lt;0.02$ ).</p>
<p>In sum, papers that provided a link to the source code were more often cited than those that did not. Although this may suggest that providing a link to the source code results in a greater uptake of the paper, this relationship is not necessarily causal. Even though providing the source code may make it easier for other authors to build upon the approach of the other authors, it is also possible that authors who provide links to the source code may have spent more time carefully planning and working on the paper, thereby increasing the quality of the work and thus the uptake by the community.</p>
<h1>4. Discussion</h1>
<p>In this article we have assessed how often data and/or source code is provided in order to enable a reproducibility study. Although data are often available, source code is made available less often. Fortunately, there is a clear improvement from 2011 to 2016, with the percentage of papers providing a (working) link to the source code approximately doubling (from $18.6 \%$ to $36.2 \%$ ). Unfortunately, requesting the source code (if it was not already provided) is unlikely to be successful, as only about a third of the requests was (or could be) granted. It is likely that the (relatively low) success of our requests is an upper bound. The reason for this is that we signed our e-mails requesting the data and/or source code with the name of an established member of the ACL community (a past ACL president).</p>
<p>Finally, even if the source code and data are available, there is no guarantee that the results are reproducible. On the basis of five studies selected from 2011 and five studies from 2016, we found that at most $60 \%$ of the studies were reproducible when not enforcing an exact reproduction. If an exact reproduction was required, then only a single study (from 2011) was reproducible. Approaches such as providing a virtual (e.g., Docker) image with all software, source code, and data, or using CodaLab worksheets as done by Liang, Jordan, and Klein (2011) might prove to be worthwhile in order to ensure a more effortless reproduction.</p>
<p>We would like to end with the following recommendation of Pedersen (2008, page 470) made ten years ago, but remaining relevant today:
[Another course of action] is to accept (and in fact insist) that highly detailed empirical studies must be reproducible to be credible, and that it is unreasonable to expect that reproducibility be possible based on the description provided in a publication. Thus, releasing software that makes it easy to reproduce and modify experiments should be an essential part of the publication process, to the point where we might one day only accept for publication articles that are accompanied by working software that allows for immediate and reliable reproduction of results.</p>
<p>Because we established that papers that provide links to the code are typically more often cited than papers that do not, we hope to provide researchers in computational linguistics with additional motivation to make their source code available in future publications.</p>
<h2>Acknowledgments</h2>
<p>We thank all authors who took the effort to respond to our request for their data and code.</p>
<p>The (anonymized) count data, the reproduced values for the ten studies, listings of the reasons for replication failure, the $R$ code used for the statistical analyses (including box plots), and the e-mail text requesting the data and code can be downloaded at http://www.let.rug.nl/ wieling/CL-repro/repro.xlsx.</p>
<h2>References</h2>
<p>Barba, Lorena A. 2018. Terminologies for reproducible research. arXiv preprint arXiv:1802.03311.
Bikel, Daniel M. 2004. Intricacies of Collins' parsing model. Computational Linguistics, 30(4):479-511.
Branavan, S. R. K., David Silver, and Regina Barzilay. 2011. Learning to win by reading manuals in a Monte-Carlo framework. In Proceedings of the 49th Annual Meeting of the ACL, pages 268-277, Portland, OR.
Branco, António, Kevin Bretonnel Cohen, Piek Vossen, Nancy Ide, and Nicoletta Calzolari. 2017. Replicability and reproducibility of research results for human language technology: Introducing an LRE special section. Language Resources and Evaluation, 51(1):1-5.
Coavoux, Maximin and Benoit Crabbé. 2016. Neural greedy constituent parsing with dynamic oracles. In Proceedings of the 54th Annual Meeting of the ACL, pages 172-182, Berlin.
Collberg, Christian, Todd Proebsting, and Alex M. Warren. 2015. Repeatability
and benefaction in computer systems research. Technical report. http:// reproducibility.cs.arizona.edu/.
Collins, Michael. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.
Drummond, Chris. 2009. Replicability is not reproducibility: Nor is it good science. In Proceedings of the Twenty-Sixth International Conference on Machine Learning: Workshop on Evaluation Methods for Machine Learning IV, Montreal.
Fokkens, Antske, Marieke Erp, Marten Postma, Ted Pedersen, Piek Vossen, and Nuno Freire. 2013. Offspring from reproduction problems: What replication failure teaches us. In Proceedings of the 51st Annual Meeting of the ACL, volume 1, pages 1691-1701, Sofia.
Gao, Qiaozi, Malcolm Doering, Shaohua Yang, and Joyce Chai. 2016. Physical causality of action verbs in grounded language understanding. In Proceedings of the 54th Annual Meeting of the ACL, pages 1814-1824, Berlin.
He, Yulan, Chenghua Lin, and Harith Alani. 2011. Automatically extracting polarity-bearing topics for cross-domain sentiment classification. In Proceedings of the 49th Annual Meeting of the ACL, pages 123-131, Portland, OR.
Hu, Zhiting, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, and Eric Xing. 2016. Harnessing deep neural networks with logic rules. In Proceedings of the 54th Annual Meeting of the ACL, pages 2410-2420, Berlin.
Liang, Percy, Michael I. Jordan, and Dan Klein. 2011. Learning dependency-based compositional semantics. In Proceedings of the 49th Annual Meeting of the ACL, pages 590-599, Portland, OR.</p>
<p>Liberman, Mark. 2015. Replicability vs. reproducibility-or is it the other way around? Language Log, 31 October; http://languagelog.ldc.upenn.edu/ nll/?p=21956.
Mieskes, Margot. 2017. A quantitative study of data in the NLP community. In Proceedings of the First ACL Workshop on Ethics in Natural Language Processing, pages 23-29, Valencia.
Nakov, Preslav and Hwee Tou Ng. 2011. Translating from morphologically complex languages: A paraphrase-based approach. In Proceedings of the 49th Annual Meeting of the ACL, pages 1298-1307, Portland, OR.
Nicolai, Garrett and Grzegorz Kondrak. 2016. Leveraging inflection tables for stemming and lemmatization.</p>
<p>In Proceedings of the 54th Annual Meeting of the ACL, pages 1138-1147, Berlin.
Pedersen, Ted. 2008. Empiricism is not a matter of faith. Computational Linguistics, 34(3):465-470.
Sauper, Christina, Aria Haghighi, and Regina Barzilay. 2011. Content models with attitude. In Proceedings of the 49th Annual Meeting of the ACL, pages 350-358, Portland, OR.
Tian, Ran, Naoaki Okazaki, and Kentaro Inui. 2016. Learning semantically and additively compositional distributional representations. In Proceedings of the 54th Annual Meeting of the ACL, pages 1277-1287, Berlin.
Zar, Jerrold H. 1999. Biostatistical Analysis. Prentice Hall, Upper Saddle River, NJ.</p>
<p>.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>6 The study of Nicolai and Kondrak (2016) was included as the authors explicitly asked if we could include them in the experimentation process.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
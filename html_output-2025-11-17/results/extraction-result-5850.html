<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5850 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5850</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5850</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-53d8b356551a2361020a948f64454a6d599af69f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/53d8b356551a2361020a948f64454a6d599af69f" target="_blank">Prefix-Tuning: Optimizing Continuous Prompts for Generation</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> Prefix-tuning is proposed, a lightweight alternative to fine- Tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which is called the prefix.</p>
                <p><strong>Paper Abstract:</strong> Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were “virtual tokens”. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5850.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5850.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prefix-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prefix-tuning (continuous task-specific prefix)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that freezes a pretrained LM and learns a small sequence of continuous task-specific vectors (a 'prefix') prepended to the input; the LM attends to the prefix like virtual tokens and only the prefix parameters are optimized. Applied to GPT-2 for table-to-text and BART for summarization in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (MEDIUM, LARGE) and BART_LARGE</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Table-to-text (E2E, WebNLG, DART) and Abstractive Summarization (XSUM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Conditional NLG: generate textual descriptions from linearized tables (E2E, WebNLG, DART) and generate abstractive summaries from news articles (XSUM).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prepend a learned continuous prefix (sequence of activation vectors P_θ) to the model input: [PREFIX; x; y] for autoregressive models or [PREFIX; x; PREFIX'; y] for encoder-decoder; prefix vectors are free parameters (reparametrized via a small matrix + MLP for stability). Prefix length is a hyperparameter.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to (a) full fine-tuning (update all LM parameters), (b) adapter-tuning, (c) top-2-layer fine-tuning (FT-TOP2), (d) embedding-only continuous prompts, and (e) infix-tuning (placing learned activations between x and y).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table-to-text (GPT-2 MEDIUM, Prefix 0.1% params): E2E BLEU=74.8; WebNLG BLEU=64.52; DART BLEU=51.11. Summarization (BART_LARGE): Prefix(2% params) ROUGE-L=35.21 (validation); Prefix(0.1%) ROUGE-L=33.13 (validation). (See paper tables for full metric set.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Full fine-tuning (FT-FULL): E2E BLEU=74.2, WebNLG BLEU=66.03, DART BLEU=50.46. Summarization FT-FULL ROUGE-L reported ≈37.25. Adapter and FT-TOP2 reported in tables; embedding-only and infix ablations also compared (see separate entries).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Relative to FT-FULL: E2E +0.6 BLEU (74.8 vs 74.2); WebNLG -1.51 BLEU (64.52 vs 66.03); DART +0.65 BLEU (51.11 vs 50.46). Summarization: Prefix(2%) ROUGE-L ≈ -2.04 vs FT (35.21 vs 37.25); Prefix(0.1%) ROUGE-L ≈ -4.12.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>Mixed — prefix-tuning matches or slightly improves performance on table-to-text (comparable/Pareto-efficient) but underperforms full fine-tuning on XSUM summarization, with the gap larger at very small parameter budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize that keeping pretrained LM parameters frozen preserves general-purpose representations and provides an inductive bias that helps generalization and extrapolation; prefix-tuning is more parameter-efficient and modular, can influence encoding of x and decoding of y, and can better extrapolate to unseen topics in low-data regimes because it steers the LM without overwriting pretrained knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Summarization on XSUM: prefix-tuning (especially at 0.1% params) shows a clear performance drop vs full fine-tuning, indicating the format is not universally superior; in full-data summarization prefix-tuning suffers a small degradation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5850.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5850.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Embedding-only prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embedding-only continuous prompt (optimize only virtual-token embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reduced variant where only the input token embeddings of virtual prompt tokens are optimized while all deeper activations are computed normally by the Transformer; evaluated as an ablation of prefix-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (MEDIUM) (evaluated in table-to-text experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Table-to-text (E2E; intrinsic evaluation reported in Table 5)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate text descriptions from linearized table inputs; embedding-only ablation compares to full prefix-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Optimize only the embedding vectors of 'virtual tokens' prepended to the input (embedding-only), not the full-layer activations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to full prefix-tuning (trainable activations at all layers) and to different prefix lengths (EmB-1, EmB-10, EmB-20 reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Intrinsic metrics (Table 5): Prefix (full) BLEU=70.3; Embedding-only EmB-10 BLEU=62.2, EmB-1 BLEU=48.1, EmB-20 BLEU=61.9. Other metrics (NIST/MET/ROUGE/CIDEr) also substantially lower for embedding-only.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Full prefix-tuning BLEU=70.3 vs Embedding-only EmB-10 BLEU=62.2 (difference -8.1 BLEU).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>-8.1 BLEU (EmB-10 vs full prefix) in intrinsic table-to-text eval.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>Reduced — optimizing only embeddings is significantly worse than optimizing full-layer prefix activations.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Embedding-only is less expressive because it only changes the embedding layer and cannot directly modify deeper activation layers; it upper-bounds discrete prompt optimization and therefore underperforms prefix-tuning which can modify representations throughout the network.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5850.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5850.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Infix-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Infix-tuning (trainable activations between input and output)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Place learned, trainable activation vectors between the input x and the target y (format [x; INFIX; y]) instead of at the front; evaluated to test positional effects of trainable activations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 (MEDIUM) (table-to-text experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Table-to-text (intrinsic eval in Table 5)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate text from linearized table inputs; infix-tuning compares effect of placing learnable activations between x and y vs prepending them.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Learnable activations inserted between x and y: [x; Infix; y].</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared directly to prefix-tuning (learnable activations prepended: [Prefix; x; y]).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Intrinsic metrics (Table 5): Infix-1 BLEU=67.9, Infix-10 BLEU=67.2, Infix-20 BLEU=66.7 (compared to Prefix BLEU=70.3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Prefix (70.3 BLEU) vs Infix-1 (67.9) => -2.4 BLEU.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>-2.4 BLEU (Infix-1 vs Prefix)</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>Reduced — infix-tuning slightly underperforms prefix-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors suggest prefix-tuning can influence encoding of x and subsequent decoding of y, whereas infix-tuning can only directly influence the decoder-side activations (y), so prefix placement gives more steering power over both input encoding and generation.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5850.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5850.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prefix length ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prefix-length sensitivity (length vs performance)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systematic study of how varying the number of prefix tokens (length of the learned prefix) affects training loss and test performance, showing an improvement up to a dataset-dependent threshold and possible overfitting beyond it.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART_LARGE (summarization) and GPT-2 (table-to-text)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Abstractive Summarization (XSUM) and Table-to-text (E2E/WebNLG)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Assess effect of different prefix lengths on generation quality across tasks and datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Vary prefix length hyperparameter (examples: table-to-text searched {1,5,10,20,40}; summarization searched up to 300).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Observed: performance increases as prefix length increases up to a threshold, then slightly drops. Thresholds reported: ~10 for table-to-text, ~200 for summarization (Figure 4). Longer prefixes reduce training loss but can decrease test performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Not reported as a single scalar; qualitative thresholds: performance peaks at prefix length ≈10 for table-to-text and ≈200 for summarization, then small drop.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>Nonlinear: increasing prefix length improves performance up to a task-dependent threshold, then reduces performance (overfitting).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Longer prefixes give more capacity, lowering training loss, but can overfit training data resulting in slightly worse generalization on test sets; optimal length depends on input size/complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5850.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5850.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prefix initialization (real-word activations)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prefix initialization with activations of real words</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Initialize learned prefix vectors using the LM's activations for real words (task-relevant or irrelevant words) rather than random initialization; this significantly improves stability and generation quality in low-data regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 and BART (low-data experiments reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Table-to-text (low-data E2E) and Summarization (low-data XSUM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Low-data experiments where only small numbers of training examples are available (e.g., 50–500).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Initialize prefix P_θ from the LM's activations for chosen real words (e.g., 'summarization', 'table-to-text') vs random uniform initialization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Random initialization vs real-word initialization; task-relevant words vs task-irrelevant words.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative/plot results: initializing with real-word activations significantly outperforms random initialization in low-data settings (Figure 5 and 8); task-relevant words slightly outperform irrelevant words. In full-data settings initialization has little/no impact.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Random init: high variance and lower quality in low-data; Real-word init: markedly better and more stable generation. (No single scalar reported in main text; plotted improvements shown in figures.)</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Not summarized numerically in paper; authors state 'significant' improvement in low-data plots (Figure 5/8).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>Improved (in low-data regimes); no meaningful effect in full-data regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Initializing with real-word activations provides a LM-consistent starting point that preserves pretrained structure and reduces sensitivity/variance when optimizing few parameters on small datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>In full-data settings initialization trick has no impact—random initialization reaches similar performance given sufficient data.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5850.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5850.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>In-context learning / prompting (GPT-3)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-context learning (few-shot prompting with natural language examples)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prepending a natural language task instruction and a few examples to the LM input so the model performs the task at generation time without updating parameters (exemplified by GPT-3's few-shot capabilities). Mentioned in related work as context-based deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (discussed), contrasted with GPT-2 and BART</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Few-shot adaptation across many tasks via in-context examples</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Adapt a pretrained autoregressive LM to a downstream task by prepending instructions and K examples within the context window and generating outputs autoregressively.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural language prompt: instruction + a few exemplars (few-shot) appended to the input; zero parameter updates (in-context).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Contrasted with prefix-tuning and parameter-updating methods; limitation discussed relative to prefix-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not directly evaluated in this paper; noted as able to adapt models like GPT-3 to tasks without parameter updates. The paper cites GPT-3 few-shot success but emphasizes context-length limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Not quantified in this paper; contextual limitation noted (e.g., context window ≈2048 tokens constrains number of examples).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>Enables few-shot adaptation (improved few-shot usability for very large models like GPT-3) but constrained by context length.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>In-context learning can steer generation via natural language exemplars/instructions but is limited by the bounded-length context (e.g., 2048 tokens for GPT-3), restricting the effective training set size that can be provided at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Authors note that natural-language task instructions (prompting) tend to fail for moderately-sized pretrained LMs like GPT-2 and BART in their preliminary experiments; GPT-3 is an exception that succeeds with in-context learning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5850.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5850.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Natural-language instruction prompting (GPT-2/BART)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prepending NL task instructions to GPT-2 / BART</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using human-readable natural language instructions (e.g., 'summarize the following') as a prompt for moderately-sized LMs (GPT-2, BART); paper reports these instructions generally fail to steer these models effectively in preliminary experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 and BART (preliminary experiments mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Generic downstream NLG tasks (e.g., summarization) using NL instructions</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Attempt to get pretrained LMs to perform tasks by prepending human-written instructions (no parameter updates).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural language task instruction prepended to input (discrete prompting).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared qualitatively to continuous prefix-tuning and to GPT-3 in-context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported to 'fail' (no successful steering) in preliminary experiments for GPT-2 and BART (exception: GPT-3 succeeds with in-context prompting). No numeric metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>Reduced / ineffective for GPT-2 and BART; successful only for much larger models (GPT-3) per cited literature.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Discrete natural-language instructions are insufficient for moderately-sized LMs to perform complex NLG tasks; continuous optimized prefixes are strictly more expressive (and more effective) because they are not constrained to embeddings of real words and can modify deeper-layer activations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners. <em>(Rating: 2)</em></li>
                <li>Autoprompt: Eliciting knowledge from language models with automatically generated prompts. <em>(Rating: 2)</em></li>
                <li>The power of scale for parameter-efficient prompt tuning. <em>(Rating: 2)</em></li>
                <li>Learning how to ask: Querying LMs with mixtures of soft prompts. <em>(Rating: 2)</em></li>
                <li>WARP: word-level adversarial reprogramming. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5850",
    "paper_id": "paper-53d8b356551a2361020a948f64454a6d599af69f",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "Prefix-tuning",
            "name_full": "Prefix-tuning (continuous task-specific prefix)",
            "brief_description": "A method that freezes a pretrained LM and learns a small sequence of continuous task-specific vectors (a 'prefix') prepended to the input; the LM attends to the prefix like virtual tokens and only the prefix parameters are optimized. Applied to GPT-2 for table-to-text and BART for summarization in this paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (MEDIUM, LARGE) and BART_LARGE",
            "model_size": null,
            "task_name": "Table-to-text (E2E, WebNLG, DART) and Abstractive Summarization (XSUM)",
            "task_description": "Conditional NLG: generate textual descriptions from linearized tables (E2E, WebNLG, DART) and generate abstractive summaries from news articles (XSUM).",
            "problem_format": "Prepend a learned continuous prefix (sequence of activation vectors P_θ) to the model input: [PREFIX; x; y] for autoregressive models or [PREFIX; x; PREFIX'; y] for encoder-decoder; prefix vectors are free parameters (reparametrized via a small matrix + MLP for stability). Prefix length is a hyperparameter.",
            "comparison_format": "Compared to (a) full fine-tuning (update all LM parameters), (b) adapter-tuning, (c) top-2-layer fine-tuning (FT-TOP2), (d) embedding-only continuous prompts, and (e) infix-tuning (placing learned activations between x and y).",
            "performance": "Table-to-text (GPT-2 MEDIUM, Prefix 0.1% params): E2E BLEU=74.8; WebNLG BLEU=64.52; DART BLEU=51.11. Summarization (BART_LARGE): Prefix(2% params) ROUGE-L=35.21 (validation); Prefix(0.1%) ROUGE-L=33.13 (validation). (See paper tables for full metric set.)",
            "performance_comparison": "Full fine-tuning (FT-FULL): E2E BLEU=74.2, WebNLG BLEU=66.03, DART BLEU=50.46. Summarization FT-FULL ROUGE-L reported ≈37.25. Adapter and FT-TOP2 reported in tables; embedding-only and infix ablations also compared (see separate entries).",
            "format_effect_size": "Relative to FT-FULL: E2E +0.6 BLEU (74.8 vs 74.2); WebNLG -1.51 BLEU (64.52 vs 66.03); DART +0.65 BLEU (51.11 vs 50.46). Summarization: Prefix(2%) ROUGE-L ≈ -2.04 vs FT (35.21 vs 37.25); Prefix(0.1%) ROUGE-L ≈ -4.12.",
            "format_effect_direction": "Mixed — prefix-tuning matches or slightly improves performance on table-to-text (comparable/Pareto-efficient) but underperforms full fine-tuning on XSUM summarization, with the gap larger at very small parameter budgets.",
            "explanation_or_hypothesis": "Authors hypothesize that keeping pretrained LM parameters frozen preserves general-purpose representations and provides an inductive bias that helps generalization and extrapolation; prefix-tuning is more parameter-efficient and modular, can influence encoding of x and decoding of y, and can better extrapolate to unseen topics in low-data regimes because it steers the LM without overwriting pretrained knowledge.",
            "counterexample_or_null_result": "Summarization on XSUM: prefix-tuning (especially at 0.1% params) shows a clear performance drop vs full fine-tuning, indicating the format is not universally superior; in full-data summarization prefix-tuning suffers a small degradation.",
            "uuid": "e5850.0"
        },
        {
            "name_short": "Embedding-only prompt",
            "name_full": "Embedding-only continuous prompt (optimize only virtual-token embeddings)",
            "brief_description": "A reduced variant where only the input token embeddings of virtual prompt tokens are optimized while all deeper activations are computed normally by the Transformer; evaluated as an ablation of prefix-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (MEDIUM) (evaluated in table-to-text experiments)",
            "model_size": null,
            "task_name": "Table-to-text (E2E; intrinsic evaluation reported in Table 5)",
            "task_description": "Generate text descriptions from linearized table inputs; embedding-only ablation compares to full prefix-tuning.",
            "problem_format": "Optimize only the embedding vectors of 'virtual tokens' prepended to the input (embedding-only), not the full-layer activations.",
            "comparison_format": "Compared to full prefix-tuning (trainable activations at all layers) and to different prefix lengths (EmB-1, EmB-10, EmB-20 reported).",
            "performance": "Intrinsic metrics (Table 5): Prefix (full) BLEU=70.3; Embedding-only EmB-10 BLEU=62.2, EmB-1 BLEU=48.1, EmB-20 BLEU=61.9. Other metrics (NIST/MET/ROUGE/CIDEr) also substantially lower for embedding-only.",
            "performance_comparison": "Full prefix-tuning BLEU=70.3 vs Embedding-only EmB-10 BLEU=62.2 (difference -8.1 BLEU).",
            "format_effect_size": "-8.1 BLEU (EmB-10 vs full prefix) in intrinsic table-to-text eval.",
            "format_effect_direction": "Reduced — optimizing only embeddings is significantly worse than optimizing full-layer prefix activations.",
            "explanation_or_hypothesis": "Embedding-only is less expressive because it only changes the embedding layer and cannot directly modify deeper activation layers; it upper-bounds discrete prompt optimization and therefore underperforms prefix-tuning which can modify representations throughout the network.",
            "counterexample_or_null_result": null,
            "uuid": "e5850.1"
        },
        {
            "name_short": "Infix-tuning",
            "name_full": "Infix-tuning (trainable activations between input and output)",
            "brief_description": "Place learned, trainable activation vectors between the input x and the target y (format [x; INFIX; y]) instead of at the front; evaluated to test positional effects of trainable activations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 (MEDIUM) (table-to-text experiments)",
            "model_size": null,
            "task_name": "Table-to-text (intrinsic eval in Table 5)",
            "task_description": "Generate text from linearized table inputs; infix-tuning compares effect of placing learnable activations between x and y vs prepending them.",
            "problem_format": "Learnable activations inserted between x and y: [x; Infix; y].",
            "comparison_format": "Compared directly to prefix-tuning (learnable activations prepended: [Prefix; x; y]).",
            "performance": "Intrinsic metrics (Table 5): Infix-1 BLEU=67.9, Infix-10 BLEU=67.2, Infix-20 BLEU=66.7 (compared to Prefix BLEU=70.3).",
            "performance_comparison": "Prefix (70.3 BLEU) vs Infix-1 (67.9) =&gt; -2.4 BLEU.",
            "format_effect_size": "-2.4 BLEU (Infix-1 vs Prefix)",
            "format_effect_direction": "Reduced — infix-tuning slightly underperforms prefix-tuning.",
            "explanation_or_hypothesis": "Authors suggest prefix-tuning can influence encoding of x and subsequent decoding of y, whereas infix-tuning can only directly influence the decoder-side activations (y), so prefix placement gives more steering power over both input encoding and generation.",
            "counterexample_or_null_result": null,
            "uuid": "e5850.2"
        },
        {
            "name_short": "Prefix length ablation",
            "name_full": "Prefix-length sensitivity (length vs performance)",
            "brief_description": "Systematic study of how varying the number of prefix tokens (length of the learned prefix) affects training loss and test performance, showing an improvement up to a dataset-dependent threshold and possible overfitting beyond it.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BART_LARGE (summarization) and GPT-2 (table-to-text)",
            "model_size": null,
            "task_name": "Abstractive Summarization (XSUM) and Table-to-text (E2E/WebNLG)",
            "task_description": "Assess effect of different prefix lengths on generation quality across tasks and datasets.",
            "problem_format": "Vary prefix length hyperparameter (examples: table-to-text searched {1,5,10,20,40}; summarization searched up to 300).",
            "comparison_format": null,
            "performance": "Observed: performance increases as prefix length increases up to a threshold, then slightly drops. Thresholds reported: ~10 for table-to-text, ~200 for summarization (Figure 4). Longer prefixes reduce training loss but can decrease test performance.",
            "performance_comparison": null,
            "format_effect_size": "Not reported as a single scalar; qualitative thresholds: performance peaks at prefix length ≈10 for table-to-text and ≈200 for summarization, then small drop.",
            "format_effect_direction": "Nonlinear: increasing prefix length improves performance up to a task-dependent threshold, then reduces performance (overfitting).",
            "explanation_or_hypothesis": "Longer prefixes give more capacity, lowering training loss, but can overfit training data resulting in slightly worse generalization on test sets; optimal length depends on input size/complexity.",
            "counterexample_or_null_result": null,
            "uuid": "e5850.3"
        },
        {
            "name_short": "Prefix initialization (real-word activations)",
            "name_full": "Prefix initialization with activations of real words",
            "brief_description": "Initialize learned prefix vectors using the LM's activations for real words (task-relevant or irrelevant words) rather than random initialization; this significantly improves stability and generation quality in low-data regimes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 and BART (low-data experiments reported)",
            "model_size": null,
            "task_name": "Table-to-text (low-data E2E) and Summarization (low-data XSUM)",
            "task_description": "Low-data experiments where only small numbers of training examples are available (e.g., 50–500).",
            "problem_format": "Initialize prefix P_θ from the LM's activations for chosen real words (e.g., 'summarization', 'table-to-text') vs random uniform initialization.",
            "comparison_format": "Random initialization vs real-word initialization; task-relevant words vs task-irrelevant words.",
            "performance": "Qualitative/plot results: initializing with real-word activations significantly outperforms random initialization in low-data settings (Figure 5 and 8); task-relevant words slightly outperform irrelevant words. In full-data settings initialization has little/no impact.",
            "performance_comparison": "Random init: high variance and lower quality in low-data; Real-word init: markedly better and more stable generation. (No single scalar reported in main text; plotted improvements shown in figures.)",
            "format_effect_size": "Not summarized numerically in paper; authors state 'significant' improvement in low-data plots (Figure 5/8).",
            "format_effect_direction": "Improved (in low-data regimes); no meaningful effect in full-data regimes.",
            "explanation_or_hypothesis": "Initializing with real-word activations provides a LM-consistent starting point that preserves pretrained structure and reduces sensitivity/variance when optimizing few parameters on small datasets.",
            "counterexample_or_null_result": "In full-data settings initialization trick has no impact—random initialization reaches similar performance given sufficient data.",
            "uuid": "e5850.4"
        },
        {
            "name_short": "In-context learning / prompting (GPT-3)",
            "name_full": "In-context learning (few-shot prompting with natural language examples)",
            "brief_description": "Prepending a natural language task instruction and a few examples to the LM input so the model performs the task at generation time without updating parameters (exemplified by GPT-3's few-shot capabilities). Mentioned in related work as context-based deployment.",
            "citation_title": "Language models are few-shot learners.",
            "mention_or_use": "mention",
            "model_name": "GPT-3 (discussed), contrasted with GPT-2 and BART",
            "model_size": "175B",
            "task_name": "Few-shot adaptation across many tasks via in-context examples",
            "task_description": "Adapt a pretrained autoregressive LM to a downstream task by prepending instructions and K examples within the context window and generating outputs autoregressively.",
            "problem_format": "Natural language prompt: instruction + a few exemplars (few-shot) appended to the input; zero parameter updates (in-context).",
            "comparison_format": "Contrasted with prefix-tuning and parameter-updating methods; limitation discussed relative to prefix-tuning.",
            "performance": "Not directly evaluated in this paper; noted as able to adapt models like GPT-3 to tasks without parameter updates. The paper cites GPT-3 few-shot success but emphasizes context-length limitations.",
            "performance_comparison": null,
            "format_effect_size": "Not quantified in this paper; contextual limitation noted (e.g., context window ≈2048 tokens constrains number of examples).",
            "format_effect_direction": "Enables few-shot adaptation (improved few-shot usability for very large models like GPT-3) but constrained by context length.",
            "explanation_or_hypothesis": "In-context learning can steer generation via natural language exemplars/instructions but is limited by the bounded-length context (e.g., 2048 tokens for GPT-3), restricting the effective training set size that can be provided at inference time.",
            "counterexample_or_null_result": "Authors note that natural-language task instructions (prompting) tend to fail for moderately-sized pretrained LMs like GPT-2 and BART in their preliminary experiments; GPT-3 is an exception that succeeds with in-context learning.",
            "uuid": "e5850.5"
        },
        {
            "name_short": "Natural-language instruction prompting (GPT-2/BART)",
            "name_full": "Prepending NL task instructions to GPT-2 / BART",
            "brief_description": "Using human-readable natural language instructions (e.g., 'summarize the following') as a prompt for moderately-sized LMs (GPT-2, BART); paper reports these instructions generally fail to steer these models effectively in preliminary experiments.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "GPT-2 and BART (preliminary experiments mentioned)",
            "model_size": null,
            "task_name": "Generic downstream NLG tasks (e.g., summarization) using NL instructions",
            "task_description": "Attempt to get pretrained LMs to perform tasks by prepending human-written instructions (no parameter updates).",
            "problem_format": "Natural language task instruction prepended to input (discrete prompting).",
            "comparison_format": "Compared qualitatively to continuous prefix-tuning and to GPT-3 in-context learning.",
            "performance": "Reported to 'fail' (no successful steering) in preliminary experiments for GPT-2 and BART (exception: GPT-3 succeeds with in-context prompting). No numeric metrics provided.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "Reduced / ineffective for GPT-2 and BART; successful only for much larger models (GPT-3) per cited literature.",
            "explanation_or_hypothesis": "Discrete natural-language instructions are insufficient for moderately-sized LMs to perform complex NLG tasks; continuous optimized prefixes are strictly more expressive (and more effective) because they are not constrained to embeddings of real words and can modify deeper-layer activations.",
            "uuid": "e5850.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners.",
            "rating": 2
        },
        {
            "paper_title": "Autoprompt: Eliciting knowledge from language models with automatically generated prompts.",
            "rating": 2
        },
        {
            "paper_title": "The power of scale for parameter-efficient prompt tuning.",
            "rating": 2
        },
        {
            "paper_title": "Learning how to ask: Querying LMs with mixtures of soft prompts.",
            "rating": 2
        },
        {
            "paper_title": "WARP: word-level adversarial reprogramming.",
            "rating": 1
        }
    ],
    "cost": 0.019726,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Prefix-Tuning: Optimizing Continuous Prompts for Generation</h1>
<p>Xiang Lisa Li<br>Stanford University<br>xlisali@stanford.edu</p>
<h2>Percy Liang</h2>
<p>Stanford University
pliang@cs.stanford.edu</p>
<h2>Abstract</h2>
<p>Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were "virtual tokens". We apply prefix-tuning to GPT-2 for table-totext generation and to BART for summarization. We show that by modifying only $0.1 \%$ of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.</p>
<h2>1 Introduction</h2>
<p>Fine-tuning is the prevalent paradigm for using large pretrained language models (LMs) (Radford et al., 2019; Devlin et al., 2019) to perform downstream tasks (e.g., summarization), but it requires updating and storing all the parameters of the LM. Consequently, to build and deploy NLP systems that rely on large pretrained LMs, one currently needs to store a modified copy of all the LM parameters for each task. This can be prohibitively expensive given the size of current LMs; for example, GPT-2 has 774M parameters (Radford et al., 2019) and GPT-3 has 175B parameters (Brown et al., 2020).</p>
<p>A natural approach to this problem is lightweight fine-tuning, which freezes most of the pretrained parameters and only tunes a smaller set of parameters. For example, adapter-tuning (Rebuffi et al.,</p>
<p>Figure 1: Fine-tuning (top) updates all LM parameters (the red Transformer box) and requires storing a full model copy for each task. We propose prefixtuning (bottom), which freezes the LM parameters and only optimizes the prefix (the red prefix blocks). Consequently, we only need to store the prefix for each task, making prefix-tuning modular and space-efficient. Note that each vertical block denote transformer activations at one time step.</p>
<p>2017; Houlsby et al., 2019) inserts additional taskspecific layers between the layers of pretrained language models. Adapter-tuning has promising performance on natural language understanding and generation benchmarks, attaining comparable performance with fine-tuning while adding only around $2-4 \%$ task-specific parameters (Houlsby et al., 2019; Lin et al., 2020).</p>
<p>At the limit, GPT-3 (Brown et al., 2020) can be deployed using in-context learning, which is a form of prompting, without modifying any LM parameters. In in-context learning, Brown et al. (2020) prepend a natural language task instruction (e.g., $T L ; D R$ for summarization) and a few examples to the task input, and then generate the task output from the LM. However, since Transformers can only condition on a bounded-length context (e.g., 2048 tokens for GPT-3), in-context learning is restricted to very small training sets.</p>
<p>In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation (NLG) tasks, inspired by prompting. Consider the task of generating a textual description of a data table, as shown in Figure 1, where the task input is a linearized table (e.g., "name: Starbucks | type: coffee shop") and the output is a textual description (e.g., "Starbucks serves coffee."). Prefix-tuning prepends a sequence of continuous task-specific vectors to the input, which we call a prefix, depicted by red blocks in Figure 1 (bottom). To generate each token, the LM can attend to the prefix as if it were a sequence of "virtual tokens", but unlike prompting, the prefix consists entirely of free parameters which do not correspond to real tokens. In contrast to fine-tuning in Figure 1 (top), which updates all LM parameters and thus requires storing a tuned copy of the model for each task, prefix-tuning only optimizes the prefix. Consequently, we only need to store one copy of the large LM and a learned task-specific prefix, yielding a very small overhead for each additional task (e.g., 250 K parameters for table-to-text).</p>
<p>In contrast to full fine-tuning, prefix-tuning is also modular: we train an upstream prefix which steers an unmodified LM, and therefore, a single LM can support many tasks at once. In the context of personalization where the tasks correspond to users (Shokri and Shmatikov, 2015; McMahan et al., 2016), we would have a separate prefix for each user trained only on that user's data, thereby avoiding data cross-contamination. Moreover, the prefix-based architecture enables us to even process examples from multiple users/tasks in a single batch, something that is not possible with other lightweight fine-tuning approaches like adaptertuning.</p>
<p>We evaluate prefix-tuning on table-to-text generation using GPT-2 and abstractive summarization using BART. In terms of storage, prefix-tuning stores 1000x fewer parameters than full fine-tuning. In terms of performance when trained on full datasets, prefix-tuning and fine-tuning are comparable for table-to-text ( $\S 6.1$ ), while prefix-tuning suffers a small degradation for summarization (§6.2). In low-data settings, prefix-tuning outperforms finetuning on both tasks (§6.3). Prefix-tuning also extrapolates better to tables (for table-to-text) and articles (for summarization) with unseen topics (§6.4).</p>
<h2>2 Related Work</h2>
<p>Fine-tuning for natural language generation. Current state-of-the-art systems for natural language generation (NLG) are based on fine-tuning pretrained LMs. For table-to-text generation, Kale (2020) fine-tunes a sequence-to-sequence model (T5; Raffel et al., 2020). For extractive and abstractive summarization, researchers fine-tune masked language models (e.g., BERT; Devlin et al., 2019) and encode-decoder models (e.g., BART; Lewis et al., 2020), respectively (Zhong et al., 2020; Liu and Lapata, 2019; Raffel et al., 2020). For other conditional NLG tasks such as machine translation and dialogue generation, fine-tuning is also the prevalent paradigm (Zhang et al., 2020c; Stickland et al., 2020; Zhu et al., 2020; Liu et al., 2020). In this paper, we focus on table-to-text using GPT-2 and summarization using BART, but prefix-tuning in principle can be applied to other generation tasks and pretrained models, such as masked LMs.</p>
<p>Lightweight fine-tuning. Prefix-tuning falls under the broad class of lightweight fine-tuning methods, which freeze most of the pretrained parameters and only tune a smaller set of parameters. The key question is how to augment the LM architecture and decide which subset of pretrained parameters to tune. One line of research learns a task-specific parameter mask (Zhao et al., 2020; Radiya-Dixit and Wang, 2020). Another line of research inserts new modules with trainable parameters. For example, Zhang et al. (2020a) trains a "side" network that is fused with the pretrained model via summation; adapter-tuning inserts task-specific layers (adapters) between each layer of the pretrained LM (Houlsby et al., 2019; Lin et al., 2020; Rebuffi et al., 2017; Pfeiffer et al., 2020). Compared to this line of work, which tunes around $3.6 \%$ of the LM parameters, our method obtains a further 30x reduction in task-specific parameters, tuning only $0.1 \%$ while maintaining comparable performance on table-to-text tasks.</p>
<p>Prompting. Prompting is a way of leveraging a pretrained LM by prepending instructions and a few examples to the task input and generating the task output from the LM. For autoregressive LMs, the most successful form of prompting is GPT-3's in-context learning (Brown et al., 2020), which uses manually designed prompts to adapt its generation for different tasks in few-shot settings. For masked LMs like BERT and RoBERTa (Liu et al.,</p>
<p>2019), prompt engineering has been explored for natural language understanding tasks (Jiang et al., 2020; Schick and Schütze, 2020). For example, AutoPrompt (Shin et al., 2020) searches for a sequence of discrete trigger words and concatenates it with each input to elicit sentiment or factual knowledge from BERT and RoBERTa. In contrast with AutoPrompt, our method optimizes continuous prefixes, which are more expressive (§7.2); moreover, we focus on language generation tasks.</p>
<p>Continuous vectors have been used to steer LMs; for example, Subramani et al. (2020) showed that a pretrained LSTM language model can reconstruct arbitrary sentences by optimizing a continuous vector for each sentence, making the vector inputspecific. In contrast, prefix-tuning optimizes a taskspecific prefix that applies to all instances of that task. As a result, unlike the previous work whose application is limited to sentence reconstruction, prefix-tuning can be applied to NLG tasks.</p>
<p>Controllable generation. Controllable generation aims to steer a pretrained language model to match a sentence-level attribute (e.g., positive sentiment or sports). Such control can happen at training time: Keskar et al. (2019) pretrains the language model (CTRL) to condition on metadata such as keywords or URLs. The control can also happen at decoding time, by weighted decoding (GeDi, Krause et al., 2020) or iteratively updating the past activations (PPLM, Dathathri et al., 2020). However, there is no straightforward way to apply these controllable generation techniques to enforce fine-grained control over generated contents, as demanded by tasks like table-to-text and summarization.</p>
<p>P<em>-tuning. Prefix tuning is an instance of a new class of methods that has emerged, which we call $\mathrm{p}^{</em>}$-tuning (since the other prominent instances, ptuning and prompt-tuning, also start with p ), all based on the idea of optimizing a continuous prefix or prompt. Concurrent with our work, Qin and Eisner (2021) learn mixtures of soft fill-in-the-blank prompts to elicit knowledge from LMs such as BERT and BART. Hambardzumyan et al. (2021) learns task-specific embeddings that adapts BERT for sentiment classification. Both works show that tuning soft prompts outperforms previous work, which optimizes over discrete prompts. P-tuning (Liu et al., 2021) shows that jointly updating the prompt embeddings and LM parameters improves</p>
<p>GPT-2's performance on natural language understanding tasks, in both few-shot and full data settings. In a followup work, Prompt-tuning (Lester et al., 2021) simplifies our approach and applies it to T5 (Raffel et al., 2020), demonstrating that the performance gap between fine-tuning and $\mathrm{p}^{*}$ tuning vanishes as the model size grows.</p>
<h2>3 Problem Statement</h2>
<p>Consider a conditional generation task where the input $x$ is a context and the output $y$ is a sequence of tokens. We focus on two tasks, shown in Figure 2 (right): In table-to-text, $x$ corresponds to a linearized data table and $y$ is a textual description; in summarization, $x$ is an article and $y$ is a summary.</p>
<h3>3.1 Autoregressive LM</h3>
<p>Assume we have an autoregressive neural language model $p_{\phi}(y \mid x)$ parametrized by $\phi$ (e.g., GPT-2; Radford et al., 2019). As shown in Figure 2 (top), let $z=[x ; y]$ be the concatenation of $x$ and $y$; let $\mathrm{X}<em _mathrm_idx="\mathrm{idx">{\mathrm{idx}}$ denote the sequence of indices that corresponds to $x$, and $\mathrm{Y}</em>$ denote the same for $y$.}</p>
<p>The activation vector at time step $i$ is $h_{i} \in \mathbb{R}^{d}$, where $h_{i}=\left[h_{i}^{(1)} ; \cdots ; h_{i}^{(n)}\right]$ is a concatenation of all activation layers at this time step, and $h_{i}^{(j)}$ is the activation vector of the $j$-th layer at time step $i .{ }^{1}$</p>
<p>An autoregressive neural LM computes $h_{i}$ as a function of $z_{i}$ and the past activations in its left context, as follows:</p>
<p>$$
h_{i}=\mathrm{LM}<em i="i">{\phi}\left(z</em>\right)
$$}, h_{&lt;i</p>
<p>where the last layer of $h_{i}$ is used to compute the distribution for the next token: $p_{\phi}\left(z_{i+1} \mid h_{\leq i}\right)=$ $\operatorname{softmax}\left(W_{\phi} h_{i}^{(n)}\right)$ and $W_{\phi}$ is a matrix that maps $h_{i}^{(n)}$ to logits over the vocabulary.</p>
<h3>3.2 Encoder-Decoder Architecture</h3>
<p>We can also use an encoder-decoder architecture (e.g., BART; Lewis et al., 2020) to model $p_{\phi}(y \mid x)$, where $x$ is encoded by the bidirectional encoder, and the decoder predicts $y$ autoregressively (conditioned on the encoded $x$ and its left context). We use the same indexing and activation notation, as shown in Figure 2 (bottom): each $h_{i}$ for $i \in \mathrm{X}<em i="i">{\mathrm{idx}}$ is computed by the a bidirectional encoder; each $h</em>$ is computed by an autoregressive decoder using the same equation (1).}$ for $i \in \mathrm{Y}_{\mathrm{idx}</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: An annotated example of prefix-tuning using an autoregressive LM (top) and an encoder-decoder model (bottom). The prefix activations $\forall i \in \mathrm{P}<em i="i">{\mathrm{idx}}, h</em>$. The remaining activations are computed by the Transformer.}$ are drawn from a trainable matrix $P_{\theta</p>
<h3>3.3 Fine-tuning</h3>
<p>In the full fine-tuning framework, we initialize with the pretrained parameters $\phi$. Here $p_{\phi}$ is a trainable language model distribution and we perform gradient updates on the following log-likelihood objective:
$\max <em _phi="\phi">{\phi} \log p</em>(y \mid x)=\max <em _in="\in" _mathrm_Y="\mathrm{Y" i="i">{\phi} \sum</em><em _phi="\phi">{\mathrm{idx}}} \log p</em>\right)$.}\left(z_{i} \mid h_{&lt;i</p>
<h2>4 Prefix-Tuning</h2>
<p>We propose prefix-tuning as an alternative to full fine-tuning for conditional generation tasks. We first provide intuition in $\S 4.1$ before defining our method formally in $\S 4.2$.</p>
<h3>4.1 Intuition</h3>
<p>Prompting has demonstrated that conditioning on a proper context can steer the LM without changing its parameters. For example, if we want the LM to generate a word (e.g., Obama), we can prepend its common collocations as context (e.g., Barack), and the LM will assign much higher probability to the desired word. Extending this intuition beyond generating a single word or sentence, we want to find a context that steers the LM to solve an NLG task. Intuitively, the context could influence the encoding of the task input $x$ by guiding what to extract from $x$, and it could influence the generation of the task output $y$ by steering the next token distribution. However, it's non-obvious whether such a context exists. Using natural language task instructions (e.g., "summarize the following table in one sentence") for the context might guide a human to
solve the task, but this fails for moderately-sized pretrained LMs. ${ }^{2}$ Optimizing over the discrete instructions might help, but discrete optimization is computationally challenging.</p>
<p>Instead of optimizing over discrete tokens, we can optimize the instruction as continuous word embeddings, whose effects will be propagated upward to all Transformer activation layers and rightward to subsequent tokens. This is strictly more expressive than a discrete prompt which is constrained to the embeddings of real words. Prefix-tuning goes one step further in increasing expressivity by optimizing the activations of all the layers, not just the embedding layer. As another benefit, prefixtuning can directly modify representations deeper in the network, therefore, avoiding long computation paths across the depth of the network.</p>
<h3>4.2 Method</h3>
<p>Prefix-tuning prepends a prefix for an autoregressive LM to obtain $z=[\operatorname{REFIX} ; x ; y]$, or prepends prefixes for both encoder and decoder to obtain $z=[\operatorname{REFIX} ; x ; \operatorname{Prefix}^{\prime} ; y]$, as shown in Figure 2. Here, $\mathrm{P}<em _mathrm_idx="\mathrm{idx">{\mathrm{idx}}$ denotes the sequence of prefix indices, and we use $\left|\mathrm{P}</em>\right|$ to denote the length of the prefix.}</p>
<p>We follow the recurrence relation in equation (1), except that the activations of the prefix indices are free parameters, given by a matrix $P_{\theta}$ (parametrized by $\theta$ ) of dimension $\left|\mathrm{P}<em i="i">{\mathrm{idx}}\right| \times \operatorname{dim}\left(h</em>\right)$.</p>
<p>$$
h_{i}= \begin{cases}P_{\theta}[i,:], &amp; \text { if } i \in \mathrm{P}<em _phi="\phi">{\mathrm{idx}} \ \mathrm{LM}</em>
$$}\left(z_{i}, h_{&lt;i}\right), &amp; \text { otherwise }\end{cases</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>The training objective is the same as equation (2), but the set of trainable parameters changes: the language model parameters $\phi$ are fixed and the prefix parameters $\theta$ are the only trainable parameters.</p>
<p>Here, each $h_{i}$ is a function of the trainable $P_{\theta}$. When $i \in \mathrm{P}<em i="i">{\mathrm{idx}}$, this is clear because $h</em>}$ copies directly from $P_{\theta}$. When $i \notin \mathrm{P<em i="i">{\mathrm{idx}}, h</em>$, because the prefix activations are always in the left context and will therefore affect any activations to the right.}$ still depends on $P_{\theta</p>
<h3>4.3 Parametrization of $P_{\theta}$</h3>
<p>Empirically, directly updating the $P_{\theta}$ parameters leads to unstable optimization and a slight drop in performance. ${ }^{3}$ So we reparametrize the matrix $P_{\theta}[i,:]=\operatorname{MLP}<em _theta="\theta">{\theta}\left(P</em>}^{\prime}[i,:]\right)$ by a smaller matrix $\left(P_{\theta}^{\prime}\right)$ composed with a large feedforward neural network $\left(\mathrm{MLP<em _theta="\theta">{\theta}\right)$. Now, the trainable parameters include $P</em>}^{\prime}$ and the parameters of $\mathrm{MLP<em _theta="\theta">{\theta}$. Note that $P</em>$}$ and $P_{\theta}^{\prime}$ has the same number of rows (i.e., the prefix length), but different number of columns. ${ }^{4</p>
<p>Once training is complete, these reparametrization parameters can be dropped, and only the prefix $\left(P_{\theta}\right)$ needs to be saved.</p>
<h2>5 Experimental Setup</h2>
<h3>5.1 Datasets and Metrics</h3>
<p>We evaluate on three standard neural generation datasets for the table-to-text task: E2E (Novikova et al., 2017), WebNLG (Gardent et al., 2017), and DART (Radev et al., 2020), as shown in Table 1. The datasets are ordered by increasing complexity and size. E2E only has 1 domain (i.e. restaurant reviews); WebNLG has 14 domains, and DART is open-domain, using open-domain tables from Wikipedia. For evaluation, we report the metrics using the official evaluation scripts (see details in Appendix A.1).</p>
<p>For the summarization task, we use the XSUM (Narayan et al., 2018) dataset, which is an abstractive summarization dataset on news articles. We report ROUGE-1, ROUGE-2 and ROUGE-L.</p>
<h3>5.2 Methods</h3>
<p>For table-to-text generation, we compare prefixtuning with three other methods: full fine-tuning</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>(FT-FULL), fine-tuning only the top 2 layers (FTTOP2), and adapter-tuning (ADAPTER). ${ }^{5}$ We also report the current state-of-the-art results on these datasets: On E2E, Shen et al. (2019) uses a pragmatically informed model without pretraining. On WebNLG, Kale (2020) fine-tunes T5-large. On DART, no official models trained on this dataset version are released. ${ }^{6}$ For summarization, we compare against fine-tuning BART (Lewis et al., 2020).</p>
<h3>5.3 Architectures and Hyperparameters</h3>
<p>For table-to-text, we use GPT-2 MEDIUM and GPT$2_{\text {LARGE }}$. For summarization, we use BART $_{\text {LARGE }}$. Our implementation is based on the Hugging Face Transformers (Wolf et al., 2020).</p>
<p>At training time, we use the AdamW optimizer (Loshchilov and Hutter, 2019) and a linear learning rate scheduler, as suggested by the Hugging Face default setup. The hyperparameters we tune include the number of epochs, batch size, learning rate, and prefix length. Hyperparameter details are in the appendix. The default setting is 10 epochs, batch size 5 , learning rate $5 \cdot 10^{-5}$ and prefix length 10. The table-to-text models are trained on TITAN Xp or GeForce GTX TITAN X machines. Prefixtuning takes 0.2 hours per epoch to train on 22 K examples, whereas fine-tuning takes around 0.3 hours per epoch. The summarization models are trained on Tesla V100 machines, taking 1.25 hours per epoch on the XSUM dataset. For time efficiency, prefix-tuning is around $30 \%$ faster than fine-tuning. For GPU memory efficiency, prefixtuning with batchsize 1 takes $18 \%$ of the total GPU memory, whereas fine-tuning takes $50 \%$.</p>
<p>At decoding time, for table-to-text, we use beam search with beam size 5 . For summarization, we use beam size 6 and length normalization 0.8 . Decoding takes 1.2 seconds per sentence (without</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>batching) for table-to-text, and 2.6 seconds per batch (using a batch size of 10) for summarization.</p>
<h2>6 Main Results</h2>
<h3>6.1 Table-to-text Generation</h3>
<p>We find that by updating only $0.1 \%$ task-specific parameters, ${ }^{7}$ prefix-tuning is effective in table-to-text generation, outperforming other lightweight baselines (ADAPTER and FT-TOP2) even by updating 30x fewer parameters and achieving a comparable performance with (full) fine-tuning. This trend holds for all datasets: E2E, WebNLG, ${ }^{8}$ and DART.</p>
<p>If we match the number of parameters for prefixtuning and adapter-tuning to be $0.1 \%$, Table 2 shows that prefix-tuning is significantly better than ADAPTER ( $0.1 \%$ ), attaining 4.1 BLEU improvement per dataset on average. Even when we compare with fine-tuning ( $100 \%$ ) and adapter-tuning (3.0\%), which update significantly more parameters than prefix-tuning, prefix-tuning still achieves results comparable or better than those two systems. This demonstrates that prefix-tuning is more Pareto efficient than adapter-tuning, significantly reducing parameters while improving generation quality.</p>
<p>Additionally, attaining good performance on DART suggests that prefix-tuning can generalize to tables with diverse domains and a large number of relations. We will delve deeper into extrapolation performance (i.e., generalization to unseen categories or topics) in $\S 6.4$.</p>
<p>In summary, prefix-tuning is an effective and space-efficient method to adapt GPT-2 to table-totext generation. It also maintains the performance gains when scaling up to GPT-2 ${ }_{\text {LARGE }}$, suggesting it has the potential to scale to even larger models with a similar architecture, like GPT-3.</p>
<h3>6.2 Summarization</h3>
<p>As shown in Table 3, with 2\% parameters, prefixtuning obtains slightly lower performance than finetuning ( 36.05 vs. 37.25 in ROUGE-L). With only $0.1 \%$ parameters, prefix-tuning underperforms full fine-tuning ( 35.05 vs. 37.25 ). There are several differences between XSUM and the three table-totext datasets which could account for why prefixtuning has comparative advantage in table-to-text:</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>(1) XSUM contains 4 x more examples than the three table-to-text datasets on average; (2) the input articles are 17 x longer than the linearized table input of table-to-text datasets on average; (3) summarization is more complex than table-to-text because it requires selecting key contents from an article.</p>
<h3>6.3 Low-data Setting</h3>
<p>Based on the results from table-to-text (§6.1) and summarization (§6.2), we observe that prefixtuning has a comparative advantage when the number of training examples is smaller. To explore the low-data setting more systematically, we subsample the full dataset (E2E for table-to-text and XSUM for summarization) to obtain small datasets of size ${50,100,200,500}$. For each size, we sample 5 different datasets and average over 2 training random seeds. Thus, we average over 10 models for each low-data setting. ${ }^{9}$</p>
<p>Figure 3 (right) shows that prefix-tuning outperforms fine-tuning in low-data regimes by 2.9 BLEU on average, in addition to requiring much fewer parameters, but the gap narrows as the dataset size increases.</p>
<p>Qualitatively, Figure 3 (left) shows 8 examples generated by both prefix-tuning and fine-tuning models trained on different data levels. While both methods tend to undergenerate (missing table contents) in low data regimes, prefix-tuning tends to be more faithful than fine-tuning. For example, finetuning $(100,200)^{10}$ falsely claims a low customer rating while the true rating is average, whereas prefix-tuning $(100,200)$ generates a description that is faithful to the table.</p>
<h3>6.4 Extrapolation</h3>
<p>We now investigate extrapolation performance to unseen topics for both table-to-text and summarization. In order to construct an extrapolation setting, we split the existing datasets so that training and test cover different topics. For table-to-text, the WebNLG dataset is labeled with table topics. There are 9 categories that appear in training and dev, denoted as SEEN and 5 categories that only appear at test time, denoted as UNSEEN. So we evaluate extrapolation by training on the SEEN categories and testing on the UNSEEN categories. For summarization, we construct two extrapolation data splits:</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>|  | E2E |  |  |  |  |  | WebNLG |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
|  |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 4: Prefix length vs. performance on summerization (left) and table-to-text (right). Performance increases as the prefix length increases up to a threshold (200 for summarization and 10 for table-to-text) and then a slight performance drop occurs. Each plot reports two metrics (on two vertical axes).</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>BLEU</td>
<td>NIST</td>
<td>MET</td>
<td>ROUGE</td>
<td>CIDEr</td>
</tr>
<tr>
<td>Prefix</td>
<td>70.3</td>
<td>8.82</td>
<td>46.3</td>
<td>72.1</td>
<td>2.46</td>
</tr>
<tr>
<td></td>
<td>Embedding-only: EmB-{PrefixLength}</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>EmB-1</td>
<td>48.1</td>
<td>3.33</td>
<td>32.1</td>
<td>60.2</td>
<td>1.10</td>
</tr>
<tr>
<td>EmB-10</td>
<td>62.2</td>
<td>6.70</td>
<td>38.6</td>
<td>66.4</td>
<td>1.75</td>
</tr>
<tr>
<td>EmB-20</td>
<td>61.9</td>
<td>7.11</td>
<td>39.3</td>
<td>65.6</td>
<td>1.85</td>
</tr>
<tr>
<td></td>
<td>Infix-tuning: Infix-{PrefixLength}</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Infix-1</td>
<td>67.9</td>
<td>8.63</td>
<td>45.8</td>
<td>69.4</td>
<td>2.42</td>
</tr>
<tr>
<td>Infix-10</td>
<td>67.2</td>
<td>8.48</td>
<td>45.8</td>
<td>69.9</td>
<td>2.40</td>
</tr>
<tr>
<td>Infix-20</td>
<td>66.7</td>
<td>8.47</td>
<td>45.8</td>
<td>70.0</td>
<td>2.42</td>
</tr>
</tbody>
</table>
<p>Table 5: Intrinsic evaluation of Embedding-only (§7.2) and Infixing (§7.3). Both Embedding-only ablation and Infix-tuning underperforms full prefix-tuning.</p>
<p>Length increases up to a threshold (200 for summarization, 10 for table-to-text) and then a slight performance drop occurs. Prefixes longer than the threshold lead to lower training loss, but slightly worse test performance, suggesting that they tend to overfit the training data.</p>
<h3>7.2 Full vs Embedding-only</h3>
<p>Recall in §4.1, we discussed optimizing the continuous embeddings of the "virtual tokens." We instantiate that idea and call it <em>embedding-only</em>. The word embeddings are free parameters, and the remaining activation layers are computed by the Transformer. Table 5 (top) shows that the performance drops significantly, suggesting that tuning only the embedding layer is not sufficiently expressive.</p>
<p>Embedding-only upper bounds the performance of discrete prompt optimization (Shin et al., 2020), because discrete prompt restricts the embedding layer to exactly match the embedding of a real word. Consequently, we have this chain of increasing expressive power: discrete prompting &lt; embedding-only &lt; prefix-tuning.</p>
<h3>7.3 Prefix-tuning vs Infix-tuning</h3>
<p>We also investigate how the trainable activations' position in the sequence affects performance. In</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 5: Initializing the prefix with activations of real words significantly outperforms random initialization, in low-data settings.</p>
<p>Prefix-tuning, we place them at the beginning [Prefix; <em>x; y</em>]. We can also place the trainable activations between <em>x</em> and <em>y</em> (i.e. [<em>x</em>; Infix; <em>y</em>]) and call this infix-tuning. Table 5 (bottom) shows that infix-tuning slightly underperforms prefix-tuning. We believe this is because prefix-tuning can affect the activations of <em>x</em> and <em>y</em> whereas infix-tuning can only influence the activations of <em>y</em>.</p>
<h3>7.4 Initialization</h3>
<p>We find that how the prefix is initialized has a large impact in low-data settings. Random initialization leads to low performance with high variance. Initializing the prefix with activations of real words significantly improves generation, as shown in Figure 5. In particular, initializing with task relevant words such as "summarization" and "table-to-text" obtains slightly better performance than task irrelevant words such as "elephant" and "divide", but using real words is still better than random. Moreover, in full data settings, the initialization trick has no impact, and random initialization leads to equally good performance.</p>
<p>Since we initialize the prefix with activations of real words computed by the LM, this initialization strategy is concordant with prefix-tuning's philosophy, which preserves the pretrained LM as much as possible.</p>
<h3>7.5 Data Efficiency</h3>
<p>We also investigate the data efficiency of prefix-tuning (without initialization trick, a.k.a random initialization) and full fine-tuning by comparing their performance on 5 different data scales of the E2E task (10%, 20%, 40%, 60%, and 80%). Figure 6 shows that prefix-tuning has better performance than fine-tuning when using more than 20% of the data. For data scale of 10%, prefix-tuning with random initialization yields comparable or slightly lower performance than full fine-tuning,</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 6: Data efficiency curves: percentage of training set vs. performance on table-to-text (E2E). Prefixtuning (without the initialization trick) is more dataefficient than fine-tuning when using more than $20 \%$ of the data.
necessitating the initialization trick ( $\S 6.3$ ) to improve the performance in this low-data regime.</p>
<h2>8 Discussion</h2>
<p>We will discuss several favorable properties of prefix-tuning and some open problems.</p>
<p>Personalization. As we note in $\S 1$, prefix-tuning is advantageous when there are a large number of tasks that needs to be trained independently. One practical setting is user privacy (Shokri and Shmatikov, 2015; McMahan et al., 2016). In order to preserve user privacy, each user's data needs to be separated and a personalized model needs to be trained independently for each user. Consequently, each user can be regarded as an independent task. If there are millions of users, prefix-tuning can scale to this setting and maintain modularity, enabling flexible addition or deletion of users by adding or deleting their prefixes without cross-contamination.</p>
<p>Batching across users. Under the same personalization setting, prefix-tuning allows batching different users' queries even though they are backed by different prefixes. When multiple users query a cloud GPU device with their inputs, it is computationally efficient to put these users in the same batch. Prefix-tuning keeps the shared LM intact; consequently, batching requires a simple step of prepending the personalized prefix to user input, and all the remaining computation is unchanged. In contrast, we can't batch across different users in adapter-tuning, which has personalized adapters between shared Transformer layers.</p>
<p>This batching benefit could also help create efficient ensembles of multiple prefixes trained on the same task (Lester et al., 2021).</p>
<p>Inductive bias of prefix-tuning. Recall that finetuning updates all pretrained parameters, whereas prefix-tuning and adapter-tuning preserve them.</p>
<p>Since the language models are pretrained on general purpose corpora, preserving the LM parameters might help generalization to domains unseen during training. In concordance with this intuition, we observe that both prefix-tuning and adaptertuning have significant performance gain in extrapolation settings ( $\S 6.4$ ); however, how these methods improve extrapolation is an open question.</p>
<p>While prefix-tuning and adapter-tuning both freeze the pretrained parameters, they tune different sets of parameters to affect the activation layers of the Transformer. Recall that prefix-tuning keeps the LM intact and uses the prefix and the pretrained attention blocks to affect the subsequent activations; adapter-tuning inserts trainable modules between LM layers, which directly add residual vectors to the activations. Moreover, we observe that prefixtuning requires vastly fewer parameters compared to adapter-tuning while maintaining comparable performance. We think this gain in parameter efficiency is because prefix-tuning keeps the pretrained LM intact as much as possible, and therefore exploits the LM more than adapter-tuning.</p>
<p>Recent work by Aghajanyan et al. (2020) uses intrinsic dimension to show that there exists a lowdimensional reparameterization that is as effective for fine-tuning as the full parametrization. This explains why good accuracy on downstream tasks can be obtained by updating only a small number of parameters. Our work echoes this finding by showing that good generation performance can also be attained by updating a very small prefix. However, prefix-tuning is not just about the size of trainable parameters, but more importantly, which subset of parameters to modify. Therefore, it would be interesting future work to explore other lightweight fine-tuning methods that achieve an even better accuracy-size tradeoff.</p>
<h2>Acknowledgments</h2>
<p>We thank the members of p-lambda group as well as anonymous reviewers for valuable feedback. We gratefully acknowledge the support of a PECASE award. XLL is supported by a Stanford Graduate Fellowship.</p>
<h2>Reproducibility</h2>
<p>Our code is available at https://github.com/ XiangLi1999/PrefixTuning.
Experiments and data are available at https: //worksheets.codalab.org/worksheets/ 0x16e0c8e7ab1f4b22aaccddc8b586541f.</p>
<h2>References</h2>
<p>Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. 2020. Intrinsic dimensionality explains the effectiveness of language model fine-tuning.</p>
<p>Anja Belz and Ehud Reiter. 2006. Comparing automatic and human evaluation of NLG systems. In 11th Conference of the European Chapter of the Association for Computational Linguistics, Trento, Italy. Association for Computational Linguistics.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners.</p>
<p>Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. 2020. Plug and play language models: A simple approach to controlled text generation. In International Conference on Learning Representations.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. The WebNLG challenge: Generating text from RDF data. In Proceedings of the 10th International Conference on Natural Language Generation, pages 124-133, Santiago de Compostela, Spain. Association for Computational Linguistics.</p>
<p>Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May. 2021. WARP: word-level adversarial reprogramming. CoRR, abs/2101.00121.</p>
<p>Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 2790-2799, Long Beach, California, USA. PMLR.</p>
<p>Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language
models know? Transactions of the Association for Computational Linguistics, 8:423-438.</p>
<p>Mihir Kale. 2020. Text-to-text pre-training for data-totext tasks.
N. Keskar, B. McCann, L. R. Varshney, Caiming Xiong, and R. Socher. 2019. Ctrl: A conditional transformer language model for controllable generation. ArXiv, abs/1909.05858.</p>
<p>Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shafiq Joty, Richard Socher, and Nazneen Fatema Rajani. 2020. GeDi: Generative Discriminator Guided Sequence Generation. arXiv preprint arXiv:2009.06367.</p>
<p>Alon Lavie and Abhaya Agarwal. 2007. Meteor: An automatic metric for mt evaluation with high levels of correlation with human judgments. In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT '07, pages 228-231, Stroudsburg, PA, USA. Association for Computational Linguistics.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics.</p>
<p>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.</p>
<p>Zhaojiang Lin, Andrea Madotto, and Pascale Fung. 2020. Exploring versatile generative language model via parameter-efficient transfer learning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 441-459, Online. Association for Computational Linguistics.</p>
<p>Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2021. Gpt understands, too. arXiv preprint arXiv:2103.10385.</p>
<p>Yang Liu and Mirella Lapata. 2019. Text summarization with pretrained encoders. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3730-3740, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020. Multilingual denoising pre-training for neural machine translation.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692.</p>
<p>Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In International Conference on Learning Representations.
H. Brendan McMahan, Eider Moore, Daniel Ramage, and Blaise Agüera y Arcas. 2016. Federated learning of deep networks using model averaging. Proceedings of the 20 th International Conference on Artificial Intelligence and Statistics (AISTATS) 2017, abs/1602.05629.</p>
<p>Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don't give me the details, just the summary! Topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium.</p>
<p>Jekaterina Novikova, Ondrej Dusek, and Verena Rieser. 2017. The E2E dataset: New challenges for end-toend generation. CoRR, abs/1706.09254.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL '02, pages 311-318, Stroudsburg, PA, USA. Association for Computational Linguistics.</p>
<p>Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. 2020. Adapterfusion: Non-destructive task composition for transfer learning.</p>
<p>Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying LMs with mixtures of soft prompts. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), Mexico City.</p>
<p>Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Nazneen Fatema Rajani, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, Yangxiaokang Liu, Nadia Irwanto, Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Murori Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, and Richard Socher. 2020. Dart: Open-domain structured data record to text generation.
A. Radford, Jeffrey Wu, R. Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.</p>
<p>Evani Radiya-Dixit and Xin Wang. 2020. How fine can fine-tuning be? learning efficient language models. In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics,
volume 108 of Proceedings of Machine Learning Research, pages 2435-2443, Online. PMLR.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-totext transformer. Journal of Machine Learning Research, 21(140):1-67.</p>
<p>Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. 2017. Learning multiple visual domains with residual adapters. In Advances in Neural Information Processing Systems, volume 30, pages 506516. Curran Associates, Inc.</p>
<p>Timo Schick and Hinrich Schütze. 2020. Exploiting cloze questions for few shot text classification and natural language inference.</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881-7892, Online. Association for Computational Linguistics.</p>
<p>Sheng Shen, Daniel Fried, Jacob Andreas, and Dan Klein. 2019. Pragmatically informative text generation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4060-4067, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Taylor Shin, Yasaman Razeghi, Robert L. Logan IV au2, Eric Wallace, and Sameer Singh. 2020. Autoprompt: Eliciting knowledge from language models with automatically generated prompts.</p>
<p>Reza Shokri and Vitaly Shmatikov. 2015. Privacypreserving deep learning. In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security, CCS '15, page 1310-1321, New York, NY, USA. Association for Computing Machinery.</p>
<p>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and Ralph Weischedel. 2006. A study of translation error rate with targeted human annotation. In In Proceedings of the Association for Machine Transaltion in the Americas (AMTA 2006.</p>
<p>Asa Cooper Stickland, Xian Li, and Marjan Ghazvininejad. 2020. Recipes for adapting pre-trained monolingual and multilingual models to machine translation.</p>
<p>Nishant Subramani, Samuel R. Bowman, and Kyunghyun Cho. 2020. Can unconditional language models recover arbitrary sentences?</p>
<p>Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. 2015. Cider: Consensus-based image description evaluation. In CVPR, pages 4566-4575. IEEE Computer Society.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Jeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas Guibas, and Jitendra Malik. 2020a. Sidetuning: A baseline for network adaptation via additive side networks.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020b. BERTScore: Evaluating text generation with bert. In International Conference on Learning Representations.</p>
<p>Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. 2020c. DIALOGPT : Largescale generative pre-training for conversational response generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 270278, Online. Association for Computational Linguistics.</p>
<p>Mengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and Hinrich Schütze. 2020. Masking as an efficient alternative to finetuning for pretrained language models.</p>
<p>Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and Steffen Eger. 2019. MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 563-578, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Ming Zhong, Pengfei Liu, Yiran Chen, Danqing Wang, Xipeng Qiu, and Xuanjing Huang. 2020. Extractive summarization as text matching. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6197-6208, Online. Association for Computational Linguistics.</p>
<p>Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin, Wengang Zhou, Houqiang Li, and Tieyan Liu. 2020. Incorporating bert into neural machine translation. In International Conference on Learning Representations.</p>
<h2>A Supplementary Material</h2>
<h2>A. 1 Datasets and Metrics</h2>
<p>We evaluate on three standard neural generation datasets for the table-to-text task: E2E (Novikova et al., 2017), WebNLG (Gardent et al., 2017), and DART (Radev et al., 2020).</p>
<p>The E2E dataset contains approximately 50K examples with 8 distinct fields; it contains multiple test references for one source table, and the average output length is 22.9 . We use the official evaluation script, ${ }^{12}$ which reports BLEU (Papineni et al., 2002), NIST (Belz and Reiter, 2006), METEOR (Lavie and Agarwal, 2007), ROUGE-L (Lin, 2004), and CIDEr (Vedantam et al., 2015).</p>
<p>The WebNLG (Gardent et al., 2017) dataset consists of 22 K examples, and the input $x$ is a sequence of (subject, property, object) triples. The average output length is 22.5 . In the training and validation splits, the input describes entities from 9 distinct DBpedia categories (e.g., Monument). The test split consists of two parts: the first half contains DB categories seen in training data, and the second half contains 5 unseen categories. These unseen categories are used to evaluate extrapolation. We use the official evaluation script, which reports BLEU, METEOR and TER (Snover et al., 2006).</p>
<p>DART (Radev et al., 2020) is an open domain table-to-text dataset, with similar input format (entity-relation-entity triples) as WebNLG. The average output length is 21.6 . It consists of 82 K examples from WikiSQL, WikiTableQuestions, E2E, and WebNLG and applies some manual or automated conversion. We use the official evaluation script ${ }^{13}$ and report BLEU, METEOR, TER, MoverScore (Zhao et al., 2019), BERTScore (Zhang et al., 2020b) and BLEURT (Sellam et al., 2020).</p>
<p>For the summarization task, we use the XSUM (Narayan et al., 2018) dataset, which is an abstractive summarization dataset on news articles. There are 225 K examples. The average length of the articles is 431 words and the average length of the summaries is 23.3. We report ROUGE-1, ROUGE2 and ROUGE-L, computed by the python package rouge-score.</p>
<p>Data pre-processing. For table-to-text, we linearize a table $x$ in order to fit into a language model context. In the E2E dataset, for example, "(field A,</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>value A), (field B, value B)" is linearized to "field A : value A | field B : value B". Also, in WebNLG and DART, a sequence of triple "(entity1.1, relation1, entity1.2), (entity2.1, relation2, entity2.2)" is linearlized as "entity1.1 : relation1 : entity1.2 | entity2.1 : relation2 : entity2.2".</p>
<p>For summarization, we truncate the articles $x$ to 512 BPE tokens.</p>
<p>Extrapolation data splits. We construct two extrapolation data splits news-to-sports and within-news from the original XSUM dataset. XSUM dataset is drawn from BBC news, and we identify the topic of each article based on its URL. Since "news" and "sports" are the two domains with the most articles, we create our first train/test split. Additionally, "news" has subdomains such as "UK", "world", and "technology". Consequently, we create a second data split, using the top 3 news subdomains (i.e. {world, UK, business }) as training data and the rest as test data.</p>
<h2>A. 2 Hyperparameters</h2>
<p>In Table 6, we report the hyperparameters used to train the best-performing models documented in the experiment section.</p>
<p>As for the search range of each hyperparameters: the learning rates are selected from ${1 \mathrm{e}-5,5 \mathrm{e}-05$, $8 \mathrm{e}-05}$; the number of epochs are selected from ${5$, 10} for table-to-text and ${5,25,30}$ for summarization; We select the largest batch size that can fit into GPU memory and didn't explicitly tune for an optimal batch size. Prefix length are selected from ${1,5,10,20,40}$ for table-to-text and ${1,10,20$, $50,80,100,200,300}$ for summarization. We use perplexity and automatic generation metrics on the validation set to select the best-performing models.</p>
<p>For table-to-text in the low data settings, we use a learning rate of $5 \mathrm{e}-5$, and a batch size of 10 . We use a prefix length of 6 , since we apply the initialization trick and initialize the prefix with "table-to-text:", which contains 6 BPE tokens. Instead of tuning the number of epochs, we tune the max steps of updates in ${100,200,400,600}$, as shown in Table 8. We apply early stopping based on the performance of validation set, where the validation size $=30 \%$ training size.</p>
<p>For summarization in the low data settings, we use a learning rate of $5 \mathrm{e}-5$ and a warmup step of 100. We use a batch size of 5 for prefix-tuning and 6 for fine-tuning. We apply the initialization trick and use the word "summarize" to initialize</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">learning rate</th>
<th style="text-align: center;"># epoch</th>
<th style="text-align: center;">batch size</th>
<th style="text-align: center;">prefix length</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Prefix:</td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">E2E</td>
<td style="text-align: left;">$8 \mathrm{e}-05$</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">WebNLG</td>
<td style="text-align: left;">$5 \mathrm{e}-05$</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">DART</td>
<td style="text-align: left;">$5 \mathrm{e}-05$</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: left;">XSUM</td>
<td style="text-align: left;">$5 \mathrm{e}-05$</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: left;">Adapter:</td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">E2E (3\%)</td>
<td style="text-align: left;">$5 \mathrm{e}-05$</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">E2E (0.1\%)</td>
<td style="text-align: left;">$8 \mathrm{e}-05$</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">WebNLG (3\%)</td>
<td style="text-align: left;">$5 \mathrm{e}-05$</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">WebNLG (0.1\%)</td>
<td style="text-align: left;">$5 \mathrm{e}-05$</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">DART (3\%)</td>
<td style="text-align: left;">$5 \mathrm{e}-05$</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">DART (0.1\%)</td>
<td style="text-align: left;">$8 \mathrm{e}-05$</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Fine-tune:</td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">E2E</td>
<td style="text-align: left;">$5 \mathrm{e}-05$</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">WebNLG</td>
<td style="text-align: left;">$1 \mathrm{e}-05$</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">DART</td>
<td style="text-align: left;">$1 \mathrm{e}-05$</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">FT-top2:</td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">E2E</td>
<td style="text-align: left;">$5 \mathrm{e}-05$</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">WebNLG</td>
<td style="text-align: left;">$5 \mathrm{e}-05$</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">DART</td>
<td style="text-align: left;">$5 \mathrm{e}-05$</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">within-news</td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Fine-tune</td>
<td style="text-align: left;">$3 \mathrm{e}-5$</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Prefix</td>
<td style="text-align: left;">$5 \mathrm{e}-5$</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">80</td>
</tr>
<tr>
<td style="text-align: left;">news-to-sports</td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Fine-tune</td>
<td style="text-align: left;">$3 \mathrm{e}-5$</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Prefix</td>
<td style="text-align: left;">$5 \mathrm{e}-5$</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">40</td>
</tr>
</tbody>
</table>
<p>Table 6: Hyperparameter settings for our method and baseline methods.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">R-1 $\uparrow$</th>
<th style="text-align: center;">R-2 $\uparrow$</th>
<th style="text-align: center;">R-L $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\operatorname{PrEfiX}(2 \%)$</td>
<td style="text-align: center;">43.30</td>
<td style="text-align: center;">20.35</td>
<td style="text-align: center;">35.21</td>
</tr>
<tr>
<td style="text-align: left;">$\operatorname{PrEfiX}(0.1 \%)$</td>
<td style="text-align: center;">41.54</td>
<td style="text-align: center;">18.56</td>
<td style="text-align: center;">33.13</td>
</tr>
</tbody>
</table>
<p>Table 7: Metrics for summarization on XSUM validation set.
the prefix, resulting in a prefix length of 1 . We tune the number of epochs in ${3,5,10,20,30}$, shown in Table 8 . We also apply early stopping based on validation performance.</p>
<p>For the extrapolation setting, the hyperparameters for our table-to-text model is the same as the hyperparameters of WebNLG. The hyperparameters for summarization is shown in the last block of Table 6.</p>
<h2>A. 3 Validation Performance</h2>
<p>Table 9 shows the validation performance on the three table-to-text datasets. Table 7 shows the validation performance on XSUM.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">size $=50$</th>
<th style="text-align: center;">size $=100$</th>
<th style="text-align: center;">size $=200$</th>
<th style="text-align: center;">size $=500$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Prefix (max steps)</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">400</td>
</tr>
<tr>
<td style="text-align: left;">Finetune (max steps)</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">400</td>
</tr>
<tr>
<td style="text-align: left;">Prefix (epoch)</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: left;">Finetune (epoch)</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">3</td>
</tr>
</tbody>
</table>
<p>Table 8: Max # update steps for low data settings.</p>
<h2>A. 4 Additional Results for Low-data Settings</h2>
<p>Figure 7 supplements the low-data performance curves in Figure 3 by plotting the relationship between training size and generation metrics for both prefix-tuning and fine-tuning.</p>
<h2>A. 5 Additional Results for the Initialization Experiment</h2>
<p>Figure 8 supplements Figure 3 by plotting additional metrics for our initialization technique $\S 7.4$. It validates that random initialization (from a uniform $(0,1)$ distirbution) significantly underperforms initializing with real words; Additionally, initializing with task-relevant words (e.g., "summarization" and "table-to-text") attains slightly better generation scores than initializing with task-irrelevant words (e.g., "elephant" and "banana").</p>
<h2>A. 6 Qualitative Examples for Extrapolation</h2>
<p>Table 10 contains qualitative examples from both seen and unseen categories in WebNLG. We find that for unseen categories, both prefix-tuning and fine-tuning tend to undergenerate (generated output do not cover full table contents) or generate untruthfully (generated output is inconsistent with table contents). In particular, prefix-tuning tends to undergenerate whereas fine-tuning tends to generate untruthfully. For seen categories, both perform fairly well in terms of coverage and truthfulness.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">E2E</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">WebNLG</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">DART</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">NIST</td>
<td style="text-align: center;">MET</td>
<td style="text-align: center;">R-L</td>
<td style="text-align: center;">CIDEr</td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">MET</td>
<td style="text-align: center;">TER $\downarrow$</td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">MET</td>
<td style="text-align: center;">TER $\downarrow$</td>
<td style="text-align: center;">Mover</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">BLEURT</td>
</tr>
<tr>
<td style="text-align: center;">GPT-2 MEDIUM</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">FT-FULL</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">8.76</td>
<td style="text-align: center;">49.3</td>
<td style="text-align: center;">76.9</td>
<td style="text-align: center;">2.66</td>
<td style="text-align: center;">66.03</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">50.46</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.41</td>
</tr>
<tr>
<td style="text-align: center;">FT-TOP2</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">8.51</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">75.3</td>
<td style="text-align: center;">2.60</td>
<td style="text-align: center;">54.61</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">48.41</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: center;">ADAPTER(3\%)</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">8.53</td>
<td style="text-align: center;">48.4</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">2.60</td>
<td style="text-align: center;">60.63</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">48.56</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.40</td>
</tr>
<tr>
<td style="text-align: center;">ADAPTER( $0.1 \%)$</td>
<td style="text-align: center;">68.1</td>
<td style="text-align: center;">8.30</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">71.4</td>
<td style="text-align: center;">2.41</td>
<td style="text-align: center;">53.24</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">44.72</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">0.35</td>
</tr>
<tr>
<td style="text-align: center;">Prefix( $0.1 \%)$</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">8.80</td>
<td style="text-align: center;">49.4</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">2.69</td>
<td style="text-align: center;">64.52</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">51.11</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.42</td>
</tr>
<tr>
<td style="text-align: center;">GPT-2_LARGE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">FT-FULL</td>
<td style="text-align: center;">72.1</td>
<td style="text-align: center;">8.62</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">75.1</td>
<td style="text-align: center;">2.56</td>
<td style="text-align: center;">64.69</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">51.00</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.43</td>
</tr>
<tr>
<td style="text-align: center;">Prefix</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">8.81</td>
<td style="text-align: center;">49.5</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">2.72</td>
<td style="text-align: center;">64.11</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">50.84</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.42</td>
</tr>
</tbody>
</table>
<p>Table 9: Metrics on the development set (higher is better, except for TER) for table-to-text generation on E2E (left), WebNLG (middle) and DART (right).
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 7: Prefix-tuning (orange) outperforms fine-tuning (blue) in low-data regimes in addition to requiring many fewer parameters. The top three plots correspond to summarization, measured by ROUGE-1, ROUGE-2, and ROUGE-L. The bottom three plots correspond to table-to-text, measured by NIST, METEOR, and CIDEr. The x -axis is the training size and the y -axis is the evaluation metric (higher is better).
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 8: Initializing the prefix with activations of real words significantly outperforms random initialization, in a low-data setting with 100 training data.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Source [Unseen, Athelete]</th>
<th style="text-align: center;">(Al Kharaitiyat SC, club, Alaa Abdul-Zahra), (Al Khor, ground, Al Kharaitiyat SC), (Shabab Al-Ordon Club, club, Alaa Abdul-Zahra) (Amar Osim, manager, Al Kharaitiyat SC)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Prefix-tuning</td>
<td style="text-align: center;">Al Kharaitiyat SC are managed by Amar Osim and play at their ground at Al Khor. Al Kharaitiyat SC are also the club for which Alaa Abdul-Zahra is a player.</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tuning</td>
<td style="text-align: center;">Alaa Abdul-Zahra plays for Al-Kharaitiyat SC and Shabab Al-Ordon Club. He also plays for Al-Khor and manages Al-Kharaitiyat SC.</td>
</tr>
<tr>
<td style="text-align: center;">Reference</td>
<td style="text-align: center;">Alaa Abdul Zahra plays for Al Kharaitiyat SC which is located at Al Khor and managed by Amar Osim. The Shabab Al-Ordon club is associated with Alaa Abdul-Zahra.</td>
</tr>
<tr>
<td style="text-align: center;">Source [Unseen, Transportation]</td>
<td style="text-align: center;">(Genoa, location, Costa Crociere), (AIDA Cruises, operator, AIDAstella), (Costa Crociere, owner, AIDAstella)</td>
</tr>
<tr>
<td style="text-align: center;">Prefix-tuning <br> Fine-tuning <br> Reference</td>
<td style="text-align: center;">AID Astella is operated by Aida Cruises and is owned by the Costa Rican tourist resort of Genoa. <br> AID Astella, operated by AIDA-Cruises, is located in Genoa and is owned by the Costa Rican government. Costa Crociere is the owner of the AIDAstella and are based in Genoa. The operator of AIDAstella is AIDA Cruises.</td>
</tr>
<tr>
<td style="text-align: center;">Source [Unseen, Politician]</td>
<td style="text-align: center;">(Euro, currency, Netherlands), (Stellendam, birthPlace, Ab Klink ), (Netherlands, nationality, Ab Klink)</td>
</tr>
<tr>
<td style="text-align: center;">Prefix-tuning <br> Fine-tuning <br> Reference</td>
<td style="text-align: center;">Ab Klink was born in Stellendam and is a national of the Netherlands where the currency is the Euro. Ab Klink is a national of the Netherlands where the currency is the Euro. He was born in Stellendam. Ab Klink was born in Stellendam in the Netherlands, where the national currency is the euro.</td>
</tr>
<tr>
<td style="text-align: center;">Source [Unseen, Politician]</td>
<td style="text-align: center;">(Robert E, Lee, commander, Battle of Salem Church), (American Civil War, isPartOfMilitaryConflict, Battle of Salem Church), (Battle of Salem Church, battles, Aaron S. Daggett)</td>
</tr>
<tr>
<td style="text-align: center;">Prefix-tuning</td>
<td style="text-align: center;">Robert E. Lee was the commander of the Battle of Salem Church which was part of the military conflict in the American Civil war.</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tuning <br> Reference</td>
<td style="text-align: center;">The Battle of Salem Church is part of the American Civil War and was commanded by Robert E. Lee. Robert E Lee was a commander in the Battle of Salem Church, which was one of the military conflicts in the American Civil War. Aaron S Daggett fought in the same battle.</td>
</tr>
<tr>
<td style="text-align: center;">Source [Unseen, Artist]</td>
<td style="text-align: center;">(Christian alternative rock, musicSubgenre, Alternative rock), (Alternative rock, genre, Andrew White (musician))</td>
</tr>
<tr>
<td style="text-align: center;">Prefix-tuning <br> Fine-tuning <br> Reference</td>
<td style="text-align: center;">Andrew White is a Christian alternative rock musician. <br> Andrew White, a Christian alternative rocker, performs. <br> The musician Andrew White's genre is alternative rock, the genre which has the sub genre Christian alternative rock.</td>
</tr>
<tr>
<td style="text-align: center;">Source [Unseen, Artist]</td>
<td style="text-align: center;">(Hip hop music, genre, Allen Forrest), (solo singer, background, Allen Forrest)</td>
</tr>
<tr>
<td style="text-align: center;">Prefix-tuning <br> Fine-tuning <br> Reference</td>
<td style="text-align: center;">Allen Forrest is a solo singer. <br> Born in <br> Allen Forrest is a solo singer whose genre is Hip Hop music.</td>
</tr>
<tr>
<td style="text-align: center;">Source [Seen, ComicsCharacter]</td>
<td style="text-align: center;">(Americans, nationality, Ducan Rouleau), (Ducan Rouleau, creator, Baymax),(Alan Tudyk, starring, Big Hero 6 (film)), (Steven T Segle, creator, Baymax), (Big Hero 6 (film), serires, Baymax)</td>
</tr>
<tr>
<td style="text-align: center;">Prefix-tuning</td>
<td style="text-align: center;">Baymax is a character in Big Hero 6 which stars Alan Tudyk. He was created by Steven T. Seagle and the American, Duncan Rouleau.</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tuning</td>
<td style="text-align: center;">Alan Tudyk stars in the film Big Hero 6 in which Baymax is a character created by Steven T. Seagle and the American, Duncan Rouleau.</td>
</tr>
<tr>
<td style="text-align: center;">Reference</td>
<td style="text-align: center;">Baymax is a character who appeared in Big Hero 6 starring Alan Tudyk. It was created by Steven T Seagle and the American, Duncan Rouleau.</td>
</tr>
<tr>
<td style="text-align: center;">Source [Seen, City]</td>
<td style="text-align: center;">(Washington, D.C., capital, United States), (White Americans, ethnicGroup, United States), (United States, country, New Jersey), (New York City, largest City, United States), (New Jersy, isPartOf, Atlantic City)</td>
</tr>
<tr>
<td style="text-align: center;">Prefix-tuning</td>
<td style="text-align: center;">Washington D.C. is the capital of the United States where the largest city is New York City and the White Americans are an ethnic group. Atlantic City, New Jersey is also part of the United States.</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tuning</td>
<td style="text-align: center;">Atlantic City, New Jersey is part of New Jersey in the United States. The capital city is Washington D.C. and one of the ethnic groups is White Americans.</td>
</tr>
<tr>
<td style="text-align: center;">Reference</td>
<td style="text-align: center;">New York City (NYC) is the largest U.S. city. Atlantic City, New Jersey are also part of the United States with its capital as Washington, DC and home to White Americans.</td>
</tr>
</tbody>
</table>
<p>Table 10: Qualitative examples from WebNLG. The first 6 examples are from the unseen categories, labeled next to source; the last two examples are from the seen categories. For unseen categories, both prefix-tuning and finetuning tend to undergenerate (generated output do not cover full table contents) or generate untruthfully (generated output is inconsistent with table contents). In particular, prefix-tuning tends to undergenerate more often than generate untruthfully whereas fine-tuning tends to generate untruthfully. For seen categories, both perform fairly well in terms of coverage and truthfulness.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{12}$ https://github.com/tuetschek/ e2e-metrics
${ }^{13}$ https://github.com/Yale-LILY/dart&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{9}$ We also sample a dev split (with dev size $=30 \% \times$ training size) for each training set. We use the dev split to choose hyperparameters and perform early stopping.
${ }^{10}$ The number in the parenthesis refers to the training size.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
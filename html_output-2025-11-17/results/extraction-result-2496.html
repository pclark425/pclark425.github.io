<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2496 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2496</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2496</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-270045418</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.15031v1.pdf" target="_blank">Amortized nonmyopic active search via deep imitation learning</a></p>
                <p><strong>Paper Abstract:</strong> Active search formalizes a specialized active learning setting where the goal is to collect members of a rare, valuable class. The state-of-the-art algorithm approximates the optimal Bayesian policy in a budget-aware manner, and has been shown to achieve impressive empirical performance in previous work. However, even this approximate policy has a superlinear computational complexity with respect to the size of the search problem, rendering its application impractical in large spaces or in real-time systems where decisions must be made quickly. We study the amortization of this policy by training a neural network to learn to search. To circumvent the difficulty of learning from scratch, we appeal to imitation learning techniques to mimic the behavior of the expert, expensive-to-compute policy. Our policy network, trained on synthetic data, learns a beneficial search strategy that yields nonmyopic decisions carefully balancing exploration and exploitation. Extensive experiments demonstrate our policy achieves competitive performance at real-world tasks that closely approximates the expert's at a fraction of the cost, while outperforming cheaper baselines.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2496.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2496.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ANS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Amortized Nonmyopic Search (policy network)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small feedforward neural network policy trained via imitation learning (DAGGER) to mimic the budget-aware ENS expert, amortizing expensive nonmyopic active search decisions to permit real-time and large-scale deployment while preserving nonmyopic exploration–exploitation behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Amortized Nonmyopic Search (ANS)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ANS is a learned policy network (5 fully connected ReLU layers with small width) that maps a state representation of an active search problem to a next-query choice. It is trained via DAGGER to imitate the Efficient Nonmyopic Search (ENS) expert on many synthetic, GP-sampled training problems. The state representation concatenates per-candidate features: posterior probability Pr(y=1|x,D), remaining budget ℓ, sum of posterior probabilities of the (ℓ−1) nearest unlabeled neighbors, and sum of similarities to those neighbors. At runtime ANS performs cheap forward passes to select queries, replacing the expensive ENS computation for fast per-iteration decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General active search tasks (demonstrated on drug discovery, materials discovery, disease hotspot detection, product recommendation).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Learned imitation of ENS: ANS selects the unlabeled candidate that the trained network predicts ENS would choose given the current state; decisions implicitly account for remaining budget through included features and through behavior learned from ENS (i.e., initial exploration when budget large, transition to exploitation as budget dwindles).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Wall-clock time per decision / per-iteration time; amortized cost is forward-pass time plus nearest-neighbor feature computation. Example reported: on 6.7M-molecule search, ANS averaged 36.94 ± 0.15 minutes per iteration (including approximate neighbor search precomputation), versus an estimated ~10 hours per iteration for ENS.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>ANS does not explicitly compute information-theoretic information gain; it imitates ENS which optimizes expected terminal utility (expected target count); features include neighbors' posterior sums as proxies for future yield.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Implicit: learned from ENS via imitation; the policy's input includes remaining budget and neighbor-based features that enable budget-aware nonmyopic behavior (explore when budget large, exploit when small). Behavior is emergent from training, not an explicit UCB-style formula.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity objective; diversity arises indirectly via ENS-like nonmyopic behavior that values querying points that open up regions of multiple targets (neighbor-sum features lead the network to prefer cluster centers when exploring).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed labeling/query budget (fixed number of experiments T).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Budget is encoded as feature ℓ = T − t and the training target (ENS) is budget-aware; ANS therefore learns to incorporate remaining budget into decisions, producing more exploratory selections when many queries remain and more exploitative ones as budget decreases.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Operationalized as discovering labeled 'targets' (e.g., actives); in some large-molecule tasks breakthroughs defined as top-percentile (e.g., top 1% by objective function) and targets are those above threshold; success measured by number of targets found within T queries.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Number of targets discovered within T = 100 queries (primary), and wall-clock time per iteration. Example: on 6.7M-molecule problems ANS takes 36.94 ± 0.15 minutes per iteration and finds substantially more targets than myopic baselines and nearly as many as ENS; exact counts per dataset reported in paper tables/figures (per-task numbers across many datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against ENS (expert), one-step greedy policy, UCB-family policies (p + β p(1−p) for β∈{0.1,0.3,1}), Explore-Then-Commit (ETC) with varied m, and Information-Directed Sampling (IDS) where computationally feasible.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>ANS closely approximates ENS's search performance across tasks while being much faster; it outperforms greedy one-step and other baselines on many tasks, and yields the best performance on the largest-scale problems in the paper. Quantitatively, ANS finds nearly as many targets as ENS but at a fraction of per-iteration runtime (example: ANS ≈ 37 minutes vs ENS ≈ 10 hours per iteration on 6.7M).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Empirical wall-clock speedup relative to ENS is large (orders of magnitude lower runtime per iteration in very large problems); exact speedup depends on dataset and implementation (paper reports ANS reduces ENS's per-iteration runtime from estimated ~10 hours to ~37 minutes on the largest task).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper presents a Pareto analysis (targets found vs time per iteration) showing ANS sits near ENS on performance while much cheaper computationally; it also shows ANS initially explores more than greedy one-step and then shifts to exploitation, yielding superior final performance. Ablations show the importance of the four engineered features and of DAGGER for achieving ENS-like behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Key practical insight: amortizing a budget-aware nonmyopic expert via imitation learning can deliver near-expert allocation behavior (balancing exploration and exploitation) at vastly reduced computational cost, enabling practical deployment under strict per-decision time or very large search spaces; representing remaining budget and neighbor-based future-value proxies is crucial.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Amortized nonmyopic active search via deep imitation learning', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2496.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2496.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ENS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Efficient Nonmyopic Search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A budget-aware approximation to the Bayesian optimal active search policy that assumes after the next query the remaining queries are made in a batch and approximates future selection by taking the top (ℓ−1) most-likely targets, yielding nonmyopic exploratory behavior while remaining computationally tractable relative to exhaustive dynamic programming.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Efficient Nonmyopic Active Search</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Efficient Nonmyopic Search (ENS)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ENS approximates the Bayesian optimal active search policy by making a simplifying batch assumption: after a candidate query, remaining ℓ−1 future queries are assumed to be selected simultaneously as the ℓ−1 highest-probability unlabeled points for each possible label outcome of the current query. ENS evaluates expected terminal utility (expected number of targets) of querying each candidate by summing over possible labels and their induced future top-(ℓ−1) sets, yielding a budget-aware score per candidate that induces nonmyopic exploration/exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Active search problems across domains (drug discovery, materials discovery, hotspot detection, recommender systems).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate queries by maximizing expected terminal utility (expected number of targets found by the end of budget) under the batch future-queries approximation; for each candidate, compute expected utility of querying it plus the top (ℓ−1) adaptively-selected future points under each possible label outcome.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Algorithmic complexity (big-O) and wall-clock time per decision. Complexity: naïve O(n^2 log n); optimized implementation with k-NN classifier reduces to O(n (log n + m log m + T)) where m is max degree in nearest-neighbor graph; runtime in large problems can be hours per decision (estimated ~10 hours/iteration at 6.7M scale).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected terminal utility measured as expected number of targets (expected sum of labels) — the selection objective is maximizing expected discoveries rather than mutual-information style metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Budget-awareness: because ENS sets its lookahead horizon equal to remaining budget ℓ, it naturally transitions: when budget large it prefers exploratory queries that could open up larger clusters, when budget small it favors exploitation; this emerges from expected-utility computation across possible label outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity-promoting term; diversity in explored hypotheses emerges indirectly by valuing queries that enable many future discoveries (via the top-(ℓ−1) neighbor mechanism).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of queries (fixed labeling budget T).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Explicitly accounts for remaining budget ℓ in lookahead: the policy computes scores using ℓ and the batch assumption to estimate terminal utility for the remaining horizon.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Breakthroughs considered as targets (rare valuable class members); performance measured by final count of targets found under budget. In large-molecule GuacaMol tasks, top-k percentile counting used to define 'breakthrough' targets (top 1%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Number of targets found under T queries (primary). ENS consistently achieves best performance among considered policies in settings where it is computationally feasible (reported across multiple datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to one-step greedy, UCB variants, ETC, IDS where feasible, and ANS (the amortized mimic).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>ENS is state-of-the-art on many problem instances and outperforms greedy one-step and other heuristics when computationally feasible; ANS approximates ENS closely while being far cheaper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Not an efficiency gain per se; ENS trades increased computation for substantial performance gains over myopic policies. However ENS is still computationally heavy and the target of amortization by ANS.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Authors note ENS's budget-awareness induces nonmyopic behavior that yields higher discovery counts at the cost of higher computational complexity; this tradeoff motivates amortization strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Budget-aware lookahead (matching lookahead horizon to remaining budget) is key to effective nonmyopic allocation: it yields strategic exploration early and exploitation later and substantially improves discovery over one-step greedy decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Amortized nonmyopic active search via deep imitation learning', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2496.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2496.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IDS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Information-Directed Sampling (heuristic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A heuristic that scores candidate queries by the ratio of expected information gain about currently high-value hypotheses to expected instantaneous regret, aiming to prioritize queries that yield the most informative reduction in uncertainty per unit of immediate loss.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning to Optimize via Information-Directed Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Information-Directed Sampling (IDS)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>IDS computes for each candidate x a score: (expected information gained about labels of the current ℓ most-likely targets when querying x) divided by (expected instantaneous regret from querying x). The heuristic seeks queries that maximize information per unit regret, thereby trading off learning (information acquisition) and immediate performance loss.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Active search / active learning; applied to small-to-moderate sized active search problems where information-theoretic quantities can be computed.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Rank candidates by information-to-regret ratio and select the highest-scoring candidate; this favors queries that most reduce uncertainty about high-value hypotheses relative to their immediate cost.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Computational feasibility is limited by the cost of computing expected information gains and regrets; therefore IDS was only applied to small search spaces in the paper (problems with fewer than ~1000 points were feasible).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Explicit information-theoretic information gain about labels (mutual information / expected reduction in uncertainty) for the ℓ most-likely targets; used in the numerator of the score.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Explicit: IDS balances exploration and exploitation by maximizing information gained per expected regret — inherently promotes exploration if it yields high information relative to immediate cost.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Implicit: by seeking information about the set of likely targets, IDS can select queries that diversify what is learned about the hypothesis set, but there is no explicit diversity objective beyond information.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed labeling budget; ℓ (remaining budget) figures in which set of most-likely targets the information is computed over.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>IDS computes information with respect to the current horizon (ℓ) and thus implicitly accounts for remaining budget in its information calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not directly optimized for breakthrough count; goal is to reduce uncertainty about high-value hypotheses which can improve discovery rates indirectly; in experiments performance measured by targets found.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Number of targets found under T queries when computationally feasible. IDS performed less well than ENS in reported comparisons for the problem sizes where it was applied.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to ENS, ANS, one-step, UCB, ETC.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>IDS was computationally expensive and did not outperform ENS in the small problems where it was feasible to run; IDS also did not match ENS's performance in authors' experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>None reported in this paper; IDS is usually more computationally intensive than simpler heuristics and was only applied on small spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>IDS explicitly quantifies information vs regret tradeoff; paper uses IDS as a baseline to contrast objectives that explicitly optimize information-theoretic gains with ENS's expected-terminal-utility objective.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Information-per-regret is a principled allocation heuristic, but computational cost of computing information limits its applicability to large search spaces without approximation or amortization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Amortized nonmyopic active search via deep imitation learning', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2496.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2496.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UCB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Upper Confidence Bound family (active learning variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of scoring heuristics that combine posterior mean (exploitation) and an uncertainty term (exploration) via p + β p(1−p), where β controls exploration weight; used as computationally cheap baselines in active search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Using Upper Confidence Bounds for Online Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Upper Confidence Bound (UCB) policies for active search</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Rank candidates by score s(x)=p(x)+β·p(x)(1−p(x)), where p(x)=Pr(y=1|x,D); the second term is a proxy for uncertainty (variance of a Bernoulli) and β tunes exploration vs exploitation. Low β prioritizes exploitation (high p), higher β increases exploration for uncertain candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Active learning / active search across applications used in experiments (product recommendation, disease hotspot detection, materials, drug discovery).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Per-iteration greedy ranking by p + β p(1−p); allocate budget by repeatedly selecting top-scoring candidates until budget exhausted (or sequentially updating posteriors after each label).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Per-iteration time is small (simple scoring per candidate); wall-clock time dominated by posterior updates and nearest-neighbor operations when used with k-NN classifiers; complexity scales linearly in n per iteration for score computations.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Exploration term p(1−p) is a surrogate for uncertainty reduction rather than explicit mutual information; not an explicit information-gain computation.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Explicit via parameter β controlling tradeoff: β=0 is greedy exploitation (one-step), larger β increases weight on uncertain (exploratory) candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity mechanism beyond the uncertainty term which may encourage sampling of uncertain regions; no explicit multi-hypothesis diversity objective.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed query budget; handled implicitly by repeated application of the scoring rule until budget exhausted.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>No explicit lookahead for remaining budget; myopic greedy selection each iteration (though β can encourage exploration across iterations).</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Targets found (count) within budget; no specialized breakthrough metric.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Number of targets found under fixed T; experiments showed UCB variants with low β (favoring exploitation) tended to perform better than higher β in many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against ENS, ANS, one-step, ETC, IDS.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>UCB performed reasonably but generally worse than ENS and ANS; tuning β is sensitive and no single β performed best across all tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Computationally cheap relative to ENS; lower wall-clock cost but generally lower discovery performance when lack of lookahead hurts outcome.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper notes tradeoffs in β selection: more exploration (higher β) can help find diverse clusters but risks lower immediate yield; prioritizing exploitation (small β) often beneficial in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Simple uncertainty-weighted scores can provide a computationally inexpensive allocation rule, but without budget-aware nonmyopic reasoning they can be arbitrarily suboptimal compared to nonmyopic policies like ENS.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Amortized nonmyopic active search via deep imitation learning', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2496.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2496.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ETC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explore-Then-Commit (ETC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple two-phase baseline: uniformly sample the space for m iterations to explore, then switch to greedy one-step exploitation for the remaining budget; used to illustrate the value of structured exploration schedules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Active Covering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Explore-Then-Commit (ETC)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ETC uniformly samples (randomly) for m initial iterations to gather global information, then commits to one-step greedy selection for the remaining budget. Parameter m is the number of exploration rounds and must be chosen a priori.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Active search tasks used as baseline analyses (product recommendation, disease hotpots, materials, drug discovery).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate first m queries uniformly at random to broadly probe the space, then greedily exploit (select highest posterior-probability points) for the rest of the budget.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Very low per-iteration computational cost (random sampling and greedy ranking); wall-clock time minimal compared to ENS.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>No explicit information-theoretic objective; initial uniform sampling is purely exploratory to increase coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Explicit schedule: fixed exploration phase of m iterations followed by exploitation; does not adaptively change based on observed labels.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Initial uniform exploration promotes coverage/diversity early, but no ongoing diversity-aware selection beyond the initial phase.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed budget T; user-specified m determines how much of budget is reserved for exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Allocates a pre-specified fraction of the budget to exploration (m), then uses remaining budget for exploitation; performance sensitive to choice of m.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Targets found; initial exploration can increase chance of locating distinct clusters (breakthroughs) but m must be well-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Number of targets found under different m settings; authors found no single m performs best across tasks, highlighting difficulty in tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against one-step, UCB, ENS, ANS, IDS.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>ETC can outperform naive greedy if m is well-chosen, but generally underperforms ENS and ANS which adaptively balance exploration and exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Computationally cheap but suboptimal in allocation when m poorly chosen.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper highlights sensitivity of ETC to m; fixed schedules cannot adapt to varying structure across problems and budgets, unlike ENS/ANS.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Fixed-schedule exploration is simple but brittle; adaptive, budget-aware methods (ENS/ANS) are preferred for robust allocation across varied problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Amortized nonmyopic active search via deep imitation learning', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2496.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2496.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DAGGER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dataset Aggregation (DAGGER) imitation learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative imitation learning algorithm that aggregates states visited by the current policy and labels them with an expert policy to reduce distributional shift and improve learned policy robustness; used to train ANS by repeatedly querying ENS on states the network visits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DAGGER (Dataset Aggregation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>DAGGER iteratively (1) rolls out the current policy on training problem instances to collect states encountered, (2) queries the expert policy for the optimal action at those states, (3) aggregates the newly labeled (state,expert-action) pairs into the training set, and (4) retrains the policy. This addresses covariate shift between states seen during expert rollouts and states visited by the learned policy.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Imitation learning for active search policy training (used to train ANS to mimic ENS); broadly applicable to imitation learning in RL and sequential decision problems.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Not an allocation policy itself; DAGGER is a training procedure enabling a learned policy to mimic a resource-allocation expert (ENS) so that the learned policy can make fast allocation decisions at runtime.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Training cost measured in number of expert queries (calls to ENS) and training iterations; DAGGER reduces long-term deployment costs by investing in expert queries during offline training.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Not an information-acquisition policy; does not compute information gain for decisions—its objective is minimizing supervised loss (cross-entropy) on expert actions aggregated over visited states.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Not directly applicable; DAGGER's role is to gather training states that reflect the learned policy's distribution, which can include exploratory states if the policy explores during rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>By rolling out the current policy and aggregating visited states, DAGGER encourages diversity of training states that reflect the learned policy's behavior; no explicit diversity objective beyond that.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Indirect: DAGGER training requires many simulated episodes but these are run on synthetic cheap problems to avoid consuming real labeling budget.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>The paper reduces real labeling cost by generating synthetic GP-based problems where ENS is cheap to query, using DAGGER to amortize expert knowledge into a learned policy that eliminates the need to run ENS at deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not applicable directly; DAGGER's success measured by how well the learned policy mimics expert decisions and by downstream discovery counts when the learned policy is deployed.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Measured via final policy's performance (targets found) and stability across seeds; ablations show DAGGER outperforms behavior cloning without aggregation and learning-from-scratch.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared DAGGER-trained ANS vs imitation learning without DAGGER vs REINFORCE training from scratch; DAGGER yields better downstream search performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>DAGGER-trained policies significantly outperform networks trained by cloning only on expert rollouts (no aggregation) and policies trained from scratch with REINFORCE in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Offline investment in ENS expert queries during DAGGER training produces a learned policy that is much cheaper at deployment (orders-of-magnitude lower per-decision computation) while retaining most expert performance.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper shows that iterative aggregation is crucial to avoid covariate shift and achieve robust imitation of ENS; training on synthetic problems avoids consuming expensive real labels but still yields policies that generalize to real tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Practical recommendation: use DAGGER with diverse synthetic problem generation to amortize expensive budget-aware expert policies into fast learned policies suitable for large-scale and real-time allocation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Amortized nonmyopic active search via deep imitation learning', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2496.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2496.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deep Adaptive Design</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Adaptive Design: Amortizing Sequential Bayesian Experimental Design</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior work that trains a design network to amortize the computation of maximizing expected information gain in sequential Bayesian experimental design, serving as inspiration for amortization of expensive experimental-design objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep Adaptive Design: Amortizing Sequential Bayesian Experimental Design</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Deep Adaptive Design (amortized BED)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A neural-network based approach that trains a design network to predict or produce experiment designs that approximately maximize expected information gain for Bayesian experimental design problems, thereby amortizing expensive per-experiment information-gain optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Bayesian experimental design (BED) — generic scientific experimental design tasks where information gain is the objective.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Learn a mapping from current posterior (or context) to next experiment design that approximates the argmax of expected information gain, thus enabling fast allocation decisions at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Amortized per-decision computation (forward pass) vs expensive repeated optimization for information-theoretic objectives; training cost is upfront but evaluation cost is low.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Explicitly maximizes expected information gain (mutual information / reduction in posterior uncertainty) in the original formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Directly optimizes information gain which trades off learning vs immediate performance implicitly; not framed in exploitation terms but maximizes expected information about model parameters or targets.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Information-maximization tends to promote diversity in selected experiments since high-information designs often probe different regions of model uncertainty, but no explicit diversity penalty beyond information metric.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Sequential budgeted experimental design (fixed number or horizon); amortization permits rapid sequential decisions under budget constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Amortization itself reduces per-query optimization cost; original BED objective includes horizon and can be applied sequentially until budget exhausted.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Typically assessed via reduction in posterior uncertainty or downstream decision quality; in practice may translate to improved ability to identify high-performing designs or targets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported improvements in speed (amortized evaluation) with competitive or improved design quality compared to online optimization of information metrics; used as inspiration in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against online optimization of information-theoretic objectives and other heuristics in cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Amortization yields substantial runtime savings and competitive design quality in the cited work; the current paper cites it as an inspiration for amortizing ENS.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Substantial per-decision speedups due to replacing online optimization with network forward passes; exact numbers depend on problem and implementation in cited paper.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Tradeoff is between upfront training cost (and need for representative training problems) and per-decision speed at deployment; amortized networks can generalize but require careful training/problem generation.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Amortization is a viable strategy to bring information-theoretic experimental design to large-scale or real-time settings, provided representative training distributions and architectures are used.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Amortized nonmyopic active search via deep imitation learning', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2496.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2496.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FAISS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FAISS (Approximate Nearest Neighbor Search)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A similarity-search library used to accelerate nearest-neighbor computations in very large candidate pools, enabling efficient construction of neighbor-based features needed by ANS and ENS approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Billion-Scale Similarity Search with GPUs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>FAISS approximate nearest-neighbor search</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>FAISS is a high-performance library (CPU/GPU) for approximate nearest neighbor (ANN) search on large-scale embeddings; the authors use FAISS to precompute neighbor indices and similarities so that per-iteration feature construction (neighbor sums) scales to millions of points.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Engineering/performance component to enable active search at billion-scale candidate databases (drug-molecule databases, large recommender item pools).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Not an allocation policy; FAISS reduces computational cost of computing neighbor-based features used by allocation policies (ANS/ENS approximations), enabling practical per-iteration runtimes.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Wall-clock time for nearest-neighbor precomputation and per-iteration feature construction; reported: neighbor search for 6.7M points completed in roughly one hour (precomputation). After precomputation per-iteration feature construction is O(n).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Computational/time budget for per-iteration feature computations; FAISS trades approximation accuracy for speed.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Uses ANN approximations and clustering to reduce query time; authors set number of clusters following practical heuristics (⌊4√n⌋).</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Measured as time to compute neighbors (approx. one hour for 6.7M points) and enabling overall per-iteration runtimes reported for ANS.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Exact nearest-neighbor search (not feasible at scale); FAISS provides tractable approximate alternative.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>FAISS enables tractable preprocessing; without ANN methods neighbor computations would be a bottleneck rendering large-scale deployment infeasible.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Makes neighbor-feature construction feasible at multi-million scale by reducing neighbor search time from infeasible to ~1 hour precompute plus O(n) per-iteration feature updates.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Approximation in neighbor search introduces potential small errors in features, but the learned policy remains robust and yields strong downstream performance; precomputation amortizes cost across many search episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Engineering-level finding: approximate neighbor search libraries are critical enabling components when allocation strategies require neighborhood-based features at massive scales.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Amortized nonmyopic active search via deep imitation learning', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Efficient Nonmyopic Active Search <em>(Rating: 2)</em></li>
                <li>Deep Adaptive Design: Amortizing Sequential Bayesian Experimental Design <em>(Rating: 2)</em></li>
                <li>Learning to Optimize via Information-Directed Sampling <em>(Rating: 2)</em></li>
                <li>Adaptive Sampling for Discovery <em>(Rating: 2)</em></li>
                <li>Bayesian Optimal Active Search and Surveying <em>(Rating: 2)</em></li>
                <li>Learning Active Learning from Data <em>(Rating: 1)</em></li>
                <li>Quality-Weighted Vendi Scores And Their Application To Diverse Experimental Design <em>(Rating: 1)</em></li>
                <li>Cost effective active search <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2496",
    "paper_id": "paper-270045418",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "ANS",
            "name_full": "Amortized Nonmyopic Search (policy network)",
            "brief_description": "A small feedforward neural network policy trained via imitation learning (DAGGER) to mimic the budget-aware ENS expert, amortizing expensive nonmyopic active search decisions to permit real-time and large-scale deployment while preserving nonmyopic exploration–exploitation behavior.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Amortized Nonmyopic Search (ANS)",
            "system_description": "ANS is a learned policy network (5 fully connected ReLU layers with small width) that maps a state representation of an active search problem to a next-query choice. It is trained via DAGGER to imitate the Efficient Nonmyopic Search (ENS) expert on many synthetic, GP-sampled training problems. The state representation concatenates per-candidate features: posterior probability Pr(y=1|x,D), remaining budget ℓ, sum of posterior probabilities of the (ℓ−1) nearest unlabeled neighbors, and sum of similarities to those neighbors. At runtime ANS performs cheap forward passes to select queries, replacing the expensive ENS computation for fast per-iteration decisions.",
            "application_domain": "General active search tasks (demonstrated on drug discovery, materials discovery, disease hotspot detection, product recommendation).",
            "resource_allocation_strategy": "Learned imitation of ENS: ANS selects the unlabeled candidate that the trained network predicts ENS would choose given the current state; decisions implicitly account for remaining budget through included features and through behavior learned from ENS (i.e., initial exploration when budget large, transition to exploitation as budget dwindles).",
            "computational_cost_metric": "Wall-clock time per decision / per-iteration time; amortized cost is forward-pass time plus nearest-neighbor feature computation. Example reported: on 6.7M-molecule search, ANS averaged 36.94 ± 0.15 minutes per iteration (including approximate neighbor search precomputation), versus an estimated ~10 hours per iteration for ENS.",
            "information_gain_metric": "ANS does not explicitly compute information-theoretic information gain; it imitates ENS which optimizes expected terminal utility (expected target count); features include neighbors' posterior sums as proxies for future yield.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Implicit: learned from ENS via imitation; the policy's input includes remaining budget and neighbor-based features that enable budget-aware nonmyopic behavior (explore when budget large, exploit when small). Behavior is emergent from training, not an explicit UCB-style formula.",
            "diversity_mechanism": "No explicit diversity objective; diversity arises indirectly via ENS-like nonmyopic behavior that values querying points that open up regions of multiple targets (neighbor-sum features lead the network to prefer cluster centers when exploring).",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed labeling/query budget (fixed number of experiments T).",
            "budget_constraint_handling": "Budget is encoded as feature ℓ = T − t and the training target (ENS) is budget-aware; ANS therefore learns to incorporate remaining budget into decisions, producing more exploratory selections when many queries remain and more exploitative ones as budget decreases.",
            "breakthrough_discovery_metric": "Operationalized as discovering labeled 'targets' (e.g., actives); in some large-molecule tasks breakthroughs defined as top-percentile (e.g., top 1% by objective function) and targets are those above threshold; success measured by number of targets found within T queries.",
            "performance_metrics": "Number of targets discovered within T = 100 queries (primary), and wall-clock time per iteration. Example: on 6.7M-molecule problems ANS takes 36.94 ± 0.15 minutes per iteration and finds substantially more targets than myopic baselines and nearly as many as ENS; exact counts per dataset reported in paper tables/figures (per-task numbers across many datasets).",
            "comparison_baseline": "Compared against ENS (expert), one-step greedy policy, UCB-family policies (p + β p(1−p) for β∈{0.1,0.3,1}), Explore-Then-Commit (ETC) with varied m, and Information-Directed Sampling (IDS) where computationally feasible.",
            "performance_vs_baseline": "ANS closely approximates ENS's search performance across tasks while being much faster; it outperforms greedy one-step and other baselines on many tasks, and yields the best performance on the largest-scale problems in the paper. Quantitatively, ANS finds nearly as many targets as ENS but at a fraction of per-iteration runtime (example: ANS ≈ 37 minutes vs ENS ≈ 10 hours per iteration on 6.7M).",
            "efficiency_gain": "Empirical wall-clock speedup relative to ENS is large (orders of magnitude lower runtime per iteration in very large problems); exact speedup depends on dataset and implementation (paper reports ANS reduces ENS's per-iteration runtime from estimated ~10 hours to ~37 minutes on the largest task).",
            "tradeoff_analysis": "Paper presents a Pareto analysis (targets found vs time per iteration) showing ANS sits near ENS on performance while much cheaper computationally; it also shows ANS initially explores more than greedy one-step and then shifts to exploitation, yielding superior final performance. Ablations show the importance of the four engineered features and of DAGGER for achieving ENS-like behavior.",
            "optimal_allocation_findings": "Key practical insight: amortizing a budget-aware nonmyopic expert via imitation learning can deliver near-expert allocation behavior (balancing exploration and exploitation) at vastly reduced computational cost, enabling practical deployment under strict per-decision time or very large search spaces; representing remaining budget and neighbor-based future-value proxies is crucial.",
            "uuid": "e2496.0",
            "source_info": {
                "paper_title": "Amortized nonmyopic active search via deep imitation learning",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "ENS",
            "name_full": "Efficient Nonmyopic Search",
            "brief_description": "A budget-aware approximation to the Bayesian optimal active search policy that assumes after the next query the remaining queries are made in a batch and approximates future selection by taking the top (ℓ−1) most-likely targets, yielding nonmyopic exploratory behavior while remaining computationally tractable relative to exhaustive dynamic programming.",
            "citation_title": "Efficient Nonmyopic Active Search",
            "mention_or_use": "use",
            "system_name": "Efficient Nonmyopic Search (ENS)",
            "system_description": "ENS approximates the Bayesian optimal active search policy by making a simplifying batch assumption: after a candidate query, remaining ℓ−1 future queries are assumed to be selected simultaneously as the ℓ−1 highest-probability unlabeled points for each possible label outcome of the current query. ENS evaluates expected terminal utility (expected number of targets) of querying each candidate by summing over possible labels and their induced future top-(ℓ−1) sets, yielding a budget-aware score per candidate that induces nonmyopic exploration/exploitation.",
            "application_domain": "Active search problems across domains (drug discovery, materials discovery, hotspot detection, recommender systems).",
            "resource_allocation_strategy": "Allocate queries by maximizing expected terminal utility (expected number of targets found by the end of budget) under the batch future-queries approximation; for each candidate, compute expected utility of querying it plus the top (ℓ−1) adaptively-selected future points under each possible label outcome.",
            "computational_cost_metric": "Algorithmic complexity (big-O) and wall-clock time per decision. Complexity: naïve O(n^2 log n); optimized implementation with k-NN classifier reduces to O(n (log n + m log m + T)) where m is max degree in nearest-neighbor graph; runtime in large problems can be hours per decision (estimated ~10 hours/iteration at 6.7M scale).",
            "information_gain_metric": "Expected terminal utility measured as expected number of targets (expected sum of labels) — the selection objective is maximizing expected discoveries rather than mutual-information style metrics.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Budget-awareness: because ENS sets its lookahead horizon equal to remaining budget ℓ, it naturally transitions: when budget large it prefers exploratory queries that could open up larger clusters, when budget small it favors exploitation; this emerges from expected-utility computation across possible label outcomes.",
            "diversity_mechanism": "No explicit diversity-promoting term; diversity in explored hypotheses emerges indirectly by valuing queries that enable many future discoveries (via the top-(ℓ−1) neighbor mechanism).",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed number of queries (fixed labeling budget T).",
            "budget_constraint_handling": "Explicitly accounts for remaining budget ℓ in lookahead: the policy computes scores using ℓ and the batch assumption to estimate terminal utility for the remaining horizon.",
            "breakthrough_discovery_metric": "Breakthroughs considered as targets (rare valuable class members); performance measured by final count of targets found under budget. In large-molecule GuacaMol tasks, top-k percentile counting used to define 'breakthrough' targets (top 1%).",
            "performance_metrics": "Number of targets found under T queries (primary). ENS consistently achieves best performance among considered policies in settings where it is computationally feasible (reported across multiple datasets).",
            "comparison_baseline": "Compared to one-step greedy, UCB variants, ETC, IDS where feasible, and ANS (the amortized mimic).",
            "performance_vs_baseline": "ENS is state-of-the-art on many problem instances and outperforms greedy one-step and other heuristics when computationally feasible; ANS approximates ENS closely while being far cheaper.",
            "efficiency_gain": "Not an efficiency gain per se; ENS trades increased computation for substantial performance gains over myopic policies. However ENS is still computationally heavy and the target of amortization by ANS.",
            "tradeoff_analysis": "Authors note ENS's budget-awareness induces nonmyopic behavior that yields higher discovery counts at the cost of higher computational complexity; this tradeoff motivates amortization strategies.",
            "optimal_allocation_findings": "Budget-aware lookahead (matching lookahead horizon to remaining budget) is key to effective nonmyopic allocation: it yields strategic exploration early and exploitation later and substantially improves discovery over one-step greedy decisions.",
            "uuid": "e2496.1",
            "source_info": {
                "paper_title": "Amortized nonmyopic active search via deep imitation learning",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "IDS",
            "name_full": "Information-Directed Sampling (heuristic)",
            "brief_description": "A heuristic that scores candidate queries by the ratio of expected information gain about currently high-value hypotheses to expected instantaneous regret, aiming to prioritize queries that yield the most informative reduction in uncertainty per unit of immediate loss.",
            "citation_title": "Learning to Optimize via Information-Directed Sampling",
            "mention_or_use": "use",
            "system_name": "Information-Directed Sampling (IDS)",
            "system_description": "IDS computes for each candidate x a score: (expected information gained about labels of the current ℓ most-likely targets when querying x) divided by (expected instantaneous regret from querying x). The heuristic seeks queries that maximize information per unit regret, thereby trading off learning (information acquisition) and immediate performance loss.",
            "application_domain": "Active search / active learning; applied to small-to-moderate sized active search problems where information-theoretic quantities can be computed.",
            "resource_allocation_strategy": "Rank candidates by information-to-regret ratio and select the highest-scoring candidate; this favors queries that most reduce uncertainty about high-value hypotheses relative to their immediate cost.",
            "computational_cost_metric": "Computational feasibility is limited by the cost of computing expected information gains and regrets; therefore IDS was only applied to small search spaces in the paper (problems with fewer than ~1000 points were feasible).",
            "information_gain_metric": "Explicit information-theoretic information gain about labels (mutual information / expected reduction in uncertainty) for the ℓ most-likely targets; used in the numerator of the score.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Explicit: IDS balances exploration and exploitation by maximizing information gained per expected regret — inherently promotes exploration if it yields high information relative to immediate cost.",
            "diversity_mechanism": "Implicit: by seeking information about the set of likely targets, IDS can select queries that diversify what is learned about the hypothesis set, but there is no explicit diversity objective beyond information.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed labeling budget; ℓ (remaining budget) figures in which set of most-likely targets the information is computed over.",
            "budget_constraint_handling": "IDS computes information with respect to the current horizon (ℓ) and thus implicitly accounts for remaining budget in its information calculations.",
            "breakthrough_discovery_metric": "Not directly optimized for breakthrough count; goal is to reduce uncertainty about high-value hypotheses which can improve discovery rates indirectly; in experiments performance measured by targets found.",
            "performance_metrics": "Number of targets found under T queries when computationally feasible. IDS performed less well than ENS in reported comparisons for the problem sizes where it was applied.",
            "comparison_baseline": "Compared to ENS, ANS, one-step, UCB, ETC.",
            "performance_vs_baseline": "IDS was computationally expensive and did not outperform ENS in the small problems where it was feasible to run; IDS also did not match ENS's performance in authors' experiments.",
            "efficiency_gain": "None reported in this paper; IDS is usually more computationally intensive than simpler heuristics and was only applied on small spaces.",
            "tradeoff_analysis": "IDS explicitly quantifies information vs regret tradeoff; paper uses IDS as a baseline to contrast objectives that explicitly optimize information-theoretic gains with ENS's expected-terminal-utility objective.",
            "optimal_allocation_findings": "Information-per-regret is a principled allocation heuristic, but computational cost of computing information limits its applicability to large search spaces without approximation or amortization.",
            "uuid": "e2496.2",
            "source_info": {
                "paper_title": "Amortized nonmyopic active search via deep imitation learning",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "UCB",
            "name_full": "Upper Confidence Bound family (active learning variant)",
            "brief_description": "A set of scoring heuristics that combine posterior mean (exploitation) and an uncertainty term (exploration) via p + β p(1−p), where β controls exploration weight; used as computationally cheap baselines in active search.",
            "citation_title": "Using Upper Confidence Bounds for Online Learning",
            "mention_or_use": "use",
            "system_name": "Upper Confidence Bound (UCB) policies for active search",
            "system_description": "Rank candidates by score s(x)=p(x)+β·p(x)(1−p(x)), where p(x)=Pr(y=1|x,D); the second term is a proxy for uncertainty (variance of a Bernoulli) and β tunes exploration vs exploitation. Low β prioritizes exploitation (high p), higher β increases exploration for uncertain candidates.",
            "application_domain": "Active learning / active search across applications used in experiments (product recommendation, disease hotspot detection, materials, drug discovery).",
            "resource_allocation_strategy": "Per-iteration greedy ranking by p + β p(1−p); allocate budget by repeatedly selecting top-scoring candidates until budget exhausted (or sequentially updating posteriors after each label).",
            "computational_cost_metric": "Per-iteration time is small (simple scoring per candidate); wall-clock time dominated by posterior updates and nearest-neighbor operations when used with k-NN classifiers; complexity scales linearly in n per iteration for score computations.",
            "information_gain_metric": "Exploration term p(1−p) is a surrogate for uncertainty reduction rather than explicit mutual information; not an explicit information-gain computation.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Explicit via parameter β controlling tradeoff: β=0 is greedy exploitation (one-step), larger β increases weight on uncertain (exploratory) candidates.",
            "diversity_mechanism": "No explicit diversity mechanism beyond the uncertainty term which may encourage sampling of uncertain regions; no explicit multi-hypothesis diversity objective.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed query budget; handled implicitly by repeated application of the scoring rule until budget exhausted.",
            "budget_constraint_handling": "No explicit lookahead for remaining budget; myopic greedy selection each iteration (though β can encourage exploration across iterations).",
            "breakthrough_discovery_metric": "Targets found (count) within budget; no specialized breakthrough metric.",
            "performance_metrics": "Number of targets found under fixed T; experiments showed UCB variants with low β (favoring exploitation) tended to perform better than higher β in many tasks.",
            "comparison_baseline": "Compared against ENS, ANS, one-step, ETC, IDS.",
            "performance_vs_baseline": "UCB performed reasonably but generally worse than ENS and ANS; tuning β is sensitive and no single β performed best across all tasks.",
            "efficiency_gain": "Computationally cheap relative to ENS; lower wall-clock cost but generally lower discovery performance when lack of lookahead hurts outcome.",
            "tradeoff_analysis": "Paper notes tradeoffs in β selection: more exploration (higher β) can help find diverse clusters but risks lower immediate yield; prioritizing exploitation (small β) often beneficial in their experiments.",
            "optimal_allocation_findings": "Simple uncertainty-weighted scores can provide a computationally inexpensive allocation rule, but without budget-aware nonmyopic reasoning they can be arbitrarily suboptimal compared to nonmyopic policies like ENS.",
            "uuid": "e2496.3",
            "source_info": {
                "paper_title": "Amortized nonmyopic active search via deep imitation learning",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "ETC",
            "name_full": "Explore-Then-Commit (ETC)",
            "brief_description": "A simple two-phase baseline: uniformly sample the space for m iterations to explore, then switch to greedy one-step exploitation for the remaining budget; used to illustrate the value of structured exploration schedules.",
            "citation_title": "Active Covering",
            "mention_or_use": "use",
            "system_name": "Explore-Then-Commit (ETC)",
            "system_description": "ETC uniformly samples (randomly) for m initial iterations to gather global information, then commits to one-step greedy selection for the remaining budget. Parameter m is the number of exploration rounds and must be chosen a priori.",
            "application_domain": "Active search tasks used as baseline analyses (product recommendation, disease hotpots, materials, drug discovery).",
            "resource_allocation_strategy": "Allocate first m queries uniformly at random to broadly probe the space, then greedily exploit (select highest posterior-probability points) for the rest of the budget.",
            "computational_cost_metric": "Very low per-iteration computational cost (random sampling and greedy ranking); wall-clock time minimal compared to ENS.",
            "information_gain_metric": "No explicit information-theoretic objective; initial uniform sampling is purely exploratory to increase coverage.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Explicit schedule: fixed exploration phase of m iterations followed by exploitation; does not adaptively change based on observed labels.",
            "diversity_mechanism": "Initial uniform exploration promotes coverage/diversity early, but no ongoing diversity-aware selection beyond the initial phase.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed budget T; user-specified m determines how much of budget is reserved for exploration.",
            "budget_constraint_handling": "Allocates a pre-specified fraction of the budget to exploration (m), then uses remaining budget for exploitation; performance sensitive to choice of m.",
            "breakthrough_discovery_metric": "Targets found; initial exploration can increase chance of locating distinct clusters (breakthroughs) but m must be well-tuned.",
            "performance_metrics": "Number of targets found under different m settings; authors found no single m performs best across tasks, highlighting difficulty in tuning.",
            "comparison_baseline": "Compared against one-step, UCB, ENS, ANS, IDS.",
            "performance_vs_baseline": "ETC can outperform naive greedy if m is well-chosen, but generally underperforms ENS and ANS which adaptively balance exploration and exploitation.",
            "efficiency_gain": "Computationally cheap but suboptimal in allocation when m poorly chosen.",
            "tradeoff_analysis": "Paper highlights sensitivity of ETC to m; fixed schedules cannot adapt to varying structure across problems and budgets, unlike ENS/ANS.",
            "optimal_allocation_findings": "Fixed-schedule exploration is simple but brittle; adaptive, budget-aware methods (ENS/ANS) are preferred for robust allocation across varied problems.",
            "uuid": "e2496.4",
            "source_info": {
                "paper_title": "Amortized nonmyopic active search via deep imitation learning",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "DAGGER",
            "name_full": "Dataset Aggregation (DAGGER) imitation learning",
            "brief_description": "An iterative imitation learning algorithm that aggregates states visited by the current policy and labels them with an expert policy to reduce distributional shift and improve learned policy robustness; used to train ANS by repeatedly querying ENS on states the network visits.",
            "citation_title": "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning",
            "mention_or_use": "use",
            "system_name": "DAGGER (Dataset Aggregation)",
            "system_description": "DAGGER iteratively (1) rolls out the current policy on training problem instances to collect states encountered, (2) queries the expert policy for the optimal action at those states, (3) aggregates the newly labeled (state,expert-action) pairs into the training set, and (4) retrains the policy. This addresses covariate shift between states seen during expert rollouts and states visited by the learned policy.",
            "application_domain": "Imitation learning for active search policy training (used to train ANS to mimic ENS); broadly applicable to imitation learning in RL and sequential decision problems.",
            "resource_allocation_strategy": "Not an allocation policy itself; DAGGER is a training procedure enabling a learned policy to mimic a resource-allocation expert (ENS) so that the learned policy can make fast allocation decisions at runtime.",
            "computational_cost_metric": "Training cost measured in number of expert queries (calls to ENS) and training iterations; DAGGER reduces long-term deployment costs by investing in expert queries during offline training.",
            "information_gain_metric": "Not an information-acquisition policy; does not compute information gain for decisions—its objective is minimizing supervised loss (cross-entropy) on expert actions aggregated over visited states.",
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Not directly applicable; DAGGER's role is to gather training states that reflect the learned policy's distribution, which can include exploratory states if the policy explores during rollouts.",
            "diversity_mechanism": "By rolling out the current policy and aggregating visited states, DAGGER encourages diversity of training states that reflect the learned policy's behavior; no explicit diversity objective beyond that.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Indirect: DAGGER training requires many simulated episodes but these are run on synthetic cheap problems to avoid consuming real labeling budget.",
            "budget_constraint_handling": "The paper reduces real labeling cost by generating synthetic GP-based problems where ENS is cheap to query, using DAGGER to amortize expert knowledge into a learned policy that eliminates the need to run ENS at deployment.",
            "breakthrough_discovery_metric": "Not applicable directly; DAGGER's success measured by how well the learned policy mimics expert decisions and by downstream discovery counts when the learned policy is deployed.",
            "performance_metrics": "Measured via final policy's performance (targets found) and stability across seeds; ablations show DAGGER outperforms behavior cloning without aggregation and learning-from-scratch.",
            "comparison_baseline": "Compared DAGGER-trained ANS vs imitation learning without DAGGER vs REINFORCE training from scratch; DAGGER yields better downstream search performance.",
            "performance_vs_baseline": "DAGGER-trained policies significantly outperform networks trained by cloning only on expert rollouts (no aggregation) and policies trained from scratch with REINFORCE in experiments.",
            "efficiency_gain": "Offline investment in ENS expert queries during DAGGER training produces a learned policy that is much cheaper at deployment (orders-of-magnitude lower per-decision computation) while retaining most expert performance.",
            "tradeoff_analysis": "Paper shows that iterative aggregation is crucial to avoid covariate shift and achieve robust imitation of ENS; training on synthetic problems avoids consuming expensive real labels but still yields policies that generalize to real tasks.",
            "optimal_allocation_findings": "Practical recommendation: use DAGGER with diverse synthetic problem generation to amortize expensive budget-aware expert policies into fast learned policies suitable for large-scale and real-time allocation.",
            "uuid": "e2496.5",
            "source_info": {
                "paper_title": "Amortized nonmyopic active search via deep imitation learning",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Deep Adaptive Design",
            "name_full": "Deep Adaptive Design: Amortizing Sequential Bayesian Experimental Design",
            "brief_description": "A prior work that trains a design network to amortize the computation of maximizing expected information gain in sequential Bayesian experimental design, serving as inspiration for amortization of expensive experimental-design objectives.",
            "citation_title": "Deep Adaptive Design: Amortizing Sequential Bayesian Experimental Design",
            "mention_or_use": "mention",
            "system_name": "Deep Adaptive Design (amortized BED)",
            "system_description": "A neural-network based approach that trains a design network to predict or produce experiment designs that approximately maximize expected information gain for Bayesian experimental design problems, thereby amortizing expensive per-experiment information-gain optimization.",
            "application_domain": "Bayesian experimental design (BED) — generic scientific experimental design tasks where information gain is the objective.",
            "resource_allocation_strategy": "Learn a mapping from current posterior (or context) to next experiment design that approximates the argmax of expected information gain, thus enabling fast allocation decisions at test time.",
            "computational_cost_metric": "Amortized per-decision computation (forward pass) vs expensive repeated optimization for information-theoretic objectives; training cost is upfront but evaluation cost is low.",
            "information_gain_metric": "Explicitly maximizes expected information gain (mutual information / reduction in posterior uncertainty) in the original formulation.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Directly optimizes information gain which trades off learning vs immediate performance implicitly; not framed in exploitation terms but maximizes expected information about model parameters or targets.",
            "diversity_mechanism": "Information-maximization tends to promote diversity in selected experiments since high-information designs often probe different regions of model uncertainty, but no explicit diversity penalty beyond information metric.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Sequential budgeted experimental design (fixed number or horizon); amortization permits rapid sequential decisions under budget constraints.",
            "budget_constraint_handling": "Amortization itself reduces per-query optimization cost; original BED objective includes horizon and can be applied sequentially until budget exhausted.",
            "breakthrough_discovery_metric": "Typically assessed via reduction in posterior uncertainty or downstream decision quality; in practice may translate to improved ability to identify high-performing designs or targets.",
            "performance_metrics": "Reported improvements in speed (amortized evaluation) with competitive or improved design quality compared to online optimization of information metrics; used as inspiration in this paper.",
            "comparison_baseline": "Compared against online optimization of information-theoretic objectives and other heuristics in cited work.",
            "performance_vs_baseline": "Amortization yields substantial runtime savings and competitive design quality in the cited work; the current paper cites it as an inspiration for amortizing ENS.",
            "efficiency_gain": "Substantial per-decision speedups due to replacing online optimization with network forward passes; exact numbers depend on problem and implementation in cited paper.",
            "tradeoff_analysis": "Tradeoff is between upfront training cost (and need for representative training problems) and per-decision speed at deployment; amortized networks can generalize but require careful training/problem generation.",
            "optimal_allocation_findings": "Amortization is a viable strategy to bring information-theoretic experimental design to large-scale or real-time settings, provided representative training distributions and architectures are used.",
            "uuid": "e2496.6",
            "source_info": {
                "paper_title": "Amortized nonmyopic active search via deep imitation learning",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "FAISS",
            "name_full": "FAISS (Approximate Nearest Neighbor Search)",
            "brief_description": "A similarity-search library used to accelerate nearest-neighbor computations in very large candidate pools, enabling efficient construction of neighbor-based features needed by ANS and ENS approximations.",
            "citation_title": "Billion-Scale Similarity Search with GPUs",
            "mention_or_use": "use",
            "system_name": "FAISS approximate nearest-neighbor search",
            "system_description": "FAISS is a high-performance library (CPU/GPU) for approximate nearest neighbor (ANN) search on large-scale embeddings; the authors use FAISS to precompute neighbor indices and similarities so that per-iteration feature construction (neighbor sums) scales to millions of points.",
            "application_domain": "Engineering/performance component to enable active search at billion-scale candidate databases (drug-molecule databases, large recommender item pools).",
            "resource_allocation_strategy": "Not an allocation policy; FAISS reduces computational cost of computing neighbor-based features used by allocation policies (ANS/ENS approximations), enabling practical per-iteration runtimes.",
            "computational_cost_metric": "Wall-clock time for nearest-neighbor precomputation and per-iteration feature construction; reported: neighbor search for 6.7M points completed in roughly one hour (precomputation). After precomputation per-iteration feature construction is O(n).",
            "information_gain_metric": null,
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": null,
            "diversity_mechanism": null,
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Computational/time budget for per-iteration feature computations; FAISS trades approximation accuracy for speed.",
            "budget_constraint_handling": "Uses ANN approximations and clustering to reduce query time; authors set number of clusters following practical heuristics (⌊4√n⌋).",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "Measured as time to compute neighbors (approx. one hour for 6.7M points) and enabling overall per-iteration runtimes reported for ANS.",
            "comparison_baseline": "Exact nearest-neighbor search (not feasible at scale); FAISS provides tractable approximate alternative.",
            "performance_vs_baseline": "FAISS enables tractable preprocessing; without ANN methods neighbor computations would be a bottleneck rendering large-scale deployment infeasible.",
            "efficiency_gain": "Makes neighbor-feature construction feasible at multi-million scale by reducing neighbor search time from infeasible to ~1 hour precompute plus O(n) per-iteration feature updates.",
            "tradeoff_analysis": "Approximation in neighbor search introduces potential small errors in features, but the learned policy remains robust and yields strong downstream performance; precomputation amortizes cost across many search episodes.",
            "optimal_allocation_findings": "Engineering-level finding: approximate neighbor search libraries are critical enabling components when allocation strategies require neighborhood-based features at massive scales.",
            "uuid": "e2496.7",
            "source_info": {
                "paper_title": "Amortized nonmyopic active search via deep imitation learning",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Efficient Nonmyopic Active Search",
            "rating": 2,
            "sanitized_title": "efficient_nonmyopic_active_search"
        },
        {
            "paper_title": "Deep Adaptive Design: Amortizing Sequential Bayesian Experimental Design",
            "rating": 2,
            "sanitized_title": "deep_adaptive_design_amortizing_sequential_bayesian_experimental_design"
        },
        {
            "paper_title": "Learning to Optimize via Information-Directed Sampling",
            "rating": 2,
            "sanitized_title": "learning_to_optimize_via_informationdirected_sampling"
        },
        {
            "paper_title": "Adaptive Sampling for Discovery",
            "rating": 2,
            "sanitized_title": "adaptive_sampling_for_discovery"
        },
        {
            "paper_title": "Bayesian Optimal Active Search and Surveying",
            "rating": 2,
            "sanitized_title": "bayesian_optimal_active_search_and_surveying"
        },
        {
            "paper_title": "Learning Active Learning from Data",
            "rating": 1,
            "sanitized_title": "learning_active_learning_from_data"
        },
        {
            "paper_title": "Quality-Weighted Vendi Scores And Their Application To Diverse Experimental Design",
            "rating": 1,
            "sanitized_title": "qualityweighted_vendi_scores_and_their_application_to_diverse_experimental_design"
        },
        {
            "paper_title": "Cost effective active search",
            "rating": 1,
            "sanitized_title": "cost_effective_active_search"
        }
    ],
    "cost": 0.02207025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Amortized nonmyopic active search via deep imitation learning
23 May 2024</p>
<p>Quan Nguyen 
Washington University
St. Louis</p>
<p>Anindya Sarkar anindya@wustl.edu 
Washington University
St. Louis</p>
<p>Roman Garnett garnett@wustl.edu 
Washington University
St. Louis</p>
<p>Amortized nonmyopic active search via deep imitation learning
23 May 20248F1569B4DCE81F6982D682E24416552DarXiv:2405.15031v1[cs.LG]
Active search formalizes a specialized active learning setting where the goal is to collect members of a rare, valuable class.The state-of-the-art algorithm approximates the optimal Bayesian policy in a budget-aware manner, and has been shown to achieve impressive empirical performance in previous work.However, even this approximate policy has a superlinear computational complexity with respect to the size of the search problem, rendering its application impractical in large spaces or in real-time systems where decisions must be made quickly.We study the amortization of this policy by training a neural network to learn to search.To circumvent the difficulty of learning from scratch, we appeal to imitation learning techniques to mimic the behavior of the expert, expensive-to-compute policy.Our policy network, trained on synthetic data, learns a beneficial search strategy that yields nonmyopic decisions carefully balancing exploration and exploitation.Extensive experiments demonstrate our policy achieves competitive performance at real-world tasks that closely approximates the expert's at a fraction of the cost, while outperforming cheaper baselines.Preprint.Under review.</p>
<p>Introduction</p>
<p>Many problems in science and engineering share a common theme where an agent searches for rare and valuable items in a massive database; examples include fraud detection, product recommendation, and drug and materials discovery.The bottleneck of this procedure is often the cost of labeling, that is, determining whether a candidate is one of the targets of the search.For instance, in product recommendation, labeling can involve presenting a customer with a product they might enjoy, at the risk of interrupting the customer's shopping experience and consequently losing sales; in drug discovery, it takes time-consuming computer simulations and/or expensive laboratory experiments to characterize a potential drug.This labeling cost rules out exhaustive screening and motivates strategic exploration of the search space.Active search (AS) frames this problem in the language of adaptive experimental design, and aims to develop policies that iteratively choose data points to label to uncover as many valuable points as possible under a labeling budget.</p>
<p>AS has been thoroughly studied in previous work and sophisticated search policies have been developed [13,[18][19][20].Of particular interest is the work of Jiang et al. [18], who derived the Bayesian optimal policy under the simplifying assumption that future experiments are chosen simultaneously in a batch.Here, the number of future experiments is set to be the remaining labeling budget so that this remaining budget is actively accounted for during policy computation.The authors called the resulting policy efficient nonmyopic search (ENS), which can be viewed as a budget-aware approximation to the true optimal policy.They showed this budget-awareness induces nonmyopic decisions that automatically balance between strategic exploration of the space and timely exploitation of regions that likely yield many targets, and ultimately achieve state-of-the-art (SOTA) search performance across many tasks.Although the aforementioned simplifying assumption, combined with aggressive pruning, allows ENS to be feasibly applied to problems of considerable size (100 000+ in Jiang et al. [18,19,20], for example), the policy retains a superlinear computational complexity.This complexity poses a challenge in (1) deploying in real-time applications where decisions need to be made quickly and (2) scaling to large spaces.For instance, a guided data discovery task [37] in visual analytics combines a search algorithm with an interactive visualization to assist a user with their analytic goals in real time, and the time available for the search algorithm to run is thus severely constrained.Modern recommender systems such as YouTube and Amazon must quickly search over millions of items to make recommendations for a large number of users [10]; similarly, there exist databases with billions of synthesizable molecules acting as search spaces for drug discovery [51].</p>
<p>We aim to alleviate the computational cost of budget-aware search by training a small, relatively shallow feedforward neural network to mimic the behavior of the SOTA, expert policy ENS; policy computation is thus amortized as we deploy the trained network as the search policy.We train this policy using the imitation learning technique DAGGER [43], which aids the goal of behavior cloning by iteratively querying the expert's actions on states encountered by the network being trained.This procedure is done with small, synthetic search problems where ENS is cheap to query.We find that the trained policy network successfully learns a beneficial nonmyopic search behavior and, despite the synthetic training data, incurs only a minor decrease in search performance at real-world tasks, in exchange for much faster decision-making.We showcase the usefulness of this computationally lightweight policy with a wide range of search problems spanning diverse applications, including drug discovery tasks of an unprecedented multi-million scale.</p>
<p>Preliminaries</p>
<p>We first introduce the problem setting, the active search framework, the SOTA search policy and its computational complexity to motivate our goal of amortization.</p>
<p>Active search and the optimal policy</p>
<p>An active search (AS) problem is defined by a finite set of data points X ≜ {x i }, among which there exists a rare, valuable subset T ⊂ X .We use the term "targets" to refer to the members of this valuable subset, which we wish to collect from the entire space X .We further use membership in T as the labels for the points in X : y i ≜ I [x i ∈ T ], ∀x i ∈ X .The targets are not known a priori, but whether a specific data point x i is one can be determined by querying an oracle returning the requested label y i .This oracle models the process of performing laborious experiments to characterize the data point in question (e.g., suggesting a given product to a customer and observing their subsequent clicking behavior, or performing computer simulations and laboratory experiments on a candidate molecule for drug discovery).We thus assume the oracle to be expensive to query.Concretely, we assume we may only query the oracle T times, where T ≪ n ≜ |X |; T can be viewed as our querying budget.We denote the set of data points and their revealed labels as D = {(x i , y i )}, and the set of those queried up to iteration t ≤ T as D t = {(x i , y i )} t i=1 .The goal of AS is to design a policy that sequentially selects which data points to query so as to find as many targets throughout the T iterations of the search as possible.To express this preference for maximizing the number of "hits" across different terminal data sets collected at the end of the search D T , we use the utility function u(D T ) = yi∈D T y i , which simply counts the number of targets in D T .</p>
<p>Previous works on AS have derived the optimal policy under Bayesian decision theory that finds the query with the highest expected terminal utility [13,18].First, we build a probabilistic classifier that outputs the probability an unlabeled point x has a positive label given the observed data set D, denoted as Pr(y = 1 | x, D).Then, at iteration t + 1, having collected D t , we query the data point:
x * t+1 = arg max xi+1∈X \Dt E u(D T ) | x i+1 , D t ,(1)
where the expectation is with respect to the label y i+1 , and the future queries (x t+2 , x t+3 , . . ., x T ) are similarly computed in this optimal manner.While this computation can theoretically be achieved via dynamic programming [4], it has a complexity of O (2 n) ℓ , where n is again the size of the search space and ℓ = T − t is the length of the decision-making horizon (i.e., the remaining querying budget).This exponential blowup stems from the fact that Bayesian decision theory compels the optimal policy to reason about not only the possible labels of a putative query, but also how each possible label affects subsequent future queries.This computation therefore cannot be realized in almost all practical scenarios except for the very last few iterations when ℓ is sufficiently small [13].</p>
<p>A common strategy that we can adopt here is to limit the depth of the lookahead in this computation, effectively pretending ℓ is indeed small.The simplest version of this is obtained by setting ℓ = 1: we assume we have only one query remaining, and the optimal decision becomes maximizing the expected marginal utility gain, which is equivalent to greedily querying the most likely target:
x * T = arg max x T ∈X \D T −1 E u D T −1 ∪ {x T , y T } | x T , D T −1 = arg max x T ∈X \D T −1 Pr(y T = 1 | x T , D T −1 ).(2)
We call this greedy policy the one-step policy, as it computes the one-step optimal decision.Albeit extremely computationally efficient, one-step does not consider exploratory queries that may lead to future gains.As a result, it tends to get stuck in small regions of targets, failing to discover larger target sets due to insufficient exploration.Theoretically, Garnett et al. [13] even showed that by limiting its lookahead, a myopic policy could perform worse than a less myopic one by any arbitrary degree.Motivated by the potential of nonmyopia in AS, Jiang et al. [18] developed an approximation to the optimal policy that accounts for the remaining budget ℓ, which we examine next.</p>
<p>Nonmyopic search via budget-awareness</p>
<p>To avoid the high cost of reasoning about the dependence among labels of future queries, Jiang et al. [18] made the simplifying assumption that, after our next query, all remaining future queries are made at the same time in a batch.Under this assumption, the future queries in the lookahead in Eq. ( 1)-to be optimally chosen to maximize expected terminal utility-can be quickly identified as the set of (ℓ−1) most likely targets [18].Their policy ENS thus estimates the value of each putative query x i with the expected utility of the union of x i and the top (ℓ − 1) unlabeled points that are adaptively selected based on each possible label y i .Again, as the number of future queries in this policy computation is set to exactly match the true length of the decision-making horizon, ENS actively accounts for the remaining labeling budget when making its queries.The authors demonstrated the benefits of this budget-awareness by showing that ENS exhibits nonmyopic, exploratory behavior when the budget is large, and automatically transitions to more exploitative queries as search progresses.This strategic exploration ultimately allows ENS to outperform many search baselines including the one-step policy.</p>
<p>While the aforementioned batch assumption avoids an exponential blowup in computational complexity, ENS still incurs a considerable cost, especially under large values of n, the size of the search space.A naïve implementation with a generic classifier has a complexity of O n 2 log n .The official implementation by Jiang et al. [18], on the other hand, uses a lightweight k-nearest neighbor (NN) classifier that (reasonably) assumes a certain level of locality when probabilities Pr(y | x, D) are updated in light of new data.This structure allows for a faster computation of the batch of future queries in ENS's lookahead, and brings the complexity down to O n (log n + m log m + T ) , where m is the largest degree of any node within the nearest neighbor graph corresponding to the k-NN [18].Unfortunately, this reduced complexity still poses a substantial challenge in two scenarios commonly encountered in AS: large search spaces (e.g., drug discovery) and settings where queries must be rapidly computed (recommender and other real-time systems).We address this problem by training an estimator, specifically a neural network, to learn the mapping from possible candidate queries to the output of ENS, thus amortizing policy computation; the next section details our approach.</p>
<p>3 Amortizing budget-aware active search</p>
<p>Our goal is to amortize search with a neural network, replacing the time-consuming policy computation of ENS with fast forward passes through the network.Crucially, this network should learn a beneficial strategy that outperforms the greedy one-step policy, so that the cost of training and deploying the network outweighs one-step's speed and ease of use.We now discuss our approach using reinforcement learning, specifically imitation learning, to effectively train one such network.</p>
<p>Learning to search with imitation learning</p>
<p>We start with the goal of training a neural network to learn to search using reinforcement learning (RL), as the utility function in Sect.2.1 can be naturally treated as a reward function, and each search run of T iterations as belonging to a budget-constrained episodic Markov decision process.Given a search space defined by X , the current state at iteration t is given by D t , the data that we have collected thus far, while the unlabeled data points X \ D t make up the possible actions that can be taken.Unlike many RL settings, though, the size of the action space in a typical AS problem makes it challenging for common RL training algorithms. 1 This is because many of these algorithms rely on thoroughly exploring the action space to learn about the value of each specific action in a given state, and as n = |X | grows larger, this task becomes increasingly more daunting.</p>
<p>Noting that we have access to ENS, an expert policy with demonstrated superior performance throughout previous works, we forgo learning to search from scratch and seek to instead rely on ENS for guidance.This proves to be more feasible, as we can leverage imitation learning techniques in which we collect a data set of state and expert's action pairs S = {s, ENS(s)} and train a neural network to learn this mapping.Here, we wish our neural network to output the same decision generated by ENS (i.e., which unlabeled point to query) given the current state (the observed data) of a search problem.This is done by treating the goal of imitating the expert policy as a classification problem, where a data point is characterized by a given state s of a search, and the corresponding label is the expert's decision ENS(s).A neural network classifier is then trained to correctly classify ENS(s) as the desirable label among all possible actions, by minimizing the corresponding cross-entropy loss.The training data S for imitation learning can be assembled in several ways.For example, we could run ENS on training problems and record the states encountered and the decisions computed.However, this leaves the possibility that as the trained network is deployed, it will arrive at a state very different from those seen during training, and thus output unreliable decisions.We use DAGGER [43], a well-established imitation learning technique, to address this problem.DAGGER is a meta-learning algorithm that iteratively rolls out the policy currently being trained (i.e., it uses the current policy to make decisions), collects the expert's actions on the encountered states, and appends this newly collected guidance to the training set to improve the policy being trained.This iterative procedure allows the expert policy to be queried more strategically, targeting states the current policy network is likely to be in.Alg. 1 summarizes this procedure.</p>
<p>Constructing search problems for training</p>
<p>To realize DAGGER, we require access to a search problem "generator" that provides AS problems in which we are free to roll out the network being trained and observe its performance.One may consider directly using one's own real-world use case to train the policy network; however, running DAGGER on real-life AS problems might prove infeasible.This is because DAGGER is an iterative training loop that requires many training episodes to be played so that the collected training data S could cover a wide range of behaviors of the expert to be imitated.We cannot afford to dedicate many real search campaigns to this task, especially under our assumption of expensive labels.Instead, we turn to synthetic problems generated in a way that is sufficiently diverse to present a wide range of scenarios under which we may observe ENS's behavior.In addition to constructing these problems and running a policy currently being trained on them at little computational cost, we can limit the size of the problems so that ENS can be queried efficiently.</p>
<p>When called, our data-generating process constructs a randomly generated set X .We sample from a Gaussian process (GP) [41] at the locations in X to obtain a real-valued label for each x ∈ X , which is then converted to a binary label by thresholding at a chosen quantile.The generated search space and labels are returned as a training problem.Although this procedure is quite simple and, in using a GP, assumes a certain level of smoothness in the labels, we observe that the generated problems offer reasonable variety of structures with "clumps" of targets of variable number and size.This variety successfully facilitates imitation learning, as later demonstrated by the empirical performance of our trained policy on real-world problems.We include more details in Appx. A.</p>
<p>Feature engineering &amp; implementation</p>
<p>The effectiveness of any training procedure in RL crucially depends on the quality of the representation of a given state during a search.To characterize a state in a way that aids learning, we use the following features to represent each unlabeled data point x ∈ X \ D remaining in a search:</p>
<p>• the posterior probability that the data point has a positive label
Pr(y = 1 | x, D), • the remaining budget ℓ = T − t,
• the sum of posterior probabilities of the (ℓ − 1) unlabeled nearest neighbors of x:
x ′ ∈NN(x, ℓ−1) Pr(y ′ = 1 | x ′ , D),(3)
where NN(x, k) denotes the set of k unlabeled nearest neighbors of a given x ∈ X , and • the sum of similarities between x and its (ℓ − 1) unlabeled nearest neighbors:
x ′ ∈NN(x, ℓ−1) s(x, x ′ ),(4)
where s(x, x ′ ) ∈ [0, 1] denotes the similarity between two given points x, x ′ ∈ X .</p>
<p>Which similarity function s to use to compute the nearest neighbors of each point and the corresponding similarity values depends on the application.We use the radial basis function kernel
s(x, x ′ ) = exp − ∥x−x ′ ∥ 2 2 λ 2
during training and upon deployment for appropriate tasks in Sect.5, but this can be replaced with another function more applicable to a given domain.</p>
<p>We specifically design the third and fourth features to relate the value of an unlabeled point we may query to the characteristics of its nearest neighbors.Intuitively, a point whose neighbors are likely targets is a promising candidate, as it indicates a region that could yield many hits.On the other hand, points that are close to its neighbors (e.g., cluster centers) could also prove beneficial to query, as they help the policy explore the space effectively.Further, the number of nearest neighbors to include in these computations is set to match the length of the horizon, allowing these features to dynamically adjust to our remaining budget.Overall, the four features make up the feature vector of each candidate point, and concatenating all feature vectors gives the state representation of a given search iteration.We also note that our state representation is task-agnostic and applicable across AS problems of varying structures and sizes, which is crucial for training our policy on the different problems we generate, as well as for when we deploy our trained policy on unseen problems.</p>
<p>Here, finding the nearest neighbors of each point may prove challenging under large spaces.We leverage the state-of-the-art similarity search library FAISS [21] to perform efficient approximate nearest neighbor search when exact search is prohibitive. 2FAISS allows us to significantly accelerate this step, completing, for example, the neighbor search for our largest problem in Sect. 5 of 6.7 million points in roughly one hour.Once this neighbor search is done before the actual search campaign, the time complexity of constructing the features above at each search iteration is O(n).</p>
<p>We run Alg. 1 for N = 50 iterations, each consisting of 3 training problems.At the end of each iteration, we train a small policy network with 5 fully connected hidden layers (with 8, 16, 32, 16, and 8 neurons, respectively) and ReLU activation functions using minibatch gradient descent with Adam optimizer [23].The trained policy is then evaluated on a fixed set of 3 unseen validation problems.We set the labeling budget T = 100 across all generated problems.</p>
<p>Demonstration of learned search strategy</p>
<p>Before discussing our experiment results, we briefly demonstrate the learned behavior of our trained policy network using an illustrative toy example visualized in Fig. 1.The search space X consists This problem structure presents an interesting choice between exploiting the small cluster of very likely targets and exploring larger clusters that contain less likely targets.A good policy should select exploitation if the remaining budget is small and choose to further explore otherwise.The remaining panels in Fig. 1, which visualize the logits computed by our trained policy under different remaining budgets ℓ ∈ {10, 33, 100}, show that this is exactly the case: the policy targets the cluster of likely targets when the budget is small, and moves to larger clusters as the budget increases.Further, when exploring, the policy appropriately favors cluster centers, which offer more information about the space.This balance between exploitation and strategic exploration our trained policy exhibits indicates that the policy has learned a meaningful search behavior from ENS, which translates into good empirical performance, as later shown in Sect. 5.</p>
<p>Related work</p>
<p>Active search.We continue the line of research on active search (AS) [13,9], which previous works have also referred to as active learning and adaptive sampling for discovery [53,54,57] or active covering [17].Garnett et al. [13] studied the Bayesian optimal policy, and Jiang et al. [18] proposed ENS as a budget-aware approximation demonstrating impressive empirical success.ENS has since then been adopted under various settings, including batch [19], cost-aware [53,54,20], multifidelity [40], and diversity-aware AS [39].We propose to amortize policy computation of ENS using imitation learning, scaling nonmyopic search to large data sets.</p>
<p>Amortization via neural networks.Using neural networks to amortize expensive computations has seen increasing interests from the machine learning community.Of note is the work of Foster et al. [12], who tackled amortizing maximizing expected information gain [32,33] for Bayesian experimental design (BED) [27] with a design network trained on a specialized loss function, and was a major inspiration for our work.Subsequent works [6,48] have studied BED under other settings such as those with discrete action spaces.Liu et al. [30] tackled amortizing Gaussian process (GP) inference by training a transformer-based network as a regression model to predict optimal hyperparameters of GPs with stationary kernels; Bitzer et al. [5] later extended the approach to more general kernel structures.Andrychowicz et al. [2], on the other hand, learned an optimization policy with a recurrent neural network that predicts the next update to the parameters to be optimized based on query history; the policy network was shown to outperform many generic gradient-based optimizers.Also related is the work of Konyushkova et al. [26], where a regressor was trained to predict the value of querying a given unlabeled data point for active learning.</p>
<p>Reinforcement learning.Reinforcement learning (RL) has proven a useful tool for learning effective strategies for planning tasks similar to AS. Examples include active learning policies for named entity recognition [11,28], neural machine translation [29], and active learning on graphs [15].Igoe Table 1: Average number of targets found and standard errors by each search policy across 10 repeats of all instances of a given task.Settings that are computationally prohibitive for a given policy are left blank.The best policy in each setting is highlighted bold; policies not significantly worse than the best (according to a two-sided paired t-test with a significance level of α = 0.05) are in blue italics.et al. [16] were interested in path planning for drones, framed as a specialized AS setting with linear models and many agents.Sarkar et al. [46] and Sarkar et al. [47] studied the problem of visual AS, a realization of AS on images for geospatial exploration.Overall, the methodologies in these works rely on being able to generate many training episodes to learn an effective RL policy from scratch, which cannot be realized in our setting.Having access to ENS, we instead leverage imitation learning to learn to search from this expert on synthetically generated search problems.When deployed, our trained policy can be applied to a diverse set of use cases, as demonstrated in the next section.</p>
<p>Experiments</p>
<p>We tested our policy network, which we call amortized nonmyopic search, or ANS, and a number of baselines on search problems spanning a wide range of applications.For each problem included, we run each policy 10 times from the same set of initial data D 0 that contains one target and one non-target, both randomly sampled.Each of these runs has a labeling budget of T = 100.</p>
<p>Baselines.We compare our method against the expert policy ENS which ours was trained to mimic, as well as the one-step policy that greedily queries the most likely target discussed in Sect.2.1.We also implement a number of baseline policies from the literature.The first is a family of upper confidence bound (UCB) policies [3,8] that rank candidates by the following score: p + β p(1 − p), where p = Pr(y = 1 | x, D) is the posterior probability that a given candidate is a target, and β is the tradeoff parameter balancing exploitation (favoring large p) and exploration (favoring large uncertainty in the label, as measured by p(1 − p)).Here, we have β take on values from {0.1, 0.3, 1}.Another baseline is from Jiang and Rostamizadeh [17], who proposed a simple explore-then-commit (ETC) scheme that uniformly samples the space for m iterations and then switches to the greedy sampling strategy of one-step for the remaining of the search.We run this ETC policy with m ∈ {10, 20, 30}.Finally, we include the policy by Xu et al. [57], which uses the information-directed sampling (IDS) heuristic [44,45] that scores each candidate query x by the ratio between (1) information about the labels of the current ℓ most likely targets (ℓ = T − t is the length of the remaining horizon), gained by querying x and (2) the expected instant regret from querying x.We note that Xu et al. [57] proposed this policy under specialized AS settings that allow information gain to be efficiently computed; they also only considered problems with fewer than 1000 points.For our experiments, we can only apply IDS to relatively small spaces where the policy is computationally feasible.Search problems.We now discuss the search problems making up our experiments.The first was posed by Andrade-Pacheco et al. [1], who sought to identify disease hotspots within a region of interest.The provided data sets correspond to 4 distinct AS problems of finding locations with a high prevalence of schistosomiasis in Côte d'Ivoire and Malawi and of lymphatic filariasis in Haiti and the Philippines.Each problem consists of 1500 points, with targets accounting for 10%-34% of the space.For our second task, following Nguyen and Garnett [39], we simulate product recommendation problems using the Fashion-MNIST data [56], which contains 70 000 images classified into 10 classes of clothing articles.We first randomly select 3 out of the 10 classes as products a user is interested in (i.e., our search targets).We further sub-sample these 3 classes uniformly at random to increase the difficulty of the problem; the resulting prevalence rate of the targets is roughly 6%.We repeat this process 10 times to generate 10 search problems with this data.</p>
<p>Borrowing from previous works [18,19], we use a data set from the materials science literature [22,52] containing 106 810 alloys, of which 4275 can form bulk metallic glasses with high toughness and wear resistance and are our search targets.Another application comes from drug discovery, where we aim to identify "active compounds", chemical compounds that bind with a targeted protein.Garnett et al. [14] assembled a suite of such drug discovery problems, each of which consists of the active compounds for a specific protein from the BindingDB database [31], and 100 000 molecules sampled from the ZINC database [49] that act as the negative pool.Our experiments include the first 10 problems where on average the active compounds make up 0.5% of the search space.</p>
<p>Finally, to demonstrate the ability to perform search on large spaces achieved by our method ANS, we consider two large-scale, challenging drug discovery tasks.The first employs the GuacaMol database of over 1.5 million drug-like molecules that were specifically curated for drug discovery benchmarking tasks involving machine learning [7].In addition to these molecules, the database offers a family of objective functions to measure the molecules' quality using a variety of criteria.We use each objective function provided to define a search problem as follows.We first randomly sample a set of 1000 molecules which we fully label using the objective functions.We then define the search targets as those of the remaining unlabeled molecules whose scores exceed the 99-th percentile of the labeled set; in other words, the goal of our search is the top 1% molecules.In total, we assemble 9 such AS problems with GuacaMol.For our second task, we follow the procedure by Garnett et al. [14] described above with the BindingDB and ZINC databases, this time expanding the negative pool to all drug-like molecules in ZINC [51].This results in a search space of 6.7 million candidates, of which 0.03% are the active compounds we aim to search for.</p>
<p>Discussions.</p>
<p>Tab. 1 reports the performance of the search policies -measured in the number of targets discovered -in each of these tasks, where settings that are computationally prohibitive for a given policy are left blank.From these results, we observe a clear trend: the state-of-the-art policy ENS consistently achieves the best performance under all settings that it could feasibly run, while our trained policy network ANS closely follows ENS, sometimes outperforming the other baselines by a large margin.Our method also yields the best result in large-scale problems, demonstrating its usefulness in large search spaces.Among the baselines, we note the difficulty in setting the number of exploration rounds m for ETC, since no value of m performs the best across all settings.Results from UCB policies, on the other hand, indicate that prioritizing exploitation (setting the parameter β to a small value) is beneficial, a trend also observed in previous work [18].</p>
<p>To illustrate the tradeoff between performance and speed achieved by ANS, the left panel of Fig. 2 shows the average time taken by ANS, ENS, and myopic baselines per iteration as a function of the size of the data, while the right panel shows the number of discoveries by each policy vs. the same average time per iteration.These plots do not include the results from the large-scale problems so that the comparison with ENS is fair, or IDS which is slower than ENS but does not perform as well.We see that ANS finds almost as many targets as ENS but is much more computationally lightweight.We thus have established a new point on the Pareto frontier of the performance vs. speed tradeoff with our search policy.In the 6.7 million-point drug discovery problems, ANS on average takes 36.94 ± 0.15 minutes per iteration, which we deem entirely acceptable given the boost in performance compared to faster but myopic baselines, the fact that the time cost of labeling is typically much higher, and ENS, in comparison, is estimated to take roughly 10 hours per iteration on the same scale.The average difference in cumulative reward and standard errors between our policy and one-step.Our policy spends its initial budget exploring the space and finds fewer targets in the beginning but smoothly switches to more exploitative queries and outperforms one-step at the end.</p>
<p>As a demonstration of our policy's strategic explorative behavior learned from ENS, Fig. 3 shows the result of an illustrative run by the one-step policy vs. ANS from the problem of finding schistosomiasis hotspots in Côte d'Ivoire [1].We note that the queries made by onestep are localized within the center region containing the target in the initial data D 0 , while ANS is able to discover a larger cluster of targets to the south.Further, Fig. 4 visualizes the cumulative difference in utility between ANS and one-step across all experimental settings.Here, ANS initially finds fewer targets than one-step, as the former tends to dedicate its queries to exploration of the space when the remaining budget is large; however, as the search progresses, ANS smoothly transitions to more exploitative queries and ultimately outperforms the greedy policy.The same pattern of behavior has been observed from ENS in previous works [18,40,39].</p>
<p>We defer further analyses to Appx.C, which includes an ablation study showing the importance of each feature we engineer for our policy network in Sect.3.3, as well as the benefit of the DAGGER loop compared to (1) imitation learning without iteratively generating more training data and (2) learning to search from scratch without ENS.We also examine the stability between different training runs giving different policy networks, and observe that these policies yield comparable results.Finally, we investigate the effectiveness of various schemes to further improve search performance under settings where repeated searches are conducted within the same space.</p>
<p>Conclusion</p>
<p>We propose an imitation learning-based method to scale nonmyopic active search to large search spaces, enabling real-time decision-making and efficient exploration of massive databases common in product recommendation and drug discovery beyond myopic/greedy strategies.Extensive experiments showcase the usefulness of our policy, which mimics the state-of-the-art policy ENS while being significantly cheaper to run.Future directions include deriving a more effective reinforcement learning strategy to train our policy network, potentially outperforming ENS, as well as extending to other active search settings such as batch [18] and diversity-aware search [39,38].We now discuss our procedure of generating active search problems to train the policy network in DAGGER, summarized in Alg. 2, where U {m, n} denotes a discrete uniform distribution of the integers between m and n (inclusive), while U [a, b] refers to a continuous uniform distribution between a and b.</p>
<p>A Generation of training search problems</p>
<p>The search space X of each generated problem exists in a ddimensional space, where d is a random integer between 2 and 10.Once d is determined, we sample 100d points uniformly from the d-dimensional unit hypercube.This set of uniform points is combined with n cluster clusters, where n cluster is a random integer between 10 and 10d.To generate each cluster, we first sample another random integer between 10 and 10d, denoted as m, to determine the size of the cluster.We then draw m points from an isotropic Gaussian distribution with the mean vector µ randomly sampled within the unit hypercube and the diagonal covariance matrix σ 2 I d , where σ is drawn from U [0.1, 0.1d].Here, σ, which determines the spread of a given cluster, is constrained to be between 0.1 and 0.1d (relatively small numbers) to ensure that the points within this cluster are indeed close to one another.</p>
<p>Again, the union of the uniform points and the clusters make up the entire search space X .We then draw a sample from a Gaussian process (GP) at the points in X .This GP is equipped with a zero mean function and a radial basis function kernel whose length scale ℓ scales linearly with the dimensionality of the space d by a factor of 0.05.This GP sample yields a vector f of real-valued numbers.We then sample uniformly between 0.01 and 0.2 for a prevalence rate p, which determines the proportion of X corresponds to the targets.As such, we compute the binary labels y by thresholding f at the 100(1 − p)-th quantile of the values in f .We keep the prevalence rate p below 20% to ensure that the targets are sufficiently rare.The tuple (X , y) is finally returned.Fig. 5 shows an example of one such generated problem in two dimensions, showing a search space with a considerably complex structure with multiple clusters and groups of targets.</p>
<p>B Data sets</p>
<p>We now describe the data sets used in our experiments in Sect. 5.These data sets are curated from authors of respective publications respecting their licenses, as detailed below.No identifiable information or offensive content is included in the data.</p>
<p>• We downloaded the disease hotspot data set from the GitHub repository provided in Andrade-Pacheco et al. [1].We computed the nearest neighbors of each data point (a location within one of the four countries included) using its coordinates (longitude and latitude).• The Fashion-MNIST data set is published by Xiao et al. [56].We used UMAP [35] to produce a two-dimensional embedding of the images and compute the nearest neighbors on this embedding.• We obtained the bulk metal glass data from Jiang et al. [18], who, following Ward et al. [52], represented each data point with various physical attributes that were found to be informative in predicting glass-forming ability.Each feature is subsequently scaled to range between 0 and 1.The nearest neighbor search is performed on these features.• The data for the first set of drug discovery problems were also obtained from Jiang et al. [18], where the Morgan fingerprints [42] were used as the feature vectors and the Tanimoto coefficient [55] as the measure of similarity.• The molecules in the GuacaMol data are included in the publication of Brown et al. [7],</p>
<p>while those in our large drug discovery tasks were downloaded from the ZINC-22 database [51].For each of these data sets, we used the state-of-the-art transformer-based molecular variational autoencoder trained in Maus et al. [34]  X ← X ∪ x j :
x j ∼ N (µ, σ 2 I d ) m j=1
▷ use an isotropic Gaussian distribution 9: end for 10: f ∼ N (X ; 0, Σ),</p>
<p>where Σ = K(X , X ) and
K(x 1 , x 2 ) = exp − ∥x1−x2∥ 2 2ℓ 2
with length scale ℓ = 0.05d  Ablation study.We use the 10 product recommendation tasks from the FashionMNIST data to quantify the value of various components of our framework.First, we trained four additional policy networks, each learning from ENS without one of the four features discussed in Sect.3.3.We also trained another network using imitation learning but without DAGGER's iterative procedure: we ran the expert policy ENS on 3 × 50 = 150 generated search problems (the same number of problems generated to train the policy examined in the main text), kept track of the encountered states and selected actions, and used these data to train the new network until convergence only once.Finally, we trained a policy network without imitation learning using the REINFORCE policy gradient algorithm [50].The performance of these policies, along with that of our main policy ANS as a reference, is shown in Tab. 2. We see that by removing any component of our imitation learning procedure, we incur a considerable decrease in search performance, which demonstrates the importance of each of these components.Training stability.We rerun our training procedure with DAGGER for 10 times using different random seeds and evaluate the trained policy networks using the experiments with the FashionMNIST data.Each row of Fig. 6 shows the distribution of the number of targets found by each of these 10 policy networks across the 100 search problems.We observe that the variation across these 10 distributions is quite small, especially compared to the variation across different search runs by the same policy network.This shows that our training procedure is stable, resulting in policy networks that behave similarly under different random seeds.</p>
<p>Refinement under repeated search.In many settings that AS targets, multiple search campaigns may be conducted within the same search space.For example, as in our drug discovery experiments in Sect.5, a scientist may explore a molecular database to identify candidates with different desirable properties.As the search for a given property concludes, the next search stays within the same database but now targets a different property.In these situations, we may reasonably seek to refine our search strategy throughout these episodes using the results we observe, so that our search policy could improve using its past experiences.We identify two approaches to such refinement:</p>
<p>• If a neural network is used as the search policy, it can be updated by a policy gradient algorithm such as REINFORCE [50] after each episode.• If a deep autoencoder (DAE) is used to produce a representation of the search candidates (on which the nearest neighbor search described in Sect.3.3 is conducted), the autoencoder can be updated with a semisupervised loss [25] that accounts for the labels it iteratively uncovers throughout the search.</p>
<p>To investigate the effects of each of these approaches on the search performance of our policy trained with imitation learning and examined in the main text, we engineer another version of the FashionMNIST data set [56] that simulates a setting of repeated search.We first randomly choose 5 out of 10 classes in the data set to act as possible target sets throughout the repeated searches.These selected classes are then sub-sampled uniformly at random so that there are only 1000 data points per class; this yields a data set of 40 000 points in total.We then use a variational autoencoder [24] to learn a two-dimensional representation of these 40 000 candidates.</p>
<p>We allow 100 search episodes within this database, where in each episode, 1 of the chosen 5 classes is randomly selected as the target class.To implement the second approach to search refinement, we train a variational Gaussian process classifier on the observed data D and use the corresponding evidence lower bound (ELBO) to make up the supervised component of the joint loss of the semisupervised model.While we update the search policy using the REINFORCE loss at the end of each episode, an update to the semisupervised VAE is performed for every 20 iterations within one episode.Fig. 7 shows the value of each of the two update schemes as the cumulative difference in the number of targets found between each scheme compared to performing no updates (both the search policy and the representation of the data points are kept fixed) throughout 100 search episodes across 10 repeats.Surprisingly, attempting to further refine the search policy using REINFORCE actually hurts performance, resulting in an increasing gap in reward between the initial policy and the one continually updated.On the other hand, we see that updating the initial unsupervised VAE to account for the observed labels yields an improvement in performance on average, but this improvement is not consistent across the 10 repeats.Overall, we show the difficulty in further updating our trained search policy using real experiences under repeated searches, and hypothesize that more sophisticated reinforcement learning procedures such as the double Q-learning algorithm [36] are needed to improve learning, which we leave as future work.</p>
<p>D Limitations</p>
<p>Sect. 5 demonstrates that our policy ANS, trained with imitation learning, cannot perfectly capture the search strategy of the state-of-the-art ENS by Jiang et al. [18] and is outperformed by the policy.We view improving the architecture of our policy network as well as the state representation to enable more effective learning as a promising future direction.For example, as examined in Liu et al. [30], a transformer-based network with beneficial input permutation invariance properties can learn from data sets of different sizes, which could also prove useful in AS.This network architecture can be combined with a better training strategy, as mentioned in Appx.C, to potentially yield comparable performance as ENS or even to outperform it.</p>
<p>E Broader impact</p>
<p>The development of an efficient AS algorithm through imitation learning has significant potential to positively impact various fields where computational efficiency is paramount.By reducing the superlinear computational complexity of the state-of-the-art policy ENS, our approach enables the application of AS in significantly larger data sets.This scalability is essential for industries and research fields that deal with vast amounts of data, including genomics, astronomy, and environmental monitoring, among others.The ability to achieve competitive performance at a fraction of the cost also has substantial economic implications.Organizations can deploy high-performance AS solutions without the need for extensive computational resources, making advanced data analysis more accessible and affordable.Since AS aims to identify as many targets as possible, the collected data set may end up unbalanced towards the positives.It is important for the user to ensure that maximizing the number of labeled targets accurately reflects their objective, and that the collected data are used by downstream tasks that are not negatively affected by this imbalance.</p>
<p>Algorithm 1 DAGGER for imitation learning 1 :
11
inputs number of training iterations N , expert policy π * , problem generator G 2: initialize S ← ∅ 3: initialize π0 randomly 4: for i = 1 to N do 5: sample AS problems X ∼ G 6: roll out πi−1 on X to obtain states {s} 7: assemble S i = s, π * (s) 8: aggregate S ← S ∪ S i 9: train πi on S until convergence 10: end for 11: returns best πi on validation</p>
<p>Figure 1 :
1
Figure 1: Demonstration of our trained policy's budget-awareness with a toy example.Left panel: the probability that a point is a target.Remaining panels: computed logits and the point selected to be the next query under different labeling budgets.Our policy appropriately balances between exploitation under a small labeling budget and strategic exploration if the budget is large. of uniformly sampled points as well as 3 distinct clusters of different sizes.The left panel shows Pr(y = 1 | x, D), the probability that each point is a target, specifically set so that: • for each of the 100 uniformly sampled points, Pr(y = 1 | x, D) = 0.1, • for each point in the small cluster of size 10 at the top, Pr(y = 1 | x, D) = 0.9, • for each point in the medium cluster of size 30 on the right, Pr(y = 1 | x, D) = 0.3, and • for each point in the large cluster of size 100 on the bottom left, Pr(y = 1 | x, D) = 0.1.</p>
<p>Figure 2 :
2
Figure 2: The time taken per iteration by different active search policies in the small-and mediumscale experiments.Left: average number of seconds per iteration with respect to the size of the search space.Right: average number of targets found and standard errors vs. time per iteration.</p>
<p>Figure 3 :
3
Figure 3: Locations in Côte d'Ivoire selected by the one-step policy and by ours in an illustrative run with the disease hotspot data, where our policy discovers a larger target cluster.</p>
<p>Figure 4 :
4
Figure4: The average difference in cumulative reward and standard errors between our policy and one-step.Our policy spends its initial budget exploring the space and finds fewer targets in the beginning but smoothly switches to more exploitative queries and outperforms one-step at the end.</p>
<p>Figure 5 :
5
Figure 5: An example twodimensional problem generated by Alg. 2, where bright and dark points indicate targets and non-targets, respectively.The search space includes both clusters and more widely dispersed points.</p>
<p>Figure 6 :
6
Figure 6: Distributions of the number of targets found across 100 product recommendation tasks with FashionMNIST by 10 policy networks trained with different initial random seeds.The distributions are comparable, indicating that the trained policy networks behave similarly.</p>
<p>Figure 7 :
7
Figure 7: Average cumulative difference in the number of targets found and standard errors between (left) updating the policy network using REINFORCE or (right) updating the autoencoder producing the representation of the candidates vs. performing no updates.</p>
<p>to generate a 256-dimensional embedding of the molecules.The approximate nearest neighbor search described in Sect.3.3 was conducted on this embedding.
Algorithm 2 Generate synthetic search problems1: d ∼ U {2, 10} 2: X ← x j : x j ∼ U [0, 1] 3: n cluster ∼ U {10, 10d} 4: for i = 1 to n cluster do100d j=1▷ sample dimensionality of search space ▷ sample uniform points ▷ sample number of clusters5: 6: 7: 8:m ∼ U {10, 10d} µ = [µ j ] d j=1 , where µ j ∼ U [0, 1] σ ∼ U [0.1, 0.1d]▷ sample cluster size ▷ sample cluster center ▷ sample spread of cluster</p>
<p>Table 2 :
2
Average number of targets found and standard errors by each ablated policy across 100 product recommendation tasks with FashionMNIST.The best policy is highlighted bold.Experiments were performed on a small cluster built from commodity hardware comprising approximately 200 Intel Xeon CPU cores, each with approximately 10 GB of RAM.All compute amounted to roughly 15 000 CPU hours, including preliminary experiments not discussed in the paper, training the policy network, and all experiments for evaluation discussed in Sect. 5 and here.
11: p ∼ U [0.01, 0.2]
Not to mention one of our main goals, scaling AS to large search spaces.
We set the number of clusters into which the search space is split when performing approximate neighbor search at ⌊4 √ n⌋, following https://github.com/facebookresearch/faiss/issues/112.</p>
<p>Finding hotspots: development of an adaptive spatial sampling approach. Ricardo Andrade-Pacheco, Francois Rerolle, Jean Lemoine, Leda Hernandez, Aboulaye Meïté, Lazarus Juziwelo, Aurélien F Bibaut, Mark J Van Der Laan, Benjamin F Arnold, Hugh Jw Sturrock, Scientific Reports. 102020Cited on pgs. 7, 9, and 14</p>
<p>Learning to learn by gradient descent by gradient descent. Marcin Andrychowicz, Misha Denil, Sergio Gómez, Matthew W Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, Nando De Freitas, Advances in Neural Information Processing Systems. 201629Cited on pg. 6.</p>
<p>Using Upper Confidence Bounds for Online Learning. Peter Auer, Proceedings of the 41st Annual Symposium on Foundations of Computer Science. the 41st Annual Symposium on Foundations of Computer ScienceIEEE2000</p>
<p>Dynamic Programming. Richard Bellman, 1957Princeton University Press</p>
<p>Amortized Inference for Gaussian Process Hyperparameters of Structured Kernels. Matthias Bitzer, Mona Meister, Christoph Zimmer, Uncertainty in Artificial Intelligence. 2023</p>
<p>Optimizing Sequential Experimental Design with Deep Reinforcement Learning. Tom Blau, Edwin V Bonilla, Iadine Chades, Amir Dezfouli, Proceedings of the 39th International Conference on Machine Learning. the 39th International Conference on Machine Learning2022Cited on pg. 6.</p>
<p>GuacaMol: Benchmarking Models for de Novo Molecular Design. Nathan Brown, Marco Fiscato, Marwin Hs Segler, Alain C Vaucher, Journal of Chemical Information and Modeling. 5932019</p>
<p>Upper-Confidence-Bound Algorithms for Active Learning in Multi-armed Bandits. Alexandra Carpentier, Alessandro Lazaric, Mohammad Ghavamzadeh, Rémi Munos, Peter Auer, International Conference on Algorithmic Learning Theory. 2011</p>
<p>Similarity Search for Efficient Active Learning and Search of Rare Concepts. Cody Coleman, Edward Chou, Julian Katz-Samuels, Sean Culatana, Peter Bailis, Alexander C Berg, Robert Nowak, Roshan Sumbaly, Matei Zaharia, Yalniz Zeki, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236Cited on pg. 6.</p>
<p>Gabriel Dulac-Arnold, Richard Evans, Peter Hado Van Hasselt, Timothy Sunehag, Jonathan Lillicrap, Timothy Hunt, Theophane Mann, Thomas Weber, Ben Degris, Coppin, arXiv:1512.07679[cs.AI].Deep Reinforcement Learning in Large Discrete Action Spaces. 2015arXiv preprintCited on pg. 2.</p>
<p>Learning how to Active Learn: A Deep Reinforcement Learning Approach. Meng Fang, Yuan Li, Trevor Cohn, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language Processing2017Cited on pg. 6.</p>
<p>Deep Adaptive Design: Amortizing Sequential Bayesian Experimental Design. Adam Foster, Ilyas Desi R Ivanova, Tom Malik, Rainforth, Proceedings of the 38th International Conference on Machine Learning. the 38th International Conference on Machine Learning2021Cited on pg. 6.</p>
<p>Bayesian Optimal Active Search and Surveying. Roman Garnett, Yamuna Krishnamurthy, Xuehan Xiong, Jeff Schneider, Richard Mann, Proceedings of the 29th International Conference on Machine Learning. the 29th International Conference on Machine Learning2012Cited on pgs. 1, 2, 3, and 6.</p>
<p>Introducing the 'active search' method for iterative virtual screening. Roman Garnett, Thomas Gärtner, Martin Vogt, Jürgen Bajorath, Journal of Computer-Aided Molecular Design. 292015Cited on pg</p>
<p>Graph Policy Network for Transferable Active Learning on Graphs. Shengding Hu, Zheng Xiong, Meng Qu, Xingdi Yuan, Marc-Alexandre Côté, Zhiyuan Liu, Jian Tang, Advances in Neural Information Processing Systems. 202033Cited on pg. 6.</p>
<p>Multi-Agent Active Search: A Reinforcement Learning Approach. Conor Igoe, Ramina Ghods, Jeff Schneider, IEEE Robotics and Automation Letters. 722021</p>
<p>Active Covering. Heinrich Jiang, Afshin Rostamizadeh, Proceedings of the 38th International Conference on Machine Learning. the 38th International Conference on Machine Learning20217</p>
<p>Efficient Nonmyopic Active Search. Shali Jiang, Gustavo Malkomes, Geoff Converse, Alyssa Shofner, Benjamin Moseley, Roman Garnett, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning20171714Cited on pgs. 1, 2, 3, 6, 8, 9</p>
<p>Efficient nonmyopic batch active search. Shali Jiang, Gustavo Malkomes, Matthew Abbott, Benjamin Moseley, Roman Garnett, Advances in Neural Information Processing Systems 31. 2018Cited on pgs. 2, 6, and 8.</p>
<p>Cost effective active search. Shali Jiang, Roman Garnett, Benjamin Moseley, Advances in Neural Information Processing Systems. 201932Cited on pgs. 1, 2, and 6.</p>
<p>Billion-Scale Similarity Search with GPUs. Jeff Johnson, Matthijs Douze, Hervé Jégou, IEEE Transactions on Big Data. 732019Cited on pg. 5.</p>
<p>J-Z Kawazoe, A-P Yu, Tsai, Masumoto, Nonequilibrium Phase Diagrams of Ternary Amorphous Alloys. Condensed Matters. Springer-Verlag19978</p>
<p>Adam: A Method for Stochastic Optimization. P Diederik, Jimmy Kingma, Ba, Proceddings of the 3rd International Conference for Learning Representations. eddings of the 3rd International Conference for Learning Representations2015Cited on pg. 5.</p>
<p>Auto-Encoding Variational Bayes. P Diederik, Max Kingma, Welling, Proceddings of the 2nd International Conference for Learning Representations. eddings of the 2nd International Conference for Learning Representations201416Cited on pg</p>
<p>Semisupervised Learning with Deep Generative Models. Shakir Diederik P Kingma, Danilo Mohamed, Max Jimenez Rezende, Welling, Advances in Neural Information Processing Systems. 201427Cited on pg</p>
<p>Learning Active Learning from Data. Ksenia Konyushkova, Raphael Sznitman, Pascal Fua, Advances in Neural Information Processing Systems. 201730</p>
<p>On a Measure of the Information Provided by an Experiment. V Dennis, Lindley, The Annals of Mathematical Statistics. 2741956</p>
<p>Learning How to Actively Learn: A Deep Imitation Learning Approach. Ming Liu, Wray Buntine, Gholamreza Haffari, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational Linguistics2018Cited on pg. 6.</p>
<p>Learning to Actively Learn Neural Machine Translation. Ming Liu, Wray Buntine, Gholamreza Haffari, Proceedings of the 22nd Conference on Computational Natural Language Learning. the 22nd Conference on Computational Natural Language Learning2018Cited on pg. 6.</p>
<p>Task-Agnostic Amortized Inference of Gaussian Process Hyperparameters. Sulin Liu, Xingyuan Sun, Peter J Ramadge, Ryan P Adams, Advances in Neural Information Processing Systems. 202033</p>
<p>BindingDB: A web-accessible database of experimentally determined protein-ligand binding affinities. Tiqing Liu, Yuhmei Lin, Xin Wen, Robert N Jorissen, Michael K Gilson, Nucleic Acids Research. 352007Cited on pg</p>
<p>The Evidence Framework Applied to Classification Networks. Neural Computation, 1992. David Mackay, </p>
<p>Information-Based Objective Functions for Active Data Selection. David Mackay, Neural Computation. 1992</p>
<p>Local Latent Space Bayesian Optimization over Structured Inputs. Natalie Maus, Haydn Jones, Juston Moore, Matt J Kusner, John Bradshaw, Jacob Gardner, Advances in Neural Information Processing Systems. 202235Cited on pg. 14.</p>
<p>UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction. Leland Mcinnes, John Healy, James Melville, arXiv:1802.03426stat.ML].2018arXiv preprintCited on pg. 14.</p>
<p>Human-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, Demis Hassabis, Nature. 51875402015Cited on pg</p>
<p>Guided Data Discovery in Interactive Visualizations via Active Search. Shayan Monadjemi, Sunwoo Ha, Quan Nguyen, Henry Chai, Roman Garnett, Alvitta Ottley, 2022 IEEE Visualization and Visual Analytics (VIS). IEEE2022Cited on pg. 2.</p>
<p>Quality-Weighted Vendi Scores And Their Application To Diverse Experimental Design. Quan Nguyen, Adji Bousso, Dieng , Proceedings of the 41st International Conference on Machine Learning. the 41st International Conference on Machine Learning20249To appear.. Cited on pg</p>
<p>Nonmyopic Multiclass Active Search with Diminishing Returns for Diverse Discovery. Quan Nguyen, Roman Garnett, Proceedings of the 26th International Conference on Artificial Intelligence and Statistics. the 26th International Conference on Artificial Intelligence and Statistics20239</p>
<p>Nonmyopic Multifidelity Acitve Search. Quan Nguyen, Arghavan Modiri, Roman Garnett, Proceedings of the 38th International Conference on Machine Learning. the 38th International Conference on Machine Learning20219</p>
<p>Carl Edward Rasmussen, Christopher K I Williams, Gaussian Processes for Machine Learning. The MIT Press2006</p>
<p>Extended-Connectivity Fingerprints. David Rogers, Mathew Hahn, Journal of Chemical Information and Modeling. 5052010Cited on pg. 14.</p>
<p>A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning. Stéphane Ross, Geoffrey Gordon, Drew Bagnell, Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. the 14th International Conference on Artificial Intelligence and Statistics2011</p>
<p>Learning to Optimize via Information-Directed Sampling. Daniel Russo, Benjamin Van Roy, Advances in Neural Information Processing Systems. 201427</p>
<p>Learning to Optimize via Information-Directed Sampling. Daniel Russo, Benjamin Van Roy, Operations Research. 662018Cited on pg</p>
<p>A Partially Supervised Reinforcement Learning Framework for Visual Active Search. Anindya Sarkar, Nathan Jacobs, Yevgeniy Vorobeychik, Advances in Neural Information Processing Systems. 202336</p>
<p>A Visual Active Search Framework for Geospatial Exploration. Anindya Sarkar, Michael Lanier, Scott Alfeld, Jiarui Feng, Roman Garnett, Nathan Jacobs, Yevgeniy Vorobeychik, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision2024</p>
<p>Bayesian sequential optimal experimental design for nonlinear models using policy gradient reinforcement learning. Wanggang Shen, Xun Huan, Computer Methods in Applied Mechanics and Engineering. 2023</p>
<p>ZINC 15-Ligand Discovery for Everyone. Teague Sterling, John J Irwin, Journal of Chemical Information and Modeling. 55112015</p>
<p>Policy Gradient Methods for Reinforcement Learning with Function Approximation. David Richard S Sutton, Satinder Mcallester, Yishay Singh, Mansour, Advances in Neural Information Processing Systems. 121999</p>
<p>ZINC-22-A Free Multi-Billion-Scale Database of Tangible Compounds for Ligand Discovery. Benjamin I Tingle, G Khanh, Mar Tang, John J Castanon, Munkhzul Gutierrez, Chinzorig Khurelbaatar, Yurii S Dandarchuluun, John J Moroz, Irwin, Journal of Chemical Information and Modeling. 6342023Cited on pgs. 2, 8, and 14</p>
<p>A general-purpose machine learning framework for predicting properties of inorganic materials. Logan Ward, Ankit Agrawal, Alok Choudhary, Christopher Wolverton, npj Computational Materials. 212016</p>
<p>Active learning in the drug discovery process. Manfred K Warmuth, Gunnar Rätsch, Michael Mathieson, Jun Liao, Christian Lemmen, Advances in Neural Information Processing Systems 15. 2002Cited on pg. 6.</p>
<p>Active Learning with Support Vector Machines in the Drug Discovery Process. Manfred K Warmuth, Jun Liao, Gunnar Rätsch, Michael Mathieson, Santosh Putta, Christian Lemmen, Journal of Chemical Information and Computer Sciences. 4322003Cited on pg. 6.</p>
<p>. Peter Willett, John M Barnard, Geoffrey M Downs, Chemical Similarity Searching. Journal of Chemical Information and Computer Sciences. 3861998Cited on pg. 14.</p>
<p>Han Xiao, Kashif Rasul, Roland Vollgraf, arXiv:1708.07747[cs.LG].Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms. 2017arXiv preprintCited on pgs. 7, 14, and 16</p>
<p>Adaptive Sampling for Discovery. Ziping Xu, Eunjae Shim, Ambuj Tewari, Paul Zimmerman, Advances in Neural Information Processing Systems. 202235</p>            </div>
        </div>

    </div>
</body>
</html>
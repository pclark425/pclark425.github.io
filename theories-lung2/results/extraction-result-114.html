<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-114 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-114</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-114</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-5.html">extraction-schema-5</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <p><strong>Paper ID:</strong> paper-e46e82951d5a3c8e5c7e34c12ba61451629e5c8b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e46e82951d5a3c8e5c7e34c12ba61451629e5c8b" target="_blank">Learning in Spiking Neural Networks with a Calcium-based Hebbian Rule for Spike-timing-dependent Plasticity</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work presents a Hebbian local learning rule that models synaptic modification as a function of calcium traces tracking neuronal activity and demonstrates how spike timing and rate can be complementary in their role of shaping the connectivity of spiking neural networks.</p>
                <p><strong>Paper Abstract:</strong> Understanding how biological neural networks are shaped via local plasticity mechanisms can lead to energy-efficient and self-adaptive information processing systems, which promises to mitigate some of the current roadblocks in edge computing systems. While biology makes use of spikes to seamless use both spike timing and mean firing rate to modulate synaptic strength, most models focus on one of the two. In this work, we present a Hebbian local learning rule that models synaptic modification as a function of calcium traces tracking neuronal activity. We show how the rule reproduces results from spike time and spike rate protocols from neuroscientific studies. Moreover, we use the model to train spiking neural networks on MNIST digit recognition to show and explain what sort of mechanisms are needed to learn real-world patterns. We show how our model is sensitive to correlated spiking activity and how this enables it to modulate the learning rate of the network without altering the mean firing rate of the neurons nor the hyparameters of the learning rule. To the best of our knowledge, this is the first work that showcases how spike timing and rate can be complementary in their role of shaping the connectivity of spiking neural networks.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e114.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e114.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BiCaLL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bistable Calcium-based Local Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A biologically inspired Hebbian local learning rule that uses exponentially decaying pre- and post-synaptic calcium traces to compute weight updates, combined with a bistable hidden weight variable and a stop-learning gating trace; reproduces both STDP and spike-rate-dependent plasticity and is applied to training spiking networks (e.g., MNIST).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Calcium-based Hebbian (BiCaLL)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Pre- and post-synaptic calcium traces x_i, x_j follow exponential decay with time constants τ_i, τ_j and are incremented at spikes by a_n[1 - x_n]; weight updates are triggered on spikes: on a pre-synaptic spike the post trace x_j is compared to threshold θ_j and a fixed depression c1^d is applied if x_j > θ_j (Eq. 3); on a post-synaptic spike potentiation is proportional to the pre trace (x_i * c^p) and a small depression c2^d is applied if x_i < θ_i (Eq. 4). Learning is gated by a stop-learning trace x_s with time constant τ_s and thresholds θ_l, θ_u (ρ = 1 only when θ_l ≤ x_s ≤ θ_u). Synapses have a slow bistable drift (τ_w) toward up/down attractors with slopes α, β around threshold θ_w, producing an effectively binary effective weight w_eff. Key hyperparameters: τ_i = τ_j = 30 ms, τ_s = 800 ms, τ_w = 40 s, a_i = 0.4, a_j = 0.5, c^p = 0.18, c1^d = -0.026, c2^d = -0.008 (see Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Multiple interacting scales: millisecond-scale spike timing (STDP window simulated -60 ms to +60 ms; τ_i, τ_j = 30 ms capture ms to tens-of-ms interactions), sub-second to second-scale gating (stop-learning trace τ_s = 800 ms controls whether weight updates are applied), tens-of-seconds slow consolidation/drift (bistability τ_w = 40 s drifts hidden weight to up/down attractors), and the paper situates these against biological short-term plasticity (tens ms to minutes) and long-term plasticity (hours to days) discussed in the introduction.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Model is generic but motivated by hippocampal and cortical plasticity (references to hippocampal LTP/LTD and cortical L5 frequency dependence); implemented in networks (feedforward single-layer sFNN and discussed for recurrent RSNN).</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Used on feedforward all-to-all input→output synapses (single-layer perceptron sFNN); also discussed for recurrent networks (RSNN) where recurrent connectivity and synchrony/oscillations can introduce correlations; modulatory inputs include virtual teacher (excitatory) and global inhibition (inhibitory) projected to the output layer; synapses are effectively binary (bistable) though the hidden variable is continuous.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Unsupervised/local Hebbian associative learning / prototype learning (used for class prototype formation on MNIST); spike-timing-dependent associative components allow temporal coding.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical model (mechanistic rule implemented in simulations using Brian2; applied to spiking feedforward networks trained on MNIST; design considers neuromorphic hardware implementation).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Fast spike-timing (ms-scale) is encoded by short calcium traces (τ_i, τ_j) and produces STDP-like updates; repeated pairings at higher frequency lead to accumulation of traces and frequency-dependent potentiation (SRDP), so timing and rate interact: timing biases the sign/amount of updates for given rates. The stop-learning trace (τ_s ≈ 800 ms) provides an intermediate timescale that gates plasticity when post-synaptic activation is strong, and the slow bistability drift (τ_w ≈ 40 s) stabilizes weight states across longer times, producing consolidation-like retention of learned synapses.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) The BiCaLL rule reproduces canonical STDP curves (pre→post potentiation, post→pre depression) and frequency-dependent plasticity (SRDP) without changing hyperparameters; 2) The same calcium-trace dynamics make the rule sensitive both to precise spike timing (ms) and mean firing rate (Hz) — timing can modulate the effective learning rate without changing mean firing rates or learning-rule hyperparameters; 3) Introducing temporal correlations in spike trains biases synaptic updates toward potentiation or depression depending on Δt, enabling acceleration or slowing of learning; 4) In a feedforward sFNN trained on MNIST, coding-level-dependent inhibition and the stop-learning gate are necessary to learn classes with differing coding levels and to stabilize learned prototypes; 5) Continuous presentation (no reset between samples) leverages transient traces to decorrelate overlapping pixels between classes and reduces overlap in learned synaptic matrices.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Applied to MNIST classification (single-layer sFNN): Correct Rate (CR) up to ≈0.68 (best, when using average-pool readout AR), smaller networks CR ≈0.55 for minimal size; hyperparameters used: τ_i = τ_j = 30 ms, τ_s = 800 ms, τ_w = 40 s, a_i = 0.4, a_j = 0.5, c^p = 0.18, c1^d = -0.026, c2^d = -0.008; other metrics reported include Hamming Distance between learned weight matrices and prototypes, mean firing-rate response ranges (e.g., ~60–150 Hz across conditions), STDP pairing Δt swept -60 ms to +60 ms, SRDP saturation around ~50 Hz in cited experiments and reproduced behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Yes — a stabilization mechanism: a stop-learning gate (x_s, τ_s = 800 ms) prevents further weight modification when postsynaptic activity is high (x_s outside [θ_l, θ_u]), while a slow bistable drift (τ_w = 40 s with drift slopes α, β and threshold θ_w) drives the hidden weight to up/down attractors producing an effectively binary consolidated weight (w_eff). This implements a local slow consolidation/stabilization of the weight after faster spike-driven changes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning in Spiking Neural Networks with a Calcium-based Hebbian Rule for Spike-timing-dependent Plasticity', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e114.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e114.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STDP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spike-Timing-Dependent Plasticity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A temporally asymmetric Hebbian rule where the relative timing of pre- and post-synaptic spikes on the millisecond scale determines whether LTP or LTD occurs; BiCaLL reproduces STDP curves by reading calcium traces at spike times.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>STDP</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>LTP for pre-before-post pairings (positive Δt), LTD for post-before-pre pairings (negative Δt); in BiCaLL this is implemented via reading the opposite-side calcium trace at spike times: pre spikes sample x_j and induce depression if x_j > θ_j (Eq. 3); post spikes sample x_i and induce potentiation proportional to x_i*c^p (plus a small gap-dependent depression c2^d if x_i < θ_i) (Eq. 4). The model allows both nearest-neighbor and all-to-all spike interactions via the amplitude of trace updates a_n (a_n = 1 yields nearest-neighbor; a_n < 1 integrates past spikes).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Milliseconds (STDP pairing window explicitly simulated from -60 ms to +60 ms; trace time constants τ_i, τ_j = 30 ms capture the relevant ms-timescale integration).</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Discussed with reference to hippocampal synapses (e.g., CA3-CA1) and cortical neurons in the introduction and reproduced generally in the model; the phenomenon is treated as a general synaptic plasticity motif.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Pairwise pre↔post synapses; amount and range of interactions (nearest-neighbor vs all-to-all) depend on trace update amplitude a_i/a_j; implemented on input→output synapses in a fully connected (all-to-all) feedforward network.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Temporal associative / timing-based coding (supporting spike-timing dependent associative learning).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Phenomenon reproduced and analyzed in computational model (BiCaLL), comparisons to experimental STDP curves discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>STDP (ms) interacts with rate-dependent plasticity because repeated pairings at higher frequencies lead to accumulation of calcium traces, shifting net sign/magnitude of plasticity; BiCaLL shows how trace integration (via a_n and τ_n) links ms-scale pairings to slower mean-rate outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>BiCaLL reproduces STDP shape: potentiation for pre-post, depression for post-pre; with modeling simplification a constant depression for negative Δt is used; the trace amplitude parameter a_n controls whether interactions are nearest-neighbor (a_n ≈ 1) or all-to-all (a_n < 1), shifting the mean-rate transition between net depression and potentiation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Simulated STDP curve across Δt = -60 ms..+60 ms; trace time constants τ_i = τ_j = 30 ms; potentiation scaling c^p = 0.18, depression amplitudes c1^d = -0.026, c2^d = -0.008 used to shape the curve.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Not directly — STDP events are the fast component that can be gated and then stabilized by the stop-learning and bistability mechanisms of BiCaLL.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning in Spiking Neural Networks with a Calcium-based Hebbian Rule for Spike-timing-dependent Plasticity', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e114.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e114.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SRDP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spike-Rate-Dependent Plasticity (frequency-dependent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Plasticity outcomes that depend on the frequency of repeated spike-pairings: the same timing Δt can yield depression at low pairing frequencies and potentiation at higher frequencies; reproduced by BiCaLL via trace accumulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Spike-rate-dependent plasticity (SRDP)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>With fixed Δt and increasing pairing frequency, accumulated calcium traces (x_i, x_j) increase potentiation in BiCaLL because potentiation on post spikes scales with pre-trace x_i; depression terms can dominate at low frequencies. The model therefore produces frequency-dependent sign changes (potentiation saturates around tens of Hz, consistent with cited experimental observations ~50 Hz).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Tens of milliseconds to seconds (pairing frequencies swept up to ~50 Hz in SRDP experiments; accumulation and decay of traces with τ_i/τ_j = 30 ms produce frequency dependence on the order of tens of ms to seconds).</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Discussed with reference to layer 5 visual cortex frequency dependence and general cortical/hippocampal synapses; in-model applies to all synapses trained with BiCaLL.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Pairing protocols between pre-post neuron pairs; effect modulated by whether traces implement nearest-neighbor or all-to-all interactions (trace amplitude a_n).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Rate-based associative plasticity; homeostatic modulation of potentiation threshold by mean firing rate.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Modeled computationally; compared qualitatively to experimental SRDP data.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Frequency dependence arises because ms-scale increments summed over repeated pairings accumulate into larger pre/post calcium traces on intermediate timescales, converting timing-based signals into mean-rate dependent plasticity; this bridges fast spike timing and slower mean-rate learning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>BiCaLL reproduces experimentally observed frequency dependence: pairings with negative Δt induce depression at low frequencies and potentiation at higher frequencies; potentiation with positive Δt increases with frequency and saturates (~50 Hz). Trace update amplitude (a_n) controls how frequency translates into net potentiation via integrative behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>SRDP simulations: fixed number of spike-pairs (10) simulated at increasing frequencies; experimental saturation region reported around ~50 Hz; model hyperparameters: τ_i = τ_j = 30 ms, a_i = 0.4, c^p = 0.18, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>No explicit long-term consolidation described for SRDP itself; SRDP outcomes are stabilized by the same stop-learning and bistability mechanisms in BiCaLL.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning in Spiking Neural Networks with a Calcium-based Hebbian Rule for Spike-timing-dependent Plasticity', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e114.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e114.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>sFNN (feedforward)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spiking Feedforward Neural Network (single-layer perceptron)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single-layer, fully-connected spiking network (input→output) used to evaluate BiCaLL on MNIST, where input pixels are converted to Poisson spike trains and output neurons form pools representing classes; learning occurs on the input→output synapses using BiCaLL and is modulated by teacher excitatory and inhibitory virtual neurons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>BiCaLL applied to input→output synapses</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Input pixels become Poisson spike trains (high/low rates for 1/0); input→output synapses are plastic via BiCaLL (pre/post calcium traces, gated updates, bistable drift). Teacher virtual neurons provide additional excitatory input to target pools during training; a global (or coding-level-dependent) inhibitory virtual neuron inhibits outputs to shape competition and baseline rates.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Pattern presentation t_inp = 1 s per sample; fast spike-timing interactions captured by τ_i = τ_j = 30 ms; stop-learning gating τ_s = 800 ms operates within/over presentations; bistability τ_w = 40 s operates across presentations — thus learning and readout span ms → seconds → tens of seconds.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Artificial feedforward architecture motivated by cortical prototype learning; not mapped to a specific biological sub-region.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>All-to-all (fully connected) input→output; pools of output neurons per class (ensemble), global inhibitory projection to outputs, and teacher excitatory projection to specific output pools.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Unsupervised/prototype associative learning for classification (pattern-feature imprinting into synaptic matrices), with a weak supervised 'teacher' signal used during training to bias pools.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational model/simulation (Brian2) applied to MNIST; analyses include Hamming distances of learned synaptic matrices, classification CR, response histograms, and training protocols (continuous vs quiet inter-sample periods).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Within-sample ms-scale spike timing and rate shape instantaneous weight updates; continuous presentation leverages inter-sample transients (interactions of traces across sample switches) on the 10s-1000s ms timescale to depress overlapping synapses and decorrelate classes; stop-learning (τ_s) and bistability (τ_w) operate on longer timescales to stabilize learned prototypes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Coding-level differences across classes create problems for balanced learning — resolved by coding-level-dependent inhibition and stop-learning gating; 2) Continuous presentation (no reset) reduces overlap between class representations by using transient trace interactions; 3) Readout by average-pool (AR) outperforms max-rate (MR) as pool size increases; 4) Best reported CR ≈ 0.68 (AR readout), and various analyses quantify Hamming distance, response ranges, and within-pool variability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Correct Rate (CR) up to ≈0.68 (average-pool readout); single-sample presentation t_inp = 1 s; Poisson input rates f_a = 20 Hz (active pixel), f_s = 3 Hz (inactive pixel); teacher f_t = 30 Hz; inhibitory f_i = 210 Hz; weight bounds w_max tuned (1 mV during training, 10 mV during testing for readout sweep).</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Stop-learning gating (x_s, τ_s = 800 ms) combined with bistable drift (τ_w = 40 s) produce consolidation-like stabilization of class prototypes once output neurons reach high activity; this prevents overwriting of already-learned strong representations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning in Spiking Neural Networks with a Calcium-based Hebbian Rule for Spike-timing-dependent Plasticity', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e114.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e114.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RSNN (recurrent)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent Spiking Neural Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent spiking network architecture discussed as a substrate where network synchrony and oscillatory dynamics can introduce correlations that modulate BiCaLL-driven learning rate without changing mean firing rates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>BiCaLL (intended to be used on recurrent synapses)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Same calcium-trace based BiCaLL updates apply to recurrent connections; in recurrent topology, recurrent-driven synchrony/oscillations will produce correlated spike timing patterns that bias STDP-like updates and thus modulate the effective speed of learning.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Oscillatory dynamics referenced (theta/gamma) operate across tens to hundreds of ms (gamma ~30–100 Hz, theta ~4–8 Hz); these rhythms interact with ms-scale spike timing and second-scale gating to affect learning.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Generic recurrent networks motivated by brain regions exhibiting oscillations (cortex, hippocampus); not instantiated in the provided excerpt but proposed for study.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Recurrent connectivity (feedback loops) enabling synchronization and rhythmic subthreshold fluctuations; such motifs create correlated spike timing across neurons.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Exploration of how timing correlations in recurrent circuits modulate learning speed (accelerated synaptic modifications via timing correlations) while keeping mean rate constant.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Proposed computational/theoretical application (discussion in paper); experiments/simulations for RSNN not shown in the provided excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Hypothesized: network oscillations (tens–hundreds ms) produce spike timing correlations that interact with ms-scale STDP components and slower gating/bistability (seconds to tens of seconds) to change how fast weights consolidate without altering average firing rates.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Hypothesis and planned demonstration: timing (via induced correlations/synchrony) can be used to bias and accelerate learning rate in networks governed by BiCaLL, showing complementary roles of STDP and frequency-dependent plasticity; explicit RSNN results not present in provided text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Not explicitly demonstrated in provided excerpt; expectation is that BiCaLL's stop-learning and bistability will still provide slow consolidation after fast, oscillation-induced changes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning in Spiking Neural Networks with a Calcium-based Hebbian Rule for Spike-timing-dependent Plasticity', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e114.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e114.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Stop-learning gate</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stop-learning calcium-gated mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A gating mechanism that uses a longer post-synaptic calcium trace x_s (τ_s) and two thresholds (θ_l, θ_u) to enable/disable synaptic updates, preventing uncontrolled weight change and addressing coding-level problems during training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Stop-learning gating</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>A post-synaptic trace x_s with time constant τ_s (800 ms) is incremented at post spikes and decays exponentially; updates to w_hid (Eqs. 3 and 4) are applied only when θ_l ≤ x_s ≤ θ_u (i.e., ρ = 1), otherwise updates are inhibited (ρ = 0). This reduces learning once neurons achieve high activity to stabilize learned weight patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Intermediate scale: τ_s = 800 ms (sub-second to second), placing it between ms-scale STDP and tens-of-seconds bistability drift.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Used within the sFNN output neurons to regulate learning; general mechanism not tied to a specific biological sub-region in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Gates updates on input→output synapses; interacts with excitatory teacher input and global inhibition to shape learning of output pools.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Stability/consolidation of learned prototypes (limits further plasticity when postsynaptic activation is strong).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational mechanism implemented in the BiCaLL model and used in MNIST training experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Acts as an intermediate timescale gate: fast ms-scale spike events produce x_s increments which then control whether those fast events change weights; combined with slow bistability (τ_w) this provides a pathway from fast plasticity to slower consolidation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Applying the stop-learning gate together with coding-level-dependent inhibition allows neurons encoding classes with low coding-level to retain some random initial weights (helpful to boost baseline firing) and produces more balanced output firing rates across classes; it reduces coding-level driven response disparities but can increase representational 'noise' overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Parameters used: τ_s = 800 ms, θ_u = 0.55, θ_l = 0.05; effects illustrated qualitatively by improved Hamming-distance profiles and more similar mean firing-rate response ranges (figures in the paper), no single scalar improvement metric exclusively attributed to this mechanism is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning in Spiking Neural Networks with a Calcium-based Hebbian Rule for Spike-timing-dependent Plasticity', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e114.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e114.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Coding-level-dependent inhibition</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Input coding-level-dependent inhibitory modulation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scheme where the mean rate of inhibitory input to the output layer is set proportional to the input pattern's coding-level (fraction of active pixels) so that excitation/inhibition balance is dynamically adjusted across classes with different coding densities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Inhibition modulation (not a synaptic plasticity rule per se)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Virtual inhibitory neurons provide Poisson input to all output neurons; the inhibitory mean firing rate is scaled proportionally to the coding-level of the presented input, increasing inhibition for high-coding-level samples and decreasing it for low-coding-level samples, thereby normalizing net drive to output neurons across classes.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Operates on the sample presentation timescale (t_inp = 1 s) and continuously during presentation; interacts with ms-scale spike timing and sub-second stop-learning gating.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Applied to the output layer of the sFNN (artificial network); motivated by biological E/I balance in cortical circuits.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Global inhibition (inhibitory virtual neuron → all output units), complementary to teacher excitatory input → specific output pools; adjusts net drive across fully-connected input→output architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Supports balanced associative/prototype learning across classes with varying coding-levels (helps class assignment and stability).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational mechanism used in sFNN training experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Interacts with fast trace-based plasticity by modulating post-synaptic firing rates over the duration of sample presentation (seconds), thereby affecting trace accumulation and gating thresholds (x_s) which in turn influence slower stabilization.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Necessary to properly learn classes with different coding-levels: fixed inhibition produced unbalanced learning and poor representation for low-coding-level classes, while coding-level-dependent inhibition allowed both high- and low-coding-level classes to be learned and improved selectivity during inference.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Qualitative and graphical improvements shown (Hamming distance distributions, response separability); reported settings include inhibitory rate f_i up to 210 Hz and teacher f_t = 30 Hz; no single scalar performance number solely attributable to this mechanism is isolated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning in Spiking Neural Networks with a Calcium-based Hebbian Rule for Spike-timing-dependent Plasticity', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        </div>

    </div>
</body>
</html>
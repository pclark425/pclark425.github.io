<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-96 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-96</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-96</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-5.html">extraction-schema-5</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <p><strong>Paper ID:</strong> paper-bc0ab8c3ec4c9df7ec6701e16be33793cea5ce2c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/bc0ab8c3ec4c9df7ec6701e16be33793cea5ce2c" target="_blank">Mirrored STDP Implements Autoencoder Learning in a Network of Spiking Neurons</a></p>
                <p><strong>Paper Venue:</strong> PLoS Comput. Biol.</p>
                <p><strong>Paper TL;DR:</strong> This work proposes that plasticity rules at feedforward versus feedback connections are temporally opposed versions of spike-timing dependent plasticity (STDP), leading to a symmetric combined rule the authors call Mirrored STDP (mSTDP).</p>
                <p><strong>Paper Abstract:</strong> The autoencoder algorithm is a simple but powerful unsupervised method for training neural networks. Autoencoder networks can learn sparse distributed codes similar to those seen in cortical sensory areas such as visual area V1, but they can also be stacked to learn increasingly abstract representations. Several computational neuroscience models of sensory areas, including Olshausen & Field’s Sparse Coding algorithm, can be seen as autoencoder variants, and autoencoders have seen extensive use in the machine learning community. Despite their power and versatility, autoencoders have been difficult to implement in a biologically realistic fashion. The challenges include their need to calculate differences between two neuronal activities and their requirement for learning rules which lead to identical changes at feedforward and feedback connections. Here, we study a biologically realistic network of integrate-and-fire neurons with anatomical connectivity and synaptic plasticity that closely matches that observed in cortical sensory areas. Our choice of synaptic plasticity rules is inspired by recent experimental and theoretical results suggesting that learning at feedback connections may have a different form from learning at feedforward connections, and our results depend critically on this novel choice of plasticity rules. Specifically, we propose that plasticity rules at feedforward versus feedback connections are temporally opposed versions of spike-timing dependent plasticity (STDP), leading to a symmetric combined rule we call Mirrored STDP (mSTDP). We show that with mSTDP, our network follows a learning rule that approximately minimizes an autoencoder loss function. When trained with whitened natural image patches, the learned synaptic weights resemble the receptive fields seen in V1. Our results use realistic synaptic plasticity rules to show that the powerful autoencoder learning algorithm could be within the reach of real biological networks.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e96.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e96.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>mSTDP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mirrored Spike-Timing Dependent Plasticity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A combined synaptic plasticity rule in which feedforward synapses follow standard STDP and feedback synapses follow a temporally reversed (anti-)STDP so that, when expressed with respect to visible–hidden spike pairs, the two directions produce symmetric weight changes (up to a scale).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Mirrored STDP (mSTDP); STDP and aSTDP components</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Feedforward synapses use additive STDP: pre-before-post produces potentiation, post-before-pre produces depression; changes scale with exp(-|Δt|/τ_{+}) for potentiation and exp(-|Δt|/τ_{-}) for depression. Feedback synapses use temporally reversed STDP (aSTDP): pre-before-post produces depression and post-before-pre produces potentiation, with analogous time constants. Because feedforward presynaptic identity is the visible neuron while for feedback the hidden neuron is presynaptic, writing both rules with the same visible–hidden spike-pair ordering yields identical signed plasticity profiles for the two directions up to an overall scale factor (α ≈ ηζ). Key parameters: learning rates η (feedforward) and ζ (feedback), potentiation/depression time constants τ_{+}, τ_{-}, and pairwise exponential kernels. Plasticity accumulates by summing contributions from all spike pairs between the two neurons.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Millisecond scale for STDP/aSTDP (time constants τ_{+}, τ_{-} on the order of tens of milliseconds); plasticity effects computed per spike-pair within trials (ms–tens of ms). Changes accumulate across trials (trial-by-trial learning; training involved many thousands of stimulus presentations).</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Modeled after cortical sensory circuits (e.g., feedforward and feedback between cortical areas; motivated by V1-like circuits). The paper frames mSTDP in the context of cortical feedforward (proximal synapses) and feedback (distal dendritic synapses) connections.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Fully reciprocal bipartite feedforward–feedback architecture between a visible layer and a hidden layer (all-to-all between layers), with no lateral connections within layers in the basic model; added pools of inhibitory neurons per layer to control activity. Visible units use ON/OFF split; feedforward synapses target proximal compartments, feedback target distal dendrites.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Unsupervised representation learning (autoencoder learning / reconstruction error minimization); development of receptive fields (unsupervised feature learning).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical model: spiking network simulations (leaky integrate-and-fire neurons) plus analytical derivation connecting spike-timing plasticity to the autoencoder learning rule.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Fast (ms) spike-timing plasticity (STDP/aSTDP) produces weight changes on each stimulus presentation based on early (visible), intermediate (hidden), and late (feedback-driven visible) spike bouts; these fast changes accumulate over many presentations. Slower homeostatic adjustments (synaptic scaling) operate across trials to maintain target firing rates and sparsity, interacting with mSTDP to shape the final receptive fields and to prevent runaway excitation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Combining STDP at feedforward synapses and a temporally reversed STDP at feedback synapses (mSTDP) makes plasticity symmetric for visible–hidden spike pairs (up to scale), which is required to implement the autoencoder-style weight updates. 2) Under realistic timing of three activity bouts (initial visible spikes, delayed hidden spikes, later feedback-driven visible spikes) and exponential STDP kernels, pairwise spike contributions sum to Δw_{ij} ∝ (β x_i - γ x̂_i) y_j, approximating the scaled autoencoder learning rule Δw_{ij} ∝ (x_i - (γ/β) x̂_i) y_j. 3) With mSTDP + synaptic scaling, the spiking network learns V1-like receptive fields from whitened natural image patches and attains substantial reconstruction performance on MNIST.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Reported reconstruction correlation metrics and sparsity targets: MNIST model achieved ~80% mean correlation between input and reconstruction after training (training: 50,000 images, 5,000 hidden units, target activation ρ=0.03). Natural image patches model (500 hidden units, 300,000 high-contrast patches, ρ=0.02) achieved ~35% input–reconstruction correlation. Time constants for STDP are reported as 'on the order of tens of milliseconds'; synaptic transmission delay = 2 ms; hidden onset latencies ~5–10 ms; trial duration 65 ms. Synaptic scaling update rule per trial used ΔΦ_j = β(ρ - A_j) and running averages A_j of activation probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>No separate biological consolidation process is proposed; long-term changes arise from cumulative application of mSTDP across many trials combined with slower synaptic-scaling adjustments across trials. The paper does not posit an explicit sleep- or consolidation-mediated transfer between timescales beyond accumulation of plasticity over training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mirrored STDP Implements Autoencoder Learning in a Network of Spiking Neurons', 'publication_date_yy_mm': '2015-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e96.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e96.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Synaptic scaling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Homeostatic Synaptic Scaling (synaptic offset / multiplicative scaling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A homeostatic mechanism implemented per hidden neuron that adjusts an additive offset or multiplicative gain on incoming weights to maintain a target average activity (lifetime sparsity), used to enforce sparse representations during training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Synaptic scaling (homeostatic plasticity)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Each hidden neuron maintains a running average A_j of the fraction of trials in which it fired; after each trial the neuron updates either an additive synaptic offset φ_j (used when weights can be positive or negative) or a multiplicative scaling Φ_j (used when weights are constrained nonnegative) according to Δφ_j or ΔΦ_j = β(ρ - A_j), where ρ is the target activation rate and β a small learning rate. This effectively raises or lowers net excitatory drive to move average activity toward ρ.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Operates on the timescale of repeated trials (updates applied after each stimulus presentation) and thus is slower than per-spike STDP; in simulations it accumulates over thousands to hundreds of thousands of trials during training (i.e., trial-to-trial adaptation).</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Modelled in the same cortical sensory-area-inspired network (hidden-layer neurons in the modeled cortical area receiving feedforward inputs).</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Acts on all incoming feedforward synapses to a hidden neuron (global per-postsynaptic-neuron scaling or offset), compatible with multiplicative scaling of excitatory weights or additive offset (bias-like) depending on implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Supports unsupervised sparse representation learning (lifetime sparsity) by regulating excitability to achieve a desired activation probability.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Implemented as a computational/homeostatic mechanism in the spiking model; motivated by experimental observations (cited in paper) of synaptic scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Provides a slower homeostatic process that interacts with fast mSTDP: STDP/aSTDP produces rapid, spike-timing-dependent weight changes within a trial (ms–tens ms) while synaptic scaling slowly adjusts neuronal responsiveness across trials to maintain sparsity and prevent runaway excitation, thereby shaping the distribution of activity that STDP acts upon.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Synaptic scaling (implemented as additive offsets or multiplicative gains per hidden neuron) is necessary in the model to drive neurons toward target low activation rates (ρ) and thereby obtain sparse codes; varying ρ changes receptive field structure (very low ρ leads to larger-support RFs, higher ρ yields distributed codes). Scaling keeps lifetime activity near target and prevents runaway excitation, enabling stable cumulative effects of mSTDP.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Target activation rates used: ρ = 0.03 (MNIST) and ρ = 0.02 (natural images). Update rule ΔΦ_j = β(ρ - A_j) used for multiplicative scaling; A_j is the running fraction of trials with at least one spike. The paper shows sparsity loss measure Sparsity = ||A_j - ρ||^2 / ρ^2 plotted over training, demonstrating maintenance of target activity.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Synaptic scaling functions as a slow homeostatic adjustment across trials that stabilizes the results of fast STDP-driven changes; no separate consolidation process (e.g., offline replay or systems consolidation) is described.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mirrored STDP Implements Autoencoder Learning in a Network of Spiking Neurons', 'publication_date_yy_mm': '2015-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e96.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e96.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reciprocal feedforward–feedback architecture</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>All-to-all reciprocal feedforward–feedback visible–hidden architecture with inhibitory pools and ON–OFF preprocessing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-layer spiking network architecture with all-to-all reciprocal connections between visible and hidden units, ON/OFF split of visible inputs, inhibitory pools per layer, and anatomically motivated proximal feedforward vs distal feedback synapse placement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Reciprocal feedforward–feedback architecture (connectivity motif)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Visible units (ON/OFF) are driven by stimulus input and connect to all hidden units (feedforward W); hidden units send feedback connections Q back to visible units. No lateral connections within layers in the basic model; inhibitory interneuron pools per layer provide inhibition. Anatomical detail: feedforward synapses are placed proximal to hidden-cell somata, feedback synapses are distal on visible-cell dendrites (motivating the use of aSTDP at feedback sites). Weights are allowed to be either signed or constrained nonnegative (Dale-like) depending on simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Network dynamics produce three temporally separated spike 'bouts' per trial: initial visible spiking (0–~10 ms after stimulus onset), delayed hidden spiking (onset ~5–10 ms after visible), and later feedback-driven visible spiking (~10–20 ms); synaptic transmission delays reported as ∼2 ms. These fast temporal patterns (ms to tens of ms) are crucial to whether STDP contributions are potentiating or depressing.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Cortical sensory-area-inspired circuit (analogous to connections between cortical layers or between cortical areas such as feedforward to/from V1); motivated by anatomy of excitatory feedforward and feedback projections.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Dense reciprocal (all-to-all) inter-layer connections with separate inhibitory pools per layer; ON/OFF visible split to represent signed inputs; no lateral hidden–hidden connections in the base model (population sparsity achieved via synaptic scaling rather than lateral competition).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Unsupervised autoencoder-style learning (reconstruction-driven synaptic updates), receptive field formation.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational spiking model (LIF neurons) with anatomically motivated connectivity; analytical derivation links connectivity and timing to approximate autoencoder gradient.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Fast inter-layer spike timing (ms) determines sign/magnitude of STDP contributions; accumulation of these fast changes across many trials yields long-term modification of weights. The architecture enforces weak feedback (scaled by α<1) so that feedback-driven spikes produce small reconstructions and produce the negative term needed for the autoencoder-like plasticity.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The specific feedforward/feedback motif together with temporally-separated activity bouts enables local spike-pair-based plasticity (mSTDP) to implement a scaled autoencoder learning rule approximately locally, avoiding the need for non-local credit assignment. Anatomical differences in synapse placement motivate distinct plasticity rules (STDP vs aSTDP) that are necessary to achieve symmetric learning required by tied-weight autoencoder implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Network-level timing metrics: synaptic transmission delay = 2 ms; hidden onset latencies 5–10 ms; trial length 65 ms (most spikes before 30 ms). Reconstruction performance: MNIST correlation ~80%, natural image patches correlation ~35%; architecture parameters included number of hidden units (MNIST 5,000; images 500), inhibitory pools per layer, and target activity ρ as above.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>No explicit consolidation beyond iterative accumulation of spike-timing plasticity and slow homeostatic scaling; symmetry between W and Q is maintained/approached by mSTDP over training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mirrored STDP Implements Autoencoder Learning in a Network of Spiking Neurons', 'publication_date_yy_mm': '2015-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Emergence of simple-cell receptive field properties by learning a sparse code for natural images <em>(Rating: 2)</em></li>
                <li>Synaptic modifications in cultured hippocampal neurons: dependence on spike timing, synaptic strength, and postsynaptic cell type <em>(Rating: 2)</em></li>
                <li>Activity-dependent scaling of quantal amplitude in neocortical neurons <em>(Rating: 2)</em></li>
            </ol>
        </div>

        </div>

    </div>
</body>
</html>
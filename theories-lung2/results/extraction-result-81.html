<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-81 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-81</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-81</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-5.html">extraction-schema-5</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <p><strong>Paper ID:</strong> paper-4088e09876056348c3e64f00379e1866a8852257</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4088e09876056348c3e64f00379e1866a8852257" target="_blank">Hardware-Amenable Structural Learning for Spike-Based Pattern Classification Using a Simple Model of Active Dendrites</a></p>
                <p><strong>Paper Venue:</strong> Neural Computation</p>
                <p><strong>Paper TL;DR:</strong> This letter presents a spike-based model that employs neurons with functionally distinct dendritic compartments for classifying high-dimensional binary patterns, and its performance is compared against that achieved using support vector machine and extreme learning machine techniques.</p>
                <p><strong>Paper Abstract:</strong> This letter presents a spike-based model that employs neurons with functionally distinct dendritic compartments for classifying high-dimensional binary patterns. The synaptic inputs arriving on each dendritic subunit are nonlinearly processed before being linearly integrated at the soma, giving the neuron the capacity to perform a large number of input-output mappings. The model uses sparse synaptic connectivity, where each synapse takes a binary value. The optimal connection pattern of a neuron is learned by using a simple hardware-friendly, margin-enhancing learning algorithm inspired by the mechanism of structural plasticity in biological neurons. The learning algorithm groups correlated synaptic inputs on the same dendritic branch. Since the learning results in modified connection patterns, it can be incorporated into current event-based neuromorphic systems with little overhead. This work also presents a branch-specific spike-based version of this structural plasticity rule. The proposed model is evaluated on benchmark binary classification problems, and its performance is compared against that achieved using support vector machine and extreme learning machine techniques. Our proposed method attains comparable performance while using 10% to 50% less in computational resource than the other reported techniques.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e81.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e81.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>StructPlasticity-RM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hardware-friendly structural plasticity (Reduced Model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised, hardware-amenable structural plasticity rule that learns by forming and eliminating binary synaptic connections to group correlated inputs onto dendritic branches using a local fitness signal averaged over presentations (epochs).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Structural plasticity (fitness-based, Hebbian-like)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Synapses are binary (exist / do not exist). For each synapse a local fitness value c_ij is computed as the average over training patterns of Δc_ij, where for the (+) neuron Δc_ij = x_ij * b_j * sgn(o - y) and for the (-) neuron Δc_ij = - x_ij * b_j * sgn(o - y) (b_j is branch output, x_ij is presynaptic input, o teacher label, y network output). Wrongly classified patterns (sgn(o-y) non-zero) drive the sign of Δc_ij. Training proceeds by selecting poorly performing synapses (low c_ij) from a random candidate set, creating silent candidate synapses from the pool of possible afferents, and replacing the worst synapse with the silent synapse having the highest c_ij. This is an error-modulated, Hebbian-like re-wiring (formation/elimination) rule designed for sparse binary connectivity and local implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Operates across multiple timescales: (1) fast spike / PSC dynamics captured by the kernel K(t) with rise/fall time constants τ_r and τ_f (spike-level, millisecond timescale); (2) pattern-level averaging over the presentation duration T (intermediate timescale — the model uses the time-averaged synaptic activation z_syn = (1/T) ∫ K⋆s dt to collapse spike trains into binary pattern components); (3) slow structural changes (formation/elimination of synapses) performed across training epochs based on epoch-averaged c_ij (learning timescale — implementation-dependent; in simulations this is across iterations/epochs).</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Abstract neuron model inspired by pyramidal cells with active dendrites (generic cortical/hippocampal dendritic architecture); classifier implemented as two neurons (+ and -) compared by a WTA circuit (feedforward classifier).</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Sparse, branch-specific binary connectivity: total synapses s = m × k with k ≪ d (d = number of possible afferents); learning groups correlated afferents onto the same dendritic branch (synaptic clustering); no recurrent connectivity — feedforward pattern classifier with WTA readout.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Supervised pattern classification (error-modulated structural re-wiring); Hebbian-like correlation-based objective (associative in the sense of grouping correlated inputs), margin-based modification (RMWM) used to improve generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical model designed for neuromorphic (VLSI) implementation; evaluations are in simulation on benchmark binary classification tasks (no in vivo/in vitro experiments reported in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Fast spike events (captured by K(t)) produce branch input currents which are nonlinearly transformed by branch nonlinearity b() and summed at the soma; these branch outputs (b_j) and presynaptic activations (x_ij or their time-averages) are combined to compute local fitness c_ij averaged over pattern presentations (intermediate timescale), and epoch-averaged c_ij then drives discrete structural changes (synapse formation/elimination) on a much slower learning timescale. Thus the rule links millisecond-scale PSC and spike timing through time-averaging to slower, epoch-level structural updates.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The proposed structural plasticity rule groups correlated inputs onto the same nonlinear dendritic branches, enabling the neuron to implement higher-order input-output mappings and improving classification capacity relative to a linear neuron with the same total synaptic resources; using binary synapses makes the rule robust and hardware-friendly. The reduced model (RM) can be trained faster using time-averaged activations; a margin-based variant (RMWM) improves robustness to noisy spike inputs (RM exhibited ~5× higher test error than training error under noisy spiking in prior tests; margin mechanism reduces that gap). The method attains comparable classification performance to SVM and ELM on benchmark binary datasets while using ~10–50% fewer computational resources (reported in abstract). The squaring (quadratic) branch nonlinearity with threshold x_thr≥2 ensures branches respond supra-linearly only when multiple co-active synapses are present, promoting detection of higher-order correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Primary training/test metric reported is mean absolute error (MAE) computed as fraction of misclassified patterns; statement that noisy-spike RM test error is ~5× the training error for RM (no exact numeric error rates provided in the supplied text); resource usage reported as 10–50% less computational resources compared to SVM/ELM (abstract). Theoretical capacity estimates are provided via combinatorial expressions (e.g., combinations for NL vs L neurons) to predict the number of distinct input-output mappings.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hardware-Amenable Structural Learning for Spike-Based Pattern Classification Using a Simple Model of Active Dendrites', 'publication_date_yy_mm': '2014-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e81.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e81.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BSTDSP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Branch-Specific Spike-Timing-Dependent Structural Plasticity (BSTDSP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A spike-based, branch-specific implementation of the structural plasticity rule that uses local spike timing information at branches to implement the fitness-based synapse selection and structural changes in a manner suitable for neuromorphic hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Branch-specific spike-timing-based structural plasticity (BSTDSP)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Presented in the paper as a spike-timing-based version of the structural learning rule: branch-local spike events and timing relationships are used to compute a branch-specific, spike-based correlate of fitness that can replace the epoch-averaged c_ij used in the RM. The rule is designed so the necessary information is local to branches (enabling hardware implementation) while being modulated by the global error signal for supervised learning. (The paper states that a branch-specific spike-based implementation is presented in Section 2.6; full algorithmic details are in that section.)</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Bridges fast spike-timing (millisecond timescale: pre/post spike timing, PSC kernels) to slower structural changes (synapse formation/elimination across training epochs); spike-timing events are used instantaneously or over short windows to compute local statistics that then drive slower rewiring.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Generic model of neurons with active (nonlinear) dendritic branches — inspired by cortical pyramidal neuron dendrites; branch-local computation emphasized.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Branch-specific, sparse binary connectivity; learning modifies which afferents connect to each dendritic branch based on local spike-timing signals and global error.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Supervised spike-based learning (error-modulated structural plasticity implemented via spike-timing correlates).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/model proposal intended for neuromorphic hardware implementation; a spike-based algorithmic implementation is presented (no in vivo data).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Explicitly intended to link millisecond-scale spike timing to epoch-scale structural rewiring: branch-local spike-timing measurements produce local fitness signals that, when aggregated over presentations or modulated by error, cause slow synapse formation/elimination — thereby connecting fast temporal codes to slow connectivity changes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper presents a branch-specific spike-based variant (BSTDSP) of the structural learning rule to enable local, spike-timing-based computation of synaptic fitness suitable for hardware implementation; this provides a pathway to implement the RM/RMWM structural rewiring using spike-domain signals rather than only time-averaged rates. (Quantitative performance metrics for BSTDSP are not provided in the supplied text.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hardware-Amenable Structural Learning for Spike-Based Pattern Classification Using a Simple Model of Active Dendrites', 'publication_date_yy_mm': '2014-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e81.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e81.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STDP (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spike-Timing-Dependent Plasticity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A temporally asymmetric Hebbian plasticity rule where changes in synaptic strength depend on the relative timing of pre- and postsynaptic spikes; cited in the paper as background and in related work on spiking learning systems and RBMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Spike-Timing-Dependent Plasticity (STDP)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Mentioned in related work as a widely used spike-based plasticity mechanism (e.g., used to change synaptic weights in spike-based RBMs and bistable synapse implementations in prior studies). The paper references STDP-based learning algorithms implemented in other studies (Neftci et al., Brader et al.) but does not itself adopt a standard STDP weight-updating rule as the primary learning mechanism (instead it focuses on structural plasticity).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Operates on fast spike-timing timescales (tens of milliseconds or shorter) because plasticity depends on pre/post spike intervals; this rule is typically expressed in ms-scale temporal windows in cited literature (paper cites STDP in other works but does not give numerical windows).</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Cited generally in the context of spiking network learning (no specific brain region targeted in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Used in prior work for adjusting synaptic weights (often dense or large synapse counts); the current paper contrasts the high-synapse-count STDP-based systems with their sparse binary-synapse structural approach.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Supervised or unsupervised depending on prior implementations cited; in this paper STDP appears as a background method used in related spiking-network learning work.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Mentioned as used in computational simulations and VLSI implementations in prior work (the current paper cites these studies in the introduction).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>No detailed discussion in this paper; STDP is mentioned as a fast spike-timing plasticity mechanism in related work, contrasted with the structural (slower, connection-level) plasticity advocated here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>This paper references STDP-based systems in prior work (e.g., RBMs trained with STDP, bistable synapse STDP implementations) to motivate alternatives and to situate their structural, binary-synapse approach as more hardware-friendly when synapse counts must be low.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hardware-Amenable Structural Learning for Spike-Based Pattern Classification Using a Simple Model of Active Dendrites', 'publication_date_yy_mm': '2014-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        </div>

    </div>
</body>
</html>
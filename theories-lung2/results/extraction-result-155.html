<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-155 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-155</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-155</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-5.html">extraction-schema-5</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <p><strong>Paper ID:</strong> paper-f23c0a5a94a3429d2bd3d0e064e403dace7aed84</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f23c0a5a94a3429d2bd3d0e064e403dace7aed84" target="_blank">Network Plasticity as Bayesian Inference</a></p>
                <p><strong>Paper Venue:</strong> PLoS Comput. Biol.</p>
                <p><strong>Paper TL;DR:</strong> It is proposed that inherently stochastic features of synaptic plasticity and spine motility enable cortical networks of neurons to carry out probabilistic inference by sampling from a posterior distribution of network configurations.</p>
                <p><strong>Paper Abstract:</strong> General results from statistical learning theory suggest to understand not only brain computations, but also brain plasticity as probabilistic inference. But a model for that has been missing. We propose that inherently stochastic features of synaptic plasticity and spine motility enable cortical networks of neurons to carry out probabilistic inference by sampling from a posterior distribution of network configurations. This model provides a viable alternative to existing models that propose convergence of parameters to maximum likelihood values. It explains how priors on weight distributions and connection probabilities can be merged optimally with learned experience, how cortical networks can generalize learned information so well to novel experiences, and how they can compensate continuously for unforeseen disturbances of the network. The resulting new theory of network plasticity explains from a functional perspective a number of experimental data on stochastic aspects of synaptic plasticity that previously appeared to be quite puzzling.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e155.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e155.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Synaptic sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Synaptic sampling (stochastic parameter sampling via Langevin dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A normative stochastic plasticity rule in which each synaptic/network parameter θ_i follows a stochastic differential equation (Langevin dynamics) whose drift contains an activity-dependent likelihood gradient and a prior-gradient, and whose diffusion is a Wiener (Brownian) noise term; the stationary distribution of θ is the posterior p(θ|x).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Synaptic sampling (stochastic gradient Langevin dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Each parameter θ_i evolves as dθ_i = b(∂_{θ_i} log p_S(θ) + ∂_{θ_i} log p_N(x|θ)) dt + √(2b) dW_i (Eq (3), online Eq (5)/(7)). Components: (1) activity-dependent likelihood gradient (fits network to inputs; e.g., ML/STDP-like term), (2) prior-gradient ∂_{θ_i} log p_S(θ) enforcing structural constraints, and (3) stochastic diffusion dW_i (Wiener process) representing molecular/biophysical stochasticity. A temperature parameter T and parameter-dependent sampling speed b(θ) can scale diffusion and exploration speed. Discrete-time approximation (Eq (7)) adds Gaussian noise √(2η) ν_t and includes an N factor for online updates.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Operates on multiple separable scales: fast network-state sampling (neuronal activity) at milliseconds to 100s of milliseconds (spike/state sampling), and slow parameter sampling (synaptic efficacies and connectivity θ) on minutes→hours→days (adjustable via sampling-speed b and temperature T; simulations used b values producing hours-scale changes, biological data cited on hours–days). Temperature T and learning rate b set exploration/exploitation and absolute timescale.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Modeled primarily for cortical microcircuits (pyramidal cells, dendritic spines) and discussed for hippocampus as an example of higher turnover; implemented in models of cortical circuits (RBM, WTA spiking networks).</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Supports both synaptic weight plasticity and structural rewiring: feedforward afferents, lateral recurrent excitatory connections between hidden neurons, divisive lateral inhibition (WTA), sparse connectivity priors, and stochastic spine formation/elimination (rewiring).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Unsupervised/generative learning (learning posterior over parameters for generative models), with emergent associative mapping and robustness/compensation to perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical model (stochastic differential equation formalism), with simulations on restricted Boltzmann machines and spiking WTA circuits and comparisons to experimental spine/STDP observations.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Fast neural sampling (ms–100s ms) produces activity used in the likelihood-gradient term; the activity-dependent term can quickly strengthen synapses when pre/post activity is correlated, while the prior-gradient and stochastic diffusion act continuously on slower timescales (minutes–days) to explore parameter space and enforce structural rules. Temperature T and sampling speed b control the relative rates: higher T or larger b → faster exploration (faster spine turnover), lower T or smaller b → slower, near-deterministic convergence (MAP-like). This interaction enables rapid functional adaptation (fast stabilization of strongly driven synapses) coexisting with ongoing exploratory rewiring (slow).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A local three-term stochastic plasticity rule (activity term + prior term + Wiener noise) yields sampling from the posterior p(θ|x), which (1) improves generalization compared to pure ML (prevents overfitting via priors), (2) produces realistic spine motility statistics (log-normal weight distributions; power-law survival of new spines), (3) supports inherent and fast compensation after lesions by stochastic exploration + activity-guided stabilization (network recovers function within simulation hours), and (4) unifies synaptic and structural plasticity. The framework explains experimentally observed stochasticity as functional sampling rather than mere noise.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Reported qualitative and some quantitative metrics in simulations: RBM test log-likelihood maintained when using appropriate prior (vs overfitting without prior); recovery of reconstruction/classification performance after lesions to ~75% of pre-lesion level after ~2 hours (in simulation); spine survival fractions: newly formed spines stabilized ~8% after return to baseline environment (EE→SE) vs ~30% stabilized under persistent enriched environment (EE→EE); synaptic turnover timescale and survival-exponent depend on b (examples: b=1e-4 vs b=1e-6 used in Fig 3E,F).</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Not implemented as a separate process, but proposed: consolidation could be modeled as a modification of the prior p_S(θ) (i.e., altering ∂ log p_S/∂θ for a parameter to make it more persistent), which functionally corresponds to shifting exploration–exploitation balance to store long-term memory while allowing flexible responses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Network Plasticity as Bayesian Inference', 'publication_date_yy_mm': '2015-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e155.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e155.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STDP-like likelihood term</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Activity-dependent (STDP-like) likelihood-gradient term</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The activity-dependent component of the synaptic sampling drift is equivalent to a spike-timing-dependent plasticity (STDP)-like rule derived from the likelihood gradient of a generative spiking model; it is multiplicative in synaptic efficacy and matches characteristic STDP curves.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>STDP-like plasticity (likelihood-derived)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>From the generative spiking model, the likelihood-gradient produces a term proportional to w_i S(t)(x_i(t) - α e^{w_i}) (Eq (11) / Eq (10) specialization), where S(t) is postsynaptic spike train and x_i(t) is normalized presynaptic input; this yields a multiplicative STDP-like update whose timing dependence produces the classical STDP curve (comparison made to a measured curve at 20 Hz). The w_i factor makes changes depend on current synaptic efficacy (stronger synapses undergo larger absolute changes).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Fast: spike-timing dependence at millisecond scale (STDP windows), pairing-frequency effects observed at tens of milliseconds (20 Hz pairing, ~50 ms intervals); repeated pairings produce changes accumulating over seconds→minutes and contribute to synapse stabilization over minutes→hours when combined with the slower sampling dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Cortical pyramidal cell synapses in a Winner-Take-All (WTA) cortical microcircuit motif; comparisons to cortical STDP experimental data.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Feedforward synapses from input neurons to pyramidal cells in WTA circuits; combined with lateral inhibition (divisive normalization). The rule applies per-synapse and interacts with priors that shape population connectivity (e.g., sparsity).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Unsupervised/generative learning (fits mixture/exponential-family models to spike inputs), supports feature extraction and association.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical derivation and simulation (derivation of STDP-like term from likelihood gradient of spiking generative model) with comparison to experimental STDP curve (20 Hz).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Provides fast, spike-timing-dependent changes that can rapidly increase synaptic efficacy for strongly correlated inputs; when a synapse becomes large, the activity-dependent term dominates and stabilizes it on slower timescales, while the prior and stochastic diffusion can still cause exploration for weaker synapses—thus fast STDP-like potentiation interacts with slow stochastic/prior-mediated processes to produce both short-term adaptation and long-term structural organization.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The likelihood-gradient derived from the spiking generative model yields an STDP-like rule whose shape matches experimentally measured curves (e.g., at 20 Hz). The multiplicative factor w_i explains value-dependent plasticity (current weight dependence), and combined with prior and noise results in stability of large, reliably driven synapses and turnover of weak/transient ones.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Qualitative match to experimentally measured STDP curve at 20 Hz (no absolute quantitative fit numbers provided besides the comparison); STDP-driven plasticity contributed to rapid formation/stabilization of synapses in simulations (timescales minutes→hours).</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Implicit: strong, repeatedly potentiated synapses become dominated by the activity-dependent term and thereby stabilized (functional consolidation), and longer-term consolidation is hypothesized to be representable by changing the prior p_S(θ).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Network Plasticity as Bayesian Inference', 'publication_date_yy_mm': '2015-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e155.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e155.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spine motility (structural plasticity) model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multiplicative spine motility model with exponential mapping (w = exp(θ - θ0))</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A unified parametrization of structural and synaptic efficacy plasticity where a single parameter θ_i encodes both presence/absence of a connection and synaptic efficacy via w_i = exp(θ_i - θ0); θ follows synaptic sampling dynamics producing multiplicative spine dynamics, spontaneous regrowth, and realistic survival statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Multiplicative spine motility with stochastic sampling</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>θ_i encodes connectivity and efficacy: θ_i>0 functional synapse with efficacy w_i = exp(θ_i - θ0); θ_i<0 models retracted/non-functional synapse (mapped to tiny w). Dynamics: dθ_i = b(∂_{θ_i} log p_S(θ) + N w_i ∂_{w_i} log p_N(x^n|w)) dt + √(2b) dW_i (Eq (10)). This yields multiplicative size-dependent changes (consistent with experimental multiplicative spine dynamics), spontaneous regrowth due to noise, and activity-dependent stabilization for large θ.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Slow: spine-volume and connection turnover described at hours→days in biology; simulations used accelerated rates (hours) by choosing larger b for tractable simulation times; PSD-95 turnover and molecular contributors operate hours→days. Newly formed spines often decay within hours unless environment persists.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Dendritic spines of excitatory cortical synapses (cortex), comparisons to in vivo spine-turnover experiments (mouse barrel cortex, visual cortex); hippocampus discussed in context of different turnover rates.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Models formation/elimination of individual synaptic connections (rewiring), leads to emergent sparse connectivity and heavy-tailed (log-normal) weight distributions; supports both transient and persistent synapse populations and network-level rewiring (new lateral/excitatory connections can form between hidden neurons).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Structural adaptation to input statistics (unsupervised); supports environment-driven pruning/formation and long-term maintenance of functionally relevant connections.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical model with simulations; compared qualitatively to multiple experimental spine/motility datasets (survival statistics, enrichment-induced turnover).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Noise-driven spontaneous formation happens at slow timescales (hours–days); activity-dependent likelihood term biases which new spines are stabilized quickly if they contribute to explaining inputs, so brief enriched experience produces transient spine formation that decays unless the enriched input persists (persistent exposure yields higher stabilization). Multiplicative mapping makes activity effects negligible for retracted spines and dominant for large synapses, creating a separation of timescales between vulnerable/new spines and stable ones.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The model reproduces key experimental phenomena: (1) increased spine formation under enriched environment (EE) followed by decay if enrichment is transient, (2) a small fraction of newly formed spines become long-lived (~8% stable after EE→SE, ~30% stable under continuous EE in simulations), (3) emergent log-normal weight distribution from Gaussian prior in θ-space, and (4) power-law behavior in survival statistics of new spines, with exponent tunable by sampling speed b.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Quantitative simulation observations: ~8% of new spines stabilized after brief enrichment then return to baseline (EE→SE) vs ~30% stabilized under continuous EE; different b values (e.g., b=1e-4 vs b=1e-6) produced different survival-exponent/time constants (figures show fits; no absolute biological-time numerical exponents quoted in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Model suggests consolidation corresponds to making a synapse's θ more persistent (e.g., via modifying its prior p_S(θ) to favor current θ), thereby reducing diffusion and making the synapse long-lived; this is discussed as a hypothesis rather than implemented explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Network Plasticity as Bayesian Inference', 'publication_date_yy_mm': '2015-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e155.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e155.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WTA connectivity motif</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Winner-Take-All (WTA) cortical microcircuit with divisive normalization and lateral excitation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A canonical cortical microcircuit motif used in the paper: ensembles of pyramidal neurons with lateral inhibition (modeled as divisive normalization) plus feedforward input connections and allowed recurrent lateral excitatory connections between hidden neurons; used to derive tractable likelihood gradients and emergent STDP-like rules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Network motif enabling STDP-derived learning (WTA with divisive normalization)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Network: groups of pyramidal neurons form WTA circuits via divisive normalization (models lateral inhibition), receive feedforward input from many input neurons, and can have arbitrary excitatory lateral connections across WTA units (structural plasticity allowed). Learning via synaptic sampling updates feedforward and lateral weights and connections according to the three-term rule.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Neural competition and spike dynamics at millisecond scales; synaptic/structural changes occurring over minutes→hours (simulations used hours-scale).</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Cortical microcircuit motif (pyramidal cells with lateral inhibition), used as canonical model for cortical computation.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Feedforward afferents, divisive lateral inhibition (winner selection), lateral recurrent excitatory connectivity between hidden neurons; connectivity emerges sparsely via structural plasticity and priors.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Unsupervised/generative feature learning and multimodal association (associating auditory and visual inputs), plus robustness/compensation (recovery after lesions).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical model and simulations (used to show assembly formation, sensory reconstruction, spine turnover under different environments, and lesion recovery).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Fast competition and spiking dynamics govern moment-to-moment representation (ms–100s ms), while synaptic sampling slowly rewires and reweights connections (minutes→hours), enabling the circuit to explore new wiring and recover function after perturbations while maintaining momentary WTA computations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>WTA-based network with synaptic sampling develops assemblies encoding stimuli, can reconstruct visual stimuli from auditory input (generative association), and shows rapid recovery after lesions: removal of tuned neurons caused impairment but recovery within ~1 hour, full removal of lateral connections required ~2 hours to regrow a subset (~294) of connections and reach ~75% of pre-lesion performance. Network codes drift over longer timescales due to ongoing sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Lesion recovery metrics: recovery of reconstruction/classification performance to ~75% of pre-lesion level within ~2 hours after complete removal of lateral connections; assembly emergence evaluated via peri-event time histograms and PCA trajectories (qualitative), counts of functional lateral connections (e.g., 2936 removed; ~294 regrew).</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Not explicitly implemented for WTA motif; same proposal as elsewhere that consolidation could be represented by changing priors to make certain connections more persistent.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Network Plasticity as Bayesian Inference', 'publication_date_yy_mm': '2015-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-156 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-156</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-156</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-5.html">extraction-schema-5</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <p><strong>Paper ID:</strong> paper-98b4d4e24aab57ab4e1124ff8106909050645cfa</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/98b4d4e24aab57ab4e1124ff8106909050645cfa" target="_blank">Neural networks and physical systems with emergent collective computational abilities.</a></p>
                <p><strong>Paper Venue:</strong> Proceedings of the National Academy of Sciences of the United States of America</p>
                <p><strong>Paper TL;DR:</strong> A model of a system having a large number of simple equivalent components, based on aspects of neurobiology but readily adapted to integrated circuits, produces a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e156.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e156.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hebbian learning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hebbian synaptic modification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A correlation-based plasticity rule in which synaptic change is proportional to the (time-)average product of pre- and postsynaptic activity; invoked as the biological substrate for constructing the weight matrix used for associative memory storage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Hebbian learning</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>ΔT_ij ∝ [V_i(t) V_j(t)]_average: synaptic strength increments according to the time-averaged correlation of pre- and postsynaptic binary firing rates; the paper allows inclusion of decay and averages over past history and notes that the Hebbian property could be implemented by grouped cells instead of single synapses.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Operates as an average over past activity (paper: 'average' over past history) — no absolute times given; conceptually a slower timescale than single-spike events (paper models spiking detail as negligible and uses short-time averages). Practically represented in the model as changes to the fixed weight matrix T_ij (longer than neural update times set by 1/W).</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Generic cortical-like recurrent networks; cited applicability to cortical regions and simple ganglia (models intended as abstract for cortex-like associative areas).</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Implemented atop recurrent connections (bidirectional or unidirectional synapses allowed); Hebbian updates accumulate into an all-to-all-ish weight matrix T_ij (with many nonzero mutual interconnections in the modeled circuits).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Associative (content-addressable) memory formation; storage of exemplar memory patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical model (conceptual biological interpretation supported by references to neurobiology).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Fast neuronal dynamics (asynchronous updates with mean attempt rate W) produce immediate attractor retrieval, while Hebbian updates (time-averaged correlations) change the synaptic matrix T_ij slowly, thus creating/stabilizing attractor states; the paper treats synaptic change as a separate, slower process than single-neuron updates.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Hebbian-type accumulation of correlations can produce the associative T_ij used for attractor memory storage; the paper assumes such a T_ij may be produced by past experience (or inheritance) and shows that with that T_ij the network has stable attractors that serve as content-addressable memories.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>No absolute learning-rate units for Hebbian updates given; performance of Hebbian-constructed T_ij evaluated via attractor recall metrics (see other entries for numerical performance).</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>No explicit biological consolidation protocol is given; synaptic changes accumulate in T_ij (slower than neural dynamics) and thus act as a long-lived store, but the paper does not describe a separation into distinct fast-learning vs offline consolidation processes beyond the slow accumulation/decay notion.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural networks and physical systems with emergent collective computational abilities.', 'publication_date_yy_mm': '1982-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e156.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e156.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hopfield storage rule</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hebb-like outer-product memory storage (T_ij = Σ_s (2V_i^s-1)(2V_j^s-1))</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's explicit synaptic weight prescription for storing multiple binary memory vectors as attractors: weights are the sum over outer products of centered binary memory vectors (diagonal elements zeroed).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Outer-product associative storage (Hopfield rule)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>T_ij = Σ_s (2 V_i^s - 1)(2 V_j^s - 1) with T_ii = 0: each stored pattern s contributes an outer-product term to the synaptic matrix, so synapses are the sum of contributions from all stored patterns; signal-to-noise properties derive from pseudo-orthogonality of random patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Weights T_ij are treated as fixed during network retrieval (i.e., slow relative to neural updates). The paper does not give absolute times; neural update times are parameterized by 1/W, while writing into T_ij is conceptualized as accumulation over past experience (much slower).</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Abstract model intended to represent local strongly recurrent cortical pools (authors note typical cortical regions or simple ganglia with intense mutual interconnections).</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Dense recurrent connectivity (all-to-all possible, with many nonzero T_ij); both symmetric and nonsymmetric variants studied; clipped/sign-only variants also tested; can include one-directional-only connections (T_ij ≠ 0, T_ji = 0).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Long-term associative memory storage and content-addressable recall (retrieval from partial or noisy cues); categorization and familiarity recognition emerge from attractor dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical model and numerical simulations (Monte Carlo simulations, N=30 and N=100 examples).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Fast asynchronous neural updates (mean attempt rate W; settling to attractors on timescales of order a few/W) implement recall; the outer-product rule encodes memories into the slower synaptic substrate T_ij so that retrieval dynamics operate on the fast timescale while storage evolves on a slower, cumulative timescale.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Storage rule yields attractor states that act as content-addressable memories; capacity about 0.15·N patterns for random patterns before severe recall errors; performance robust to some asymmetry and to clipping of weights; asymmetric or clipped variants reduce signal-to-noise by known factors (e.g., sign-clipping reduces capacity by factor ~2/π^(1/2) in SNR, and one-direction-only connectivity reduces SNR by factor 1/√2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Capacity: ~0.15·N stored random patterns retained with good recall; example: for N=100, n=10 gives single-bit error probability P=0.0091 (theoretical) and probability of zero errors ≈ e^{-0.91} ≈ 0.40 (simulation ≈0.6). Clipped algorithm: maximal stored memories for N=100 around n≈13; Shannon information ~ N(N/8) bits for that regime.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Storage is direct accumulation into T_ij (no staged consolidation described). Forgetting/recency is achieved by weight saturation/digitization (see separate entry) rather than explicit offline consolidation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural networks and physical systems with emergent collective computational abilities.', 'publication_date_yy_mm': '1982-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e156.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e156.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Asymmetric sequential update term</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Temporally asymmetric Hebb-like modification for sequence encoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed modification to the weight matrix that encodes transitions between successive memory states by correlating the postsynaptic activity at time s+1 with presynaptic activity at time s, enabling metastable sequential transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Temporally asymmetric Hebbian rule (sequence term)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>ΔT_ij = A Σ_s (2 V_i^{s+1} - 1)(2 V_j^s - 1): adds a directional (time-lagged) contribution proportional to co-occurrence of presynaptic activity at time s and postsynaptic activity at the next step s+1, scaled by A; creates nonsymmetric weights that bias transitions from V^s → V^{s+1}.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Encodes transitions between discrete stored states ordered in sequence (indices s and s+1). The rule is inherently about one-step temporal relations between stored patterns; absolute time per step corresponds to network dynamics timescale (order 1/W) but no explicit milliseconds/sec provided.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Abstract recurrent network (same generic cortical-like recurrent pool as main model); designed to impose directed flow among attractor states.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Introduces explicit asymmetry into otherwise recurrent connectivity (T_ij ≠ T_ji) to create directional couplings that bias state-to-state transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Time-sequence retention / sequential memory (encoding of short ordered sequences).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical model (addition to the main Hopfield-style model; numerically tested for small sequences).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Fast attractor dynamics cause the network to dwell near V^s for some time (determined by dynamics and magnitude A) before the asymmetric term pushes it toward V^{s+1}; thus sequence generation arises from an interaction of fast attractor stability and a slower (or modulatory) asymmetric bias in weights.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Adding the asymmetric term can produce metastable dwell near a pattern V_s followed by a transition to V_{s+1} when A is tuned; however, sequences longer than about four states were not reliably produced and even short sequences were not faithfully followed, indicating limitations of simple asymmetric Hebbian additions for robust long sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Qualitative: successful short sequences up to length ~4 in simulations; no quantitative timing constants beyond dependence on parameter A and baseline dynamics parameter W were given.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>No explicit consolidation; sequence behavior emerges from static (but asymmetrically biased) synaptic matrix—i.e., encoding of temporal order is stored in synaptic asymmetry rather than staged consolidation across timescales.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural networks and physical systems with emergent collective computational abilities.', 'publication_date_yy_mm': '1982-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e156.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e156.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Weight saturation / digitized forgetting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Saturation-limited synaptic increment scheme (digitized T_ij)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A practical hardware-inspired rule limiting synaptic value magnitude (e.g., bounding T_ij to 0, ±1, ±2, ±3) so that further increments saturate, producing natural forgetting of distant memories and retention of recent ones.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Saturation-limited plasticity (digitized weights)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Allow T_ij values only in a bounded discrete set; increments beyond the upper bound are ignored and decrements reduce saturation level; as new memories are written, older contributions are gradually overwritten because saturation prevents indefinite growth, producing an effective recency-dependent memory retention.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Implements gradual forgetting over the (unspecified) long-term timescale of accumulated writes into T_ij; no absolute clock units given, but conceptually slower than retrieval dynamics and dependent on rate of memory additions.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Applied to the abstract Hopfield-type recurrent network; motivated as an implementable hardware constraint but posited as biologically plausible as a mechanism for limited synaptic dynamic range.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Works on the same all-to-all or dense recurrent connectivity; the bounded weight range affects global capacity and recency properties uniformly across connections.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Long-term memory storage with recency-dependent forgetting; supports continual learning with limited interference.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical model (simulations and reasoning about digitized weights).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Fast retrieval dynamics remain unchanged; slow accumulation of pattern contributions into bounded T_ij causes older memories to be lost as new memories push weights toward saturation — thus a slow overwriting mechanism interacting with fast attractor dynamics to determine which memories remain retrievable.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Bounding T_ij values yields retention of recent memories while older memories decay (are no longer stable); for N=100 a discrete range 0,...,±3 is proposed as appropriate; this scheme increases noise slightly but is robust and requires no delicate balance. It is proposed as a natural forgetting mechanism when new memories are continually added.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>No explicit time constants; qualitative effect is that recall of distant-past memories disappears depending on digitizing depth; for N=100 the proposed discretization level ±3 was judged appropriate based on signal-to-noise tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>This is not a consolidation mechanism per se; it is an online overwriting/recency-based forgetting rule: newer memories overwrite older ones due to bounded synaptic magnitude, producing effective short-term retention among the set of recent writes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural networks and physical systems with emergent collective computational abilities.', 'publication_date_yy_mm': '1982-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e156.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e156.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Recurrent attractor connectivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrently connected symmetric/asymmetric networks (attractor networks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Dense recurrent networks with strong back-coupling among neurons that produce an attractor phase-space flow enabling content-addressable memory retrieval, categorization, and error-correction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Not a single plasticity rule — network connectivity motif (recurrent attractor network)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Network architecture characterized by intensive mutual interconnections (potentially many nonzero weights T_ij), allowing strong backward coupling (feedback) and asynchronous stochastic neuron updates (mean attempt rate W); weight symmetry yields an energy function guaranteeing monotonic decrease to attractors, whereas asymmetry induces metastability/finite-temperature-like dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Neural update (retrieval) timescale parameterized by mean processing time 1/W (network typically settles in a few/W); asymmetry can lead to slow metastable transitions between attractors whose timing depends on asymmetry strength A and dynamics, but no absolute ms/s values are given.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Modeled after local cortical pools and simple ganglia — suggested scale of 100–10,000 neurons with intense mutual connections.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Recurrent feedback-dominated topology; studied both symmetric (T_ij = T_ji) and nonsymmetric cases; also considered clipped/sign-only weights and one-direction-only connections; topology mostly dense/random rather than strictly sparse or layered feedforward.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Associative memory retrieval, categorization, familiarity detection, limited sequence generation, error correction, generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical model (analytic arguments and Monte Carlo simulations).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Fast asynchronous neural updates implement attractor convergence; slower changes to connectivity (Hebbian accumulation, saturation) create and reshape attractors; asymmetric connectivity biases inter-attractor transitions on intermediate timescales, enabling limited temporal ordering; initial transient processing (within ~1/2W) distinguishes familiarity vs novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Recurrent attractor connectivity yields robust content-addressable memory behavior even with asynchronous updates and some weight asymmetry or failure; symmetric weights admit an energy function guaranteeing monotonic descent to local minima; capacity scales ≈0.15·N for random patterns; asymmetry reduces stability but can support metastable sequences; clipped/sign weights and one-directional connectivity degrade performance by predictable SNR factors but the system fails softly (graceful degradation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Convergence times ~a few/W in simulations; capacity ≈0.15·N; SNR penalties: sign-clipping reduces SNR by factor (2/π)^{1/2}, one-direction-only connectivity reduces SNR by 1/√2; for N=30 and N=100 simulations reported examples of attractor landscapes and probabilistic recall rates.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Attractor structure is stored in the synaptic matrix T_ij (the slow substrate). No explicit staged consolidation is described beyond accumulation/overwriting dynamics and saturation-based forgetting; sequential/asymmetric terms embed temporal order in the static weight matrix rather than via an explicit fast-to-slow consolidation process.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural networks and physical systems with emergent collective computational abilities.', 'publication_date_yy_mm': '1982-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The Organization of Behavior <em>(Rating: 2)</em></li>
                <li>Content Addressable Memories <em>(Rating: 2)</em></li>
                <li>Perceptrons: An Introduction to Computational Geometry <em>(Rating: 1)</em></li>
                <li>Principles of Perceptrons <em>(Rating: 1)</em></li>
            </ol>
        </div>

        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-91 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-91</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-91</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-5.html">extraction-schema-5</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <p><strong>Paper ID:</strong> paper-c67a7aa3953913fb4d06417b235c89e01d72cff9</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c67a7aa3953913fb4d06417b235c89e01d72cff9" target="_blank">Optimal percentage of inhibitory synapses in multi-task learning</a></p>
                <p><strong>Paper Venue:</strong> Scientific Reports</p>
                <p><strong>Paper TL;DR:</strong> It is shown that 30% inhibitory synapses is the percentage maximizing the learning performance since it guarantees, at the same time, the network excitability necessary to express the response and the variability required to confine the employment of resources.</p>
                <p><strong>Paper Abstract:</strong> Performing more tasks in parallel is a typical feature of complex brains. These are characterized by the coexistence of excitatory and inhibitory synapses, whose percentage in mammals is measured to have a typical value of 20-30%. Here we investigate parallel learning of more Boolean rules in neuronal networks. We find that multi-task learning results from the alternation of learning and forgetting of the individual rules. Interestingly, a fraction of 30% inhibitory synapses optimizes the overall performance, carving a complex backbone supporting information transmission with a minimal shortest path length. We show that 30% inhibitory synapses is the percentage maximizing the learning performance since it guarantees, at the same time, the network excitability necessary to express the response and the variability required to confine the employment of resources.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e91.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e91.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Non-uniform negative-feedback plasticity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Non-uniform negative-feedback synaptic plasticity (distance-weighted)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model learning rule used in this study that modifies strengths of synapses active during an incorrect response by an amount inversely proportional to their chemical distance from the output neuron (±α/d_k), strengthening when a '1' output was missed and weakening when a '0' output was missed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Non-uniform negative-feedback plasticity</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>When the network gives an incorrect output, every synapse that was active in the avalanche is adjusted by Δg = ±α / d_k, where α is the plastic adaptation strength and d_k is the chemical distance of the presynaptic neuron from the output; signs: +α/d_k if the output neuron failed to fire (potentiate), −α/d_k if the output neuron erroneously fired (depress). Synapses with g_ij < 10^-4 are pruned. The rule implements a backward-propagating feedback signal that is strongest at synapses closest to the output.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Two interacting scales in the model: fast neural dynamics operate per model time-step (unit time-step ~ propagation between neurons; stated as of order 10 ms; refractory period = 1 time-step), and slow synaptic change across many trials governed by α (plastic adaptation is applied after each full avalanche/trial; smaller α = slower adaptation). The paper does not assign absolute minute/hour/day values to α-driven changes.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Model-level cortical-like network (no specific biological region simulated); network obeys Dale's law and includes excitatory and inhibitory neurons with hub neurons inhibitory, mimicking cortical functional networks.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Used on a spatial, scale-free outgoing-degree network (n(k_out) ∝ k_out^−2), connections placed with distance-dependent probability p(r) ∝ e^(−r/r_0); synapses are directed and weighted.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Supervised multi-rule Boolean learning (parallel learning of Boolean rules such as AND, OR, XOR, and random mappings) implemented via feedback after each trial.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical model (network simulations).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Fast avalanche propagation encodes trial responses (ms timescale per propagation step); incorrect outputs trigger slow synaptic updates (Δg ∝ α/d_k) applied between trials. Slow adaptation accumulates over many trials to reshape network backbone for multi-task learning; the paper shows slower adaptation (smaller α) improves learning and memory retention.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The non-uniform distance-weighted negative-feedback rule enables credit-assignment focused near the output and sculpts a backbone of active paths that supports multi-task learning; slow adaptation (small α) is crucial to learn and remember multiple rules, and pruning of very weak synapses refines the connectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Reported network-level performance: percentage of networks giving correct answers to rules across repeated trials; typical simulation parameters: N up to 1000, k_d = 3, α = 0.001 in reported figures. Performance curves (accuracy vs. number of trial applications) and asymptotic success rates (percent correct) are the main quantitative metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Implicit consolidation via iterative slow synaptic updates across many trials: alternating phases of learning and partial forgetting (unlearning) allow reorganization and stabilization of synaptic backbone; no explicit biological consolidation protocol (e.g., sleep replay) is modeled, but pruning and accumulation of Δg provide long-term retention.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Optimal percentage of inhibitory synapses in multi-task learning', 'publication_date_yy_mm': '2015-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e91.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e91.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Homeostatic plasticity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Balanced depression and potentiation (homeostatic plasticity conserving total synaptic weight)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A plasticity constraint tested/used in the model where excitatory and inhibitory adaptations have opposite signs to conserve average synaptic strength, implementing a global balance between excitation and inhibition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Conservation of total synaptic weight through balanced synaptic depression and potentiation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Homeostatic plasticity (balanced potentiation/depression)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Plastic adaptation applied with opposite signs to excitatory and inhibitory synapses, such that overall average synaptic strength is conserved; implemented as one of the adaptation procedures (homeostatic, uniform, restricted) and found to optimize performance when combined with inhibitory hubs.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Operates at the same slow adaptation timescale as the trial-by-trial plastic updates (accumulates across many trials); the paper frames it as a long-term balancing process rather than fast spike-triggered changes.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Applied in the cortical-like model network; motivated by cortical observations where homeostatic mechanisms maintain excitation/inhibition balance.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Interacts with the model's scale-free connectivity and with the fraction of inhibitory synapses p_in; homeostatic adaptation leads to inhibitory neurons becoming on average stronger after learning for p_in ≈ 30%.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Supports robust multi-task supervised learning by preventing runaway strengthening/weakening and preserving network excitability.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Used within the computational/theoretical model; referenced to experimental observations.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Homeostatic adjustments act over the same slow, cumulative timescale as the Δg updates, stabilizing network excitability while fast avalanche dynamics encode trial responses.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Including homeostatic plasticity and assigning hubs as inhibitory optimizes single- and multi-rule learning; plastic adaptation drives average inhibitory synaptic strengths higher and prevents creation of excessively strong inhibitory synapses for p_in ≈ 30%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Improved learning performance (accuracy/time-to-learn) relative to uniform or restricted adaptation, reported qualitatively and shown in supplemental figures (no single scalar reported in main text beyond comparative performance curves).</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Homeostatic balancing conserves total synaptic strength as adaptation proceeds across trials, thereby stabilizing learned configurations (no explicit multi-stage consolidation protocol given).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Optimal percentage of inhibitory synapses in multi-task learning', 'publication_date_yy_mm': '2015-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e91.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e91.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Plastic adaptation modes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Uniform, restricted, and homeostatic adaptation procedures</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three tested plasticity procedures: (i) homeostatic (opposite sign for excitatory/inhibitory updates to conserve mean strength), (ii) uniform (all active synapses updated equally), (iii) restricted (only excitatory synapses updated).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Uniform / Restricted / Homeostatic adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Variants of the trial-by-trial adaptation: (a) Uniform: Δg applied equally to all active synapses; (b) Restricted: Δg applied only to excitatory synapses; (c) Homeostatic: Δg applied with opposite signs on excitatory vs inhibitory synapses to conserve average strength. In all cases non-uniform distance weighting (1/d_k) can be used.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>All operate on the slow adaptation timescale between avalanches/trials (accumulation across many trials); no direct mapping to absolute minutes/hours in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Used in the cortical-like simulated network.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Their effects were tested across networks with varying inhibitory fraction p_in and scale-free connectivity; homeostatic mode combined with inhibitory hubs produced best performance.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Supervised multi-rule learning (Boolean rules) in the model.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/modeling study (variants assessed in simulations).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Fast trial dynamics produce avalanches; between trials these three adaptation modes sculpt connectivity differently, affecting long-term ability to learn multiple rules; slower adaptation (small α) universally improves learning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Homeostatic adaptation and inhibitory hubs optimize learning; uniform or restricted rules perform worse for multi-task learning. Slow plastic adaptation is beneficial under all modes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Learning accuracy (percentage of networks that have learned each rule) over number of trials; qualitative and collapsed scaling curves (supplemental) demonstrate improved performance for slower α.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Different adaptation modes change how synaptic modifications accumulate across trials, with homeostatic mode providing the most stable long-term configurations observed in simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Optimal percentage of inhibitory synapses in multi-task learning', 'publication_date_yy_mm': '2015-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e91.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e91.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scale-free spatial network</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scale-free outgoing-degree, distance-dependent spatial connectivity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Structural model of the network used: outgoing-degree distribution n(k_out) ∝ k_out^−2 (k_out in [2,100]) combined with distance-dependent connection probability p(r) ∝ e^(−r/r0), producing hub neurons and spatial embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Scale-free spatial connectivity (power-law degree with distance-dependent links)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Network wiring: each neuron assigned a random outgoing degree drawn from a power-law distribution (exponent −2); connection targets chosen with probability decaying exponentially with Euclidean distance (scale parameter r0 ≃ 16). Dale's law enforced (neurons exclusively excitatory or inhibitory); inhibitory neurons biased to be highly connected (hubs).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Connectivity itself is structural (static during trials) and does not have an intrinsic temporal scale; it shapes fast avalanche propagation (per-step ~10 ms) and long-term learning by determining which synapses are eligible for plastic updates.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Generic cortex-like network topology reflecting functional brain networks; hubs resemble experimentally observed inhibitory hub neurons in developing hippocampus/cortex.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Scale-free degree distribution, spatially constrained (exponential distance kernel), directed and weighted synapses, with fraction of inhibitory neurons p_in varied in experiments (0–50%).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Supports supervised multi-task learning by providing heterogeneous routing and hub-mediated coordination of activity.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/model.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Static topology determines fast propagation routes of avalanches (ms) and the set of synapses that will undergo slow plastic adaptation across trials; changes in effective functional backbone arise from slow synaptic changes, not from rewiring of the static anatomical graph.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The combination of scale-free connectivity and spatial constraints yields hub neurons (inhibitory hubs were advantageous); fraction of inhibitory synapses p_in ≈ 20–30% in this topology optimizes multi-task learning by producing a complex backbone with many alternative short and long paths and minimal average shortest path between input and output.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Asymptotic task performance (percent correct across trials) as a function of p_in; backbone size and average shortest path measured across network configurations (peak backbone size and minimal shortest paths around p_in ≈ 20–30%).</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Topology provides substrate; consolidation of function arises via slow synaptic strengthening/weakening on this static topology, including pruning of very weak synapses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Optimal percentage of inhibitory synapses in multi-task learning', 'publication_date_yy_mm': '2015-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e91.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e91.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neuronal avalanches / criticality</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neuronal avalanches and critical-state activity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Spontaneous and evoked bursts of activity ('avalanches') with power-law size/duration distributions; the model reproduces avalanche statistics and treats learning as occurring in a near-critical dynamical regime.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Avalanche-driven dynamics (critical-state activity)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Fast network dynamics: stimulation of inputs leads to cascades of neuronal firings (avalanches); avalanche size measured as the number of neurons activated during a trial that gives the correct response. Dynamics follow propagation rule v_j(t+1) = v_j(t) ± (v_i k_out^i / k_in^j) (g_ij / Σ_k g_ik), unit step ~10 ms.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Fast: propagation per step ~10 ms; avalanches unfold over multiple steps (thus tens to hundreds of ms depending on path lengths). Slow: learning uses statistics of avalanches accumulated across many trials to guide synaptic updates over longer timescales.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Model captures cortex-like spontaneous/evoked dynamics consistent with in vitro/in vivo cortical observations.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Avalanche statistics and spatial-temporal propagation are shaped by the scale-free, distance-dependent connectivity and by the fraction of inhibitory synapses p_in.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Learning interpreted as a phenomenon in a critical dynamical regime; successful responses are produced by avalanches of appropriate sizes and patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational model reproducing empirical avalanche statistics (cites Beggs & Plenz etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Fast avalanche events provide the trial-by-trial patterns that are evaluated and used to trigger slow synaptic updates; critical-like avalanches (no characteristic size) relate to spontaneous activity, but evoked correct responses may require confinement of avalanches (inhibition) to be efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Purely excitatory networks require full-system avalanches (s* = N) to give correct answers, i.e., inefficient resource use; introducing inhibition (p_in > 0) confines avalanches, enabling correct responses with smaller recruited populations. The distribution of successful-avalanche sizes shifts to smaller sizes with increasing p_in, and optimal learning occurs near p_in ≈ 20–30%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Avalanche size distributions (P(s*)) measured when correct responses occur; peak at s* = N for purely excitatory networks, peak shifts to smaller s* and distribution broadens for increasing p_in. Entropy S computed from P(s*) and used in functional F = E p_in S.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>No explicit biological consolidation modeled; slow synaptic adaptation uses statistics of avalanche occurrences to iteratively shape functional backbones that support consistent, efficient avalanche responses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Optimal percentage of inhibitory synapses in multi-task learning', 'publication_date_yy_mm': '2015-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e91.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e91.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Backbone of active paths</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Union/backbone of active synaptic paths supporting correct responses</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Structural subgraph formed by the union of synaptic paths that become repeatedly active when the network gives the right answers for all rules; its complexity and path multiplicity depend on inhibitory fraction p_in and predict learning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Active-path backbone (functional subnetwork)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>The backbone is defined as the union of all synapses/neuron activations that connect inputs to the output during avalanches that produce correct responses; it includes temporal ordering of firings and identifies multiple alternative paths and a preferential route.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Backbone emerges over slow adaptation timescale (across many trials) but maps the temporal order of fast avalanche propagation (per-step ~10 ms); connections in the backbone can show short one-step temporal links or longer delayed links depending on firing delays.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Model cortical-like network; the backbone is a functional (not anatomical) subnetwork determined by plastic changes.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Backbone complexity (size and multiplicity of independent paths) is maximal at p_in ≈ 20–30%; average shortest path between inputs and output within backbone is minimal near this p_in.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Multi-task parallel supervised learning: backbone must accommodate multiple, possibly interfering, mapping rules to the same output.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computed from simulations (model).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Fast avalanches populate candidate paths each trial; slow synaptic updates reinforce or prune these such that a stable backbone (collection of frequently used paths) forms over many trials enabling multi-rule performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Multi-task learning requires alternation of learning and partial forgetting; a complex backbone with a broad multiplicity of paths and minimal shortest-path lengths emerges at p_in ≈ 20–30%, optimizing information transmission and resource efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Backbone size (number of neurons in the union of active paths), distribution of path multiplicity per neuron, and average shortest path length between inputs and output measured across configurations; backbone size peaks and average shortest path minimizes near p_in ≈ 20–30%.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Backbone stabilization is the emergent result of repeated slow plastic updates guided by trial-by-trial fast dynamics; partial forgetting (temporary performance drop) is necessary to reconfigure backbone for multiple rules.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Optimal percentage of inhibitory synapses in multi-task learning', 'publication_date_yy_mm': '2015-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e91.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e91.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Functional F = E p_in S</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Functional combining excitability, inhibition fraction and entropy (F = E p_in S)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scalar functional defined in the paper combining average synaptic strength (E), inhibitory fraction p_in, and response entropy S to quantify the balance between excitability and variability that supports learning; it peaks near the empirically observed 30% inhibitory fraction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>None (analytic/diagnostic functional)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>F = E p_in S where E = ⟨g_ij⟩ is average synaptic strength (positive for excitatory, negative for inhibitory), p_in is fraction of inhibitory synapses, and S = −Σ_s* P(s*) ln P(s*) is Shannon entropy of avalanche-size distribution for successful responses. F captures tradeoff: must be nonzero excitability (E) and variability (p_in S) to support learning.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>E and S are measured from ensemble of trials (statistics accumulated over many trials); E reflects long-term synaptic strengths resulting from slow plastic adaptation; S is computed from the distribution of successful avalanche sizes (fast events aggregated).</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Diagnostic measure applied to the model cortical-like network to identify optimal p_in.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Depends on emergent functional backbone and underlying scale-free spatial topology; F peaks at p_in ≈ 30% in the simulated networks.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Used to explain and predict multi-task learning performance (supervised Boolean rule learning) as a function of network parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Analytic diagnostic computed from model simulation outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>F explicitly couples fast-event variability (S, measured from avalanches) and slow synaptic state (E) with structural parameter p_in, formalizing how fast variability and slow excitability combine to yield optimal learning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>F is non-monotonic in p_in and has a pronounced maximum at p_in ≈ 30%, providing a quantitative explanation for why the empirically observed inhibitory fraction in mammalian brains optimizes the balance between excitability and variability for multi-task learning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Functional F values plotted versus p_in; entropy S and average synaptic strength E also plotted. Maximum of F located at p_in = 30% for reported simulations (N = 1000, α = 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>The functional describes how slow consolidation of synaptic strengths (E) and the statistical variability of fast events (S) jointly determine learning efficacy; no separate biological consolidation steps beyond plastic adaptation are modeled.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Optimal percentage of inhibitory synapses in multi-task learning', 'publication_date_yy_mm': '2015-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Conservation of total synaptic weight through balanced synaptic depression and potentiation. <em>(Rating: 2)</em></li>
                <li>Adaptive learning by extremal dynamics and negative feedback. <em>(Rating: 2)</em></li>
                <li>Learning as a phenomenon occurring in a critical state. <em>(Rating: 2)</em></li>
                <li>Self-organized criticality model for brain plasticity. <em>(Rating: 2)</em></li>
                <li>Neuronal avalanches in neocortical circuits. <em>(Rating: 2)</em></li>
                <li>Synaptic tagging and long-term potentiation. <em>(Rating: 1)</em></li>
                <li>Dynamics of Sparsely Connected Networks of Excitatory and Inhibitory Spiking Neurons. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        </div>

    </div>
</body>
</html>
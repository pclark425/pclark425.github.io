<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-83 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-83</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-83</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-5.html">extraction-schema-5</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <p><strong>Paper ID:</strong> paper-6a1ca656fe9bba092e477aecf7bf0ad896c4ea81</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6a1ca656fe9bba092e477aecf7bf0ad896c4ea81" target="_blank">A theoretical framework for learning through structural plasticity</a></p>
                <p><strong>Paper TL;DR:</strong> A mean-field approach is exploited to develop a theoretical framework of learning through this kind of plasticity, capable of taking into account several features of the connectivity and pattern of activity of biological neural networks, including probability distributions of neuron firing rates and probabilistic connection rules.</p>
                <p><strong>Paper Abstract:</strong> A growing body of research indicates that structural plasticity mechanisms are crucial for learning and memory consolidation. Starting from a simple phenomenological model, we exploit a mean-field approach to develop a theoretical framework of learning through this kind of plasticity, capable of taking into account several features of the connectivity and pattern of activity of biological neural networks, including probability distributions of neuron firing rates, selectivity of the responses of single neurons to multiple stimuli, probabilistic connection rules and noisy stimuli. More importantly, it describes the effects of stabilization, pruning and reorganization of synaptic connections. This framework is used to compute the values of some relevant quantities used to characterize the learning and memory capabilities of the neuronal network in training and testing procedures as the number of training patterns and other model parameters vary. The results are then compared with those obtained through simulations with firing-rate-based neuronal network models.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e83.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e83.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Structural plasticity (activity-dependent + homeostatic)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structural plasticity combining activity-dependent synapse stabilization and homeostatic pruning/rewiring</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A phenomenological model in which synapses are potentiated and stabilized when pre- and postsynaptic neurons are concurrently in a high firing-rate regime (activity-dependent), and non-stabilized synapses are periodically pruned and replaced to preserve overall connectivity (homeostatic rewiring). Stabilized synapses are assigned a larger, irreversible weight and are protected from pruning, supporting long-lived memory traces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Activity-dependent stabilization + Homeostatic pruning/rewiring</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>During each training example a connection is stabilized (potentiated) if both pre- and postsynaptic neurons have firing rates above thresholds (ν_th); stabilization sets the synaptic weight from baseline W_b to a larger value W_s and is irreversible in the model. Homeostatic structural plasticity is implemented as periodic rewiring every r training steps: all non-stabilized (baseline-weight) connections are pruned and an equal number of new baseline connections are randomly created (preserving the indegree distribution). The probability that a connection is stabilized at least once in T examples is p_T = 1 - (1 - α1 α2)^T, where α1 and α2 are the fractions of high-rate neurons in the two populations.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Relative scales only (paper does not assign absolute ms/seconds/days): structural plasticity is explicitly described as operating on longer time scales than short- or long-term functional plasticity; activity-dependent stabilization occurs during individual training examples (faster), while homeostatic pruning/rewiring is applied periodically on a slower timescale (modelled by the rewiring step r; example used r = 100 training examples). Conceptually spans from repeated training-example timescales up to long-term consolidation across many thousands of examples.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Not specific to any brain region; model uses two generic populations P1→P2 in a feed-forward architecture to represent e.g., sensory input → contextual target population.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Feed-forward P1→P2 with either fixed indegree or Poisson-distributed indegree; typical parameters in the paper: N1 = N2 = 100000, mean incoming connections per P2 neuron ⟨C⟩ ≈ 5000; presynaptic neurons chosen at random; stabilized connections accumulate between co-active pre/post neurons producing selective (coding) neurons in P2.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Associative mapping / pattern association and long-term memory consolidation (input patterns in P1 associated to contextual stimuli in P2); also used to study memory capacity and pattern recall.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical model (phenomenological structural-plasticity rule + mean-field analytical derivations) validated by firing-rate network simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Paper explicitly describes interaction between faster, activity-dependent stabilization (occurring per training example when pre/post rates exceed thresholds) and slower, homeostatic rewiring (periodic pruning/creation every r examples). Stabilization makes synapses permanent (W_s) and protects them from later pruning, thus transferring transient activity correlations into durable connectivity changes; rewiring removes unimportant baseline connections and randomly redistributes them, increasing discriminability (SDNR) and preventing accumulation of unused synapses.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Structural-plasticity rule (stabilization + periodic rewiring) enables a feed-forward network to learn and recall very large numbers of patterns: SDNR-based memory capacity T_max ~ 28,000–30,000 patterns depending on noise level; periodic rewiring increases SDNR and memory capacity by ~10–20% compared to stabilization without rewiring; stabilized connections are irreversible and prevent catastrophic forgetting by protecting potentiated synapses from later pruning; theory and simulations agree within ~1–2% for most metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Quantitative metrics used: SDNR = |⟨S_c⟩ - ⟨S_b⟩| / σ_b (signal-difference-to-noise ratio); relation to recall probability P_C ≈ 0.5[1 + erf(SDNR/√8)]. Example numeric results: mean indegree ⟨C⟩ = 5000, N = 1e5, rewiring step r = 100 (typical), T_max ≈ 28,000 with noise std 2 Hz, T_max ≳ 30,000 with no noise; rewiring gives ≈20% increase in T_max relative to no-rewiring in reported simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Yes — stabilization of co-active synapses (W_b → W_s, irreversible in model) constitutes consolidation: transient co-activity during training examples produces durable synaptic changes that are protected from future pruning; periodic homeostatic rewiring prunes unstabilized synapses while preserving consolidated ones, thereby consolidating learned associations into long-term connectivity structure and reducing catastrophic forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A theoretical framework for learning through structural plasticity', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e83.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e83.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STP / STDP / LTP (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Short-term plasticity (STP), spike-timing-dependent plasticity (STDP), and long-term potentiation (LTP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Functional synaptic plasticity mechanisms cited as complementary to structural plasticity: STP and STDP alter synaptic efficacy on fast timescales, and LTP is associated with potentiation and formation of durable synaptic contacts that can trigger structural changes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Short-term plasticity (STP), Spike-timing-dependent plasticity (STDP), Long-term potentiation (LTP)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Paper cites STP (e.g., Tsodyks et al.) and STDP (e.g., Gütig et al.) as fast synaptic-efficacy mechanisms studied in literature; LTP is referenced as a mechanism that can both increase synaptic efficacy and promote structural changes (spine formation) near potentiated sites. The paper does not implement STDP/STP dynamics but notes that functional plasticity can precede or co-occur with structural stabilization.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Referred to generically as 'short-term' and 'long-term' plasticity that operate on timescales shorter than structural plasticity; the paper does not assign explicit numeric timescales but contrasts these faster functional mechanisms with the slower structural processes.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>General cortical/spiking network context; cited works include cortex and hippocampus in background discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Mentioned in prior-art context: combined with structural plasticity (e.g., spiking network simulations that combined STDP with structural rewiring showed reduced network noise and faster learning — Spiess et al. 2016).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Short-term and long-term plasticity mechanisms support working memory, fast learning and initial encoding; LTP and STDP are implicated in initial potentiation that may lead to subsequent structural stabilization and memory consolidation.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Mentioned as subjects of prior computational and experimental studies; referenced computational spiking-network simulations and experimental observations (no new experiments here).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Paper discusses that functional plasticity (STDP/LTP/STP) can act on faster timescales and may trigger structural changes (e.g., potentiation followed by spine formation), so fast functional changes can be converted into slower structural consolidation; the present model encapsulates this by treating stabilization as an activity-driven event (conceptually downstream of functional potentiation).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited findings from the literature (not derived in this paper) include that combining structural plasticity with STDP can reduce network noise and speed learning (Spiess et al. 2016) and that LTP is associated with new synapse formation adjacent to potentiated contacts. The paper uses these citations to motivate its activity-dependent stabilization rule.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>No new numerical performance measures for STP/STDP/LTP are reported in this paper (these mechanisms are cited as background).</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Described conceptually: fast functional potentiation (LTP/STDP) can induce formation/proliferation of structural contacts (spines), which leads to longer-lived synaptic changes; the paper models the end effect (stabilization) phenomenologically rather than modelling LTP/STDP dynamics explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A theoretical framework for learning through structural plasticity', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e83.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e83.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Feed-forward Poisson/Fixed-indegree connectivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Feed-forward connectivity with fixed or Poisson-distributed indegree</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Network wiring used in the model: two large neuron populations connected in a feed-forward manner (P1 → P2), where each P2 neuron receives many inputs (typical mean indegree ⟨C⟩ ≈ 5000) drawn either as a fixed indegree or from a Poisson distribution; presynaptic indices are sampled randomly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Feed-forward connectivity with fixed or Poisson indegree</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Connectivity is instantiated either with a fixed indegree C for every neuron in P2 or with C sampled from a Poisson distribution with mean ⟨C⟩; presynaptic neuron identities are chosen uniformly at random from P1. Rewiring preserves the chosen indegree distribution. Model assumes high average indegree (≈5000) and very large populations (N1 = N2 = 100000).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Connectivity is static on short timescales but is modified on the structural-plasticity timescale via pruning and rewiring (i.e., non-stabilized connections are periodically replaced every r training examples).</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Abstract/generic two-population feed-forward circuit (not mapped to a specific anatomical region), intended to represent sensory → contextual/target mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Feed-forward architecture, sparse but high indegree per neuron; supports multiple-selectivity (neurons in P2 can code multiple training patterns, average number α2 T). Poisson indegree introduces heterogeneity in input degree across P2 neurons.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Associative pattern mapping: structural changes in feed-forward connections encode associations between P1 input patterns and P2 contextual representations, enabling pattern recall during test.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational model (network topology used in simulations and theoretical calculations).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Structural rewiring acts across training episodes to reshuffle non-stabilized baseline connections while leaving stabilized (potentiated) connections intact; this separation of structural change timing (fast stabilization vs slow rewiring) is implemented on top of the static baseline feed-forward wiring.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using this connectivity, the model analytically predicts and simulations confirm large pattern memory capacities (T_max ≈ 28k–30k) and shows that Poisson indegree can be handled by averaging over indegree distributions; rewiring that preserves indegree distribution improves learning performance compared to no rewiring.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Connectivity numerical parameters: N1 = N2 = 100000, ⟨C⟩ = 5000, rewiring step r typically 100 examples in simulations; memory-capacity results reported above are measured with this connectivity setup.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Topology interacts with structural plasticity: durable (stabilized) connections embedded into the feed-forward topology support consolidated representations; homeostatic rewiring preserves overall degree distribution while reallocating baseline synapses, enabling refinement of feed-forward projections over longer timescales.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A theoretical framework for learning through structural plasticity', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Opposing effects of neuronal activity on structural plasticity <em>(Rating: 2)</em></li>
                <li>Structural plasticity and memory capacity: effectual connectivity and synaptic states <em>(Rating: 2)</em></li>
                <li>Spiking network simulations combining structural plasticity and STDP reduce noise and speed learning <em>(Rating: 2)</em></li>
                <li>Synaptic density changes across development (Huttenlocher, 1979) <em>(Rating: 1)</em></li>
                <li>Hippocampal synaptic turnover and memory retention (Pfeiffer et al., 2018) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        </div>

    </div>
</body>
</html>